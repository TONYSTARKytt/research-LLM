Title: 
PromptIR: Prompting for All-in-One Blind Image Restoration

Abstract: Image restoration involves recovering a high-quality clean image from its
degraded version. Deep learning-based methods have significantly improved image
restoration performance, however, they have limited generalization ability to
different degradation types and levels. This restricts their real-world
application since it requires training individual models for each specific
degradation and knowing the input degradation type to apply the relevant model.
We present a prompt-based learning approach, PromptIR, for All-In-One image
restoration that can effectively restore images from various types and levels
of degradation. In particular, our method uses prompts to encode
degradation-specific information, which is then used to dynamically guide the
restoration network. This allows our method to generalize to different
degradation types and levels, while still achieving state-of-the-art results on
image denoising, deraining, and dehazing. Overall, PromptIR offers a generic
and efficient plugin module with few lightweight prompts that can be used to
restore images of various types and levels of degradation with no prior
information on the corruptions present in the image. Our code and pretrained
models are available here: this https URL


------------------------------------------------------------------------------

Title:
A Comparison of Time-based Models for Multimodal Emotion Recognition

Abstract: Emotion recognition has become an important research topic in the field of
human-computer interaction. Studies on sound and videos to understand emotions
focused mainly on analyzing facial expressions and classified 6 basic emotions.
In this study, the performance of different sequence models in multi-modal
emotion recognition was compared. The sound and images were first processed by
multi-layered CNN models, and the outputs of these models were fed into various
sequence models. The sequence model is GRU, Transformer, LSTM and Max Pooling.
Accuracy, precision, and F1 Score values of all models were calculated. The
multi-modal CREMA-D dataset was used in the experiments. As a result of the
comparison of the CREMA-D dataset, GRU-based architecture with 0.640 showed the
best result in F1 score, LSTM-based architecture with 0.699 in precision
metric, while sensitivity showed the best results over time with Max
Pooling-based architecture with 0.620. As a result, it has been observed that
the sequence models compare performances close to each other.


------------------------------------------------------------------------------

Title:
Continuous Layout Editing of Single Images with Diffusion Models

Abstract: Recent advancements in large-scale text-to-image diffusion models have
enabled many applications in image editing. However, none of these methods have
been able to edit the layout of single existing images. To address this gap, we
propose the first framework for layout editing of a single image while
preserving its visual properties, thus allowing for continuous editing on a
single image. Our approach is achieved through two key modules. First, to
preserve the characteristics of multiple objects within an image, we
disentangle the concepts of different objects and embed them into separate
textual tokens using a novel method called masked textual inversion. Next, we
propose a training-free optimization method to perform layout control for a
pre-trained diffusion model, which allows us to regenerate images with learned
concepts and align them with user-specified layouts. As the first framework to
edit the layout of existing images, we demonstrate that our method is effective
and outperforms other baselines that were modified to support this task. Our
code will be freely available for public use upon acceptance.


------------------------------------------------------------------------------

Title:
GIMLET: A Unified Graph-Text Model for Instruction-Based Molecule  Zero-Shot Learning

Abstract: Molecule property prediction has gained significant attention in recent
years. The main bottleneck is the label insufficiency caused by expensive lab
experiments. In order to alleviate this issue and to better leverage textual
knowledge for tasks, this study investigates the feasibility of employing
natural language instructions to accomplish molecule-related tasks in a
zero-shot setting. We discover that existing molecule-text models perform
poorly in this setting due to inadequate treatment of instructions and limited
capacity for graphs. To overcome these issues, we propose GIMLET, which unifies
language models for both graph and text data. By adopting generalized position
embedding, our model is extended to encode both graph structures and
instruction text without additional graph encoding modules. GIMLET also
decouples encoding of the graph from tasks instructions in the attention
mechanism, enhancing the generalization of graph features across novel tasks.
We construct a dataset consisting of more than two thousand molecule tasks with
corresponding instructions derived from task descriptions. We pretrain GIMLET
on the molecule tasks along with instructions, enabling the model to transfer
effectively to a broad range of tasks. Experimental results demonstrate that
GIMLET significantly outperforms molecule-text baselines in instruction-based
zero-shot learning, even achieving closed results to supervised GNN models on
tasks such as toxcast and muv.


------------------------------------------------------------------------------

Title:
Iterative Scale-Up ExpansionIoU and Deep Features Association for  Multi-Object Tracking in Sports

Abstract: Multi-object tracking algorithms have made significant advancements due to
the recent developments in object detection. However, most existing methods
primarily focus on tracking pedestrians or vehicles, which exhibit relatively
simple and regular motion patterns. Consequently, there is a scarcity of
algorithms that address the tracking of targets with irregular or non-linear
motion, such as multi-athlete tracking. Furthermore, popular tracking
algorithms often rely on the Kalman filter for object motion modeling, which
fails to track objects when their motion contradicts the linear motion
assumption of the Kalman filter. Due to this reason, we proposed a novel online
and robust multi-object tracking approach, named Iterative Scale-Up
ExpansionIoU and Deep Features for multi-object tracking. Unlike conventional
methods, we abandon the use of the Kalman filter and propose utilizing the
iterative scale-up expansion IoU. This approach achieves superior tracking
performance without requiring additional training data or adopting a more
robust detector, all while maintaining a lower computational cost compared to
other appearance-based methods. Our proposed method demonstrates remarkable
effectiveness in tracking irregular motion objects, achieving a score of 75.3%
in HOTA. It outperforms all state-of-the-art online tracking algorithms on the
SportsMOT dataset, covering various kinds of sport scenarios.


------------------------------------------------------------------------------

Title:
Harnessing Mixed Offline Reinforcement Learning Datasets via Trajectory  Weighting

Abstract: Most offline reinforcement learning (RL) algorithms return a target policy
maximizing a trade-off between (1) the expected performance gain over the
behavior policy that collected the dataset, and (2) the risk stemming from the
out-of-distribution-ness of the induced state-action occupancy. It follows that
the performance of the target policy is strongly related to the performance of
the behavior policy and, thus, the trajectory return distribution of the
dataset. We show that in mixed datasets consisting of mostly low-return
trajectories and minor high-return trajectories, state-of-the-art offline RL
algorithms are overly restrained by low-return trajectories and fail to exploit
high-performing trajectories to the fullest. To overcome this issue, we show
that, in deterministic MDPs with stochastic initial states, the dataset
sampling can be re-weighted to induce an artificial dataset whose behavior
policy has a higher return. This re-weighted sampling strategy may be combined
with any offline RL algorithm. We further analyze that the opportunity for
performance improvement over the behavior policy correlates with the
positive-sided variance of the returns of the trajectories in the dataset. We
empirically show that while CQL, IQL, and TD3+BC achieve only a part of this
potential policy improvement, these same algorithms combined with our
reweighted sampling strategy fully exploit the dataset. Furthermore, we
empirically demonstrate that, despite its theoretical limitation, the approach
may still be efficient in stochastic environments. The code is available at
this https URL


------------------------------------------------------------------------------

Title:
Evading Forensic Classifiers with Attribute-Conditioned Adversarial  Faces

Abstract: The ability of generative models to produce highly realistic synthetic face
images has raised security and ethical concerns. As a first line of defense
against such fake faces, deep learning based forensic classifiers have been
developed. While these forensic models can detect whether a face image is
synthetic or real with high accuracy, they are also vulnerable to adversarial
attacks. Although such attacks can be highly successful in evading detection by
forensic classifiers, they introduce visible noise patterns that are detectable
through careful human scrutiny. Additionally, these attacks assume access to
the target model(s) which may not always be true. Attempts have been made to
directly perturb the latent space of GANs to produce adversarial fake faces
that can circumvent forensic classifiers. In this work, we go one step further
and show that it is possible to successfully generate adversarial fake faces
with a specified set of attributes (e.g., hair color, eye size, race, gender,
etc.). To achieve this goal, we leverage the state-of-the-art generative model
StyleGAN with disentangled representations, which enables a range of
modifications without leaving the manifold of natural images. We propose a
framework to search for adversarial latent codes within the feature space of
StyleGAN, where the search can be guided either by a text prompt or a reference
image. We also propose a meta-learning based optimization strategy to achieve
transferable performance on unknown target models. Extensive experiments
demonstrate that the proposed approach can produce semantically manipulated
adversarial fake faces, which are true to the specified attribute set and can
successfully fool forensic face classifiers, while remaining undetectable by
humans. Code: this https URL


------------------------------------------------------------------------------

Title:
Semi-automated extraction of research topics and trends from NCI funding  in radiological sciences from 2000-2020

Abstract: Investigators, funders, and the public desire knowledge on topics and trends
in publicly funded research but current efforts in manual categorization are
limited in scale and understanding. We developed a semi-automated approach to
extract and name research topics, and applied this to \$1.9B of NCI funding
over 21 years in the radiological sciences to determine micro- and macro-scale
research topics and funding trends. Our method relies on sequential clustering
of existing biomedical-based word embeddings, naming using subject matter
experts, and visualization to discover trends at a macroscopic scale above
individual topics. We present results using 15 and 60 cluster topics, where we
found that 2D projection of grant embeddings reveals two dominant axes:
physics-biology and therapeutic-diagnostic. For our dataset, we found that
funding for therapeutics- and physics-based research have outpaced diagnostics-
and biology-based research, respectively. We hope these results may (1) give
insight to funders on the appropriateness of their funding allocation, (2)
assist investigators in contextualizing their work and explore neighboring
research domains, and (3) allow the public to review where their tax dollars
are being allocated.


------------------------------------------------------------------------------

Title:
Squeeze, Recover and Relabel: Dataset Condensation at ImageNet Scale  From A New Perspective

Abstract: We present a new dataset condensation framework termed Squeeze, Recover and
Relabel (SRe$^2$L) that decouples the bilevel optimization of model and
synthetic data during training, to handle varying scales of datasets, model
architectures and image resolutions for effective dataset condensation. The
proposed method demonstrates flexibility across diverse dataset scales and
exhibits multiple advantages in terms of arbitrary resolutions of synthesized
images, low training cost and memory consumption with high-resolution training,
and the ability to scale up to arbitrary evaluation network architectures.
Extensive experiments are conducted on Tiny-ImageNet and full ImageNet-1K
datasets. Under 50 IPC, our approach achieves the highest 42.5% and 60.8%
validation accuracy on Tiny-ImageNet and ImageNet-1K, outperforming all
previous state-of-the-art methods by margins of 14.5% and 32.9%, respectively.
Our approach also outperforms MTT by approximately 52$\times$ (ConvNet-4) and
16$\times$ (ResNet-18) faster in speed with less memory consumption of
11.6$\times$ and 6.4$\times$ during data synthesis. Our code and condensed
datasets of 50, 200 IPC with 4K recovery budget are available at
this https URL


------------------------------------------------------------------------------

Title:
Mapping the German Diamond Open Access Journal Landscape

Abstract: In the current scientific and political discourse surrounding the
transformation of the scientific publication system, significant attention is
focused on Diamond Open Access (OA). This article explores the potential and
challenges of Diamond OA journals, using Germany as a case study. Two questions
are addressed: first, the current role of such journals in the scientific
publication system is determined through bibliometric analysis across various
disciplines. Second, an investigation is conducted to assess the sustainability
of Diamond OA journals and identify associated structural problems or potential
breaking points. This investigation includes an in-depth expert interview study
involving 20 editors of Diamond OA journals. The empirical results are
presented using a landscape map that considers two dimensions: 'monetized and
gift-based completion of tasks' and 'journal team size.' The bibliometric
analysis reveals a substantial number of Diamond OA journals in the social
sciences and humanities, but limited adoption in other fields. The model proves
effective for small to mid-sized journals, but not for larger ones.
Additionally, it was found that 23 Diamond OA journals have recently
discontinued their operations. The expert interviews demonstrate the usefulness
of the two dimensions in understanding key differences. Journals in two of the
four quadrants of the map exemplify sustainable conditions, while the other two
quadrants raise concerns about long-term stability. These concerns include
limited funding leading to a lack of division of labor and an excessive burden
on highly committed members. These findings underscore the need for the
development of more sustainable funding models to ensure the success of Diamond
OA journals.


------------------------------------------------------------------------------

Title:
Improved Signal Detection for Ambient Backscatter Communications

Abstract: In ambient backscatter communication (AmBC) systems, passive tags connect to
a reader by reflecting an ambient radio frequency (RF) signal. However, the
reader may not know the channel states and RF source parameters and can
experience interference. The traditional energy detector (TED) appears to be an
ideal solution. However, it performs poorly under these conditions. To address
this, we propose two new detectors: (1) A joint correlation-energy detector
(JCED) based on the first-order correlation of the received samples and (2) An
improved energy detector (IED) based on the p-th norm of the received signal
vector. We compare the performance of the IED and TED under generalized noise
modeled using the McLeish distribution and derive a general analytical formula
for the area under the receiver operating characteristic (ROC) curves. Based on
our results, both detectors outperform TED. For example, the probability of
detection with a false alarm rate of 1% for JCED and IED is 14% and 5% higher,
respectively, compared to TED. These gains are even higher using the direct
interference cancellation (DIC) technique, with increases of 16% and 7%,
respectively. Overall, our proposed detectors offer better performance than the
TED, making them useful tools for improving AmBC system performance.


------------------------------------------------------------------------------

Title:
Towards Explainable Evaluation Metrics for Machine Translation

Abstract: Unlike classical lexical overlap metrics such as BLEU, most current
evaluation metrics for machine translation (for example, COMET or BERTScore)
are based on black-box large language models. They often achieve strong
correlations with human judgments, but recent research indicates that the
lower-quality classical metrics remain dominant, one of the potential reasons
being that their decision processes are more transparent. To foster more
widespread acceptance of novel high-quality metrics, explainability thus
becomes crucial. In this concept paper, we identify key properties as well as
key goals of explainable machine translation metrics and provide a
comprehensive synthesis of recent techniques, relating them to our established
goals and properties. In this context, we also discuss the latest
state-of-the-art approaches to explainable metrics based on generative models
such as ChatGPT and GPT4. Finally, we contribute a vision of next-generation
approaches, including natural language explanations. We hope that our work can
help catalyze and guide future research on explainable evaluation metrics and,
mediately, also contribute to better and more transparent machine translation
systems.


------------------------------------------------------------------------------

Title:
CamChoice: A Corpus of Multiple Choice Questions and Candidate Response  Distributions

Abstract: Multiple Choice examinations are a ubiquitous form of assessment that is used
to measure the ability of candidates across various domains and tasks.
Maintaining the quality of proposed questions is of great importance to test
designers, and therefore newly proposed questions go through several pre-test
evaluation stages before they can be deployed into real-world exams. This
process is currently quite manual, which can lead to time lags in the question
development cycle. Automating this process would lead to a large improvement in
efficiency, however, current datasets do not contain sufficient pre-test
analysis information. In this paper, we introduce CamChoice; a multiple-choice
comprehension dataset with questions at different target levels, where
questions have the true candidate selected options distributions. We introduce
the task of candidate distribution matching, propose several evaluation metrics
for the task, and demonstrate that automatic systems trained on RACE++ can be
leveraged as baselines for our task. We further demonstrate that these
automatic systems can be used for practical pre-test evaluation tasks such as
detecting underperforming distractors, where our detection systems can
automatically identify poor distractors that few candidates select. We release
the data publicly for future research.


------------------------------------------------------------------------------

Title:
Checking Refinement of Asynchronous Programs against Context-Free  Specifications

Abstract: In the language-theoretic approach to refinement verification, we check that
the language of traces of an implementation all belong to the language of a
specification. We consider the refinement verification problem for asynchronous
programs against specifications given by a Dyck language. We show that this
problem is EXPSPACE-complete -- the same complexity as that of language
emptiness and for refinement verification against a regular specification. Our
algorithm uses several technical ingredients. First, we show that checking if
the coverability language of a succinctly described vector addition system with
states (VASS) is contained in a Dyck language is EXPSPACE-complete. Second, in
the more technical part of the proof, we define an ordering on words and show a
downward closure construction that allows replacing the (context-free) language
of each task in an asynchronous program by a regular language. Unlike downward
closure operations usually considered in infinite-state verification, our
ordering is not a well-quasi-ordering, and we have to construct the regular
language ab initio. Once the tasks can be replaced, we show a reduction to an
appropriate VASS and use our first ingredient. In addition to the inherent
theoretical interest, refinement verification with Dyck specifications captures
common practical resource usage patterns based on reference counting, for which
few algorithmic techniques were known.


------------------------------------------------------------------------------

Title:
GT-TSCH: Game-Theoretic Distributed TSCH Scheduler for Low-Power IoT  Networks

Abstract: Time-Slotted Channel Hopping (TSCH) is a synchronous medium access mode of
the IEEE 802.15.4e standard designed for providing low-latency and
highly-reliable end-to-end communication. TSCH constructs a communication
schedule by combining frequency channel hopping with Time Division Multiple
Access (TDMA). In recent years, IETF designed several standards to define
general mechanisms for the implementation of TSCH. However, the problem of
updating the TSCH schedule according to the changes of the wireless link
quality and node's traffic load left unresolved. In this paper, we use
non-cooperative game theory to propose GT-TSCH, a distributed TSCH scheduler
designed for low-power IoT applications. By considering selfish behavior of
nodes in packet forwarding, GT-TSCH updates the TSCH schedule in a distributed
approach with low control overhead by monitoring the queue length, the place of
the node in the Directed Acyclic Graph (DAG) topology, the quality of the
wireless link, and the data packet generation rate. We prove the existence and
uniqueness of Nash equilibrium in our game model and we find the optimal number
of TSCH Tx timeslots to update the TSCH slotframe. To examine the performance
of our contribution, we implement GT-TSCH on Zolertia Firefly IoT motes and the
Contiki-NG Operating System (OS). The evaluation results reveal that GT-TSCH
improves performance in terms of throughput and end-to-end delay compared to
the state-of-the-art method.


------------------------------------------------------------------------------

Title:
Can Differentiable Decision Trees Learn Interpretable Reward Functions?

Abstract: There is an increasing interest in learning reward functions that model human
intent and human preferences. However, many frameworks use blackbox learning
methods that, while expressive, are difficult to interpret. We propose and
evaluate a novel approach for learning expressive and interpretable reward
functions from preferences using Differentiable Decision Trees (DDTs) for both
low- and high-dimensional state inputs. We explore and discuss the viability of
learning interpretable reward functions using DDTs by evaluating our algorithm
on Cartpole, Visual Gridworld environments, and Atari games. We provide
evidence that that the tree structure of our learned reward function is useful
in determining the extent to which a reward function is aligned with human
preferences. We visualize the learned reward DDTs and find that they are
capable of learning interpretable reward functions but that the discrete nature
of the trees hurts the performance of reinforcement learning at test time.
However, we also show evidence that using soft outputs (averaged over all leaf
nodes) results in competitive performance when compared with larger capacity
deep neural network reward functions.


------------------------------------------------------------------------------

Title:
Multi-Task Learning with Loop Specific Attention for CDR Structure  Prediction

Abstract: The Complementarity Determining Region (CDR) structure prediction of loops in
antibody engineering has gained a lot of attraction by researchers. When
designing antibodies, a main challenge is to predict the CDR structure of the
H3 loop. Compared with the other CDR loops, that is the H1 and H2 loops, the
CDR structure of the H3 loop is more challenging due to its varying length and
flexible structure. In this paper, we propose a Multi-task learning model with
Loop Specific Attention, namely MLSA. In particular, to the best of our
knowledge we are the first to jointly learn the three CDR loops, via a novel
multi-task learning strategy. In addition, to account for the structural and
functional similarities and differences of the three CDR loops, we propose a
loop specific attention mechanism to control the influence of each CDR loop on
the training of MLSA. Our experimental evaluation on widely used benchmark data
shows that the proposed MLSA method significantly reduces the prediction error
of the CDR structure of the H3 loop, by at least 19%, when compared with other
baseline strategies. Finally, for reproduction purposes we make the
implementation of MLSA publicly available at
this https URL


------------------------------------------------------------------------------

Title:
Sixth-Order Hybrid FDMs and/or the M-Matrix Property for Elliptic  Interface Problems with Mixed Boundary Conditions

Abstract: In this paper, we develop sixth-order hybrid finite difference methods (FDMs)
for the elliptic interface problem $-\nabla \cdot( a\nabla u)=f$ in
$\Omega\backslash \Gamma$, where $\Gamma$ is a smooth interface inside
$\Omega$. The variable scalar coefficient $a>0$ and source $f$ are possibly
discontinuous across $\Gamma$. The hybrid FDMs utilize a 9-point compact
stencil at any interior regular point of the grid and a 13-point stencil at
irregular points near $\Gamma$. For interior regular points away from $\Gamma$,
we obtain a sixth-order 9-point compact FDM satisfying the M-matrix property.
Consequently, for the elliptic problem without interface (i.e., $\Gamma$ is
empty), our compact FDM satisfies the discrete maximum principle, which
guarantees the theoretical sixth-order convergence. We also derive sixth-order
compact (4-point for corners and 6-point for edges) FDMs having the M-matrix
property at any boundary point subject to (mixed) Dirichlet/Neumann/Robin
boundary conditions. For irregular points near $\Gamma$, we propose fifth-order
13-point FDMs, whose stencil coefficients can be effectively calculated by
recursively solving several small linear systems. Theoretically, the proposed
high order FDMs use high order (partial) derivatives of the coefficient $a$,
the source term $f$, the interface curve $\Gamma$, the two jump functions along
$\Gamma$, and the functions on $\partial \Omega$. Numerically, we always use
function values to approximate all required high order (partial) derivatives in
our hybrid FDMs without losing accuracy. Our proposed FDMs are independent of
the choice representing $\Gamma$ and are also applicable if the jump conditions
on $\Gamma$ only depend on the geometry (e.g., curvature) of the curve
$\Gamma$. Our numerical experiments confirm the sixth-order convergence in the
$l_{\infty}$ norm of the proposed hybrid FDMs for the elliptic interface
problem.


------------------------------------------------------------------------------

Title:
Analysing Mechanisms for Virtual Channel Management in Low-Diameter  networks

Abstract: To interconnect their growing number of servers, current supercomputers and
data centers are starting to adopt low-diameter networks, such as HyperX,
Dragonfly and Dragonfly+. These emergent topologies require balancing the load
over their links and finding suitable non-minimal routing mechanisms for them
becomes particularly challenging. The Valiant load balancing scheme is a very
popular choice for non-minimal routing. Evolved adaptive routing mechanisms
implemented in real systems are based on this Valiant scheme.
All these low-diameter networks are deadlock-prone when non-minimal routing
is employed. Routing deadlocks occur when packets cannot progress due to cyclic
dependencies. Therefore, developing efficient deadlock-free packet routing
mechanisms is critical for the progress of these emergent networks. The routing
function includes the routing algorithm for path selection and the buffers
management policy that dictates how packets allocate the buffers of the
switches on their paths. For the same routing algorithm, a different buffer
management mechanism can lead to a very different performance. Moreover,
certain mechanisms considered efficient for avoiding deadlocks, may still
suffer from hard to pinpoint instabilities that make erratic the network
response. This paper focuses on exploring the impact of these buffers
management policies on the performance of current interconnection networks,
showing a 90\% of performance drop if an incorrect buffers management policy is
used. Moreover, this study not only characterizes some of these undesirable
scenarios but also proposes practicable solutions.


------------------------------------------------------------------------------

Title:
Investigating the Usability of Collaborative Robot control through  Hands-Free Operation using Eye gaze and Augmented Reality

Abstract: This paper proposes a novel operation for controlling a mobile robot using a
head-mounted device. Conventionally, robots are operated using computers or a
joystick, which creates limitations in usability and flexibility because
control equipment has to be carried by hand. This lack of flexibility may
prevent workers from multitasking or carrying objects while operating the
robot. To address this limitation, we propose a hands-free method to operate
the mobile robot with a human gaze in an Augmented Reality (AR) environment.
The proposed work is demonstrated using the HoloLens 2 to control the mobile
robot, Robotnik Summit-XL, through the eye-gaze in AR. Stable speed control and
navigation of the mobile robot were achieved through admittance control which
was calculated using the gaze position. The experiment was conducted to compare
the usability between the joystick and the proposed operation, and the results
were validated through surveys (i.e., SUS, SEQ). The survey results from the
participants after the experiments showed that the wearer of the HoloLens
accurately operated the mobile robot in a collaborative manner. The results for
both the joystick and the HoloLens were marked as easy to use with
above-average usability. This suggests that the HoloLens can be used as a
replacement for the joystick to allow hands-free robot operation and has the
potential to increase the efficiency of human-robot collaboration in situations
when hands-free controls are needed.


------------------------------------------------------------------------------

Title:
Auditing Predictive Models for Intersectional Biases

Abstract: Predictive models that satisfy group fairness criteria in aggregate for
members of a protected class, but do not guarantee subgroup fairness, could
produce biased predictions for individuals at the intersection of two or more
protected classes. To address this risk, we propose Conditional Bias Scan
(CBS), a flexible auditing framework for detecting intersectional biases in
classification models. CBS identifies the subgroup for which there is the most
significant bias against the protected class, as compared to the equivalent
subgroup in the non-protected class, and can incorporate multiple commonly used
fairness definitions for both probabilistic and binarized predictions. We show
that this methodology can detect previously unidentified intersectional and
contextual biases in the COMPAS pre-trial risk assessment tool and has higher
bias detection power compared to similar methods that audit for subgroup
fairness.


------------------------------------------------------------------------------

Title:
Can LLMs Express Their Uncertainty? An Empirical Evaluation of  Confidence Elicitation in LLMs

Abstract: The task of empowering large language models (LLMs) to accurately express
their confidence, referred to as confidence elicitation, is essential in
ensuring reliable and trustworthy decision-making processes. Previous methods,
which primarily rely on model logits, have become less suitable for LLMs and
even infeasible with the rise of closed-source LLMs (e.g., commercialized LLM
APIs). This leads to a growing need to explore the untapped area of
\emph{non-logit-based} approaches to estimate the uncertainty of LLMs. Hence,
in this study, we investigate approaches for confidence elicitation that do not
require model fine-tuning or access to proprietary information. We introduce
three categories of methods: verbalize-based, consistency-based, and their
hybrid methods for benchmarking, and evaluate their performance across five
types of datasets and four widely-used LLMs. Our analysis of these methods
uncovers several key insights: 1) LLMs often exhibit a high degree of
overconfidence when verbalizing their confidence; 2) Prompting strategies such
as CoT, Top-K and Multi-step confidences improve calibration of verbalized
confidence; 3) Consistency-based methods outperform the verbalized confidences
in most cases, with particularly notable improvements on the arithmetic
reasoning task; 4) Hybrid methods consistently deliver the best performance
over their baselines, thereby emerging as a promising state-of-the-art
approach; 5) Despite these advancements, all investigated methods continue to
struggle with challenging tasks, such as those requiring professional
knowledge, leaving significant scope for improvement of confidence elicitation.


------------------------------------------------------------------------------

Title:
SQ Lower Bounds for Learning Bounded Covariance GMMs

Abstract: We study the complexity of learning mixtures of separated Gaussians with
common unknown bounded covariance matrix. Specifically, we focus on learning
Gaussian mixture models (GMMs) on $\mathbb{R}^d$ of the form $P= \sum_{i=1}^k
w_i \mathcal{N}(\boldsymbol \mu_i,\mathbf \Sigma_i)$, where $\mathbf \Sigma_i =
\mathbf \Sigma \preceq \mathbf I$ and $\min_{i \neq j} \| \boldsymbol \mu_i -
\boldsymbol \mu_j\|_2 \geq k^\epsilon$ for some $\epsilon>0$. Known learning
algorithms for this family of GMMs have complexity $(dk)^{O(1/\epsilon)}$. In
this work, we prove that any Statistical Query (SQ) algorithm for this problem
requires complexity at least $d^{\Omega(1/\epsilon)}$. In the special case
where the separation is on the order of $k^{1/2}$, we additionally obtain
fine-grained SQ lower bounds with the correct exponent. Our SQ lower bounds
imply similar lower bounds for low-degree polynomial tests. Conceptually, our
results provide evidence that known algorithms for this problem are nearly best
possible.


------------------------------------------------------------------------------

Title:
PyKoopman: A Python Package for Data-Driven Approximation of the Koopman  Operator

Abstract: PyKoopman is a Python package for the data-driven approximation of the
Koopman operator associated with a dynamical system. The Koopman operator is a
principled linear embedding of nonlinear dynamics and facilitates the
prediction, estimation, and control of strongly nonlinear dynamics using linear
systems theory. In particular, PyKoopman provides tools for data-driven system
identification for unforced and actuated systems that build on the
equation-free dynamic mode decomposition (DMD) and its variants. In this work,
we provide a brief description of the mathematical underpinnings of the Koopman
operator, an overview and demonstration of the features implemented in
PyKoopman (with code examples), practical advice for users, and a list of
potential extensions to PyKoopman. Software is available at
this http URL


------------------------------------------------------------------------------

Title:
Named entity recognition in resumes

Abstract: Named entity recognition (NER) is used to extract information from various
documents and texts such as names and dates. It is important to extract
education and work experience information from resumes in order to filter them.
Considering the fact that all information in a resume has to be entered to the
companys system manually, automatizing this process will save time of the
companies. In this study, a deep learning-based semi-automatic named entity
recognition system has been implemented with a focus on resumes in the field of
IT. Firstly, resumes of employees from five different IT related fields has
been annotated. Six transformer based pre-trained models have been adapted to
named entity recognition problem using the annotated data. These models have
been selected among popular models in the natural language processing field.
The obtained system can recognize eight different entity types which are city,
date, degree, diploma major, job title, language, country and skill. Models
used in the experiments are compared using micro, macro and weighted F1 scores
and the performance of the methods was evaluated. Taking these scores into
account for test set the best micro and weighted F1 score is obtained by
RoBERTa and the best macro F1 score is obtained by Electra model.


------------------------------------------------------------------------------

Title:
Design Considerations and Robustness to Parameter Uncertainty in  Wire-Wrapped Cam Mechanisms

Abstract: Collaborative robots must simultaneously be safe enough to operate in close
proximity to human operators and powerful enough to assist users in industrial
tasks such as lifting heavy equipment. The requirement for safety necessitates
that collaborative robots are designed with low-powered actuators. However,
some industrial tasks may require the robot to have high payload capacity
and/or long reach. For collaborative robot designs to be successful, they must
find ways of addressing these conflicting design requirements. One promising
strategy for navigating this tradeoff is through the use of static balancing
mechanisms to offset the robot's self weight, thus enabling the selection of
lower-powered actuators. In this paper, we introduce a novel, 2 degree of
freedom static balancing mechanism based on spring-loaded, wire-wrapped cams.
We also present an optimization-based cam design method that guarantees the
cams stay convex, ensures the springs stay below their extensions limits, and
minimizes sensitivity to unmodeled deviations from the nominal spring constant.
Additionally, we present a model of the effect of friction between the wire and
the cam. Lastly, we show experimentally that the torque generated by the cam
mechanism matches the torque predicted in our modeling approach. Our results
also suggest that the effects of wire-cam friction are significant for
non-circular cams.


------------------------------------------------------------------------------

Title:
Impacts and Risk of Generative AI Technology on Cyber Defense

Abstract: Generative Artificial Intelligence (GenAI) has emerged as a powerful
technology capable of autonomously producing highly realistic content in
various domains, such as text, images, audio, and videos. With its potential
for positive applications in creative arts, content generation, virtual
assistants, and data synthesis, GenAI has garnered significant attention and
adoption. However, the increasing adoption of GenAI raises concerns about its
potential misuse for crafting convincing phishing emails, generating
disinformation through deepfake videos, and spreading misinformation via
authentic-looking social media posts, posing a new set of challenges and risks
in the realm of cybersecurity. To combat the threats posed by GenAI, we propose
leveraging the Cyber Kill Chain (CKC) to understand the lifecycle of
cyberattacks, as a foundational model for cyber defense. This paper aims to
provide a comprehensive analysis of the risk areas introduced by the offensive
use of GenAI techniques in each phase of the CKC framework. We also analyze the
strategies employed by threat actors and examine their utilization throughout
different phases of the CKC, highlighting the implications for cyber defense.
Additionally, we propose GenAI-enabled defense strategies that are both
attack-aware and adaptive. These strategies encompass various techniques such
as detection, deception, and adversarial training, among others, aiming to
effectively mitigate the risks posed by GenAI-induced cyber threats.


------------------------------------------------------------------------------

Title:
Online Self-Supervised Learning in Machine Learning Intrusion Detection  for the Internet of Things

Abstract: This paper proposes a novel Self-Supervised Intrusion Detection (SSID)
framework, which enables a fully online Machine Learning (ML) based Intrusion
Detection System (IDS) that requires no human intervention or prior off-line
learning. The proposed framework analyzes and labels incoming traffic packets
based only on the decisions of the IDS itself using an Auto-Associative Deep
Random Neural Network, and on an online estimate of its statistically measured
trustworthiness. The SSID framework enables IDS to adapt rapidly to
time-varying characteristics of the network traffic, and eliminates the need
for offline data collection. This approach avoids human errors in data
labeling, and human labor and computational costs of model training and data
collection. The approach is experimentally evaluated on public datasets and
compared with well-known ML models, showing that this SSID framework is very
useful and advantageous as an accurate and online learning ML-based IDS for IoT
systems.


------------------------------------------------------------------------------

Title:
Tracking public attitudes toward ChatGPT on Twitter using sentiment  analysis and topic modeling

Abstract: ChatGPT sets a new record with the fastest-growing user base, as a chatbot
powered by a large language model (LLM). While it demonstrates state-of-the-art
capabilities in a variety of language-generating tasks, it also raises
widespread public concerns regarding its societal impact. In this paper, we
utilize natural language processing approaches to investigate the public
attitudes towards ChatGPT by applying sentiment analysis and topic modeling
techniques to Twitter data. Our result shows that the overall sentiment is
largely neutral to positive, which also holds true across different occupation
groups. Among a wide range of topics mentioned in tweets, the most popular
topics are Artificial Intelligence, Search Engines, Education, Writing, and
Question Answering.


------------------------------------------------------------------------------

Title:
Achieving Sample and Computational Efficient Reinforcement Learning by  Action Space Reduction via Grouping

Abstract: Reinforcement learning often needs to deal with the exponential growth of
states and actions when exploring optimal control in high-dimensional spaces
(often known as the curse of dimensionality). In this work, we address this
issue by learning the inherent structure of action-wise similar MDP to
appropriately balance the performance degradation versus sample/computational
complexity. In particular, we partition the action spaces into multiple groups
based on the similarity in transition distribution and reward function, and
build a linear decomposition model to capture the difference between the
intra-group transition kernel and the intra-group rewards. Both our theoretical
analysis and experiments reveal a \emph{surprising and counter-intuitive
result}: while a more refined grouping strategy can reduce the approximation
error caused by treating actions in the same group as identical, it also leads
to increased estimation error when the size of samples or the computation
resources is limited. This finding highlights the grouping strategy as a new
degree of freedom that can be optimized to minimize the overall performance
loss. To address this issue, we formulate a general optimization problem for
determining the optimal grouping strategy, which strikes a balance between
performance loss and sample/computational complexity. We further propose a
computationally efficient method for selecting a nearly-optimal grouping
strategy, which maintains its computational complexity independent of the size
of the action space.


------------------------------------------------------------------------------

Title:
Sum-Rate Maximization of RSMA-based Aerial Communications with Energy  Harvesting: A Reinforcement Learning Approach

Abstract: In this letter, we investigate a joint power and beamforming design problem
for rate-splitting multiple access (RSMA)-based aerial communications with
energy harvesting, where a self-sustainable aerial base station serves multiple
users by utilizing the harvested energy. Considering maximizing the sum-rate
from the long-term perspective, we utilize a deep reinforcement learning (DRL)
approach, namely the soft actor-critic algorithm, to restrict the maximum
transmission power at each time based on the stochastic property of the channel
environment, harvested energy, and battery power information. Moreover, for
designing precoders and power allocation among all the private/common streams
of the RSMA, we employ sequential least squares programming (SLSQP) using the
Han-Powell quasi-Newton method to maximize the sum-rate for the given
transmission power via DRL. Numerical results show the superiority of the
proposed scheme over several baseline methods in terms of the average sum-rate
performance.


------------------------------------------------------------------------------

Title:
Decentralized Online Federated G-Network Learning for Lightweight  Intrusion Detection

Abstract: Cyberattacks are increasingly threatening networked systems, often with the
emergence of new types of unknown (zero-day) attacks and the rise of vulnerable
devices. While Machine Learning (ML)-based Intrusion Detection Systems (IDSs)
have been shown to be extremely promising in detecting these attacks, the need
to learn large amounts of labelled data often limits the applicability of
ML-based IDSs to cybersystems that only have access to private local data. To
address this issue, this paper proposes a novel Decentralized and Online
Federated Learning Intrusion Detection (DOF-ID) architecture. DOF-ID is a
collaborative learning system that allows each IDS used for a cybersystem to
learn from experience gained in other cybersystems in addition to its own local
data without violating the data privacy of other systems. As the performance
evaluation results using public Kitsune and Bot-IoT datasets show, DOF-ID
significantly improves the intrusion detection performance in all collaborating
nodes simultaneously with acceptable computation time for online learning.


------------------------------------------------------------------------------

Title:
Deep Metric Learning with Soft Orthogonal Proxies

Abstract: Deep Metric Learning (DML) models rely on strong representations and
similarity-based measures with specific loss functions. Proxy-based losses have
shown great performance compared to pair-based losses in terms of convergence
speed. However, proxies that are assigned to different classes may end up being
closely located in the embedding space and hence having a hard time to
distinguish between positive and negative items. Alternatively, they may become
highly correlated and hence provide redundant information with the model. To
address these issues, we propose a novel approach that introduces Soft
Orthogonality (SO) constraint on proxies. The constraint ensures the proxies to
be as orthogonal as possible and hence control their positions in the embedding
space. Our approach leverages Data-Efficient Image Transformer (DeiT) as an
encoder to extract contextual features from images along with a DML objective.
The objective is made of the Proxy Anchor loss along with the SO
regularization. We evaluate our method on four public benchmarks for
category-level image retrieval and demonstrate its effectiveness with
comprehensive experimental results and ablation studies. Our evaluations
demonstrate the superiority of our proposed approach over state-of-the-art
methods by a significant margin.


------------------------------------------------------------------------------

Title:
Apolitical Intelligence? Auditing Delphi's responses on controversial  political issues in the US

Abstract: As generative language models are deployed in ever-wider contexts, concerns
about their political values have come to the forefront with critique from all
parts of the political spectrum that the models are biased and lack neutrality.
However, the question of what neutrality is and whether it is desirable remains
underexplored. In this paper, I examine neutrality through an audit of Delphi
[arXiv:2110.07574], a large language model designed for crowdsourced ethics. I
analyse how Delphi responds to politically controversial questions compared to
different US political subgroups. I find that Delphi is poorly calibrated with
respect to confidence and exhibits a significant political skew. Based on these
results, I examine the question of neutrality from a data-feminist lens, in
terms of how notions of neutrality shift power and further marginalise unheard
voices. These findings can hopefully contribute to a more reflexive debate
about the normative questions of alignment and what role we want generative
models to play in society.


------------------------------------------------------------------------------

Title:
Adaptive Bernstein Change Detector for High-Dimensional Data Streams

Abstract: Change detection is of fundamental importance when analyzing data streams.
Detecting changes both quickly and accurately enables monitoring and prediction
systems to react, e.g., by issuing an alarm or by updating a learning
algorithm. However, detecting changes is challenging when observations are
high-dimensional. In high-dimensional data, change detectors should not only be
able to identify when changes happen, but also in which subspace they occur.
Ideally, one should also quantify how severe they are. Our approach, ABCD, has
these properties. ABCD learns an encoder-decoder model and monitors its
accuracy over a window of adaptive size. ABCD derives a change score based on
Bernstein's inequality to detect deviations in terms of accuracy, which
indicate changes. Our experiments demonstrate that ABCD outperforms its best
competitor by at least 8% and up to 23% in F1-score on average. It can also
accurately estimate changes' subspace, together with a severity measure that
correlates with the ground truth.


------------------------------------------------------------------------------

Title:
Rate-Splitting Multiple Access for 6G Networks: Ten Promising Scenarios  and Applications

Abstract: In the upcoming 6G era, multiple access (MA) will play an essential role in
achieving high throughput performances required in a wide range of wireless
applications. Since MA and interference management are closely related issues,
the conventional MA techniques are limited in that they cannot provide
near-optimal performance in universal interference regimes. Recently,
rate-splitting multiple access (RSMA) has been gaining much attention. RSMA
splits an individual message into two parts: a common part, decodable by every
user, and a private part, decodable only by the intended user. Each user first
decodes the common message and then decodes its private message by applying
successive interference cancellation (SIC). By doing so, RSMA not only embraces
the existing MA techniques as special cases but also provides significant
performance gains by efficiently mitigating inter-user interference in a broad
range of interference regimes. In this article, we first present the
theoretical foundation of RSMA. Subsequently, we put forth four key benefits of
RSMA: spectral efficiency, robustness, scalability, and flexibility. Upon this,
we describe how RSMA can enable ten promising scenarios and applications along
with future research directions to pave the way for 6G.


------------------------------------------------------------------------------

Title:
A Survey of Link Prediction Algorithms

Abstract: The problem of link prediction, predicting if two nodes in a network have a
connection between them, is a theoretical problem with numerous field-agnostic
real-world applications. This paper investigates the efficacy of three classes
of link prediction algorithms: local node similarity heuristics, the global
index Random Walk with Restart, and Node2Vec embeddings. Furthermore, this
paper provides insight into the performance of canonical link prediction
algorithms on small graphs. The graphs included in this study are sampled from
various domains, including infrastructure and ecological networks.


------------------------------------------------------------------------------

Title:
Minimalist and High-Quality Panoramic Imaging with PSF-aware  Transformers

Abstract: High-quality panoramic images with a Field of View (FoV) of 360-degree are
essential for contemporary panoramic computer vision tasks. However,
conventional imaging systems come with sophisticated lens designs and heavy
optical components. This disqualifies their usage in many mobile and wearable
applications where thin and portable, minimalist imaging systems are desired.
In this paper, we propose a Panoramic Computational Imaging Engine (PCIE) to
address minimalist and high-quality panoramic imaging. With less than three
spherical lenses, a Minimalist Panoramic Imaging Prototype (MPIP) is
constructed based on the design of the Panoramic Annular Lens (PAL), but with
low-quality imaging results due to aberrations and small image plane size. We
propose two pipelines, i.e. Aberration Correction (AC) and Super-Resolution and
Aberration Correction (SR&AC), to solve the image quality problems of MPIP,
with imaging sensors of small and large pixel size, respectively. To provide a
universal network for the two pipelines, we leverage the information from the
Point Spread Function (PSF) of the optical system and design a PSF-aware
Aberration-image Recovery Transformer (PART), in which the self-attention
calculation and feature extraction are guided via PSF-aware mechanisms. We
train PART on synthetic image pairs from simulation and put forward the PALHQ
dataset to fill the gap of real-world high-quality PAL images for low-level
vision. A comprehensive variety of experiments on synthetic and real-world
benchmarks demonstrates the impressive imaging results of PCIE and the
effectiveness of plug-and-play PSF-aware mechanisms. We further deliver
heuristic experimental findings for minimalist and high-quality panoramic
imaging. Our dataset and code will be available at
this https URL


------------------------------------------------------------------------------

Title:
Designing Individualized Policy and Technology Interventions to Improve  Gig Work Conditions

Abstract: The gig economy is characterized by short-term contract work completed by
independent workers who are paid to perform "gigs", and who have control over
when, whether and how they conduct work. Gig economy platforms (e.g., Uber,
Lyft, Instacart) offer workers increased job opportunities, lower barriers to
entry, and improved flexibility. However, growing evidence suggests that worker
well-being and gig work conditions have become significant societal issues. In
designing public-facing policies and technologies for improving gig work
conditions, inherent tradeoffs exist between offering individual flexibility
and when attempting to meet all community needs. In platform-based gig work,
contractors pursue the flexibility of short-term tasks, but policymakers resist
segmenting the population when designing policies to support their work. As
platforms offer an ever-increasing variety of services, we argue that
policymakers and platform designers must provide more targeted and personalized
policies, benefits, and protections for platform-based workers, so that they
can lead more successful and sustainable gig work careers. We present in this
paper relevant legal and scholarly evidence from the United States to support
this position, and make recommendations for future innovations in policy and
technology.


------------------------------------------------------------------------------

Title:
What to Learn: Features, Image Transformations, or Both?

Abstract: Long-term visual localization is an essential problem in robotics and
computer vision, but remains challenging due to the environmental appearance
changes caused by lighting and seasons. While many existing works have
attempted to solve it by directly learning invariant sparse keypoints and
descriptors to match scenes, these approaches still struggle with adverse
appearance changes. Recent developments in image transformations such as neural
style transfer have emerged as an alternative to address such appearance gaps.
In this work, we propose to combine an image transformation network and a
feature-learning network to improve long-term localization performance. Given
night-to-day image pairs, the image transformation network transforms the night
images into day-like conditions prior to feature matching; the feature network
learns to detect keypoint locations with their associated descriptor values,
which can be passed to a classical pose estimator to compute the relative
poses. We conducted various experiments to examine the effectiveness of
combining style transfer and feature learning and its training strategy,
showing that such a combination greatly improves long-term localization
performance.


------------------------------------------------------------------------------

Title:
Affine Correspondences between Multi-Camera Systems for Relative Pose  Estimation

Abstract: We present a novel method to compute the relative pose of multi-camera
systems using two affine correspondences (ACs). Existing solutions to the
multi-camera relative pose estimation are either restricted to special cases of
motion, have too high computational complexity, or require too many point
correspondences (PCs). Thus, these solvers impede an efficient or accurate
relative pose estimation when applying RANSAC as a robust estimator. This paper
shows that the 6DOF relative pose estimation problem using ACs permits a
feasible minimal solution, when exploiting the geometric constraints between
ACs and multi-camera systems using a special parameterization. We present a
problem formulation based on two ACs that encompass two common types of ACs
across two views, i.e., inter-camera and intra-camera. Moreover, the framework
for generating the minimal solvers can be extended to solve various relative
pose estimation problems, e.g., 5DOF relative pose estimation with known
rotation angle prior. Experiments on both virtual and real multi-camera systems
prove that the proposed solvers are more efficient than the state-of-the-art
algorithms, while resulting in a better relative pose accuracy. Source code is
available at this https URL


------------------------------------------------------------------------------

Title:
Instance-Optimal Cluster Recovery in the Labeled Stochastic Block Model

Abstract: We consider the problem of recovering hidden communities in the Labeled
Stochastic Block Model (LSBM) with a finite number of clusters, where cluster
sizes grow linearly with the total number $n$ of items. In the LSBM, a label is
(independently) observed for each pair of items. Our objective is to devise an
efficient algorithm that recovers clusters using the observed labels. To this
end, we revisit instance-specific lower bounds on the expected number of
misclassified items satisfied by any clustering algorithm. We present
Instance-Adaptive Clustering (IAC), the first algorithm whose performance
matches these lower bounds both in expectation and with high probability. IAC
consists of a one-time spectral clustering algorithm followed by an iterative
likelihood-based cluster assignment improvement. This approach is based on the
instance-specific lower bound and does not require any model parameters,
including the number of clusters. By performing the spectral clustering only
once, IAC maintains an overall computational complexity of $\mathcal{O}(n
\text{polylog}(n))$. We illustrate the effectiveness of our approach through
numerical experiments.


------------------------------------------------------------------------------

Title:
From ontology design to user-centred interfaces for music heritage

Abstract: In this article we investigate the bridge between ontology design and UI/UX
design methodologies to assist designers in prototyping web applications for
information seeking purposes. We briefly review the state of the art in
ontology design and UI/UX methodologies, then we illustrate our approach
applied to a case study in the music heritage domain.


------------------------------------------------------------------------------

Title:
ACC Saturator: Automatic Kernel Optimization for Directive-Based GPU  Code

Abstract: Automatic code optimization is a complex process that typically involves the
application of multiple discrete algorithms that modify the program structure
irreversibly. However, the design of these algorithms is often monolithic, and
they require repetitive implementation to perform similar analyses due to the
lack of cooperation. To address this issue, modern optimization techniques,
such as equality saturation, allow for exhaustive term rewriting at various
levels of inputs, thereby simplifying compiler design.
In this paper, we propose equality saturation to optimize sequential codes
utilized in directive-based programming for GPUs. Our approach simultaneously
realizes less computation, less memory access, and high memory throughput. Our
fully-automated framework constructs single-assignment forms from inputs to be
entirely rewritten while keeping dependencies and extracts optimal cases.
Through practical benchmarks, we demonstrate a significant performance
improvement on several compilers. Furthermore, we highlight the advantages of
computational reordering and emphasize the significance of memory-access order
for modern GPUs.


------------------------------------------------------------------------------

Title:
Speech Emotion Diarization: Which Emotion Appears When?

Abstract: Speech Emotion Recognition (SER) typically relies on utterance-level
solutions. However, emotions conveyed through speech should be considered as
discrete speech events with definite temporal boundaries, rather than
attributes of the entire utterance. To reflect the fine-grained nature of
speech emotions, we propose a new task: Speech Emotion Diarization (SED). Just
as Speaker Diarization answers the question of "Who speaks when?", Speech
Emotion Diarization answers the question of "Which emotion appears when?". To
facilitate the evaluation of the performance and establish a common benchmark
for researchers, we introduce the Zaion Emotion Dataset (ZED), an openly
accessible speech emotion dataset that includes non-acted emotions recorded in
real-life conditions, along with manually-annotated boundaries of emotion
segments within the utterance. We provide competitive baselines and open-source
the code and the pre-trained models.


------------------------------------------------------------------------------

Title:
AugDMC: Data Augmentation Guided Deep Multiple Clustering

Abstract: Clustering aims to group similar objects together while separating dissimilar
ones apart. Thereafter, structures hidden in data can be identified to help
understand data in an unsupervised manner. Traditional clustering methods such
as k-means provide only a single clustering for one data set. Deep clustering
methods such as auto-encoder based clustering methods have shown a better
performance, but still provide a single clustering. However, a given dataset
might have multiple clustering structures and each represents a unique
perspective of the data. Therefore, some multiple clustering methods have been
developed to discover multiple independent structures hidden in data. Although
deep multiple clustering methods provide better performance, how to efficiently
capture the alternative perspectives in data is still a problem. In this paper,
we propose AugDMC, a novel data Augmentation guided Deep Multiple Clustering
method, to tackle the challenge. Specifically, AugDMC leverages data
augmentations to automatically extract features related to a certain aspect of
the data using a self-supervised prototype-based representation learning, where
different aspects of the data can be preserved under different data
augmentations. Moreover, a stable optimization strategy is proposed to
alleviate the unstable problem from different augmentations. Thereafter,
multiple clusterings based on different aspects of the data can be obtained.
Experimental results on three real-world datasets compared with
state-of-the-art methods validate the effectiveness of the proposed method.


------------------------------------------------------------------------------

Title:
Towards More Realistic Membership Inference Attacks on Large Diffusion  Models

Abstract: Generative diffusion models, including Stable Diffusion and Midjourney, can
generate visually appealing, diverse, and high-resolution images for various
applications. These models are trained on billions of internet-sourced images,
raising significant concerns about the potential unauthorized use of
copyright-protected images. In this paper, we examine whether it is possible to
determine if a specific image was used in the training set, a problem known in
the cybersecurity community and referred to as a membership inference attack.
Our focus is on Stable Diffusion, and we address the challenge of designing a
fair evaluation framework to answer this membership question. We propose a
methodology to establish a fair evaluation setup and apply it to Stable
Diffusion, enabling potential extensions to other generative models. Utilizing
this evaluation setup, we execute membership attacks (both known and newly
introduced). Our research reveals that previously proposed evaluation setups do
not provide a full understanding of the effectiveness of membership inference
attacks. We conclude that the membership inference attack remains a significant
challenge for large diffusion models (often deployed as black-box systems),
indicating that related privacy and copyright issues will persist in the
foreseeable future.


------------------------------------------------------------------------------

Title:
Conversation Derailment Forecasting with Graph Convolutional Networks

Abstract: Online conversations are particularly susceptible to derailment, which can
manifest itself in the form of toxic communication patterns like disrespectful
comments or verbal abuse. Forecasting conversation derailment predicts signs of
derailment in advance enabling proactive moderation of conversations. Current
state-of-the-art approaches to address this problem rely on sequence models
that treat dialogues as text streams. We propose a novel model based on a graph
convolutional neural network that considers dialogue user dynamics and the
influence of public perception on conversation utterances. Through empirical
evaluation, we show that our model effectively captures conversation dynamics
and outperforms the state-of-the-art models on the CGA and CMV benchmark
datasets by 1.5\% and 1.7\%, respectively.


------------------------------------------------------------------------------

Title:
An Energy Stable Discontinuous Galerkin Time-Domain Finite Element  Method in Optics and Photonics

Abstract: In this paper, a time-domain discontinuous Galerkin (TDdG) finite element
method for the full system of Maxwell's equations in optics and photonics is
investigated, including a complete proof of a semi-discrete error estimate. The
new capabilities of methods of this type are to efficiently model linear and
nonlinear effects, for example of Kerr nonlinearities. Energy stable
discretizations both at the semi-discrete and the fully discrete levels are
presented. In particular, the proposed semi-discrete scheme is optimally
convergent in the spatial variable on Cartesian meshes with $Q_k$-type
elements, and the fully discrete scheme is conditionally stable with respect to
a specially defined nonlinear electromagnetic energy. The approaches presented
prove to be robust and allow the modeling of optical problems and the treatment
of complex nonlinearities as well as geometries of various physical systems
coupled with electromagnetic fields.


------------------------------------------------------------------------------

Title:
A balanced finite-element method for an axisymmetrically loaded thin  shell

Abstract: We analyse a finite-element discretisation of a differential equation
describing an axisymmetrically loaded thin shell. The problem is singularly
perturbed when the thickness of the shell becomes small. We prove robust
convergence of the method in a balanced norm that captures the layers present
in the solution. Numerical results confirm our findings.


------------------------------------------------------------------------------

Title:
Transferable Curricula through Difficulty Conditioned Generators

Abstract: Advancements in reinforcement learning (RL) have demonstrated superhuman
performance in complex tasks such as Starcraft, Go, Chess etc. However,
knowledge transfer from Artificial "Experts" to humans remain a significant
challenge. A promising avenue for such transfer would be the use of curricula.
Recent methods in curricula generation focuses on training RL agents
efficiently, yet such methods rely on surrogate measures to track student
progress, and are not suited for training robots in the real world (or more
ambitiously humans). In this paper, we introduce a method named Parameterized
Environment Response Model (PERM) that shows promising results in training RL
agents in parameterized environments. Inspired by Item Response Theory, PERM
seeks to model difficulty of environments and ability of RL agents directly.
Given that RL agents and humans are trained more efficiently under the "zone of
proximal development", our method generates a curriculum by matching the
difficulty of an environment to the current ability of the student. In
addition, PERM can be trained offline and does not employ non-stationary
measures of student ability, making it suitable for transfer between students.
We demonstrate PERM's ability to represent the environment parameter space, and
training with RL agents with PERM produces a strong performance in
deterministic environments. Lastly, we show that our method is transferable
between students, without any sacrifice in training quality.


------------------------------------------------------------------------------

Title:
Conceptual Design and Analysis of No-Insulation High-Temperature  Superconductor Tubular Wave Energy Converter

Abstract: So far, a number of wave energy converters (WEC) have been proposed to
increase efficiency and economic feasibility. Particularly, tubular WEC with
permanent magnets and coil winding packs is mostly used to convert the wave
energy. Due to the demand for high magnetic flux density in WEC, research has
been conducted on high-temperature superconductors (HTS) WEC. In this paper,
the conceptual design of no-insulation (NI) HTS tubular WEC and its
optimization process are proposed. Using NI technology, it has become possible
to design WEC with high volumetric efficiency and cost-effectiveness.
Furthermore, the design is analyzed in the aspect of electromagnetism,
mechanical force, and cryogen. The performance of the proposed WEC is evaluated
as a response to various waveforms and their amplitudes. A rectifying circuit
of WEC connected in parallel with load resistance is used for the output power
study.


------------------------------------------------------------------------------

Title:
Context-lumpable stochastic bandits

Abstract: We consider a contextual bandit problem with $S $ contexts and $A $ actions.
In each round $t=1,2,\dots$ the learner observes a random context and chooses
an action based on its past experience. The learner then observes a random
reward whose mean is a function of the context and the action for the round.
Under the assumption that the contexts can be lumped into $r\le \min\{S ,A \}$
groups such that the mean reward for the various actions is the same for any
two contexts that are in the same group, we give an algorithm that outputs an
$\epsilon$-optimal policy after using at most $\widetilde O(r (S +A
)/\epsilon^2)$ samples with high probability and provide a matching
$\widetilde\Omega(r (S +A )/\epsilon^2)$ lower bound. In the regret
minimization setting, we give an algorithm whose cumulative regret up to time
$T$ is bounded by $\widetilde O(\sqrt{r^3(S +A )T})$. To the best of our
knowledge, we are the first to show the near-optimal sample complexity in the
PAC setting and $\widetilde O(\sqrt{{poly}(r)(S+K)T})$ minimax regret in the
online setting for this problem. We also show our algorithms can be applied to
more general low-rank bandits and get improved regret bounds in some scenarios.


------------------------------------------------------------------------------

Title:
Data augmentation for recommender system: A semi-supervised approach  using maximum margin matrix factorization

Abstract: Collaborative filtering (CF) has become a popular method for developing
recommender systems (RS) where ratings of a user for new items is predicted
based on her past preferences and available preference information of other
users. Despite the popularity of CF-based methods, their performance is often
greatly limited by the sparsity of observed entries. In this study, we explore
the data augmentation and refinement aspects of Maximum Margin Matrix
Factorization (MMMF), a widely accepted CF technique for the rating
predictions, which have not been investigated before. We exploit the inherent
characteristics of CF algorithms to assess the confidence level of individual
ratings and propose a semi-supervised approach for rating augmentation based on
self-training. We hypothesize that any CF algorithm's predictions with low
confidence are due to some deficiency in the training data and hence, the
performance of the algorithm can be improved by adopting a systematic data
augmentation strategy. We iteratively use some of the ratings predicted with
high confidence to augment the training data and remove low-confidence entries
through a refinement process. By repeating this process, the system learns to
improve prediction accuracy. Our method is experimentally evaluated on several
state-of-the-art CF algorithms and leads to informative rating augmentation,
improving the performance of the baseline approaches.


------------------------------------------------------------------------------

Title:
Robust Semantic Segmentation: Strong Adversarial Attacks and Fast  Training of Robust Models

Abstract: While a large amount of work has focused on designing adversarial attacks
against image classifiers, only a few methods exist to attack semantic
segmentation models. We show that attacking segmentation models presents
task-specific challenges, for which we propose novel solutions. Our final
evaluation protocol outperforms existing methods, and shows that those can
overestimate the robustness of the models. Additionally, so far adversarial
training, the most successful way for obtaining robust image classifiers, could
not be successfully applied to semantic segmentation. We argue that this is
because the task to be learned is more challenging, and requires significantly
higher computational effort than for image classification. As a remedy, we show
that by taking advantage of recent advances in robust ImageNet classifiers, one
can train adversarially robust segmentation models at limited computational
cost by fine-tuning robust backbones.


------------------------------------------------------------------------------

Title:
Evolving Computation Graphs

Abstract: Graph neural networks (GNNs) have demonstrated success in modeling relational
data, especially for data that exhibits homophily: when a connection between
nodes tends to imply that they belong to the same class. However, while this
assumption is true in many relevant situations, there are important real-world
scenarios that violate this assumption, and this has spurred research into
improving GNNs for these cases. In this work, we propose Evolving Computation
Graphs (ECGs), a novel method for enhancing GNNs on heterophilic datasets. Our
approach builds on prior theoretical insights linking node degree, high
homophily, and inter vs intra-class embedding similarity by rewiring the GNNs'
computation graph towards adding edges that connect nodes that are likely to be
in the same class. We utilise weaker classifiers to identify these edges,
ultimately improving GNN performance on non-homophilic data as a result. We
evaluate ECGs on a diverse set of recently-proposed heterophilous datasets and
demonstrate improvements over the relevant baselines. ECG presents a simple,
intuitive and elegant approach for improving GNN performance on heterophilic
datasets without requiring prior domain knowledge.


------------------------------------------------------------------------------

Title:
Siamese SIREN: Audio Compression with Implicit Neural Representations

Abstract: Implicit Neural Representations (INRs) have emerged as a promising method for
representing diverse data modalities, including 3D shapes, images, and audio.
While recent research has demonstrated successful applications of INRs in image
and 3D shape compression, their potential for audio compression remains largely
unexplored. Motivated by this, we present a preliminary investigation into the
use of INRs for audio compression. Our study introduces Siamese SIREN, a novel
approach based on the popular SIREN architecture. Our experimental results
indicate that Siamese SIREN achieves superior audio reconstruction fidelity
while utilizing fewer network parameters compared to previous INR
architectures.


------------------------------------------------------------------------------

Title:
Special Delivery: Programming with Mailbox Types (Extended Version)

Abstract: The asynchronous and unidirectional communication model supported by
mailboxes is a key reason for the success of actor languages like Erlang and
Elixir for implementing reliable and scalable distributed systems. While many
actors may send messages to some actor, only the actor may (selectively)
receive from its mailbox. Although actors eliminate many of the issues stemming
from shared memory concurrency, they remain vulnerable to communication errors
such as protocol violations and deadlocks.
Mailbox types are a novel behavioural type system for mailboxes first
introduced for a process calculus by de'Liguoro and Padovani in 2018, which
capture the contents of a mailbox as a commutative regular expression. Due to
aliasing and nested evaluation contexts, moving from a process calculus to a
programming language is challenging.
This paper presents Pat, the first programming language design incorporating
mailbox types, and describes an algorithmic type system. We make essential use
of quasi-linear typing to tame some of the complexity introduced by aliasing.
Our algorithmic type system is necessarily co-contextual, achieved through a
novel use of backwards bidirectional typing, and we prove it sound and complete
with respect to our declarative type system. We implement a prototype type
checker, and use it to demonstrate the expressiveness of Pat on a factory
automation case study and a series of examples from the Savina actor benchmark
suite.


------------------------------------------------------------------------------

Title:
Feature Mixing for Writer Retrieval and Identification on Papyri  Fragments

Abstract: This paper proposes a deep-learning-based approach to writer retrieval and
identification for papyri, with a focus on identifying fragments associated
with a specific writer and those corresponding to the same image. We present a
novel neural network architecture that combines a residual backbone with a
feature mixing stage to improve retrieval performance, and the final descriptor
is derived from a projection layer. The methodology is evaluated on two
benchmarks: PapyRow, where we achieve a mAP of 26.6 % and 24.9 % on writer and
page retrieval, and HisFragIR20, showing state-of-the-art performance (44.0 %
and 29.3 % mAP). Furthermore, our network has an accuracy of 28.7 % for writer
identification. Additionally, we conduct experiments on the influence of two
binarization techniques on fragments and show that binarizing does not enhance
performance. Our code and models are available to the community.


------------------------------------------------------------------------------

Title:
Decentralized Multi-Agent Reinforcement Learning with Global State  Prediction

Abstract: Deep reinforcement learning (DRL) has seen remarkable success in the control
of single robots. However, applying DRL to robot swarms presents significant
challenges. A critical challenge is non-stationarity, which occurs when two or
more robots update individual or shared policies concurrently, thereby engaging
in an interdependent training process with no guarantees of convergence.
Circumventing non-stationarity typically involves training the robots with
global information about other agents' states and/or actions. In contrast, in
this paper we explore how to remove the need for global information. We pose
our problem as a Partially Observable Markov Decision Process, due to the
absence of global knowledge on other agents. Using collective transport as a
testbed scenario, we study two approaches to multi-agent training. In the
first, the robots exchange no messages, and are trained to rely on implicit
communication through push-and-pull on the object to transport. In the second
approach, we introduce Global State Prediction (GSP), a network trained to
forma a belief over the swarm as a whole and predict its future states. We
provide a comprehensive study over four well-known deep reinforcement learning
algorithms in environments with obstacles, measuring performance as the
successful transport of the object to the goal within a desired time-frame.
Through an ablation study, we show that including GSP boosts performance and
increases robustness when compared with methods that use global knowledge.


------------------------------------------------------------------------------

Title:
Quantizable Transformers: Removing Outliers by Helping Attention Heads  Do Nothing

Abstract: Transformer models have been widely adopted in various domains over the last
years, and especially large language models have advanced the field of AI
significantly. Due to their size, the capability of these networks has
increased tremendously, but this has come at the cost of a significant increase
in necessary compute. Quantization is one of the most effective ways to reduce
the computational time and memory consumption of neural networks. Many studies
have shown, however, that modern transformer models tend to learn strong
outliers in their activations, making them difficult to quantize. To retain
acceptable performance, the existence of these outliers requires activations to
be in higher bitwidth or the use of different numeric formats, extra
fine-tuning, or other workarounds. We show that strong outliers are related to
very specific behavior of attention heads that try to learn a "no-op" or just a
partial update of the residual. To achieve the exact zeros needed in the
attention matrix for a no-update, the input to the softmax is pushed to be
larger and larger during training, causing outliers in other parts of the
network. Based on these observations, we propose two simple (independent)
modifications to the attention mechanism - clipped softmax and gated attention.
We empirically show that models pre-trained using our methods learn
significantly smaller outliers while maintaining and sometimes even improving
the floating-point task performance. This enables us to quantize transformers
to full INT8 quantization of the activations without any additional effort. We
demonstrate the effectiveness of our methods on both language models (BERT,
OPT) and vision transformers.


------------------------------------------------------------------------------

Title:
An Interactive Interface for Novel Class Discovery in Tabular Data

Abstract: Novel Class Discovery (NCD) is the problem of trying to discover novel
classes in an unlabeled set, given a labeled set of different but related
classes. The majority of NCD methods proposed so far only deal with image data,
despite tabular data being among the most widely used type of data in practical
applications. To interpret the results of clustering or NCD algorithms, data
scientists need to understand the domain- and application-specific attributes
of tabular data. This task is difficult and can often only be performed by a
domain expert. Therefore, this interface allows a domain expert to easily run
state-of-the-art algorithms for NCD in tabular data. With minimal knowledge in
data science, interpretable results can be generated.


------------------------------------------------------------------------------

Title:
FlowBot++: Learning Generalized Articulated Objects Manipulation via  Articulation Projection

Abstract: Understanding and manipulating articulated objects, such as doors and
drawers, is crucial for robots operating in human environments. We wish to
develop a system that can learn to articulate novel objects with no prior
interaction, after training on other articulated objects. Previous approaches
for articulated object manipulation rely on either modular methods which are
brittle or end-to-end methods, which lack generalizability. This paper presents
FlowBot++, a deep 3D vision-based robotic system that predicts dense per-point
motion and dense articulation parameters of articulated objects to assist in
downstream manipulation tasks. FlowBot++ introduces a novel per-point
representation of the articulated motion and articulation parameters that are
combined to produce a more accurate estimate than either method on their own.
Simulated experiments on the PartNet-Mobility dataset validate the performance
of our system in articulating a wide range of objects, while real-world
experiments on real objects' point clouds and a Sawyer robot demonstrate the
generalizability and feasibility of our system in real-world scenarios.


------------------------------------------------------------------------------

Title:
Cross-lingual Cross-temporal Summarization: Dataset, Models, Evaluation

Abstract: While summarization has been extensively researched in natural language
processing (NLP), cross-lingual cross-temporal summarization (CLCTS) is a
largely unexplored area that has the potential to improve cross-cultural
accessibility, information sharing, and understanding. This paper
comprehensively addresses the CLCTS task, including dataset creation, modeling,
and evaluation. We build the first CLCTS corpus, leveraging historical fictive
texts and Wikipedia summaries in English and German, and examine the
effectiveness of popular transformer end-to-end models with different
intermediate task finetuning tasks. Additionally, we explore the potential of
ChatGPT for CLCTS as a summarizer and an evaluator. Overall, we report
evaluations from humans, ChatGPT, and several recent automatic evaluation
metrics where we find our intermediate task finetuned end-to-end models
generate bad to moderate quality summaries; ChatGPT as a summarizer (without
any finetuning) provides moderate to good quality outputs and as an evaluator
correlates moderately with human evaluations though it is prone to giving lower
scores. ChatGPT also seems to be very adept at normalizing historical text. We
finally test ChatGPT in a scenario with adversarially attacked and unseen
source documents and find that ChatGPT is better at omission and entity swap
than negating against its prior knowledge.


------------------------------------------------------------------------------

Title:
Towards Exascale CFD Simulations Using the Discontinuous Galerkin Solver  FLEXI

Abstract: Modern high-order discretizations bear considerable potential for the
exascale era due to their high fidelity and the high, local computational load
that allows for computational efficiency in massively parallel simulations. To
this end, the discontinuous Galerkin (DG) framework FLEXI was selected to
demonstrate exascale readiness within the Center of Excellence for Exascale CFD
(CEEC) by simulating shock buffet on a three-dimensional wing segment at
transsonic flight conditions. This paper summarizes the recent progress made to
enable the simulation of this challenging exascale problem. For this, it is
first demonstrated that FLEXI scales excellently to over 500 000 CPU cores on
HAWK at the HLRS. To tackle the considerable resolution requirements near the
wall, a novel wall model is proposed that takes compressibility effects into
account and yields decent results for the simulation of a NACA 64A-110 airfoil.
To address the shocks in the domain, a finite-volume-based shock capturing
method was implemented in FLEXI, which is validated here using the simulation
of a linear compressor cascade at supersonic flow conditions, where the method
is demonstrated to yield efficient, robust and accurate results. Lastly, we
present the TensorFlow-Fortran-Binding (TFFB) as an easy-to-use library to
deploy trained machine learning models in Fortran solvers such as FLEXI.


------------------------------------------------------------------------------

Title:
Unveiling Global Narratives: A Multilingual Twitter Dataset of News  Media on the Russo-Ukrainian Conflict

Abstract: The ongoing Russo-Ukrainian conflict has been a subject of intense media
coverage worldwide. Understanding the global narrative surrounding this topic
is crucial for researchers that aim to gain insights into its multifaceted
dimensions. In this paper, we present a novel dataset that focuses on this
topic by collecting and processing tweets posted by news or media companies on
social media across the globe. We collected tweets from February 2022 to May
2023 to acquire approximately 1.5 million tweets in 60 different languages.
Each tweet in the dataset is accompanied by processed tags, allowing for the
identification of entities, stances, concepts, and sentiments expressed. The
availability of the dataset serves as a valuable resource for researchers
aiming to investigate the global narrative surrounding the ongoing conflict
from various aspects such as who are the prominent entities involved, what
stances are taken, where do these stances originate, and how are the different
concepts related to the event portrayed.


------------------------------------------------------------------------------

Title:
Data Architecture for Digital Object Space Management Service (DOSM)  using DAT

Abstract: The Internet of Things (IoT) data and social media data are two of the
fastest-growing data segments. Having high-quality data is crucial for making
informed business decisions. The strategic process of leveraging insights from
data is known as data-driven decision-making. To achieve this, it is necessary
to collect, store, analyze, and protect data in the best ways possible. Data
architecture is a complex task that involves describing the flow of data from
its source to its destination and creating a blueprint for managing the data to
meet business needs for information. In this paper, we utilize the Data
Architecture Tool (DAT) to model data for Digital Space Management Service,
which was developed as part of the VASARI project. This work focuses on
describing the movement of data, data formats, data location, data processing
(batch or real-time), data storage technologies, and main operations on the
data.


------------------------------------------------------------------------------

Title:
Map Point Selection for Visual SLAM

Abstract: Simultaneous localisation and mapping (SLAM) play a vital role in autonomous
robotics. Robotic platforms are often resource-constrained, and this limitation
motivates resource-efficient SLAM implementations. While sparse visual SLAM
algorithms offer good accuracy for modest hardware requirements, even these
more scalable sparse approaches face limitations when applied to large-scale
and long-term scenarios. A contributing factor is that the point clouds
resulting from SLAM are inefficient to use and contain significant redundancy.
This paper proposes the use of subset selection algorithms to reduce the map
produced by sparse visual SLAM algorithms. Information-theoretic techniques
have been applied to simpler related problems before, but they do not scale if
applied to the full visual SLAM problem. This paper proposes a number of novel
information\hyp{}theoretic utility functions for map point selection and
optimises these functions using greedy algorithms. The reduced maps are
evaluated using practical data alongside an existing visual SLAM implementation
(ORB-SLAM 2). Approximate selection techniques proposed in this paper achieve
trajectory accuracy comparable to an offline baseline while being suitable for
online use. These techniques enable the practical reduction of maps for visual
SLAM with competitive trajectory accuracy.
Results also demonstrate that SLAM front-end performance can significantly
impact the performance of map point selection. This shows the importance of
testing map point selection with a front-end implementation. To exploit this,
this paper proposes an approach that includes a model of the front-end in the
utility function when additional information is available. This approach
outperforms alternatives on applicable datasets and highlights future research
directions.


------------------------------------------------------------------------------

Title:
xSIM++: An Improved Proxy to Bitext Mining Performance for Low-Resource  Languages

Abstract: We introduce a new proxy score for evaluating bitext mining based on
similarity in a multilingual embedding space: xSIM++. In comparison to xSIM,
this improved proxy leverages rule-based approaches to extend English sentences
in any evaluation set with synthetic, hard-to-distinguish examples which more
closely mirror the scenarios we encounter during large-scale mining. We
validate this proxy by running a significant number of bitext mining
experiments for a set of low-resource languages, and subsequently train NMT
systems on the mined data. In comparison to xSIM, we show that xSIM++ is better
correlated with the downstream BLEU scores of translation systems trained on
mined bitexts, providing a reliable proxy of bitext mining performance without
needing to run expensive bitext mining pipelines. xSIM++ also reports
performance for different error types, offering more fine-grained feedback for
model development.


------------------------------------------------------------------------------

Title:
In Situ Framework for Coupling Simulation and Machine Learning with  Application to CFD

Abstract: Recent years have seen many successful applications of machine learning (ML)
to facilitate fluid dynamic computations. As simulations grow, generating new
training datasets for traditional offline learning creates I/O and storage
bottlenecks. Additionally, performing inference at runtime requires non-trivial
coupling of ML framework libraries with simulation codes. This work offers a
solution to both limitations by simplifying this coupling and enabling in situ
training and inference workflows on heterogeneous clusters. Leveraging
SmartSim, the presented framework deploys a database to store data and ML
models in memory, thus circumventing the file system. On the Polaris
supercomputer, we demonstrate perfect scaling efficiency to the full machine
size of the data transfer and inference costs thanks to a novel co-located
deployment of the database. Moreover, we train an autoencoder in situ from a
turbulent flow simulation, showing that the framework overhead is negligible
relative to a solver time step and training epoch.


------------------------------------------------------------------------------

Title:
Combination of Measurement Data and Domain Knowledge for Simulation of  Halbach Arrays with Bayesian Inference

Abstract: Accelerator magnets made from blocks of permanent magnets in a zero-clearance
configuration are known as Halbach arrays. The objective of this work is the
fusion of knowledge from different measurement sources (material and field) and
domain knowledge (magnetostatics) to obtain an updated magnet model of a
Halbach array. From Helmholtz-coil measurements of the magnetized blocks, a
prior distribution of the magnetization is estimated. Measurements of the
magnetic flux density are used to derive, by means of Bayesian inference, a
posterior distribution. The method is validated on simulated data and applied
to measurements of a dipole of the FASER detector. The updated magnet model of
the FASER dipole describes the magnetic flux density one order of magnitude
better than the prior magnet model.


------------------------------------------------------------------------------

Title:
Learning from Visual Observation via Offline Pretrained State-to-Go  Transformer

Abstract: Learning from visual observation (LfVO), aiming at recovering policies from
only visual observation data, is promising yet a challenging problem. Existing
LfVO approaches either only adopt inefficient online learning schemes or
require additional task-specific information like goal states, making them not
suited for open-ended tasks. To address these issues, we propose a two-stage
framework for learning from visual observation. In the first stage, we
introduce and pretrain State-to-Go (STG) Transformer offline to predict and
differentiate latent transitions of demonstrations. Subsequently, in the second
stage, the STG Transformer provides intrinsic rewards for downstream
reinforcement learning tasks where an agent learns merely from intrinsic
rewards. Empirical results on Atari and Minecraft show that our proposed method
outperforms baselines and in some tasks even achieves performance comparable to
the policy learned from environmental rewards. These results shed light on the
potential of utilizing video-only data to solve difficult visual reinforcement
learning tasks rather than relying on complete offline datasets containing
states, actions, and rewards. The project's website and code can be found at
this https URL


------------------------------------------------------------------------------

Title:
Multi-Objective Hull Form Optimization with CAD Engine-based Deep  Learning Physics for 3D Flow Prediction

Abstract: In this work, we propose a built-in Deep Learning Physics Optimization (DLPO)
framework to set up a shape optimization study of the Duisburg Test Case (DTC)
container vessel. We present two different applications: (1) sensitivity
analysis to detect the most promising generic basis hull shapes, and (2)
multi-objective optimization to quantify the trade-off between optimal hull
forms. DLPO framework allows for the evaluation of design iterations
automatically in an end-to-end manner. We achieved these results by coupling
Extrality's Deep Learning Physics (DLP) model to a CAD engine and an optimizer.
Our proposed DLP model is trained on full 3D volume data coming from RANS
simulations, and it can provide accurate and high-quality 3D flow predictions
in real-time, which makes it a good evaluator to perform optimization of new
container vessel designs w.r.t the hydrodynamic efficiency. In particular, it
is able to recover the forces acting on the vessel by integration on the hull
surface with a mean relative error of 3.84\% \pm 2.179\% on the total
resistance. Each iteration takes only 20 seconds, thus leading to a drastic
saving of time and engineering efforts, while delivering valuable insight into
the performance of the vessel, including RANS-like detailed flow information.
We conclude that DLPO framework is a promising tool to accelerate the ship
design process and lead to more efficient ships with better hydrodynamic
performance.


------------------------------------------------------------------------------

Title:
A new 3-DOF 2T1R parallel mechanism: Topology design and kinematics

Abstract: This article presents a new three-degree-of-freedom (3-DOF) parallel
mechanism (PM) with two translations and one rotation (2T1R), designed based on
the topological design theory of the parallel mechanism using position and
orientation characteristics (POC). The PM is primarily intended for use in
package sorting and delivery. The mobile platform of the PM moves along a
translation axis, picks up objects from a conveyor belt, and tilts them to
either side of the axis. We first calculate the PM's topological
characteristics, such as the degree of freedom (DOF) and the degree of
coupling, and provide its topological analytical formula to represent the
topological information of the PM. Next, we solve the direct and inverse
kinematic models based on the kinematic modelling principle using the
topological features. The models are purely analytic and are broken down into a
series of quadratic equations, making them suitable for use in an industrial
robot. We also study the singular configurations to identify the serial and
parallel singularities. Using the decoupling properties, we size the mechanism
to address the package sorting and depositing problem using an algebraic
approach. To determine the smallest segment lengths, we use a cylindrical
algebraic decomposition to solve a system with inequalities.


------------------------------------------------------------------------------

Title:
AudioPaLM: A Large Language Model That Can Speak and Listen

Abstract: We introduce AudioPaLM, a large language model for speech understanding and
generation. AudioPaLM fuses text-based and speech-based language models, PaLM-2
[Anil et al., 2023] and AudioLM [Borsos et al., 2022], into a unified
multimodal architecture that can process and generate text and speech with
applications including speech recognition and speech-to-speech translation.
AudioPaLM inherits the capability to preserve paralinguistic information such
as speaker identity and intonation from AudioLM and the linguistic knowledge
present only in text large language models such as PaLM-2. We demonstrate that
initializing AudioPaLM with the weights of a text-only large language model
improves speech processing, successfully leveraging the larger quantity of text
training data used in pretraining to assist with the speech tasks. The
resulting model significantly outperforms existing systems for speech
translation tasks and has the ability to perform zero-shot speech-to-text
translation for many languages for which input/target language combinations
were not seen in training. AudioPaLM also demonstrates features of audio
language models, such as transferring a voice across languages based on a short
spoken prompt. We release examples of our method at
this https URL


------------------------------------------------------------------------------

Title:
Data-Free Backbone Fine-Tuning for Pruned Neural Networks

Abstract: Model compression techniques reduce the computational load and memory
consumption of deep neural networks. After the compression operation, e.g.
parameter pruning, the model is normally fine-tuned on the original training
dataset to recover from the performance drop caused by compression. However,
the training data is not always available due to privacy issues or other
factors. In this work, we present a data-free fine-tuning approach for pruning
the backbone of deep neural networks. In particular, the pruned network
backbone is trained with synthetically generated images, and our proposed
intermediate supervision to mimic the unpruned backbone's output feature map.
Afterwards, the pruned backbone can be combined with the original network head
to make predictions. We generate synthetic images by back-propagating gradients
to noise images while relying on L1-pruning for the backbone pruning. In our
experiments, we show that our approach is task-independent due to pruning only
the backbone. By evaluating our approach on 2D human pose estimation, object
detection, and image classification, we demonstrate promising performance
compared to the unpruned model. Our code is available at
this https URL


------------------------------------------------------------------------------

Title:
Data-Driven Update of B(H) Curves of Iron Yokes in Normal Conducting  Accelerator Magnets

Abstract: Constitutive equations are used in electromagnetic field simulations to model
a material response to applied fields or forces. The $B(H)$ characteristic of
iron laminations depends on thermal and mechanical stresses that may have
occurred during the manufacturing process. Data-driven modelling and updating
of the $B(H)$ characteristic are therefore well known necessities. In this work
the $B(H)$ curve of an iron yoke of an accelerator magnet is updated based on
observed magnetic flux density data by solving a non-linear inverse problem.
The inverse problem is regularized by restricting the solution to the function
space that is spanned by the truncated Karhunen Loeve expansion of a stochastic
$B(H)$-curve model based on material measurements. It is shown that this method
is able to retrieve a previously selected ground truth $B(H)$-curve. With the
update of the $B(H)$ characteristic, the numerical model gains predictive
capacities for excitation currents that were not included in the data.


------------------------------------------------------------------------------

Title:
Let's Resonate! How to Elicit Improvisation and Letting Go in  Interactive Digital Art

Abstract: Participatory art allows for the spectator to be a participant or a viewer
able to engage actively with interactive art. Real-time technologies offer new
ways to create participative artworks. We hereby investigate how to engage
participation through movement in interactive digital art, and what this
engagement can awaken, focusing on the ways to elicit improvisation and letting
go. We analyze two Virtual Reality installations, ''InterACTE'' and ''Eve,
dance is an unplaceable place,'' involving body movement, dance, creativity and
the presence of an observing audience. We evaluate the premises, the setup, and
the feedback of the spectators in the two installations. We propose a model
following three different perspectives of resonance: 1. Inter Resonance between
Spectator and Artwork, which involves curiosity, imitation, playfulness and
improvisation. 2. Inner Resonance of Spectator him/herself, where embodiment
and creativity contribute to the sense of being present and letting go. 3.
Collective Resonance between Spectator/Artwork and Audience, which is
stimulated by curiosity, and triggers motor contagion, engagement and
gathering. The two analyzed examples seek to awaken open-minded communicative
possibilities through the use of interactive digital artworks. Moreover, the
need to recognize and develop the idea of resonance becomes increasingly
important in this time of urgency to communicate, understand and support
collectivity.


------------------------------------------------------------------------------

Title:
Microscopic, kinetic and hydrodynamic hybrid models of collective  motions with chemotaxis: a numerical study

Abstract: A general class of hybrid models has been introduced recently, gathering the
advantages multiscale descriptions. Concerning biological applications, the
particular coupled structure fits to collective cell migrations and pattern
formation scenarios. In this context, cells are modelled as discrete entities
and their dynamics is given by ODEs, while the chemical signal influencing the
motion is considered as a continuous signal which solves a diffusive equation.
From the analytical point of view, this class of model has been proved to have
a mean-field limit in the Wasserstein distance towards a system given by the
coupling of a Vlasov-type equation with the chemoattractant equation. Moreover,
a pressureless nonlocal Euler-type system has been derived for these models,
rigorously equivalent to the Vlasov one for monokinetic initial data. In the
present paper, we present a numerical study of the solutions to the Vlasov and
Euler systems, exploring general settings for inital data, far from the
monokinetic ones.


------------------------------------------------------------------------------

Title:
Accuracy evaluation of a Low-Cost Differential Global Positioning System  for mobile robotics

Abstract: Differential GPS, commonly referred as DGPS, is a well-known and very
accurate localization system for many outdoor applications in particular for
mobile outdoor robotics. The most common drawback of DGPS systems are the high
costs for both base station and receivers. In this paper, we present a setup
that uses third-party open-source software and a Ublox ZED-F9P chip to build a
ROS-enabled low-cost DGPS setup that is ready to use in a few hours. The main
goal of this paper is to analyze and evaluate the repetitive and absolute
accuracy of the system. The first measurement also examines the differences
between a SAPOS base station and a locally installed one consisting of low-cost
components. During the evaluation process of the absolute accuracy, a moving
mobile robot is used on the receiver side. It is tracked through a highly
accurate VICON motion capture system.


------------------------------------------------------------------------------

Title:
Reinforcement Federated Learning Method Based on Adaptive OPTICS  Clustering

Abstract: Federated learning is a distributed machine learning technology, which
realizes the balance between data privacy protection and data sharing
computing. To protect data privacy, feder-ated learning learns shared models by
locally executing distributed training on participating devices and aggregating
local models into global models. There is a problem in federated learning, that
is, the negative impact caused by the non-independent and identical
distribu-tion of data across different user terminals. In order to alleviate
this problem, this paper pro-poses a strengthened federation aggregation method
based on adaptive OPTICS clustering. Specifically, this method perceives the
clustering environment as a Markov decision process, and models the adjustment
process of parameter search direction, so as to find the best clus-tering
parameters to achieve the best federated aggregation method. The core
contribution of this paper is to propose an adaptive OPTICS clustering
algorithm for federated learning. The algorithm combines OPTICS clustering and
adaptive learning technology, and can effective-ly deal with the problem of
non-independent and identically distributed data across different user
terminals. By perceiving the clustering environment as a Markov decision
process, the goal is to find the best parameters of the OPTICS cluster without
artificial assistance, so as to obtain the best federated aggregation method
and achieve better performance. The reliability and practicability of this
method have been verified on the experimental data, and its effec-tiveness and
superiority have been proved.


------------------------------------------------------------------------------

Title:
Otter-Knowledge: benchmarks of multimodal knowledge graph representation  learning from different sources for drug discovery

Abstract: Recent research in representation learning utilizes large databases of
proteins or molecules to acquire knowledge of drug and protein structures
through unsupervised learning techniques. These pre-trained representations
have proven to significantly enhance the accuracy of subsequent tasks, such as
predicting the affinity between drugs and target proteins. In this study, we
demonstrate that by incorporating knowledge graphs from diverse sources and
modalities into the sequences or SMILES representation, we can further enrich
the representation and achieve state-of-the-art results on established
benchmark datasets. We provide preprocessed and integrated data obtained from 7
public sources, which encompass over 30M triples. Additionally, we make
available the pre-trained models based on this data, along with the reported
outcomes of their performance on three widely-used benchmark datasets for
drug-target binding affinity prediction found in the Therapeutic Data Commons
(TDC) benchmarks. Additionally, we make the source code for training models on
benchmark datasets publicly available. Our objective in releasing these
pre-trained models, accompanied by clean data for model pretraining and
benchmark results, is to encourage research in knowledge-enhanced
representation learning.


------------------------------------------------------------------------------

Title:
Backstepping Control of Coupled General Hyperbolic-Parabolic PDE-PDE  Systems

Abstract: This paper considers the backstepping state feedback and observer design for
hyperbolic and parabolic PDEs, which are bidirectionally interconnected in a
general coupling structure. Both PDE subsystems consist of coupled scalar PDEs
with the heterodirectional hyperbolic PDE subsystem subject to actuation and
sensing. By making use of a multi-step approach to construct the transformation
into a stable target system, it is shown that a backstepping state feedback and
observer design only requires to solve the well-known kernel equations for the
hyperbolic and parabolic subsystems as well as additional decoupling equations.
The latter are standard initial boundary value problems for parabolic PDEs.
This significantly facilitates the well-posedness analysis and the numerical
computation of the backstepping controller. Exponential stability is verified
for the state feedback loop, the observer error dynamics, and the closed-loop
system using an observer-based compensator. The proposed backstepping design
procedures are demonstrated for numerical examples.


------------------------------------------------------------------------------

Title:
AI could create a perfect storm of climate misinformation

Abstract: We are in the midst of a transformation of the digital news ecosystem. The
expansion of online social networks, the influence of recommender systems,
increased automation, and new generative artificial intelligence tools are
rapidly changing the speed and the way misinformation about climate change and
sustainability issues moves around the world. Policymakers, researchers and the
public need to combine forces to address the dangerous combination of opaque
social media algorithms, polarizing social bots, and a new generation of
AI-generated content. This synthesis brief is the result of a collaboration
between Stockholm Resilience Centre at Stockholm University, the Beijer
Institute of Ecological Economics at the Royal Swedish Academy of Sciences, the
Complexity Science Hub Vienna, and Karolinska Institutet. It has been put
together as an independent contribution to the Nobel Prize Summit 2023, Truth,
Trust and Hope, Washington D.C., 24th to 26th of May 2023.


------------------------------------------------------------------------------

Title:
Natural Language Processing in Electronic Health Records in Relation to  Healthcare Decision-making: A Systematic Review

Abstract: Background: Natural Language Processing (NLP) is widely used to extract
clinical insights from Electronic Health Records (EHRs). However, the lack of
annotated data, automated tools, and other challenges hinder the full
utilisation of NLP for EHRs. Various Machine Learning (ML), Deep Learning (DL)
and NLP techniques are studied and compared to understand the limitations and
opportunities in this space comprehensively.
Methodology: After screening 261 articles from 11 databases, we included 127
papers for full-text review covering seven categories of articles: 1) medical
note classification, 2) clinical entity recognition, 3) text summarisation, 4)
deep learning (DL) and transfer learning architecture, 5) information
extraction, 6) Medical language translation and 7) other NLP applications. This
study follows the Preferred Reporting Items for Systematic Reviews and
Meta-Analyses (PRISMA) guidelines.
Result and Discussion: EHR was the most commonly used data type among the
selected articles, and the datasets were primarily unstructured. Various ML and
DL methods were used, with prediction or classification being the most common
application of ML or DL. The most common use cases were: the International
Classification of Diseases, Ninth Revision (ICD-9) classification, clinical
note analysis, and named entity recognition (NER) for clinical descriptions and
research on psychiatric disorders.
Conclusion: We find that the adopted ML models were not adequately assessed.
In addition, the data imbalance problem is quite important, yet we must find
techniques to address this underlining problem. Future studies should address
key limitations in studies, primarily identifying Lupus Nephritis, Suicide
Attempts, perinatal self-harmed and ICD-9 classification.


------------------------------------------------------------------------------

Title:
Critical-Reflective Human-AI Collaboration: Exploring Computational  Tools for Art Historical Image Retrieval

Abstract: Just as other disciplines, the humanities explore how computational research
approaches and tools can meaningfully contribute to scholarly knowledge
production. We approach the design of computational tools through the
analytical lens of 'human-AI collaboration.' However, there is no generalizable
concept of what constitutes 'meaningful' human-AI collaboration. In terms of
genuinely human competencies, we consider criticality and reflection as guiding
principles of scholarly knowledge production. Although (designing for)
reflection is a recurring topic in CSCW and HCI discourses, it has not been
centered in work on human-AI collaboration. We posit that integrating both
concepts is a viable approach to supporting 'meaningful' human-AI collaboration
in the humanities. Our research, thus, is guided by the question of how
critical reflection can be enabled in human-AI collaboration. We address this
question with a use case that centers on computer vision (CV) tools for art
historical image retrieval. Specifically, we conducted a qualitative interview
study with art historians and extended the interviews with a think-aloud
software exploration. We observed and recorded our participants' interaction
with a ready-to-use CV tool in a possible research scenario. We found that
critical reflection, indeed, constitutes a core prerequisite for 'meaningful'
human-AI collaboration in humanities research contexts. However, we observed
that critical reflection was not fully realized during interaction with the CV
tool. We interpret this divergence as supporting our hypothesis that
computational tools need to be intentionally designed in such a way that they
actively scaffold and support critical reflection during interaction. Based on
our findings, we suggest four empirically grounded design implications for
'critical-reflective human-AI collaboration'.


------------------------------------------------------------------------------

Title:
Don't Treat the Symptom, Find the Cause! Efficient  Artificial-Intelligence Methods for (Interactive) Debugging

Abstract: In the modern world, we are permanently using, leveraging, interacting with,
and relying upon systems of ever higher sophistication, ranging from our cars,
recommender systems in e-commerce, and networks when we go online, to
integrated circuits when using our PCs and smartphones, the power grid to
ensure our energy supply, security-critical software when accessing our bank
accounts, and spreadsheets for financial planning and decision making. The
complexity of these systems coupled with our high dependency on them implies
both a non-negligible likelihood of system failures, and a high potential that
such failures have significant negative effects on our everyday life. For that
reason, it is a vital requirement to keep the harm of emerging failures to a
minimum, which means minimizing the system downtime as well as the cost of
system repair. This is where model-based diagnosis comes into play.
Model-based diagnosis is a principled, domain-independent approach that can
be generally applied to troubleshoot systems of a wide variety of types,
including all the ones mentioned above, and many more. It exploits and
orchestrates i.a. techniques for knowledge representation, automated reasoning,
heuristic problem solving, intelligent search, optimization, stochastics,
statistics, decision making under uncertainty, machine learning, as well as
calculus, combinatorics and set theory to detect, localize, and fix faults in
abnormally behaving systems.
In this thesis, we will give an introduction to the topic of model-based
diagnosis, point out the major challenges in the field, and discuss a selection
of approaches from our research addressing these issues.


------------------------------------------------------------------------------

Title:
NoisyILRMA: Diffuse-Noise-Aware Independent Low-Rank Matrix Analysis for  Fast Blind Source Extraction

Abstract: In this paper, we address the multichannel blind source extraction (BSE) of a
single source in diffuse noise environments. To solve this problem even faster
than by fast multichannel nonnegative matrix factorization (FastMNMF) and its
variant, we propose a BSE method called NoisyILRMA, which is a modification of
independent low-rank matrix analysis (ILRMA) to account for diffuse noise.
NoisyILRMA can achieve considerably fast BSE by incorporating an algorithm
developed for independent vector extraction. In addition, to improve the BSE
performance of NoisyILRMA, we propose a mechanism to switch the source model
with ILRMA-like nonnegative matrix factorization to a more expressive source
model during optimization. In the experiment, we show that NoisyILRMA runs
faster than a FastMNMF algorithm while maintaining the BSE performance. We also
confirm that the switching mechanism improves the BSE performance of
NoisyILRMA.


------------------------------------------------------------------------------

Title:
Efficient Partitioning Method of Large-Scale Public Safety  Spatio-Temporal Data based on Information Loss Constraints

Abstract: The storage, management, and application of massive spatio-temporal data are
widely applied in various practical scenarios, including public safety.
However, due to the unique spatio-temporal distribution characteristics of
re-al-world data, most existing methods have limitations in terms of the
spatio-temporal proximity of data and load balancing in distributed storage.
There-fore, this paper proposes an efficient partitioning method of large-scale
public safety spatio-temporal data based on information loss constraints
(IFL-LSTP). The IFL-LSTP model specifically targets large-scale spatio-temporal
point da-ta by combining the spatio-temporal partitioning module (STPM) with
the graph partitioning module (GPM). This approach can significantly reduce the
scale of data while maintaining the model's accuracy, in order to improve the
partitioning efficiency. It can also ensure the load balancing of distributed
storage while maintaining spatio-temporal proximity of the data partitioning
results. This method provides a new solution for distributed storage of
mas-sive spatio-temporal data. The experimental results on multiple real-world
da-tasets demonstrate the effectiveness and superiority of IFL-LSTP.


------------------------------------------------------------------------------

Title:
XACML Extension for Graphs: Flexible Authorization Policy Specification  and Datastore-independent Enforcement

Abstract: The increasing use of graph-structured data for business- and
privacy-critical applications requires sophisticated, flexible and fine-grained
authorization and access control. Currently, role-based access control is
supported in graph databases, where access to objects is restricted via roles.
This does not take special properties of graphs into account such as vertices
and edges along the path between a given subject and resource. In previous
iterations of our research, we started to design an authorization policy
language and access control model, which considers the specification of graph
paths and enforces them in the multi-model database ArangoDB. Since this
approach is promising to consider graph characteristics in data protection, we
improve the language in this work to provide flexible path definitions and
specifying edges as protected resources. Furthermore, we introduce a method for
a datastore-independent policy enforcement. Besides discussing the latest work
in our XACML4G model, which is an extension to the Extensible Access Control
Markup Language (XACML), we demonstrate our prototypical implementation with a
real case and give an outlook on performance.


------------------------------------------------------------------------------

Title:
HOFA: Twitter Bot Detection with Homophily-Oriented Augmentation and  Frequency Adaptive Attention

Abstract: Twitter bot detection has become an increasingly important and challenging
task to combat online misinformation, facilitate social content moderation, and
safeguard the integrity of social platforms. Though existing graph-based
Twitter bot detection methods achieved state-of-the-art performance, they are
all based on the homophily assumption, which assumes users with the same label
are more likely to be connected, making it easy for Twitter bots to disguise
themselves by following a large number of genuine users. To address this issue,
we proposed HOFA, a novel graph-based Twitter bot detection framework that
combats the heterophilous disguise challenge with a homophily-oriented graph
augmentation module (Homo-Aug) and a frequency adaptive attention module
(FaAt). Specifically, the Homo-Aug extracts user representations and computes a
k-NN graph using an MLP and improves Twitter's homophily by injecting the k-NN
graph. For the FaAt, we propose an attention mechanism that adaptively serves
as a low-pass filter along a homophilic edge and a high-pass filter along a
heterophilic edge, preventing user features from being over-smoothed by their
neighborhood. We also introduce a weight guidance loss to guide the frequency
adaptive attention module. Our experiments demonstrate that HOFA achieves
state-of-the-art performance on three widely-acknowledged Twitter bot detection
benchmarks, which significantly outperforms vanilla graph-based bot detection
techniques and strong heterophilic baselines. Furthermore, extensive studies
confirm the effectiveness of our Homo-Aug and FaAt module, and HOFA's ability
to demystify the heterophilous disguise challenge.


------------------------------------------------------------------------------

Title:
HypeRS: Building a Hypergraph-driven ensemble Recommender System

Abstract: Recommender systems are designed to predict user preferences over collections
of items. These systems process users' previous interactions to decide which
items should be ranked higher to satisfy their desires. An ensemble recommender
system can achieve great recommendation performance by effectively combining
the decisions generated by individual models. In this paper, we propose a novel
ensemble recommender system that combines predictions made by different models
into a unified hypergraph ranking framework. This is the first time that
hypergraph ranking has been employed to model an ensemble of recommender
systems. Hypergraphs are generalizations of graphs where multiple vertices can
be connected via hyperedges, efficiently modeling high-order relations. We
differentiate real and predicted connections between users and items by
assigning different hyperedge weights to individual recommender systems. We
perform experiments using four datasets from the fields of movie, music and
news media recommendation. The obtained results show that the ensemble
hypergraph ranking method generates more accurate recommendations compared to
the individual models and a weighted hybrid approach. The assignment of
different hyperedge weights to the ensemble hypergraph further improves the
performance compared to a setting with identical hyperedge weights.


------------------------------------------------------------------------------

Title:
Russian assimilatory palatalization is incomplete neutralization

Abstract: Incomplete neutralization refers to phonetic traces of underlying contrasts
in phonologically neutralizing contexts. The present study examines one such
context: Russian assimilatory palatalization in C+j sequences. Russian
contrasts plain and palatalized consonants, with the plain consonants having a
secondary articulation involving retraction of the tongue dorsum
(velarization/uvularization). However, Russian also has stop-glide sequences
that form near-minimal pairs with palatalized stops. In the environment
preceding palatal glides, the contrast between palatalized and plain consonants
is neutralized, due to the palatalization of the plain stop (assimilatory
palatalization). The purpose of the study is to explore whether the
neutralization is complete. To do so, we conducted an electromagnetic
articulography (EMA) experiment examining temporal coordination and the spatial
position of the tongue body in underlyingly palatalized consonants and those
derived from assimilatory palatalization. Articulatory results from four native
speakers of Russian revealed that gestures in both conditions are coordinated
as complex segments, i.e., they are palatalized consonants. However, there are
differences across conditions consistent with the residual presence of a tongue
dorsum retraction gesture in the plain obstruents. We conclude that
neutralization of the plain-palatal contrast in Russian is incomplete;
consonants in the assimilatory palatalization condition exhibit inter-gestural
coordination characteristic of palatalized consonants along with residual
evidence of an underlying tongue dorsum retraction (velarization/uvularization)
gesture.


------------------------------------------------------------------------------

Title:
MultiTASC: A Multi-Tenancy-Aware Scheduler for Cascaded DNN Inference at  the Consumer Edge

Abstract: Cascade systems comprise a two-model sequence, with a lightweight model
processing all samples and a heavier, higher-accuracy model conditionally
refining harder samples to improve accuracy. By placing the light model on the
device side and the heavy model on a server, model cascades constitute a widely
used distributed inference approach. With the rapid expansion of intelligent
indoor environments, such as smart homes, the new setting of Multi-Device
Cascade is emerging where multiple and diverse devices are to simultaneously
use a shared heavy model on the same server, typically located within or close
to the consumer environment. This work presents MultiTASC, a
multi-tenancy-aware scheduler that adaptively controls the forwarding decision
functions of the devices in order to maximize the system throughput, while
sustaining high accuracy and low latency. By explicitly considering device
heterogeneity, our scheduler improves the latency service-level objective (SLO)
satisfaction rate by 20-25 percentage points (pp) over state-of-the-art cascade
methods in highly heterogeneous setups, while serving over 40 devices,
showcasing its scalability.


------------------------------------------------------------------------------

Title:
BPM: Blended Piecewise Moebius Maps

Abstract: We propose a novel Moebius interpolator that takes as an input a discrete map
between the vertices of two planar triangle meshes, and outputs a smooth map on
the input domain. The output map interpolates the discrete map, is continuous
between triangles, and has low quasi-conformal distortion when the input map is
discrete conformal. Our map leads to considerably smoother texture transfer
compared to the alternatives, even on very coarse triangulations. Furthermore,
our approach has a closed-form expression, is local, applicable to any discrete
map, and leads to smooth results even for extreme deformations. Finally, by
working with local intrinsic coordinates, our approach is easily generalizable
to discrete maps between a surface triangle mesh and a planar mesh, i.e., a
planar parameterization. We compare our method with existing approaches, and
demonstrate better texture transfer results, and lower quasi-conformal errors.


------------------------------------------------------------------------------

Title:
On the Direct Construction of MDS and Near-MDS Matrices

Abstract: The optimal branch number of MDS matrices makes them a preferred choice for
designing diffusion layers in many block ciphers and hash functions.
Consequently, various methods have been proposed for designing MDS matrices,
including search and direct methods. While exhaustive search is suitable for
small order MDS matrices, direct constructions are preferred for larger orders
due to the vast search space involved. In the literature, there has been
extensive research on the direct construction of MDS matrices using both
recursive and nonrecursive methods. On the other hand, in lightweight
cryptography, Near-MDS (NMDS) matrices with sub-optimal branch numbers offer a
better balance between security and efficiency as a diffusion layer compared to
MDS matrices. However, no direct construction method is available in the
literature for constructing recursive NMDS matrices. This paper introduces some
direct constructions of NMDS matrices in both nonrecursive and recursive
settings. Additionally, it presents some direct constructions of nonrecursive
MDS matrices from the generalized Vandermonde matrices. We propose a method for
constructing involutory MDS and NMDS matrices using generalized Vandermonde
matrices. Furthermore, we prove some folklore results that are used in the
literature related to the NMDS code.


------------------------------------------------------------------------------

Title:
Overview of Robust and Multilingual Automatic Evaluation Metrics for  Open-Domain Dialogue Systems at DSTC 11 Track 4

Abstract: The advent and fast development of neural networks have revolutionized the
research on dialogue systems and subsequently have triggered various challenges
regarding their automatic evaluation. Automatic evaluation of open-domain
dialogue systems as an open challenge has been the center of the attention of
many researchers. Despite the consistent efforts to improve automatic metrics'
correlations with human evaluation, there have been very few attempts to assess
their robustness over multiple domains and dimensions. Also, their focus is
mainly on the English language. All of these challenges prompt the development
of automatic evaluation metrics that are reliable in various domains,
dimensions, and languages. This track in the 11th Dialogue System Technology
Challenge (DSTC11) is part of the ongoing effort to promote robust and
multilingual automatic evaluation metrics. This article describes the datasets
and baselines provided to participants and discusses the submission and result
details of the two proposed subtasks.


------------------------------------------------------------------------------

Title:
Physics-guided neural networks for inversion-based feedforward control  applied to hybrid stepper motors

Abstract: Rotary motors, such as hybrid stepper motors (HSMs), are widely used in
industries varying from printing applications to robotics. The increasing need
for productivity and efficiency without increasing the manufacturing costs
calls for innovative control design. Feedforward control is typically used in
tracking control problems, where the desired reference is known in advance. In
most applications, this is the case for HSMs, which need to track a periodic
angular velocity and angular position reference. Performance achieved by
feedforward control is limited by the accuracy of the available model
describing the inverse system dynamics. In this work, we develop a
physics-guided neural network (PGNN) feedforward controller for HSMs, which can
learn the effect of parasitic forces from data and compensate for it, resulting
in improved accuracy. Indeed, experimental results on an HSM used in printing
industry show that the PGNN outperforms conventional benchmarks in terms of the
mean-absolute tracking error.


------------------------------------------------------------------------------

Title:
MFCCGAN: A Novel MFCC-Based Speech Synthesizer Using Adversarial  Learning

Abstract: In this paper, we introduce MFCCGAN as a novel speech synthesizer based on
adversarial learning that adopts MFCCs as input and generates raw speech
waveforms. Benefiting the GAN model capabilities, it produces speech with
higher intelligibility than a rule-based MFCC-based speech synthesizer WORLD.
We evaluated the model based on a popular intrusive objective speech
intelligibility measure (STOI) and quality (NISQA score). Experimental results
show that our proposed system outperforms Librosa MFCC- inversion (by an
increase of about 26% up to 53% in STOI and 16% up to 78% in NISQA score) and a
rise of about 10% in intelligibility and about 4% in naturalness in comparison
with conventional rule-based vocoder WORLD that used in the CycleGAN-VC family.
However, WORLD needs additional data like F0. Finally, using perceptual loss in
discriminators based on STOI could improve the quality more. WebMUSHRA-based
subjective tests also show the quality of the proposed approach.


------------------------------------------------------------------------------

Title:
Solving the complete pseudo-impulsive radiation and diffraction problem  using a spectral element method

Abstract: This paper presents a novel, efficient, high-order accurate, and stable
spectral element-based model for computing the complete three-dimensional
linear radiation and diffraction problem for floating offshore structures. We
present a solution to a pseudo-impulsive formulation in the time domain, where
the frequency-dependent quantities, such as added mass, radiation damping, and
wave excitation force for arbitrary heading angle, $\beta$, are evaluated using
Fourier transforms from the tailored time-domain responses. The spatial domain
is tessellated by an unstructured high-order hybrid configured mesh and
represented by piece-wise polynomial basis functions in the spectral element
space. Fourth-order accurate time integration is employed through an explicit
four-stage Runge-Kutta method and complemented by fourth-order finite
difference approximations for time differentiation. To reduce the computational
burden, the model can make use of symmetry boundaries in the domain
representation. The key piece of the numerical model -- the discrete Laplace
solver -- is validated through $p$- and $h$-convergence studies. Moreover, to
highlight the capabilities of the proposed model, we present prof-of-concept
examples of simple floating bodies (a sphere and a box). Lastly, a much more
involved case is performed of an oscillating water column, including
generalized modes resembling the piston motion and wave sloshing effects inside
the wave energy converter chamber. In this case, the spectral element model
trivially computes the infinite-frequency added mass, which is a singular
problem for conventional boundary element type solvers.


------------------------------------------------------------------------------

Title:
Proof of reserves and non-double spends for Chaumian Mints

Abstract: E-cash was invented in 1982 by David Chaum as an anonymous cryptographic
electronic cash system based on blind signatures. It is not a decentralized
form of money as Bitcoin. It requires trust on the server or Mint issuing the
e-cash tokens and validating the transactions for preventing double spends.
Moreover, the users also need to trust the Mint to not debase the value of
e-cash tokens by Minting an uncontrolled number. In particular, this is
critical for e-cash tokens representing a note of another asset as a currency,
or bitcoin, or another cryptocurrency. Thus it would be suitable to implement a
public auditing system providing a proof of reserves that ensures that the Mint
is not engaging into a fractional reserve system. In this article we describe
how to implement a proof of reserves system for Chaumian Mints. The protocol
also provides a proof of non-double spends.


------------------------------------------------------------------------------

Title:
Relevance-Based Compression of Cataract Surgery Videos

Abstract: In the last decade, the need for storing videos from cataract surgery has
increased significantly. Hospitals continue to improve their imaging and
recording devices (e.g., microscopes and cameras used in microscopic surgery,
such as ophthalmology) to enhance their post-surgical processing efficiency.
The video recordings enable a lot of user-cases after the actual surgery, for
example, teaching, documentation, and forensics. However, videos recorded from
operations are typically stored in the internal archive without any
domain-specific compression, leading to a massive storage space consumption. In
this work, we propose a relevance-based compression scheme for videos from
cataract surgery, which is based on content specifics of particular cataract
surgery phases. We evaluate our compression scheme with three state-of-the-art
video codecs, namely H.264/AVC, H.265/HEVC, and AV1, and ask medical experts to
evaluate the visual quality of encoded videos. Our results show significant
savings, in particular up to 95.94% when using H.264/AVC, up to 98.71% when
using H.265/HEVC, and up to 98.82% when using AV1.


------------------------------------------------------------------------------

Title:
XAI-TRIS: Non-linear benchmarks to quantify ML explanation performance

Abstract: The field of 'explainable' artificial intelligence (XAI) has produced highly
cited methods that seek to make the decisions of complex machine learning (ML)
methods 'understandable' to humans, for example by attributing 'importance'
scores to input features. Yet, a lack of formal underpinning leaves it unclear
as to what conclusions can safely be drawn from the results of a given XAI
method and has also so far hindered the theoretical verification and empirical
validation of XAI methods. This means that challenging non-linear problems,
typically solved by deep neural networks, presently lack appropriate remedies.
Here, we craft benchmark datasets for three different non-linear classification
scenarios, in which the important class-conditional features are known by
design, serving as ground truth explanations. Using novel quantitative metrics,
we benchmark the explanation performance of a wide set of XAI methods across
three deep learning model architectures. We show that popular XAI methods are
often unable to significantly outperform random performance baselines and edge
detection methods. Moreover, we demonstrate that explanations derived from
different model architectures can be vastly different; thus, prone to
misinterpretation even under controlled conditions.


------------------------------------------------------------------------------

Title:
Pure Exploration in Bandits with Linear Constraints

Abstract: We address the problem of identifying the optimal policy with a fixed
confidence level in a multi-armed bandit setup, when \emph{the arms are subject
to linear constraints}. Unlike the standard best-arm identification problem
which is well studied, the optimal policy in this case may not be deterministic
and could mix between several arms. This changes the geometry of the problem
which we characterize via an information-theoretic lower bound. We introduce
two asymptotically optimal algorithms for this setting, one based on the
Track-and-Stop method and the other based on a game-theoretic approach. Both
these algorithms try to track an optimal allocation based on the lower bound
and computed by a weighted projection onto the boundary of a normal cone.
Finally, we provide empirical results that validate our bounds and visualize
how constraints change the hardness of the problem.


------------------------------------------------------------------------------

Title:
3D Reconstruction of Spherical Images based on Incremental Structure  from Motion

Abstract: 3D reconstruction plays an increasingly important role in modern
photogrammetric systems. Conventional satellite or aerial-based remote sensing
(RS) platforms can provide the necessary data sources for the 3D reconstruction
of large-scale landforms and cities. Even with low-altitude UAVs (Unmanned
Aerial Vehicles), 3D reconstruction in complicated situations, such as urban
canyons and indoor scenes, is challenging due to the frequent tracking failures
between camera frames and high data collection costs. Recently, spherical
images have been extensively exploited due to the capability of recording
surrounding environments from one camera exposure. Classical 3D reconstruction
pipelines, however, cannot be used for spherical images. Besides, there exist
few software packages for 3D reconstruction of spherical images. Based on the
imaging geometry of spherical cameras, this study investigates the algorithms
for the relative orientation using spherical correspondences, absolute
orientation using 3D correspondences between scene and spherical points, and
the cost functions for BA (bundle adjustment) optimization. In addition, an
incremental SfM (Structure from Motion) workflow has been proposed for
spherical images using the above-mentioned algorithms. The proposed solution is
finally verified by using three spherical datasets captured by both
consumer-grade and professional spherical cameras. The results demonstrate that
the proposed SfM workflow can achieve the successful 3D reconstruction of
complex scenes and provide useful clues for the implementation in open-source
software packages. The source code of the designed SfM workflow would be made
publicly available.


------------------------------------------------------------------------------

Title:
Mapping and Optimizing Communication in ROS 2-based Applications on  Configurable System-on-Chip Platforms

Abstract: The robot operating system is the de-facto standard for designing and
implementing robotics applications. Several previous works deal with the
integration of heterogeneous accelerators into ROS-based applications. One of
these approaches is ReconROS, which enables nodes to be completely mapped to
hardware. The follow-up work fpgaDDS extends ReconROS by an intra-FPGA data
distribution service to process topic-based communication between nodes
entirely in hardware. However, the application of this approach is strictly
limited to communication between nodes implemented in hardware only. This paper
introduces gateways to close the gap between topic communication in hardware
and software. Gateways aim to reduce data transfers between hardware and
software by synchronizing a hardware-and software-mapped topic. As a result,
data must be transferred only once compared to a separate data transmission for
each subscribing hardware node in the baseline. Our measurements show
significant speedups in multi-subscriber scenarios with large message sizes.
From the conclusions of these measurements, we present a methodology for the
communication mapping of ROS 2 computation graphs. In the evaluation, an
autonomous driving real-world example benefits from the gateway and achieves a
speedup of 1.4.


------------------------------------------------------------------------------

Title:
Concept-aware clustering for decentralized deep learning under temporal  shift

Abstract: Decentralized deep learning requires dealing with non-iid data across
clients, which may also change over time due to temporal shifts. While non-iid
data has been extensively studied in distributed settings, temporal shifts have
received no attention. To the best of our knowledge, we are first with tackling
the novel and challenging problem of decentralized learning with non-iid and
dynamic data. We propose a novel algorithm that can automatically discover and
adapt to the evolving concepts in the network, without any prior knowledge or
estimation of the number of concepts. We evaluate our algorithm on standard
benchmark datasets and demonstrate that it outperforms previous methods for
decentralized learning.


------------------------------------------------------------------------------

Title:
Blended-NeRF: Zero-Shot Object Generation and Blending in Existing  Neural Radiance Fields

Abstract: Editing a local region or a specific object in a 3D scene represented by a
NeRF is challenging, mainly due to the implicit nature of the scene
representation. Consistently blending a new realistic object into the scene
adds an additional level of difficulty. We present Blended-NeRF, a robust and
flexible framework for editing a specific region of interest in an existing
NeRF scene, based on text prompts or image patches, along with a 3D ROI box.
Our method leverages a pretrained language-image model to steer the synthesis
towards a user-provided text prompt or image patch, along with a 3D MLP model
initialized on an existing NeRF scene to generate the object and blend it into
a specified region in the original scene. We allow local editing by localizing
a 3D ROI box in the input scene, and seamlessly blend the content synthesized
inside the ROI with the existing scene using a novel volumetric blending
technique. To obtain natural looking and view-consistent results, we leverage
existing and new geometric priors and 3D augmentations for improving the visual
fidelity of the final result.
We test our framework both qualitatively and quantitatively on a variety of
real 3D scenes and text prompts, demonstrating realistic multi-view consistent
results with much flexibility and diversity compared to the baselines. Finally,
we show the applicability of our framework for several 3D editing applications,
including adding new objects to a scene, removing/replacing/altering existing
objects, and texture conversion.


------------------------------------------------------------------------------

Title:
Accelerated Training via Incrementally Growing Neural Networks using  Variance Transfer and Learning Rate Adaptation

Abstract: We develop an approach to efficiently grow neural networks, within which
parameterization and optimization strategies are designed by considering their
effects on the training dynamics. Unlike existing growing methods, which follow
simple replication heuristics or utilize auxiliary gradient-based local
optimization, we craft a parameterization scheme which dynamically stabilizes
weight, activation, and gradient scaling as the architecture evolves, and
maintains the inference functionality of the network. To address the
optimization difficulty resulting from imbalanced training effort distributed
to subnetworks fading in at different growth phases, we propose a learning rate
adaption mechanism that rebalances the gradient contribution of these separate
subcomponents. Experimental results show that our method achieves comparable or
better accuracy than training large fixed-size models, while saving a
substantial portion of the original computation budget for training. We
demonstrate that these gains translate into real wall-clock training speedups.


------------------------------------------------------------------------------

Title:
Ladder Fine-tuning approach for SAM integrating complementary network

Abstract: Recently, foundation models have been introduced demonstrating various tasks
in the field of computer vision. These models such as Segment Anything Model
(SAM) are generalized models trained using huge datasets. Currently, ongoing
research focuses on exploring the effective utilization of these generalized
models for specific domains, such as medical imaging. However, in medical
imaging, the lack of training samples due to privacy concerns and other factors
presents a major challenge for applying these generalized models to medical
image segmentation task. To address this issue, the effective fine tuning of
these models is crucial to ensure their optimal utilization. In this study, we
propose to combine a complementary Convolutional Neural Network (CNN) along
with the standard SAM network for medical image segmentation. To reduce the
burden of fine tuning large foundation model and implement cost-efficient
trainnig scheme, we focus only on fine-tuning the additional CNN network and
SAM decoder part. This strategy significantly reduces trainnig time and
achieves competitive results on publicly available dataset. The code is
available at this https URL


------------------------------------------------------------------------------

Title:
Toward Leveraging Pre-Trained Self-Supervised Frontends for Automatic  Singing Voice Understanding Tasks: Three Case Studies

Abstract: Automatic singing voice understanding tasks, such as singer identification,
singing voice transcription, and singing technique classification, benefit from
data-driven approaches that utilize deep learning techniques. These approaches
work well even under the rich diversity of vocal and noisy samples owing to
their representation ability. However, the limited availability of labeled data
remains a significant obstacle to achieving satisfactory performance. In recent
years, self-supervised learning models (SSL models) have been trained using
large amounts of unlabeled data in the field of speech processing and music
classification. By fine-tuning these models for the target tasks, comparable
performance to conventional supervised learning can be achieved with limited
training data. Therefore, in this paper, we investigate the effectiveness of
SSL models for various singing voice recognition tasks. We report the results
of experiments comparing SSL models for three different tasks (i.e., singer
identification, singing voice transcription, and singing technique
classification) as initial exploration and aim to discuss these findings.
Experimental results show that each SSL model achieves comparable performance
and sometimes outperforms compared to state-of-the-art methods on each task. We
also conducted a layer-wise analysis to further understand the behavior of the
SSL models.


------------------------------------------------------------------------------

Title:
DiffWA: Diffusion Models for Watermark Attack

Abstract: With the rapid development of deep neural networks(DNNs), many robust blind
watermarking algorithms and frameworks have been proposed and achieved good
results. At present, the watermark attack algorithm can not compete with the
watermark addition algorithm. And many watermark attack algorithms only care
about interfering with the normal extraction of the watermark, and the
watermark attack will cause great visual loss to the image. To this end, we
propose DiffWA, a conditional diffusion model with distance guidance for
watermark attack, which can restore the image while removing the embedded
watermark. The core of our method is training an image-to-image conditional
diffusion model on unwatermarked images and guiding the conditional model using
a distance guidance when sampling so that the model will generate unwatermarked
images which is similar to original images. We conducted experiments on
CIFAR-10 using our proposed models. The results shows that the model can remove
the watermark with good effect and make the bit error rate of watermark
extraction higher than 0.4. At the same time, the attacked image will maintain
good visual effect with PSNR more than 31 and SSIM more than 0.97 compared with
the original image.


------------------------------------------------------------------------------

Title:
One at A Time: Multi-step Volumetric Probability Distribution Diffusion  for Depth Estimation

Abstract: Recent works have explored the fundamental role of depth estimation in
multi-view stereo (MVS) and semantic scene completion (SSC). They generally
construct 3D cost volumes to explore geometric correspondence in depth, and
estimate such volumes in a single step relying directly on the ground truth
approximation. However, such problem cannot be thoroughly handled in one step
due to complex empirical distributions, especially in challenging regions like
occlusions, reflections, etc. In this paper, we formulate the depth estimation
task as a multi-step distribution approximation process, and introduce a new
paradigm of modeling the Volumetric Probability Distribution progressively
(step-by-step) following a Markov chain with Diffusion models (VPDD).
Specifically, to constrain the multi-step generation of volume in VPDD, we
construct a meta volume guidance and a confidence-aware contextual guidance as
conditional geometry priors to facilitate the distribution approximation. For
the sampling process, we further investigate an online filtering strategy to
maintain consistency in volume representations for stable training. Experiments
demonstrate that our plug-and-play VPDD outperforms the state-of-the-arts for
tasks of MVS and SSC, and can also be easily extended to different baselines to
get improvement. It is worth mentioning that we are the first camera-based work
that surpasses LiDAR-based methods on the SemanticKITTI dataset.


------------------------------------------------------------------------------

Title:
Improved Solutions for Multidimensional Approximate Agreement via  Centroid Computation

Abstract: In this paper, we present distributed fault-tolerant algorithms that
approximate the centroid of a set of n data points in $\mathbb{R}^d$. Our work
falls into the broader area of approximate multidimensional Byzantine
agreement. The standard approach used in existing algorithms is to agree on a
vector inside the convex hull of all correct vectors. This strategy dismisses
many possibly correct data points. As a result, the algorithm does not
necessarily agree on a representative value. In fact, this does not allow us to
compute a better approximation than $2d$ of the centroid in the synchronous
case.
To find better approximation algorithms for the centroid, we investigate the
trade-off between the quality of the approximation, the resilience of the
algorithm, and the validity of the solution. For the synchronous case, we show
that it is possible to achieve a $1$-approximation of the centroid with up to
$t<n/(d+1)$ Byzantine data points. This approach however does not give any
guarantee on the validity of the solution. Therefore, we develop a second
approach that reaches a $2\sqrt{d}$-approximation of the centroid, while
satisfying the standard validity condition for agreement protocols. We are even
able to restrict the validity condition to agreement inside the box of correct
data points, while achieving optimal resilience of $t< n/3$. For the
asynchronous case, we can adapt all three algorithms to reach the same
approximation results (up to a constant factor). Our results suggest that it is
reasonable to study the trade-off between validity conditions and the quality
of the solution.


------------------------------------------------------------------------------

Title:
Radial polynomials as alternatives to smooth radial basis functions and  their applications

Abstract: Because of the high approximation power and simplicity of computation of
smooth radial basis functions (RBFs), in recent decades they have received much
attention for function approximation. These RBFs contain a shape parameter that
regulates the relation between their accuracy and stability. A difficulty in
approximation via smooth RBFs is optimal selection of shape parameter. The aim
of this paper is to introduce an alternative for smooth RBFs, which in addition
to overcoming this difficulty, its approximation power is almost equal to
RBFs....


------------------------------------------------------------------------------

Title:
FlowFace++: Explicit Semantic Flow-supervised End-to-End Face Swapping

Abstract: This work proposes a novel face-swapping framework FlowFace++, utilizing
explicit semantic flow supervision and end-to-end architecture to facilitate
shape-aware face-swapping. Specifically, our work pretrains a facial shape
discriminator to supervise the face swapping network. The discriminator is
shape-aware and relies on a semantic flow-guided operation to explicitly
calculate the shape discrepancies between the target and source faces, thus
optimizing the face swapping network to generate highly realistic results. The
face swapping network is a stack of a pre-trained face-masked autoencoder
(MAE), a cross-attention fusion module, and a convolutional decoder. The MAE
provides a fine-grained facial image representation space, which is unified for
the target and source faces and thus facilitates final realistic results. The
cross-attention fusion module carries out the source-to-target face swapping in
a fine-grained latent space while preserving other attributes of the target
image (e.g. expression, head pose, hair, background, illumination, etc).
Lastly, the convolutional decoder further synthesizes the swapping results
according to the face-swapping latent embedding from the cross-attention fusion
module. Extensive quantitative and qualitative experiments on in-the-wild faces
demonstrate that our FlowFace++ outperforms the state-of-the-art significantly,
particularly while the source face is obstructed by uneven lighting or angle
offset.


------------------------------------------------------------------------------

Title:
A Search Strategy and Vessel Detection in Maritime Environment Using  Fixed-Wing UAVs

Abstract: In this paper, we address the problem of autonomous search and vessel
detection in an unknown GNSS-denied maritime environment with fixed-wing UAVs.
The main challenge in such environments with limited localization,
communication range, and the total number of UAVs and sensors is to implement
an appropriate search strategy so that a target vessel can be detected as soon
as possible. Thus we present informed and non-informed methods used to search
the environment. The informed method relies on an obtained probabilistic map,
while the non-informed method navigates the UAVs along predefined paths
computed with respect to the environment. The vessel detection method is
trained on synthetic data collected in the simulator with data annotation
tools. Comparative experiments in simulation have shown that our combination of
sensors, search methods and a vessel detection algorithm leads to a successful
search for the target vessel in such challenging environments.


------------------------------------------------------------------------------

Title:
Mapping and Cleaning Open Commonsense Knowledge Bases with Generative  Translation

Abstract: Structured knowledge bases (KBs) are the backbone of many
know\-ledge-intensive applications, and their automated construction has
received considerable attention. In particular, open information extraction
(OpenIE) is often used to induce structure from a text. However, although it
allows high recall, the extracted knowledge tends to inherit noise from the
sources and the OpenIE algorithm. Besides, OpenIE tuples contain an open-ended,
non-canonicalized set of relations, making the extracted knowledge's downstream
exploitation harder. In this paper, we study the problem of mapping an open KB
into the fixed schema of an existing KB, specifically for the case of
commonsense knowledge. We propose approaching the problem by generative
translation, i.e., by training a language model to generate fixed-schema
assertions from open ones. Experiments show that this approach occupies a sweet
spot between traditional manual, rule-based, or classification-based
canonicalization and purely generative KB construction like COMET. Moreover, it
produces higher mapping accuracy than the former while avoiding the
association-based noise of the latter.


------------------------------------------------------------------------------

Title:
MySemCloud: Semantic-aware Word Cloud Editing

Abstract: Word clouds are a popular text visualization technique that summarize an
input text by displaying its most important words in a compact image. The
traditional layout methods do not take proximity effects between words into
account; this has been improved in semantic word clouds, where relative word
placement is controlled by edges in a word similarity graph. We introduce
MySemCloud, a new human-in-the-loop tool to visualize and edit semantic word
clouds. MySemCloud lets users perform computer-assisted local moves of words,
which improve or at least retain the semantic quality. To achieve this, we
construct a word similarity graph on which a system of forces is applied to
generate a compact initial layout with good semantic quality. The force system
also allows us to maintain these attributes after each user interaction, as
well as preserve the user's mental map. The tool provides algorithmic support
for the editing operations to help the user enhance the semantic quality of the
visualization, while adjusting it to their personal preference. We show that
MySemCloud provides high user satisfaction as well as permits users to create
layouts of higher quality than state-of-the-art semantic word cloud generation
tools.


------------------------------------------------------------------------------

Title:
On the rate of convergence of Yosida approximation for rhe nonlocal  Cahn-Hilliard equation

Abstract: It is well-known that one can construct solutions to the nonlocal
Cahn-Hilliard equation with singular potentials via Yosida approximation with
parameter $\lambda \to 0$. The usual method is based on compactness arguments
and does not provide any rate of convergence. Here, we fill the gap and we
obtain an explicit convergence rate $\sqrt{\lambda}$. The proof is based on the
theory of maximal monotone operators and an observation that the nonlocal
operator is of Hilbert-Schmidt type. Our estimate can provide convergence
result for the Galerkin methods where the parameter $\lambda$ could be linked
to the discretization parameters, yielding appropriate error estimates.


------------------------------------------------------------------------------

Title:
Vec2Vec: A Compact Neural Network Approach for Transforming Text  Embeddings with High Fidelity

Abstract: Vector embeddings have become ubiquitous tools for many language-related
tasks. A leading embedding model is OpenAI's text-ada-002 which can embed
approximately 6,000 words into a 1,536-dimensional vector. While powerful,
text-ada-002 is not open source and is only available via API. We trained a
simple neural network to convert open-source 768-dimensional MPNet embeddings
into text-ada-002 embeddings. We compiled a subset of 50,000 online food
reviews. We calculated MPNet and text-ada-002 embeddings for each review and
trained a simple neural network to for 75 epochs. The neural network was
designed to predict the corresponding text-ada-002 embedding for a given MPNET
embedding. Our model achieved an average cosine similarity of 0.932 on 10,000
unseen reviews in our held-out test dataset. We manually assessed the quality
of our predicted embeddings for vector search over text-ada-002-embedded
reviews. While not as good as real text-ada-002 embeddings, predicted
embeddings were able to retrieve highly relevant reviews. Our final model,
Vec2Vec, is lightweight (<80 MB) and fast. Future steps include training a
neural network with a more sophisticated architecture and a larger dataset of
paired embeddings to achieve greater performance. The ability to convert
between and align embedding spaces may be helpful for interoperability,
limiting dependence on proprietary models, protecting data privacy, reducing
costs, and offline operations.


------------------------------------------------------------------------------

Title:
PEBO-SLAM: Observer design for visual inertial SLAM with convergence  guarantees

Abstract: This paper introduces a new linear parameterization to the problem of visual
inertial simultaneous localization and mapping (VI-SLAM) -- without any
approximation -- for the case only using information from a single monocular
camera and an inertial measurement unit. In this problem set, the system state
evolves on the nonlinear manifold $SE(3)\times \mathbb{R}^{3n}$, on which we
design dynamic extensions carefully to generate invariant foliations, such that
the problem can be reformulated into online \emph{constant parameter}
identification, then interestingly with linear regression models obtained. It
demonstrates that VI-SLAM can be translated into a linear least squares
problem, in the deterministic sense, \emph{globally} and \emph{exactly}. Based
on this observation, we propose a novel SLAM observer, following the recently
established parameter estimation-based observer (PEBO) methodology. A notable
merit is that the proposed observer enjoys almost global asymptotic stability,
requiring neither persistency of excitation nor uniform complete observability,
which, however, are widely adopted in most existing works with provable
stability but can hardly be assured in many practical scenarios.


------------------------------------------------------------------------------

Title:
Exploring the Range of Possible Outcomes by means of Logical Scenario  Analysis and Reduction for Testing Automated Driving Systems

Abstract: With the implementation of the new EU regulation 2022/1426 regarding the
type-approval of the automated driving system (ADS) of fully automated
vehicles, scenario-based testing has gained significant importance in
evaluating the performance and safety of advanced driver assistance systems and
automated driving systems. However, the exploration and generation of concrete
scenarios from a single logical scenario can often lead to a number of similar
or redundant scenarios, which may not contribute to the testing goals.
This paper focuses on the the goal to reduce the scenario set by clustering
concrete scenarios from a single logical scenario. By employing clustering
techniques, redundant and uninteresting scenarios can be identified and
eliminated, resulting in a representative scenario set. This reduction allows
for a more focused and efficient testing process, enabling the allocation of
resources to the most relevant and critical scenarios. Furthermore, the
identified clusters can provide valuable insights into the scenario space,
revealing patterns and potential problems with the system's behavior.


------------------------------------------------------------------------------

Title:
An entropy stable discontinuous Galerkin method for the two-layer  shallow water equations on curvilinear meshes

Abstract: We present an entropy stable nodal discontinuous Galerkin spectral element
method (DGSEM) for the two-layer shallow water equations on two dimensional
curvilinear meshes. We mimic the continuous entropy analysis on the
semi-discrete level with the DGSEM constructed on Legendre-Gauss-Lobatto (LGL)
nodes. The use of LGL nodes endows the collocated nodal DGSEM with the
summation-by-parts property that is key in the discrete analysis. The
approximation exploits an equivalent flux differencing formulation for the
volume contributions, which generate an entropy conservative split-form of the
governing equations. A specific combination of an entropy conservative
numerical surface flux and discretization of the nonconservative terms is then
applied to obtain a high-order path-conservative scheme that is entropy
conservative and has the well-balanced property for discontinuous bathymetry.
Dissipation is added at the interfaces to create an entropy stable
approximation that satisfies the second law of thermodynamics in the discrete
case. We conclude with verification of the theoretical findings through
numerical tests and demonstrate results about convergence, entropy stability
and well-balancedness of the scheme.


------------------------------------------------------------------------------

Title:
Targeted collapse regularized autoencoder for anomaly detection: black  hole at the center

Abstract: Autoencoders have been extensively used in the development of recent anomaly
detection techniques. The premise of their application is based on the notion
that after training the autoencoder on normal training data, anomalous inputs
will exhibit a significant reconstruction error. Consequently, this enables a
clear differentiation between normal and anomalous samples. In practice,
however, it is observed that autoencoders can generalize beyond the normal
class and achieve a small reconstruction error on some of the anomalous
samples. To improve the performance, various techniques propose additional
components and more sophisticated training procedures. In this work, we propose
a remarkably straightforward alternative: instead of adding neural network
components, involved computations, and cumbersome training, we complement the
reconstruction loss with a computationally light term that regulates the norm
of representations in the latent space. The simplicity of our approach
minimizes the requirement for hyperparameter tuning and customization for new
applications which, paired with its permissive data modality constraint,
enhances the potential for successful adoption across a broad range of
applications. We test the method on various visual and tabular benchmarks and
demonstrate that the technique matches and frequently outperforms alternatives.
We also provide a theoretical analysis and numerical simulations that help
demonstrate the underlying process that unfolds during training and how it can
help with anomaly detection. This mitigates the black-box nature of
autoencoder-based anomaly detection algorithms and offers an avenue for further
investigation of advantages, fail cases, and potential new directions.


------------------------------------------------------------------------------

Title:
Robust Recovery Motion Control for Quadrupedal Robots via Learned  Terrain Imagination

Abstract: Quadrupedal robots have emerged as a cutting-edge platform for assisting
humans, finding applications in tasks related to inspection and exploration in
remote areas. Nevertheless, their floating base structure renders them
susceptible to fall in cluttered environments, where manual recovery by a human
operator may not always be feasible. Several recent studies have presented
recovery controllers employing deep reinforcement learning algorithms. However,
these controllers are not specifically designed to operate effectively in
cluttered environments, such as stairs and slopes, which restricts their
applicability. In this study, we propose a robust all-terrain recovery policy
to facilitate rapid and secure recovery in cluttered environments. We
substantiate the superiority of our proposed approach through simulations and
real-world tests encompassing various terrain types.


------------------------------------------------------------------------------

Title:
Natural Language Generation for Advertising: A Survey

Abstract: Natural language generation methods have emerged as effective tools to help
advertisers increase the number of online advertisements they produce. This
survey entails a review of the research trends on this topic over the past
decade, from template-based to extractive and abstractive approaches using
neural networks. Additionally, key challenges and directions revealed through
the survey, including metric optimization, faithfulness, diversity,
multimodality, and the development of benchmark datasets, are discussed.


------------------------------------------------------------------------------

Title:
Accelerating SNN Training with Stochastic Parallelizable Spiking Neurons

Abstract: Spiking neural networks (SNN) are able to learn spatiotemporal features while
using less energy, especially on neuromorphic hardware. The most widely used
spiking neuron in deep learning is the Leaky Integrate and Fire (LIF) neuron.
LIF neurons operate sequentially, however, since the computation of state at
time t relies on the state at time t-1 being computed. This limitation is
shared with Recurrent Neural Networks (RNN) and results in slow training on
Graphics Processing Units (GPU). In this paper, we propose the Stochastic
Parallelizable Spiking Neuron (SPSN) to overcome the sequential training
limitation of LIF neurons. By separating the linear integration component from
the non-linear spiking function, SPSN can be run in parallel over time. The
proposed approach results in performance comparable with the state-of-the-art
for feedforward neural networks on the Spiking Heidelberg Digits (SHD) dataset,
outperforming LIF networks while training 10 times faster and outperforming
non-spiking networks with the same network architecture. For longer input
sequences of 10000 time-steps, we show that the proposed approach results in
4000 times faster training, thus demonstrating the potential of the proposed
approach to accelerate SNN training for very large datasets.


------------------------------------------------------------------------------

Title:
Explainable Representations for Relation Prediction in Knowledge Graphs

Abstract: Knowledge graphs represent real-world entities and their relations in a
semantically-rich structure supported by ontologies. Exploring this data with
machine learning methods often relies on knowledge graph embeddings, which
produce latent representations of entities that preserve structural and local
graph neighbourhood properties, but sacrifice explainability. However, in tasks
such as link or relation prediction, understanding which specific features
better explain a relation is crucial to support complex or critical
applications.
We propose SEEK, a novel approach for explainable representations to support
relation prediction in knowledge graphs. It is based on identifying relevant
shared semantic aspects (i.e., subgraphs) between entities and learning
representations for each subgraph, producing a multi-faceted and explainable
representation.
We evaluate SEEK on two real-world highly complex relation prediction tasks:
protein-protein interaction prediction and gene-disease association prediction.
Our extensive analysis using established benchmarks demonstrates that SEEK
achieves significantly better performance than standard learning representation
methods while identifying both sufficient and necessary explanations based on
shared semantic aspects.


------------------------------------------------------------------------------

Title:
Outlier-robust Estimation of a Sparse Linear Model Using Invexity

Abstract: In this paper, we study problem of estimating a sparse regression vector with
correct support in the presence of outlier samples. The inconsistency of
lasso-type methods is well known in this scenario. We propose a combinatorial
version of outlier-robust lasso which also identifies clean samples.
Subsequently, we use these clean samples to make a good estimation. We also
provide a novel invex relaxation for the combinatorial problem and provide
provable theoretical guarantees for this relaxation. Finally, we conduct
experiments to validate our theory and compare our results against standard
lasso.


------------------------------------------------------------------------------

Title:
Analysis of divergence-preserving unfitted finite element methods for  the mixed Poisson problem

Abstract: In this paper we present a new H(div)-conforming unfitted finite element
method for the mixed Poisson problem which is robust in the cut configuration
and preserves conservation properties of body-fitted finite element methods.
The key is to formulate the divergence-constraint on the active mesh, instead
of the physical domain, in order to obtain robustness with respect to cut
configurations without the need for a stabilization that pollutes the mass
balance. This change in the formulation results in a slight inconsistency, but
does not affect the accuracy of the flux variable. By applying post-processings
for the scalar variable, in virtue of classical local post-processings in
body-fitted methods, we retain optimal convergence rates for both variables and
even the superconvergence after post-processing of the scalar variable. We
present the method and perform a rigorous a-priori error analysis of the method
and discuss several variants and extensions. Numerical experiments confirm the
theoretical results.


------------------------------------------------------------------------------

Title:
MP3: Movement Primitive-Based (Re-)Planning Policy

Abstract: We introduce a novel deep reinforcement learning (RL) approach called
Movement Prmitive-based Planning Policy (MP3). By integrating movement
primitives (MPs) into the deep RL framework, MP3 enables the generation of
smooth trajectories throughout the whole learning process while effectively
learning from sparse and non-Markovian rewards. Additionally, MP3 maintains the
capability to adapt to changes in the environment during execution. Although
many early successes in robot RL have been achieved by combining RL with MPs,
these approaches are often limited to learning single stroke-based motions,
lacking the ability to adapt to task variations or adjust motions during
execution. Building upon our previous work, which introduced an episode-based
RL method for the non-linear adaptation of MP parameters to different task
variations, this paper extends the approach to incorporating replanning
strategies. This allows adaptation of the MP parameters throughout motion
execution, addressing the lack of online motion adaptation in stochastic
domains requiring feedback. We compared our approach against state-of-the-art
deep RL and RL with MPs methods. The results demonstrated improved performance
in sophisticated, sparse reward settings and in domains requiring replanning.


------------------------------------------------------------------------------

Title:
On the Construction of Near-MDS Matrices

Abstract: The optimal branch number of MDS matrices makes them a preferred choice for
designing diffusion layers in many block ciphers and hash functions. However,
in lightweight cryptography, Near-MDS (NMDS) matrices with sub-optimal branch
numbers offer a better balance between security and efficiency as a diffusion
layer, compared to MDS matrices. In this paper, we study NMDS matrices,
exploring their construction in both recursive and nonrecursive settings. We
provide several theoretical results and explore the hardware efficiency of the
construction of NMDS matrices. Additionally, we make comparisons between the
results of NMDS and MDS matrices whenever possible. For the recursive approach,
we study the DLS matrices and provide some theoretical results on their use.
Some of the results are used to restrict the search space of the DLS matrices.
We also show that over a field of characteristic 2, any sparse matrix of order
$n\geq 4$ with fixed XOR value of 1 cannot be an NMDS when raised to a power of
$k\leq n$. Following that, we use the generalized DLS (GDLS) matrices to
provide some lightweight recursive NMDS matrices of several orders that perform
better than the existing matrices in terms of hardware cost or the number of
iterations. For the nonrecursive construction of NMDS matrices, we study
various structures, such as circulant and left-circulant matrices, and their
generalizations: Toeplitz and Hankel matrices. In addition, we prove that
Toeplitz matrices of order $n>4$ cannot be simultaneously NMDS and involutory
over a field of characteristic 2. Finally, we use GDLS matrices to provide some
lightweight NMDS matrices that can be computed in one clock cycle. The proposed
nonrecursive NMDS matrices of orders 4, 5, 6, 7, and 8 can be implemented with
24, 50, 65, 96, and 108 XORs over $\mathbb{F}_{2^4}$, respectively.


------------------------------------------------------------------------------

Title:
CEMSSL: A Unified Framework for Multi-Solution Inverse Kinematic Model  Learning of Robot Arms with High-Precision Manipulation

Abstract: Multiple solutions mainly originate from the existence of redundant degrees
of freedom in the robot arm, which may cause difficulties in inverse model
learning but they can also bring many benefits, such as higher flexibility and
robustness. Current multi-solution inverse model learning methods rely on
conditional deep generative models, yet they often fail to achieve sufficient
precision when learning multiple solutions. In this paper, we propose
Conditional Embodied Self-Supervised Learning (CEMSSL) for robot arm
multi-solution inverse model learning, and present a unified framework for
high-precision multi-solution inverse model learning that is applicable to
other conditional deep generative models. Our experimental results demonstrate
that our framework can achieve a significant improvement in precision (up to 2
orders of magnitude) while preserving the properties of the original method.
The related code will be available soon.


------------------------------------------------------------------------------

Title:
Class-Incremental Learning based on Label Generation

Abstract: Despite the great success of pre-trained language models, it is still a
challenge to use these models for continual learning, especially for the
class-incremental learning (CIL) setting due to catastrophic forgetting (CF).
This paper reports our finding that if we formulate CIL as a continual label
generation problem, CF is drastically reduced and the generalizable
representations of pre-trained models can be better retained. We thus propose a
new CIL method (VAG) that also leverages the sparsity of vocabulary to focus
the generation and creates pseudo-replay samples by using label semantics.
Experimental results show that VAG outperforms baselines by a large margin.


------------------------------------------------------------------------------

Title:
TaCA: Upgrading Your Visual Foundation Model with Task-agnostic  Compatible Adapter

Abstract: Visual foundation models like CLIP excel in learning feature representations
from extensive datasets through self-supervised methods, demonstrating
remarkable transfer learning and generalization capabilities. A growing number
of applications based on visual foundation models are emerging, including
innovative solutions such as BLIP-2. These applications employ pre-trained CLIP
models as upstream feature extractors and train various downstream modules to
accomplish diverse tasks. In situations involving system upgrades that require
updating the upstream foundation model, it becomes essential to re-train all
downstream modules to adapt to the new foundation model, which is inflexible
and inefficient. In this paper, we introduce a parameter-efficient and
task-agnostic adapter, dubbed TaCA, that facilitates compatibility across
distinct foundation models while ensuring enhanced performance for the new
models. TaCA allows downstream applications to seamlessly integrate
better-performing foundation models without necessitating retraining. We
conduct extensive experimental validation of TaCA using different scales of
models with up to one billion parameters on various tasks such as video-text
retrieval, video recognition, and visual question answering. The results
consistently demonstrate the emergent ability of TaCA on hot-plugging upgrades
for visual foundation models. Codes and models will be available at
this https URL


------------------------------------------------------------------------------

Title:
Learnability and Algorithm for Continual Learning

Abstract: This paper studies the challenging continual learning (CL) setting of Class
Incremental Learning (CIL). CIL learns a sequence of tasks consisting of
disjoint sets of concepts or classes. At any time, a single model is built that
can be applied to predict/classify test instances of any classes learned thus
far without providing any task related information for each test instance.
Although many techniques have been proposed for CIL, they are mostly empirical.
It has been shown recently that a strong CIL system needs a strong within-task
prediction (WP) and a strong out-of-distribution (OOD) detection for each task.
However, it is still not known whether CIL is actually learnable. This paper
shows that CIL is learnable. Based on the theory, a new CIL algorithm is also
proposed. Experimental results demonstrate its effectiveness.


------------------------------------------------------------------------------

Title:
DGC-GNN: Descriptor-free Geometric-Color Graph Neural Network for 2D-3D  Matching

Abstract: Direct matching of 2D keypoints in an input image to a 3D point cloud of the
scene without requiring visual descriptors has garnered increased interest due
to its lower memory requirements, inherent privacy preservation, and reduced
need for expensive 3D model maintenance compared to visual descriptor-based
methods. However, existing algorithms often compromise on performance,
resulting in a significant deterioration compared to their descriptor-based
counterparts. In this paper, we introduce DGC-GNN, a novel algorithm that
employs a global-to-local Graph Neural Network (GNN) that progressively
exploits geometric and color cues to represent keypoints, thereby improving
matching robustness. Our global-to-local procedure encodes both Euclidean and
angular relations at a coarse level, forming the geometric embedding to guide
the local point matching. We evaluate DGC-GNN on both indoor and outdoor
datasets, demonstrating that it not only doubles the accuracy of the
state-of-the-art descriptor-free algorithm but, also, substantially narrows the
performance gap between descriptor-based and descriptor-free methods. The code
and trained models will be made publicly available.


------------------------------------------------------------------------------

Title:
Novelty Accommodating Multi-Agent Planning in High Fidelity Simulated  Open World

Abstract: Autonomous agents acting in real-world environments often need to reason with
unknown novelties interfering with their plan execution. Novelty is an
unexpected phenomenon that can alter the core characteristics, composition, and
dynamics of the environment. Novelty can occur at any time in any sufficiently
complex environment without any prior notice or explanation. Previous studies
show that novelty has catastrophic impact on agent performance. Intelligent
agents reason with an internal model of the world to understand the intricacies
of their environment and to successfully execute their plans. The introduction
of novelty into the environment usually renders their internal model inaccurate
and the generated plans no longer applicable. Novelty is particularly prevalent
in the real world where domain-specific and even predicted novelty-specific
approaches are used to mitigate the novelty's impact. In this work, we
demonstrate that a domain-independent AI agent designed to detect,
characterize, and accommodate novelty in smaller-scope physics-based games such
as Angry Birds and Cartpole can be adapted to successfully perform and reason
with novelty in realistic high-fidelity simulator of the military domain.


------------------------------------------------------------------------------

Title:
Slimmable Encoders for Flexible Split DNNs in Bandwidth and Resource  Constrained IoT Systems

Abstract: The execution of large deep neural networks (DNN) at mobile edge devices
requires considerable consumption of critical resources, such as energy, while
imposing demands on hardware capabilities. In approaches based on edge
computing the execution of the models is offloaded to a compute-capable device
positioned at the edge of 5G infrastructures. The main issue of the latter
class of approaches is the need to transport information-rich signals over
wireless links with limited and time-varying capacity. The recent split
computing paradigm attempts to resolve this impasse by distributing the
execution of DNN models across the layers of the systems to reduce the amount
of data to be transmitted while imposing minimal computing load on mobile
devices. In this context, we propose a novel split computing approach based on
slimmable ensemble encoders. The key advantage of our design is the ability to
adapt computational load and transmitted data size in real-time with minimal
overhead and time. This is in contrast with existing approaches, where the same
adaptation requires costly context switching and model loading. Moreover, our
model outperforms existing solutions in terms of compression efficacy and
execution time, especially in the context of weak mobile devices. We present a
comprehensive comparison with the most advanced split computing solutions, as
well as an experimental evaluation on GPU-less devices.


------------------------------------------------------------------------------

Title:
Swarm of One: Bottom-up Emergence of Stable Robot Bodies from Identical  Cells

Abstract: Unlike most human-engineered systems, biological systems are emergent from
low-level interactions, allowing much broader diversity and superior adaptation
to the complex environments. Inspired by the process of morphogenesis in
nature, a bottom-up design approach for robot morphology is proposed to treat a
robot's body as an emergent response to underlying processes rather than a
predefined shape. This paper presents Loopy, a "Swarm-of-One" polymorphic robot
testbed that can be viewed simultaneously as a robotic swarm and a single
robot. Loopy's shape is determined jointly by self-organization and
morphological computing using physically linked homogeneous cells. Experimental
results show that Loopy can form symmetric shapes consisting of lobes. Using
the the same set of parameters, even small amounts of initial noise can change
the number of lobes formed. However, once in a stable configuration, Loopy has
an "inertia" to transfiguring in response to dynamic parameters. By making the
connections among self-organization, morphological computing, and robot design,
this paper lays the foundation for more adaptable robot designs in the future.


------------------------------------------------------------------------------

Title:
Curriculum Knowledge Switching for Pancreas Segmentation

Abstract: Pancreas segmentation is challenging due to the small proportion and highly
changeable anatomical structure. It motivates us to propose a novel
segmentation framework, namely Curriculum Knowledge Switching (CKS) framework,
which decomposes detecting pancreas into three phases with different difficulty
extent: straightforward, difficult, and challenging. The framework switches
from straightforward to challenging phases and thereby gradually learns to
detect pancreas. In addition, we adopt the momentum update parameter updating
mechanism during switching, ensuring the loss converges gradually when the
input dataset changes. Experimental results show that different neural network
backbones with the CKS framework achieved state-of-the-art performance on the
NIH dataset as measured by the DSC metric.


------------------------------------------------------------------------------

Title:
ARIES: A Corpus of Scientific Paper Edits Made in Response to Peer  Reviews

Abstract: Revising scientific papers based on peer feedback is a challenging task that
requires not only deep scientific knowledge and reasoning, but also the ability
to recognize the implicit requests in high-level feedback and to choose the
best of many possible ways to update the manuscript in response. We introduce
this task for large language models and release ARIES, a dataset of review
comments and their corresponding paper edits, to enable training and evaluating
models. We study two versions of the task: comment-edit alignment and edit
generation, and evaluate several baselines, including GPT-4. We find that
models struggle even to identify the edits that correspond to a comment,
especially in cases where the comment is phrased in an indirect way or where
the edit addresses the spirit of a comment but not the precise request. When
tasked with generating edits, GPT-4 often succeeds in addressing comments on a
surface level, but it rigidly follows the wording of the feedback rather than
the underlying intent, and includes fewer technical details than human-written
edits. We hope that our formalization, dataset, and analysis will form a
foundation for future work in this area.


------------------------------------------------------------------------------

Title:
On Differentially Private Sampling from Gaussian and Product  Distributions

Abstract: Given a dataset of $n$ i.i.d. samples from an unknown distribution $P$, we
consider the problem of generating a sample from a distribution that is close
to $P$ in total variation distance, under the constraint of differential
privacy (DP). We study the problem when $P$ is a multi-dimensional Gaussian
distribution, under different assumptions on the information available to the
DP mechanism: known covariance, unknown bounded covariance, and unknown
unbounded covariance. We present new DP sampling algorithms, and show that they
achieve near-optimal sample complexity in the first two settings. Moreover,
when $P$ is a product distribution on the binary hypercube, we obtain a pure-DP
algorithm whereas only an approximate-DP algorithm (with slightly worse sample
complexity) was previously known.


------------------------------------------------------------------------------

Title:
Uncertainty Propagation of Initial Conditions in Thermal Models

Abstract: The operation of machine tools often demands a highly accurate knowledge of
the tool center point's (TCP) position. The displacement of the TCP over time
can be inferred from thermal models, which comprise a set of geometrically
coupled heat equations. Each of these equations represents the temperature in
part of the machine, and they are often formulated on complicated geometries.
The accuracy of the TCP prediction depends highly on the accuracy of the
model parameters, such as heat exchange parameters, and the initial
temperature. Thus it is of utmost interest to determine the influence of these
parameters on the TCP displacement prediction. In turn, the accuracy of the
parameter estimate is essentially determined by the measurement accuracy and
the sensor placement.
Determining the accuracy of a given sensor configuration is a key
prerequisite of optimal sensor placement. We develop here a thermal model for a
particular machine tool. On top of this model we propose two numerical
algorithms to evaluate any given thermal sensor configuration with respect to
its accuracy. We compute the posterior variances from the posterior covariance
matrix with respect to an uncertain initial temperature field. The full matrix
is dense and potentially very large, depending on the model size. Thus, we
apply a low-rank method to approximate relevant entries, i.e. the variances on
its diagonal. We first present a straightforward way to compute this
approximation which requires computation of the model sensitivities with with
respect to the initial values. Additionally, we present a low-rank tensor
method which exploits the underlying system structure. We compare the
efficiency of both algorithms with respect to runtime and memory requirements
and discuss their respective advantages with regard to optimal sensor placement
problems.


------------------------------------------------------------------------------

Title:
Numerical analysis of the stochastic Stefan problem

Abstract: The gradient discretisation method (GDM) -- a generic framework encompassing
many numerical methods -- is studied for a general stochastic Stefan problem
with multiplicative noise. The convergence of the numerical solutions is proved
by compactness method using discrete functional analysis tools, Skorohod
theorem and the martingale representation theorem. The generic convergence
results established in the GDM framework are applicable to a range of different
numerical methods, including for example mass-lumped finite elements, but also
some finite volume methods, mimetic methods, lowest-order virtual element
methods, etc. Theoretical results are complemented by numerical tests based on
two methods that fit in GDM framework.


------------------------------------------------------------------------------

Title:
Decidable Exponentials in Nonassociative Noncommutative Linear Logic

Abstract: The use of exponentials in linear logic greatly enhances its expressive
power. In this paper we focus on nonassociative noncommutative multiplicative
linear logic, and systematically explore modal axioms K, T, and 4 as well as
the structural rules of contraction and weakening. We give sequent systems for
each subset of these axioms; these enjoy cut elimination and have analogues in
more structural logics. We then appeal to work of Bulinska extending work of
Buszkowski to show that several of these logics are PTIME decidable and
generate context free languages as categorial grammars. This contrasts
associative systems where similar logics are known to generate all recursively
enumerable languages, and are thus in particular undecidable.


------------------------------------------------------------------------------

Title:
Geometric Graphs with Unbounded Flip-Width

Abstract: We consider the flip-width of geometric graphs, a notion of graph width
recently introduced by Toru\'nczyk. We prove that many different types of
geometric graphs have unbounded flip-width. These include interval graphs,
permutation graphs, circle graphs, intersection graphs of axis-aligned line
segments or axis-aligned unit squares, unit distance graphs, unit disk graphs,
visibility graphs of simple polygons, $\beta$-skeletons, 4-polytopes, rectangle
of influence graphs, and 3d Delaunay triangulations.


------------------------------------------------------------------------------

Title:
A penalty-free Shifted Boundary Method of arbitrary order

Abstract: We introduce and analyze a penalty-free formulation of the Shifted Boundary
Method (SBM), inspired by the asymmetric version of the Nitsche method. We
prove its stability and convergence for arbitrary order finite element
interpolation spaces and we test its performance with a number of numerical
experiments. Moreover, while the SBM was previously believed to be only
asymptotically consistent (in the sense of Galerkin orthogonality), we prove
here that it is indeed exactly consistent.


------------------------------------------------------------------------------

Title:
Probabilistic Slide-support Manipulation Planning in Clutter

Abstract: To safely and efficiently extract an object from the clutter, this paper
presents a bimanual manipulation planner in which one hand of the robot is used
to slide the target object out of the clutter while the other hand is used to
support the surrounding objects to prevent the clutter from collapsing. Our
method uses a neural network to predict the physical phenomena of the clutter
when the target object is moved. We generate the most efficient action based on
the Monte Carlo tree search.The grasping and sliding actions are planned to
minimize the number of motion sequences to pick the target object. In addition,
the object to be supported is determined to minimize the position change of
surrounding objects. Experiments with a real bimanual robot confirmed that the
robot could retrieve the target object, reducing the total number of motion
sequences and improving safety.


------------------------------------------------------------------------------

Title:
Rethinking the Backward Propagation for Adversarial Transferability

Abstract: Transfer-based attacks generate adversarial examples on the surrogate model,
which can mislead other black-box models without any access, making it
promising to attack real-world applications. Recently, several works have been
proposed to boost adversarial transferability, in which the surrogate model is
usually overlooked. In this work, we identify that non-linear layers (e.g.,
ReLU, max-pooling, etc.) truncate the gradient during backward propagation,
making the gradient w.r.t.input image imprecise to the loss function. We
hypothesize and empirically validate that such truncation undermines the
transferability of adversarial examples. Based on these findings, we propose a
novel method called Backward Propagation Attack (BPA) to increase the relevance
between the gradient w.r.t. input image and loss function so as to generate
adversarial examples with higher transferability. Specifically, BPA adopts a
non-monotonic function as the derivative of ReLU and incorporates softmax with
temperature to smooth the derivative of max-pooling, thereby mitigating the
information loss during the backward propagation of gradients. Empirical
results on the ImageNet dataset demonstrate that not only does our method
substantially boost the adversarial transferability, but it also is general to
existing transfer-based attacks.


------------------------------------------------------------------------------

Title:
Auto-BI: Automatically Build BI-Models Leveraging Local Join Prediction  and Global Schema Graph

Abstract: Business Intelligence (BI) is crucial in modern enterprises and
billion-dollar business. Traditionally, technical experts like database
administrators would manually prepare BI-models (e.g., in star or snowflake
schemas) that join tables in data warehouses, before less-technical business
users can run analytics using end-user dashboarding tools. However, the
popularity of self-service BI (e.g., Tableau and Power-BI) in recent years
creates a strong demand for less technical end-users to build BI-models
themselves.
We develop an Auto-BI system that can accurately predict BI models given a
set of input tables, using a principled graph-based optimization problem we
propose called \textit{k-Min-Cost-Arborescence} (k-MCA), which holistically
considers both local join prediction and global schema-graph structures,
leveraging a graph-theoretical structure called \textit{arborescence}. While we
prove k-MCA is intractable and inapproximate in general, we develop novel
algorithms that can solve k-MCA optimally, which is shown to be efficient in
practice with sub-second latency and can scale to the largest BI-models we
encounter (with close to 100 tables).
Auto-BI is rigorously evaluated on a unique dataset with over 100K real BI
models we harvested, as well as on 4 popular TPC benchmarks. It is shown to be
both efficient and accurate, achieving over 0.9 F1-score on both real and
synthetic benchmarks.


------------------------------------------------------------------------------

Title:
From Word Models to World Models: Translating from Natural Language to  the Probabilistic Language of Thought

Abstract: How does language inform our downstream thinking? In particular, how do
humans make meaning from language -- and how can we leverage a theory of
linguistic meaning to build machines that think in more human-like ways? In
this paper, we propose \textit{rational meaning construction}, a computational
framework for language-informed thinking that combines neural models of
language with probabilistic models for rational inference. We frame linguistic
meaning as a context-sensitive mapping from natural language into a
\textit{probabilistic language of thought} (PLoT) -- a general-purpose symbolic
substrate for probabilistic, generative world modeling. Our architecture
integrates two powerful computational tools that have not previously come
together: we model thinking with \textit{probabilistic programs}, an expressive
representation for flexible commonsense reasoning; and we model meaning
construction with \textit{large language models} (LLMs), which support
broad-coverage translation from natural language utterances to code expressions
in a probabilistic programming language. We illustrate our framework in action
through examples covering four core domains from cognitive science:
probabilistic reasoning, logical and relational reasoning, visual and physical
reasoning, and social reasoning about agents and their plans. In each, we show
that LLMs can generate context-sensitive translations that capture
pragmatically-appropriate linguistic meanings, while Bayesian inference with
the generated programs supports coherent and robust commonsense reasoning. We
extend our framework to integrate cognitively-motivated symbolic modules to
provide a unified commonsense thinking interface from language. Finally, we
explore how language can drive the construction of world models themselves.


------------------------------------------------------------------------------

Title:
Multilingual Neural Machine Translation System for Indic to Indic  Languages

Abstract: This paper gives an Indic-to-Indic (IL-IL) MNMT baseline model for 11 ILs
implemented on the Samanantar corpus and analyzed on the Flores-200 corpus. All
the models are evaluated using the BLEU score. In addition, the languages are
classified under three groups namely East Indo- Aryan (EI), Dravidian (DR), and
West Indo-Aryan (WI). The effect of language relatedness on MNMT model
efficiency is studied. Owing to the presence of large corpora from English (EN)
to ILs, MNMT IL-IL models using EN as a pivot are also built and examined. To
achieve this, English- Indic (EN-IL) models are also developed, with and
without the usage of related languages. Results reveal that using related
languages is beneficial for the WI group only, while it is detrimental for the
EI group and shows an inconclusive effect on the DR group, but it is useful for
EN-IL models. Thus, related language groups are used to develop pivot MNMT
models. Furthermore, the IL corpora are transliterated from the corresponding
scripts to a modified ITRANS script, and the best MNMT models from the previous
approaches are built on the transliterated corpus. It is observed that the
usage of pivot models greatly improves MNMT baselines with AS-TA achieving the
minimum BLEU score and PA-HI achieving the maximum score. Among languages, AS,
ML, and TA achieve the lowest BLEU score, whereas HI, PA, and GU perform the
best. Transliteration also helps the models with few exceptions. The best
increment of scores is observed in ML, TA, and BN and the worst average
increment is observed in KN, HI, and PA, across all languages. The best model
obtained is the PA-HI language pair trained on PAWI transliterated corpus which
gives 24.29 BLEU.


------------------------------------------------------------------------------

Title:
Exploring the Landscape of Ubiquitous In-home Health Monitoring: A  Comprehensive Survey

Abstract: Ubiquitous in-home health monitoring systems have become popular in recent
years due to the rise of digital health technologies and the growing demand for
remote health monitoring. These systems enable individuals to increase their
independence by allowing them to monitor their health from the home and by
allowing more control over their well-being. In this study, we perform a
comprehensive survey on this topic by reviewing a large number of literature in
the area. We investigate these systems from various aspects, namely sensing
technologies, communication technologies, intelligent and computing systems,
and application areas. Specifically, we provide an overview of in-home health
monitoring systems and identify their main components. We then present each
component and discuss its role within in-home health monitoring systems. In
addition, we provide an overview of the practical use of ubiquitous
technologies in the home for health monitoring. Finally, we identify the main
challenges and limitations based on the existing literature and provide eight
recommendations for potential future research directions toward the development
of in-home health monitoring systems. We conclude that despite extensive
research on various components needed for the development of effective in-home
health monitoring systems, the development of effective in-home health
monitoring systems still requires further investigation.


------------------------------------------------------------------------------

Title:
The Power of Menus in Contract Design

Abstract: We study the power of menus of contracts in principal-agent problems with
adverse selection (agents can be one of several types) and moral hazard (we
cannot observe agent actions directly). For principal-agent problems with $T$
types and $n$ actions, we show that the best menu of contracts can obtain a
factor $\Omega(\max(n, \log T))$ more utility for the principal than the best
individual contract, partially resolving an open question of Guruganesh et al.
(2021). We then turn our attention to randomized menus of linear contracts,
where we likewise show that randomized linear menus can be $\Omega(T)$ better
than the best single linear contract. As a corollary, we show this implies an
analogous gap between deterministic menus of (general) contracts and randomized
menus of contracts (as introduced by Castiglioni et al. (2022)).


------------------------------------------------------------------------------

Title:
On Addressing the Limitations of Graph Neural Networks

Abstract: This report gives a summary of two problems about graph convolutional
networks (GCNs): over-smoothing and heterophily challenges, and outlines future
directions to explore.


------------------------------------------------------------------------------

Title:
Revisiting Image Classifier Training for Improved Certified Robust  Defense against Adversarial Patches

Abstract: Certifiably robust defenses against adversarial patches for image classifiers
ensure correct prediction against any changes to a constrained neighborhood of
pixels. PatchCleanser arXiv:2108.09135 [cs.CV], the state-of-the-art certified
defense, uses a double-masking strategy for robust classification. The success
of this strategy relies heavily on the model's invariance to image pixel
masking. In this paper, we take a closer look at model training schemes to
improve this invariance. Instead of using Random Cutout arXiv:1708.04552v2
[cs.CV] augmentations like PatchCleanser, we introduce the notion of worst-case
masking, i.e., selecting masked images which maximize classification loss.
However, finding worst-case masks requires an exhaustive search, which might be
prohibitively expensive to do on-the-fly during training. To solve this
problem, we propose a two-round greedy masking strategy (Greedy Cutout) which
finds an approximate worst-case mask location with much less compute. We show
that the models trained with our Greedy Cutout improves certified robust
accuracy over Random Cutout in PatchCleanser across a range of datasets and
architectures. Certified robust accuracy on ImageNet with a ViT-B16-224 model
increases from 58.1\% to 62.3\% against a 3\% square patch applied anywhere on
the image.


------------------------------------------------------------------------------

Title:
1st Place Solution to MultiEarth 2023 Challenge on Multimodal SAR-to-EO  Image Translation

Abstract: The Multimodal Learning for Earth and Environment Workshop (MultiEarth 2023)
aims to harness the substantial amount of remote sensing data gathered over
extensive periods for the monitoring and analysis of Earth's ecosystems'health.
The subtask, Multimodal SAR-to-EO Image Translation, involves the use of robust
SAR data, even under adverse weather and lighting conditions, transforming it
into high-quality, clear, and visually appealing EO data. In the context of the
SAR2EO task, the presence of clouds or obstructions in EO data can potentially
pose a challenge. To address this issue, we propose the Clean Collector
Algorithm (CCA), designed to take full advantage of this cloudless SAR data and
eliminate factors that may hinder the data learning process. Subsequently, we
applied pix2pixHD for the SAR-to-EO translation and Restormer for image
enhancement. In the final evaluation, the team 'CDRL' achieved an MAE of
0.07313, securing the top rank on the leaderboard.


------------------------------------------------------------------------------

Title:
Multimodal Zero-Shot Learning for Tactile Texture Recognition

Abstract: Tactile sensing plays an irreplaceable role in robotic material recognition.
It enables robots to distinguish material properties such as their local
geometry and textures, especially for materials like textiles. However, most
tactile recognition methods can only classify known materials that have been
touched and trained with tactile data, yet cannot classify unknown materials
that are not trained with tactile data. To solve this problem, we propose a
tactile zero-shot learning framework to recognise unknown materials when they
are touched for the first time without requiring training tactile samples. The
visual modality, providing tactile cues from sight, and semantic attributes,
giving high-level characteristics, are combined together to bridge the gap
between touched classes and untouched classes. A generative model is learnt to
synthesise tactile features according to corresponding visual images and
semantic embeddings, and then a classifier can be trained using the synthesised
tactile features of untouched materials for zero-shot recognition. Extensive
experiments demonstrate that our proposed multimodal generative model can
achieve a high recognition accuracy of 83.06% in classifying materials that
were not touched before. The robotic experiment demo and the dataset are
available at this https URL


------------------------------------------------------------------------------

Title:
Evaluating Large Language Models with NeuBAROCO: Syllogistic Reasoning  Ability and Human-like Biases

Abstract: This paper investigates whether current large language models exhibit biases
in logical reasoning, similar to humans. Specifically, we focus on syllogistic
reasoning, a well-studied form of inference in the cognitive science of human
deduction. To facilitate our analysis, we introduce a dataset called NeuBAROCO,
originally designed for psychological experiments that assess human logical
abilities in syllogistic reasoning. The dataset consists of syllogistic
inferences in both English and Japanese. We examine three types of biases
observed in human syllogistic reasoning: belief biases, conversion errors, and
atmosphere effects. Our findings demonstrate that current large language models
struggle more with problems involving these three types of biases.


------------------------------------------------------------------------------

Title:
SituatedGen: Incorporating Geographical and Temporal Contexts into  Generative Commonsense Reasoning

Abstract: Recently, commonsense reasoning in text generation has attracted much
attention. Generative commonsense reasoning is the task that requires machines,
given a group of keywords, to compose a single coherent sentence with
commonsense plausibility. While existing datasets targeting generative
commonsense reasoning focus on everyday scenarios, it is unclear how well
machines reason under specific geographical and temporal contexts. We formalize
this challenging task as SituatedGen, where machines with commonsense should
generate a pair of contrastive sentences given a group of keywords including
geographical or temporal entities. We introduce a corresponding English dataset
consisting of 8,268 contrastive sentence pairs, which are built upon several
existing commonsense reasoning benchmarks with minimal manual labor.
Experiments show that state-of-the-art generative language models struggle to
generate sentences with commonsense plausibility and still lag far behind human
performance. Our dataset is publicly available at
this https URL


------------------------------------------------------------------------------

Title:
On the Robustness of Generative Retrieval Models: An Out-of-Distribution  Perspective

Abstract: Recently, we have witnessed generative retrieval increasingly gaining
attention in the information retrieval (IR) field, which retrieves documents by
directly generating their identifiers. So far, much effort has been devoted to
developing effective generative retrieval models. There has been less attention
paid to the robustness perspective. When a new retrieval paradigm enters into
the real-world application, it is also critical to measure the
out-of-distribution (OOD) generalization, i.e., how would generative retrieval
models generalize to new distributions. To answer this question, firstly, we
define OOD robustness from three perspectives in retrieval problems: 1) The
query variations; 2) The unforeseen query types; and 3) The unforeseen tasks.
Based on this taxonomy, we conduct empirical studies to analyze the OOD
robustness of several representative generative retrieval models against dense
retrieval models. The empirical results indicate that the OOD robustness of
generative retrieval models requires enhancement. We hope studying the OOD
robustness of generative retrieval models would be advantageous to the IR
community.


------------------------------------------------------------------------------

Title:
Instruct-FinGPT: Financial Sentiment Analysis by Instruction Tuning of  General-Purpose Large Language Models

Abstract: Sentiment analysis is a vital tool for uncovering insights from financial
articles, news, and social media, shaping our understanding of market
movements. Despite the impressive capabilities of large language models (LLMs)
in financial natural language processing (NLP), they still struggle with
accurately interpreting numerical values and grasping financial context,
limiting their effectiveness in predicting financial sentiment. In this paper,
we introduce a simple yet effective instruction tuning approach to address
these issues. By transforming a small portion of supervised financial sentiment
analysis data into instruction data and fine-tuning a general-purpose LLM with
this method, we achieve remarkable advancements in financial sentiment
analysis. In the experiment, our approach outperforms state-of-the-art
supervised sentiment analysis models, as well as widely used LLMs like ChatGPT
and LLaMAs, particularly in scenarios where numerical understanding and
contextual comprehension are vital.


------------------------------------------------------------------------------

Title:
Solving time-dependent PDEs with the ultraspherical spectral method

Abstract: We apply the ultraspherical spectral method to solving time-dependent PDEs by
proposing two approaches to discretization based on the method of lines and
show that these approaches produce approximately same results. We analyze the
stability, the error, and the computational cost of the proposed method. In
addition, we show how adaptivity can be incorporated to offer adequate spatial
resolution efficiently. Both linear and nonlinear problems are considered. We
also explore time integration using exponential integrators with the
ultraspherical spatial discretization. Comparisons with the Chebyshev
pseudospectral method are given along the discussion and they show that the
ultraspherical spectral method is a competitive candidate for the spatial
discretization of time-dependent PDEs.


------------------------------------------------------------------------------

Title:
A Hierarchical Approach to exploiting Multiple Datasets from TalkBank

Abstract: TalkBank is an online database that facilitates the sharing of linguistics
research data. However, the existing TalkBank's API has limited data filtering
and batch processing capabilities. To overcome these limitations, this paper
introduces a pipeline framework that employs a hierarchical search approach,
enabling efficient complex data selection. This approach involves a quick
preliminary screening of relevant corpora that a researcher may need, and then
perform an in-depth search for target data based on specific criteria. The
identified files are then indexed, providing easier access for future analysis.
Furthermore, the paper demonstrates how data from different studies curated
with the framework can be integrated by standardizing and cleaning metadata,
allowing researchers to extract insights from a large, integrated dataset.
While being designed for TalkBank, the framework can also be adapted to process
data from other open-science platforms.


------------------------------------------------------------------------------

Title:
Faster Compression of Deterministic Finite Automata

Abstract: Deterministic finite automata (DFA) are a classic tool for high throughput
matching of regular expressions, both in theory and practice.
Due to their high space consumption, extensive research has been devoted to
compressed representations of DFAs that still support efficient pattern
matching queries.
Kumar~et~al.~[SIGCOMM 2006] introduced the \emph{delayed deterministic finite
automaton} (\ddfa{}) which exploits the large redundancy between inter-state
transitions in the automaton.
They showed it to obtain up to two orders of magnitude compression of
real-world DFAs, and their work formed the basis of numerous subsequent
results.
Their algorithm, as well as later algorithms based on their idea, have an
inherent quadratic-time bottleneck, as they consider every pair of states to
compute the optimal compression.
In this work we present a simple, general framework based on
locality-sensitive hashing for speeding up these algorithms to achieve
sub-quadratic construction times for \ddfa{}s.
We apply the framework to speed up several algorithms to near-linear time,
and experimentally evaluate their performance on real-world regular expression
sets extracted from modern intrusion detection systems.
We find an order of magnitude improvement in compression times, with either
little or no loss of compression, or even significantly better compression in
some cases.


------------------------------------------------------------------------------

Title:
Beyond OOD State Actions: Supported Cross-Domain Offline Reinforcement  Learning

Abstract: Offline reinforcement learning (RL) aims to learn a policy using only
pre-collected and fixed data. Although avoiding the time-consuming online
interactions in RL, it poses challenges for out-of-distribution (OOD) state
actions and often suffers from data inefficiency for training. Despite many
efforts being devoted to addressing OOD state actions, the latter (data
inefficiency) receives little attention in offline RL. To address this, this
paper proposes the cross-domain offline RL, which assumes offline data
incorporate additional source-domain data from varying transition dynamics
(environments), and expects it to contribute to the offline data efficiency. To
do so, we identify a new challenge of OOD transition dynamics, beyond the
common OOD state actions issue, when utilizing cross-domain offline data. Then,
we propose our method BOSA, which employs two support-constrained objectives to
address the above OOD issues. Through extensive experiments in the cross-domain
offline RL setting, we demonstrate BOSA can greatly improve offline data
efficiency: using only 10\% of the target data, BOSA could achieve {74.4\%} of
the SOTA offline RL performance that uses 100\% of the target data.
Additionally, we also show BOSA can be effortlessly plugged into model-based
offline RL and noising data augmentation techniques (used for generating
source-domain data), which naturally avoids the potential dynamics mismatch
between target-domain data and newly generated source-domain data.


------------------------------------------------------------------------------

Title:
Communication-Efficient Federated Learning through Importance Sampling

Abstract: The high communication cost of sending model updates from the clients to the
server is a significant bottleneck for scalable federated learning (FL). Among
existing approaches, state-of-the-art bitrate-accuracy tradeoffs have been
achieved using stochastic compression methods -- in which the client $n$ sends
a sample from a client-only probability distribution $q_{\phi^{(n)}}$, and the
server estimates the mean of the clients' distributions using these samples.
However, such methods do not take full advantage of the FL setup where the
server, throughout the training process, has side information in the form of a
pre-data distribution $p_{\theta}$ that is close to the client's distribution
$q_{\phi^{(n)}}$ in Kullback-Leibler (KL) divergence. In this work, we exploit
this closeness between the clients' distributions $q_{\phi^{(n)}}$'s and the
side information $p_{\theta}$ at the server, and propose a framework that
requires approximately $D_{KL}(q_{\phi^{(n)}}|| p_{\theta})$ bits of
communication. We show that our method can be integrated into many existing
stochastic compression frameworks such as FedPM, Federated SGLD, and QSGD to
attain the same (and often higher) test accuracy with up to $50$ times
reduction in the bitrate.


------------------------------------------------------------------------------

Title:
Identifying and Extracting Rare Disease Phenotypes with Large Language  Models

Abstract: Rare diseases (RDs) are collectively common and affect 300 million people
worldwide. Accurate phenotyping is critical for informing diagnosis and
treatment, but RD phenotypes are often embedded in unstructured text and
time-consuming to extract manually. While natural language processing (NLP)
models can perform named entity recognition (NER) to automate extraction, a
major bottleneck is the development of a large, annotated corpus for model
training. Recently, prompt learning emerged as an NLP paradigm that can lead to
more generalizable results without any (zero-shot) or few labeled samples
(few-shot). Despite growing interest in ChatGPT, a revolutionary large language
model capable of following complex human prompts and generating high-quality
responses, none have studied its NER performance for RDs in the zero- and
few-shot settings. To this end, we engineered novel prompts aimed at extracting
RD phenotypes and, to the best of our knowledge, are the first the establish a
benchmark for evaluating ChatGPT's performance in these settings. We compared
its performance to the traditional fine-tuning approach and conducted an
in-depth error analysis. Overall, fine-tuning BioClinicalBERT resulted in
higher performance (F1 of 0.689) than ChatGPT (F1 of 0.472 and 0.591 in the
zero- and few-shot settings, respectively). Despite this, ChatGPT achieved
similar or higher accuracy for certain entities (i.e., rare diseases and signs)
in the one-shot setting (F1 of 0.776 and 0.725). This suggests that with
appropriate prompt engineering, ChatGPT has the potential to match or
outperform fine-tuned language models for certain entity types with just one
labeled sample. While the proliferation of large language models may provide
opportunities for supporting RD diagnosis and treatment, researchers and
clinicians should critically evaluate model outputs and be well-informed of
their limitations.


------------------------------------------------------------------------------

Title:
Explainable Recommendation with Personalized Review Retrieval and Aspect  Learning

Abstract: Explainable recommendation is a technique that combines prediction and
generation tasks to produce more persuasive results. Among these tasks, textual
generation demands large amounts of data to achieve satisfactory accuracy.
However, historical user reviews of items are often insufficient, making it
challenging to ensure the precision of generated explanation text. To address
this issue, we propose a novel model, ERRA (Explainable Recommendation by
personalized Review retrieval and Aspect learning). With retrieval enhancement,
ERRA can obtain additional information from the training sets. With this
additional information, we can generate more accurate and informative
explanations. Furthermore, to better capture users' preferences, we incorporate
an aspect enhancement component into our model. By selecting the top-n aspects
that users are most concerned about for different items, we can model user
representation with more relevant details, making the explanation more
persuasive. To verify the effectiveness of our model, extensive experiments on
three datasets show that our model outperforms state-of-the-art baselines (for
example, 3.4% improvement in prediction and 15.8% improvement in explanation
for TripAdvisor).


------------------------------------------------------------------------------

Title:
Deep Dynamic Epidemiological Modelling for COVID-19 Forecasting in  Multi-level Districts

Abstract: Objective: COVID-19 has spread worldwide and made a huge influence across the
world. Modeling the infectious spread situation of COVID-19 is essential to
understand the current condition and to formulate intervention measurements.
Epidemiological equations based on the SEIR model simulate disease development.
The traditional parameter estimation method to solve SEIR equations could not
precisely fit real-world data due to different situations, such as social
distancing policies and intervention strategies. Additionally, learning-based
models achieve outstanding fitting performance, but cannot visualize
mechanisms. Methods: Thus, we propose a deep dynamic epidemiological (DDE)
method that combines epidemiological equations and deep-learning advantages to
obtain high accuracy and visualization. The DDE contains deep networks to fit
the effect function to simulate the ever-changing situations based on the
neural ODE method in solving variants' equations, ensuring the fitting
performance of multi-level areas. Results: We introduce four SEIR variants to
fit different situations in different countries and regions. We compare our DDE
method with traditional parameter estimation methods (Nelder-Mead, BFGS,
Powell, Truncated Newton Conjugate-Gradient, Neural ODE) in fitting the
real-world data in the cases of countries (the USA, Columbia, South Africa) and
regions (Wuhan in China, Piedmont in Italy). Our DDE method achieves the best
Mean Square Error and Pearson coefficient in all five areas. Further, compared
with the state-of-art learning-based approaches, the DDE outperforms all
techniques, including LSTM, RNN, GRU, Random Forest, Extremely Random Trees,
and Decision Tree. Conclusion: DDE presents outstanding predictive ability and
visualized display of the changes in infection rates in different regions and
countries.


------------------------------------------------------------------------------

Title:
Preprocessing Complexity for Some Graph Problems Parameterized by  Structural Parameters

Abstract: Structural graph parameters play an important role in parameterized
complexity, including in kernelization. Notably, vertex cover, neighborhood
diversity, twin-cover, and modular-width have been studied extensively in the
last few years. However, there are many fundamental problems whose
preprocessing complexity is not fully understood under these parameters.
Indeed, the existence of polynomial kernels or polynomial Turing kernels for
famous problems such as Clique, Chromatic Number, and Steiner Tree has only
been established for a subset of structural parameters. In this work, we use
several techniques to obtain a complete preprocessing complexity landscape for
over a dozen of fundamental algorithmic problems.


------------------------------------------------------------------------------

Title:
Recent Developments in Recommender Systems: A Survey

Abstract: In this technical survey, we comprehensively summarize the latest
advancements in the field of recommender systems. The objective of this study
is to provide an overview of the current state-of-the-art in the field and
highlight the latest trends in the development of recommender systems. The
study starts with a comprehensive summary of the main taxonomy of recommender
systems, including personalized and group recommender systems, and then delves
into the category of knowledge-based recommender systems. In addition, the
survey analyzes the robustness, data bias, and fairness issues in recommender
systems, summarizing the evaluation metrics used to assess the performance of
these systems. Finally, the study provides insights into the latest trends in
the development of recommender systems and highlights the new directions for
future research in the field.


------------------------------------------------------------------------------

Title:
Constructing Colloquial Dataset for Persian Sentiment Analysis of Social  Microblogs

Abstract: Introduction: Microblogging websites have massed rich data sources for
sentiment analysis and opinion mining. In this regard, sentiment classification
has frequently proven inefficient because microblog posts typically lack
syntactically consistent terms and representatives since users on these social
networks do not like to write lengthy statements. Also, there are some
limitations to low-resource languages. The Persian language has exceptional
characteristics and demands unique annotated data and models for the sentiment
analysis task, which are distinctive from text features within the English
dialect. Method: This paper first constructs a user opinion dataset called
ITRC-Opinion by collaborative environment and insource way. Our dataset
contains 60,000 informal and colloquial Persian texts from social microblogs
such as Twitter and Instagram. Second, this study proposes a new deep
convolutional neural network (CNN) model for more effective sentiment analysis
of colloquial text in social microblog posts. The constructed datasets are used
to evaluate the presented model. Furthermore, some models, such as LSTM,
CNN-RNN, BiLSTM, and BiGRU with different word embeddings, including Fasttext,
Glove, and Word2vec, investigated our dataset and evaluated the results.
Results: The results demonstrate the benefit of our dataset and the proposed
model (72% accuracy), displaying meaningful improvement in sentiment
classification performance.


------------------------------------------------------------------------------

Title:
Misinformation as Information Pollution

Abstract: Social media feed algorithms are designed to optimize online social
engagements for the purpose of maximizing advertising profits, and therefore
have an incentive to promote controversial posts including misinformation. By
thinking about misinformation as information pollution, we can draw parallels
with environmental policy for countering pollution such as carbon taxes.
Similar to pollution, a Pigouvian tax on misinformation provides economic
incentives for social media companies to control the spread of misinformation
more effectively to avoid or reduce their misinformation tax, while preserving
some degree of freedom in platforms' response. In this paper, we highlight a
bird's eye view of a Pigouvian misinformation tax and discuss the key questions
and next steps for implementing such a taxing scheme.


------------------------------------------------------------------------------

Title:
Learning Conditional Instrumental Variable Representation for Causal  Effect Estimation

Abstract: One of the fundamental challenges in causal inference is to estimate the
causal effect of a treatment on its outcome of interest from observational
data. However, causal effect estimation often suffers from the impacts of
confounding bias caused by unmeasured confounders that affect both the
treatment and the outcome. The instrumental variable (IV) approach is a
powerful way to eliminate the confounding bias from latent confounders.
However, the existing IV-based estimators require a nominated IV, and for a
conditional IV (CIV) the corresponding conditioning set too, for causal effect
estimation. This limits the application of IV-based estimators. In this paper,
by leveraging the advantage of disentangled representation learning, we propose
a novel method, named DVAE.CIV, for learning and disentangling the
representations of CIV and the representations of its conditioning set for
causal effect estimations from data with latent confounders. Extensive
experimental results on both synthetic and real-world datasets demonstrate the
superiority of the proposed DVAE.CIV method against the existing causal effect
estimators.


------------------------------------------------------------------------------

Title:
Arc-to-line frame registration method for ultrasound and photoacoustic  image-guided intraoperative robot-assisted laparoscopic prostatectomy

Abstract: Purpose: To achieve effective robot-assisted laparoscopic prostatectomy, the
integration of transrectal ultrasound (TRUS) imaging system which is the most
widely used imaging modelity in prostate imaging is essential. However, manual
manipulation of the ultrasound transducer during the procedure will
significantly interfere with the surgery. Therefore, we propose an image
co-registration algorithm based on a photoacoustic marker method, where the
ultrasound / photoacoustic (US/PA) images can be registered to the endoscopic
camera images to ultimately enable the TRUS transducer to automatically track
the surgical instrument Methods: An optimization-based algorithm is proposed to
co-register the images from the two different imaging modalities. The
principles of light propagation and an uncertainty in PM detection were assumed
in this algorithm to improve the stability and accuracy of the algorithm. The
algorithm is validated using the previously developed US/PA image-guided system
with a da Vinci surgical robot. Results: The target-registration-error (TRE) is
measured to evaluate the proposed algorithm. In both simulation and
experimental demonstration, the proposed algorithm achieved a sub-centimeter
accuracy which is acceptable in practical clinics. The result is also
comparable with our previous approach, and the proposed method can be
implemented with a normal white light stereo camera and doesn't require highly
accurate localization of the PM. Conclusion: The proposed frame registration
algorithm enabled a simple yet efficient integration of commercial US/PA
imaging system into laparoscopic surgical setting by leveraging the
characteristic properties of acoustic wave propagation and laser excitation,
contributing to automated US/PA image-guided surgical intervention
applications.


------------------------------------------------------------------------------

Title:
On boundedness of zeros of the independence polynomial of tor

Abstract: We study boundedness of zeros of the independence polynomial of tori for
sequences of tori converging to the integer lattice. We prove that zeros are
bounded for sequences of balanced tori, but unbounded for sequences of highly
unbalanced tori. Here balanced means that the size of the torus is at most
exponential in the shortest side length, while highly unbalanced means that the
longest side length of the torus is super exponential in the product over the
other side lengths cubed. We discuss implications of our results to the
existence of efficient algorithms for approximating the independence polynomial
on tori.


------------------------------------------------------------------------------

Title:
DP-BREM: Differentially-Private and Byzantine-Robust Federated Learning  with Client Momentum

Abstract: Federated Learning (FL) allows multiple participating clients to train
machine learning models collaboratively by keeping their datasets local and
only exchanging the gradient or model updates with a coordinating server.
Existing FL protocols were shown to be vulnerable to attacks that aim to
compromise data privacy and/or model robustness. Recently proposed defenses
focused on ensuring either privacy or robustness, but not both. In this paper,
we focus on simultaneously achieving differential privacy (DP) and Byzantine
robustness for cross-silo FL, based on the idea of learning from history. The
robustness is achieved via client momentum, which averages the updates of each
client over time, thus reduces the variance of the honest clients and exposes
the small malicious perturbations of Byzantine clients that are undetectable in
a single round but accumulate over time. In our initial solution DP-BREM, the
DP property is achieved via adding noise to the aggregated momentum, and we
account for the privacy cost from the momentum, which is different from the
conventional DP-SGD that accounts for the privacy cost from gradient. Since
DP-BREM assumes a trusted server (who can obtain clients' local models or
updates), we further develop the final solution called DP-BREM+, which achieves
the same DP and robustness properties as DP-BREM without a trusted server by
utilizing secure aggregation techniques, where DP noise is securely and jointly
generated by the clients. Our theoretical analysis on the convergence rate and
experimental results under different DP guarantees and attack settings
demonstrate that our proposed protocols achieve better privacy-utility tradeoff
and stronger Byzantine robustness than several baseline methods.


------------------------------------------------------------------------------

Title:
Cloud-Native Architectural Characteristics and their Impacts on Software  Quality: A Validation Survey

Abstract: Cloud-native architectures are often based on microservices and combine
different aspects that aim to leverage the capabilities of cloud platforms for
software development. Cloud-native architectural characteristics like patterns
and best practices aim to design, develop, deploy, and operate such systems
efficiently with minimal time and effort. However, architects and developers
are faced with the challenge of applying such characteristics in a targeted
manner to improve selected quality attributes. Hence, we aim to investigate
relationships, or more specifically impacts, between architectural
characteristics of cloud-native applications, and quality aspects. The
architectural characteristics in consideration are based on our recently
proposed quality model for cloud-native software architectures. To validate its
elements and revise this literature-based quality model, we conducted a
questionnaire-based survey among 42 software professionals. While the survey
results reinforce the quality model to a fair extent, they also indicate parts
requiring a revision. Thus, as an additional contribution, we present an
updated version of the quality model incorporating the survey results.
Practitioners will benefit from our work when designing and developing
cloud-native applications in a quality-oriented way. Researchers will moreover
profit from our specifically developed questionnaire-based survey tool, which
allows surveying complex structures like a hierarchical quality model.


------------------------------------------------------------------------------

Title:
To Spike or Not to Spike? A Quantitative Comparison of SNN and CNN FPGA  Implementations

Abstract: Convolutional Neural Networks (CNNs) are widely employed to solve various
problems, e.g., image classification. Due to their compute- and data-intensive
nature, CNN accelerators have been developed as ASICs or on FPGAs. Increasing
complexity of applications has caused resource costs and energy requirements of
these accelerators to grow. Spiking Neural Networks (SNNs) are an emerging
alternative to CNN implementations, promising higher resource and energy
efficiency. The main research question addressed in this paper is whether SNN
accelerators truly meet these expectations of reduced energy requirements
compared to their CNN equivalents. For this purpose, we analyze multiple SNN
hardware accelerators for FPGAs regarding performance and energy efficiency. We
present a novel encoding scheme of spike event queues and a novel memory
organization technique to improve SNN energy efficiency further. Both
techniques have been integrated into a state-of-the-art SNN architecture and
evaluated for MNIST, SVHN, and CIFAR-10 datasets and corresponding network
architectures on two differently sized modern FPGA platforms. For small-scale
benchmarks such as MNIST, SNN designs provide rather no or little latency and
energy efficiency advantages over corresponding CNN implementations. For more
complex benchmarks such as SVHN and CIFAR-10, the trend reverses.


------------------------------------------------------------------------------

Title:
Consecutive Inertia Drift of Autonomous RC Car via Primitive-based  Planning and Data-driven Control

Abstract: Inertia drift is an aggressive transitional driving maneuver, which is
challenging due to the high nonlinearity of the system and the stringent
requirement on control and planning performance. This paper presents a solution
for the consecutive inertia drift of an autonomous RC car based on
primitive-based planning and data-driven control. The planner generates complex
paths via the concatenation of path segments called primitives, and the
controller eases the burden on feedback by interpolating between multiple real
trajectories with different initial conditions into one near-feasible reference
trajectory. The proposed strategy is capable of drifting through various paths
containing consecutive turns, which is validated in both simulation and
reality.


------------------------------------------------------------------------------

Title:
State-wise Constrained Policy Optimization

Abstract: Reinforcement Learning (RL) algorithms have shown tremendous success in
simulation environments, but their application to real-world problems faces
significant challenges, with safety being a major concern. In particular,
enforcing state-wise constraints is essential for many challenging tasks such
as autonomous driving and robot manipulation. However, existing safe RL
algorithms under the framework of Constrained Markov Decision Process (CMDP) do
not consider state-wise constraints. To address this gap, we propose State-wise
Constrained Policy Optimization (SCPO), the first general-purpose policy search
algorithm for state-wise constrained reinforcement learning. SCPO provides
guarantees for state-wise constraint satisfaction in expectation. In
particular, we introduce the framework of Maximum Markov Decision Process, and
prove that the worst-case safety violation is bounded under SCPO. We
demonstrate the effectiveness of our approach on training neural network
policies for extensive robot locomotion tasks, where the agent must satisfy a
variety of state-wise safety constraints. Our results show that SCPO
significantly outperforms existing methods and can handle state-wise
constraints in high-dimensional robotics tasks.


------------------------------------------------------------------------------

Title:
Rapid building damage assessment workflow: An implementation for the  2023 Rolling Fork, Mississippi tornado event

Abstract: Rapid and accurate building damage assessments from high-resolution satellite
imagery following a natural disaster is essential to inform and optimize first
responder efforts. However, performing such building damage assessments in an
automated manner is non-trivial due to the challenges posed by variations in
disaster-specific damage, diversity in satellite imagery, and the dearth of
extensive, labeled datasets. To circumvent these issues, this paper introduces
a human-in-the-loop workflow for rapidly training building damage assessment
models after a natural disaster. This article details a case study using this
workflow, executed in partnership with the American Red Cross during a tornado
event in Rolling Fork, Mississippi in March, 2023. The output from our
human-in-the-loop modeling process achieved a precision of 0.86 and recall of
0.80 for damaged buildings when compared to ground truth data collected
post-disaster. This workflow was implemented end-to-end in under 2 hours per
satellite imagery scene, highlighting its potential for real-time deployment.


------------------------------------------------------------------------------

Title:
Quantum Pufferfish Privacy: A Flexible Privacy Framework for Quantum  Systems

Abstract: We propose a versatile privacy framework for quantum systems, termed quantum
pufferfish privacy (QPP). Inspired by classical pufferfish privacy, our
formulation generalizes and addresses limitations of quantum differential
privacy by offering flexibility in specifying private information, feasible
measurements, and domain knowledge. We show that QPP can be equivalently
formulated in terms of the Datta-Leditzky information spectrum divergence, thus
providing the first operational interpretation thereof. We reformulate this
divergence as a semi-definite program and derive several properties of it,
which are then used to prove convexity, composability, and post-processing of
QPP mechanisms. Parameters that guarantee QPP of the depolarization mechanism
are also derived. We analyze the privacy-utility tradeoff of general QPP
mechanisms and, again, study the depolarization mechanism as an explicit
instance. The QPP framework is then applied to privacy auditing for identifying
privacy violations via a hypothesis testing pipeline that leverages quantum
algorithms. Connections to quantum fairness and other quantum divergences are
also explored and several variants of QPP are examined.


------------------------------------------------------------------------------

Title:
Memory-Query Tradeoffs for Randomized Convex Optimization

Abstract: We show that any randomized first-order algorithm which minimizes a
$d$-dimensional, $1$-Lipschitz convex function over the unit ball must either
use $\Omega(d^{2-\delta})$ bits of memory or make $\Omega(d^{1+\delta/6-o(1)})$
queries, for any constant $\delta\in (0,1)$ and when the precision $\epsilon$
is quasipolynomially small in $d$. Our result implies that cutting plane
methods, which use $\tilde{O}(d^2)$ bits of memory and $\tilde{O}(d)$ queries,
are Pareto-optimal among randomized first-order algorithms, and quadratic
memory is required to achieve optimal query complexity for convex optimization.


------------------------------------------------------------------------------

Title:
Polynomial Logical Zonotopes: A Set Representation for Reachability  Analysis of Logical Systems

Abstract: In this paper, we introduce a set representation called polynomial logical
zonotopes for performing exact and computationally efficient reachability
analysis on logical systems. Polynomial logical zonotopes are a generalization
of logical zonotopes, which are able to represent up to 2^n binary vectors
using only n generators. Due to their construction, logical zonotopes are only
able to support exact computations of some logical operations (XOR, NOT, XNOR),
while other operations (AND, NAND, OR, NOR) result in over-approximations. In
order to perform all fundamental logical operations exactly, we formulate a
generalization of logical zonotopes that is constructed by additional dependent
generators and exponent matrices. We prove that through this polynomial-like
construction, we are able to perform all of the fundamental logical operations
(XOR, NOT, XNOR, AND, NAND, OR, NOR) exactly. While we are able to perform all
of the logical operations exactly, this comes with a slight increase in
computational complexity compared to logical zonotopes. We show that we can use
polynomial logical zonotopes to perform exact reachability analysis while
retaining a low computational complexity. To illustrate and showcase the
computational benefits of polynomial logical zonotopes, we present the results
of performing reachability analysis on two use cases: (1) safety verification
of an intersection crossing protocol, (2) and reachability analysis on a
high-dimensional Boolean function. Moreover, to highlight the extensibility of
logical zonotopes, we include an additional use case where we perform a
computationally tractable exhaustive search for the key of a linear-feedback
shift register.


------------------------------------------------------------------------------

Title:
A Lotka-Volterra type model analyzed through different techniques

Abstract: We consider a modified Lotka-Volterra model applied to the predator-prey
system that can also be applied to other areas, for instance the bank system.
We show that the model is well-posed (non-negativity of solutions and
conservation law) and study the local stability using different methods.
Firstly we consider the continuous model, after which the numerical schemes of
Euler and Mickens are investigated. Finally, the model is described using
Caputo fractional derivatives. For the fractional model, besides well-posedness
and local stability, we prove the existence and uniqueness of solution.
Throughout the work we compare the results graphically and present our
conclusions. To represent graphically the solutions of the fractional model we
use the modified trapezoidal method that involves the modified Euler method.


------------------------------------------------------------------------------

Title:
Unitary Complexity and the Uhlmann Transformation Problem

Abstract: State transformation problems such as compressing quantum information or
breaking quantum commitments are fundamental quantum tasks. However, their
computational difficulty cannot easily be characterized using traditional
complexity theory, which focuses on tasks with classical inputs and outputs.
To study the complexity of such state transformation tasks, we introduce a
framework for unitary synthesis problems, including notions of reductions and
unitary complexity classes. We use this framework to study the complexity of
transforming one entangled state into another via local operations. We
formalize this as the Uhlmann Transformation Problem, an algorithmic version of
Uhlmann's theorem. Then, we prove structural results relating the complexity of
the Uhlmann Transformation Problem, polynomial space quantum computation, and
zero knowledge protocols.
The Uhlmann Transformation Problem allows us to characterize the complexity
of a variety of tasks in quantum information processing, including decoding
noisy quantum channels, breaking falsifiable quantum cryptographic assumptions,
implementing optimal prover strategies in quantum interactive proofs, and
decoding the Hawking radiation of black holes. Our framework for unitary
complexity thus provides new avenues for studying the computational complexity
of many natural quantum information processing tasks.


------------------------------------------------------------------------------

Title:
Off the Radar: Uncertainty-Aware Radar Place Recognition with  Introspective Querying and Map Maintenance

Abstract: Localisation with Frequency-Modulated Continuous-Wave (FMCW) radar has gained
increasing interest due to its inherent resistance to challenging environments.
However, complex artefacts of the radar measurement process require appropriate
uncertainty estimation to ensure the safe and reliable application of this
promising sensor modality. In this work, we propose a multi-session map
management system which constructs the best maps for further localisation based
on learned variance properties in an embedding space. Using the same variance
properties, we also propose a new way to introspectively reject localisation
queries that are likely to be incorrect. For this, we apply robust noise-aware
metric learning, which both leverages the short-timescale variability of radar
data along a driven path (for data augmentation) and predicts the downstream
uncertainty in metric-space-based place recognition. We prove the effectiveness
of our method over extensive cross-validated tests of the Oxford Radar RobotCar
and MulRan dataset. In this, we outperform the current state-of-the-art in
radar place recognition and other uncertainty-aware methods when using only
single nearest-neighbour queries. We also show consistent performance increases
when rejecting queries based on uncertainty over a difficult test environment,
which we did not observe for a competing uncertainty-aware place recognition
system.


------------------------------------------------------------------------------

Title:
RXFOOD: Plug-in RGB-X Fusion for Object of Interest Detection

Abstract: The emergence of different sensors (Near-Infrared, Depth, etc.) is a remedy
for the limited application scenarios of traditional RGB camera. The RGB-X
tasks, which rely on RGB input and another type of data input to resolve
specific problems, have become a popular research topic in multimedia. A
crucial part in two-branch RGB-X deep neural networks is how to fuse
information across modalities. Given the tremendous information inside RGB-X
networks, previous works typically apply naive fusion (e.g., average or max
fusion) or only focus on the feature fusion at the same scale(s). While in this
paper, we propose a novel method called RXFOOD for the fusion of features
across different scales within the same modality branch and from different
modality branches simultaneously in a unified attention mechanism. An Energy
Exchange Module is designed for the interaction of each feature map's energy
matrix, who reflects the inter-relationship of different positions and
different channels inside a feature map. The RXFOOD method can be easily
incorporated to any dual-branch encoder-decoder network as a plug-in module,
and help the original backbone network better focus on important positions and
channels for object of interest detection. Experimental results on RGB-NIR
salient object detection, RGB-D salient object detection, and RGBFrequency
image manipulation detection demonstrate the clear effectiveness of the
proposed RXFOOD.


------------------------------------------------------------------------------

Title:
Sleptsov Nets are Turing-complete

Abstract: The present paper proves that a Sleptsov net (SN) is Turing-complete, that
considerably improves, with a brief construct, the previous result that a
strong SN is Turing-complete. Remind that, unlike Petri nets, an SN always
fires enabled transitions at their maximal firing multiplicity, as a single
step, leaving for a nondeterministic choice of which fireable transitions to
fire. A strong SN restricts nondeterministic choice to firing only the
transitions having the highest firing multiplicity.


------------------------------------------------------------------------------

Title:
Stock Price Prediction using Dynamic Neural Networks

Abstract: This paper will analyze and implement a time series dynamic neural network to
predict daily closing stock prices. Neural networks possess unsurpassed
abilities in identifying underlying patterns in chaotic, non-linear, and
seemingly random data, thus providing a mechanism to predict stock price
movements much more precisely than many current techniques. Contemporary
methods for stock analysis, including fundamental, technical, and regression
techniques, are conversed and paralleled with the performance of neural
networks. Also, the Efficient Market Hypothesis (EMH) is presented and
contrasted with Chaos theory using neural networks. This paper will refute the
EMH and support Chaos theory. Finally, recommendations for using neural
networks in stock price prediction will be presented.


------------------------------------------------------------------------------

Title:
Resources and Evaluations for Multi-Distribution Dense Information  Retrieval

Abstract: We introduce and define the novel problem of multi-distribution information
retrieval (IR) where given a query, systems need to retrieve passages from
within multiple collections, each drawn from a different distribution. Some of
these collections and distributions might not be available at training time. To
evaluate methods for multi-distribution retrieval, we design three benchmarks
for this task from existing single-distribution datasets, namely, a dataset
based on question answering and two based on entity matching. We propose simple
methods for this task which allocate the fixed retrieval budget (top-k
passages) strategically across domains to prevent the known domains from
consuming most of the budget. We show that our methods lead to an average of
3.8+ and up to 8.0 points improvements in Recall@100 across the datasets and
that improvements are consistent when fine-tuning different base retrieval
models. Our benchmarks are made publicly available.


------------------------------------------------------------------------------

Title:
Exploring the Role of Audio in Video Captioning

Abstract: Recent focus in video captioning has been on designing architectures that can
consume both video and text modalities, and using large-scale video datasets
with text transcripts for pre-training, such as HowTo100M. Though these
approaches have achieved significant improvement, the audio modality is often
ignored in video captioning. In this work, we present an audio-visual
framework, which aims to fully exploit the potential of the audio modality for
captioning. Instead of relying on text transcripts extracted via automatic
speech recognition (ASR), we argue that learning with raw audio signals can be
more beneficial, as audio has additional information including acoustic events,
speaker identity, etc. Our contributions are twofold. First, we observed that
the model overspecializes to the audio modality when pre-training with both
video and audio modality, since the ground truth (i.e., text transcripts) can
be solely predicted using audio. We proposed a Modality Balanced Pre-training
(MBP) loss to mitigate this issue and significantly improve the performance on
downstream tasks. Second, we slice and dice different design choices of the
cross-modal module, which may become an information bottleneck and generate
inferior results. We proposed new local-global fusion mechanisms to improve
information exchange across audio and video. We demonstrate significant
improvements by leveraging the audio modality on four datasets, and even
outperform the state of the art on some metrics without relying on the text
modality as the input.


------------------------------------------------------------------------------

Title:
Can a single image processing algorithm work equally well across all  phases of DCE-MRI?

Abstract: Image segmentation and registration are said to be challenging when applied
to dynamic contrast enhanced MRI sequences (DCE-MRI). The contrast agent causes
rapid changes in intensity in the region of interest and elsewhere, which can
lead to false positive predictions for segmentation tasks and confound the
image registration similarity metric. While it is widely assumed that contrast
changes increase the difficulty of these tasks, to our knowledge no work has
quantified these effects. In this paper we examine the effect of training with
different ratios of contrast enhanced (CE) data on two popular tasks:
segmentation with nnU-Net and Mask R-CNN and registration using VoxelMorph and
VTN. We experimented further by strategically using the available datasets
through pretraining and fine tuning with different splits of data. We found
that to create a generalisable model, pretraining with CE data and fine tuning
with non-CE data gave the best result. This interesting find could be expanded
to other deep learning based image processing tasks with DCE-MRI and provide
significant improvements to the models performance.


------------------------------------------------------------------------------

Title:
DreamEdit: Subject-driven Image Editing

Abstract: Subject-driven image generation aims at generating images containing
customized subjects, which has recently drawn enormous attention from the
research community. However, the previous works cannot precisely control the
background and position of the target subject. In this work, we aspire to fill
the void and propose two novel subject-driven sub-tasks, i.e., Subject
Replacement and Subject Addition. The new tasks are challenging in multiple
aspects: replacing a subject with a customized one can change its shape,
texture, and color, while adding a target subject to a designated position in a
provided scene necessitates a context-aware posture. To conquer these two novel
tasks, we first manually curate a new dataset DreamEditBench containing 22
different types of subjects, and 440 source images with different difficulty
levels. We plan to host DreamEditBench as a platform and hire trained
evaluators for standard human evaluation. We also devise an innovative method
DreamEditor to resolve these tasks by performing iterative generation, which
enables a smooth adaptation to the customized subject. In this project, we
conduct automatic and human evaluations to understand the performance of
DreamEditor and baselines on DreamEditBench. For Subject Replacement, we found
that the existing models are sensitive to the shape and color of the original
subject. The model failure rate will dramatically increase when the source and
target subjects are highly different. For Subject Addition, we found that the
existing models cannot easily blend the customized subjects into the background
smoothly, leading to noticeable artifacts in the generated image. We hope
DreamEditBench can become a standard platform to enable future investigations
toward building more controllable subject-driven image editing. Our project
homepage is this https URL


------------------------------------------------------------------------------

Title:
NTT-Based Polynomial Modular Multiplication for Homomorphic Encryption:  A Tutorial

Abstract: Homomorphic Encryption (HE) allows any third party to operate on the
encrypted data without decrypting it in advance. For the majority of HE
schemes, the multiplicative depth of circuits is the main practical limitation
in performing computations over encrypted data. Hence, Homomorphic
multiplication is one of the most important components of homomorphic
encryption. Since most of the HE schemes are constructed from the ring-learning
with errors (R-LWE) problem. Efficient polynomial modular multiplication
implementation becomes critical. This work consists of describing various
approaches to implementing polynomial modular multiplication based on number
theoretic transform.


------------------------------------------------------------------------------

Title:
Identifying and Disentangling Spurious Features in Pretrained Image  Representations

Abstract: Neural networks employ spurious correlations in their predictions, resulting
in decreased performance when these correlations do not hold. Recent works
suggest fixing pretrained representations and training a classification head
that does not use spurious features. We investigate how spurious features are
represented in pretrained representations and explore strategies for removing
information about spurious features. Considering the Waterbirds dataset and a
few pretrained representations, we find that even with full knowledge of
spurious features, their removal is not straightforward due to entangled
representation. To address this, we propose a linear autoencoder training
method to separate the representation into core, spurious, and other features.
We propose two effective spurious feature removal approaches that are applied
to the encoding and significantly improve classification performance measured
by worst group accuracy.


------------------------------------------------------------------------------

Title:
Wind Noise Reduction with a Diffusion-based Stochastic Regeneration  Model

Abstract: In this paper we present a method for single-channel wind noise reduction
using our previously proposed diffusion-based stochastic regeneration model
combining predictive and generative modelling. We introduce a non-additive
speech in noise model to account for the non-linear deformation of the membrane
caused by the wind flow and possible clipping. We show that our stochastic
regeneration model outperforms other neural-network-based wind noise reduction
methods as well as purely predictive and generative models, on a dataset using
simulated and real-recorded wind noise. We further show that the proposed
method generalizes well by testing on an unseen dataset with real-recorded wind
noise. Audio samples, data generation scripts and code for the proposed methods
can be found online (this https URL).


------------------------------------------------------------------------------

Title:
Generative Multimodal Entity Linking

Abstract: Multimodal Entity Linking (MEL) is the task of mapping mentions with
multimodal contexts to the referent entities from a knowledge base (e.g.,
Wikipedia). Prior MEL methods mainly focus on designing complex multimodal
interaction mechanisms and require fine-tuning all model parameters, which can
be prohibitively costly and difficult to scale in the era of Large Language
Models (LLMs). In this work, we propose GEMEL, a simple yet effective
Generative Multimodal Entity Linking method, which leverages the capabilities
of LLMs from large-scale pre-training to directly generate target entity names.
We keep the vision and language model frozen and only train a linear layer to
enable cross-modality interactions. To adapt LLMs to the MEL task, we take
advantage of the emerging in-context learning (ICL) capability of LLMs by
retrieving multimodal instances as demonstrations. Extensive experiments show
that with only ~0.3% of the model parameters fine-tuned, GEMEL achieves
state-of-the-art results on two well-established MEL datasets (4.1% accuracy
gains on WikiDiverse and 15.4% accuracy gains on WikiMEL). Our approach is
compatible with any off-the-shelf language model, paving the way towards an
efficient and general solution for utilizing LLMs in the MEL task.


------------------------------------------------------------------------------

Title:
Deep Language Networks: Joint Prompt Training of Stacked LLMs using  Variational Inference

Abstract: We view large language models (LLMs) as stochastic \emph{language layers} in
a network, where the learnable parameters are the natural language
\emph{prompts} at each layer. We stack two such layers, feeding the output of
one layer to the next. We call the stacked architecture a \emph{Deep Language
Network} (DLN). We first show how to effectively perform prompt optimization
for a 1-Layer language network (DLN-1). We then show how to train 2-layer DLNs
(DLN-2), where two prompts must be learnt. We consider the output of the first
layer as a latent variable to marginalize, and devise a variational inference
algorithm for joint prompt training. A DLN-2 reaches higher performance than a
single layer, sometimes comparable to few-shot GPT-4 even when each LLM in the
network is smaller and less powerful. The DLN code is open source:
this https URL .


------------------------------------------------------------------------------

Title:
LPFormer: LiDAR Pose Estimation Transformer with Multi-Task Network

Abstract: In this technical report, we present the 1st place solution for the 2023
Waymo Open Dataset Pose Estimation challenge. Due to the difficulty of
acquiring large-scale 3D human keypoint annotation, previous methods have
commonly relied on 2D image features and 2D sequential annotations for 3D human
pose estimation. In contrast, our proposed method, named LPFormer, uses only
LiDAR as its input along with its corresponding 3D annotations. LPFormer
consists of two stages: the first stage detects the human bounding box and
extracts multi-level feature representations, while the second stage employs a
transformer-based network to regress the human keypoints using these features.
Experimental results on the Waymo Open Dataset demonstrate the top performance,
and improvements even compared to previous multi-modal solutions.


------------------------------------------------------------------------------

Title:
RobustNeuralNetworks.jl: a Package for Machine Learning and Data-Driven  Control with Certified Robustness

Abstract: Neural networks are typically sensitive to small input perturbations, leading
to unexpected or brittle behaviour. We present RobustNeuralNetworks.jl: a Julia
package for neural network models that are constructed to naturally satisfy a
set of user-defined robustness constraints. The package is based on the
recently proposed Recurrent Equilibrium Network (REN) and Lipschitz-Bounded
Deep Network (LBDN) model classes, and is designed to interface directly with
Julia's most widely-used machine learning package, Flux.jl. We discuss the
theory behind our model parameterization, give an overview of the package, and
provide a tutorial demonstrating its use in image classification, reinforcement
learning, and nonlinear state-observer design.


------------------------------------------------------------------------------

Title:
Density Uncertainty Layers for Reliable Uncertainty Estimation

Abstract: Assessing the predictive uncertainty of deep neural networks is crucial for
safety-related applications of deep learning. Although Bayesian deep learning
offers a principled framework for estimating model uncertainty, the approaches
that are commonly used to approximate the posterior often fail to deliver
reliable estimates of predictive uncertainty. In this paper we propose a novel
criterion for predictive uncertainty, that a model's predictive variance should
be grounded in the empirical density of the input. It should produce higher
uncertainty for inputs that are improbable in the training data and lower
uncertainty for those inputs that are more probable. To operationalize this
criterion, we develop the density uncertainty layer, an architectural element
for a stochastic neural network that guarantees that the density uncertain
criterion is satisfied. We study neural networks with density uncertainty
layers on the CIFAR-10 and CIFAR-100 uncertainty benchmarks. Compared to
existing approaches, we find that density uncertainty layers provide reliable
uncertainty estimates and robust out-of-distribution detection performance.


------------------------------------------------------------------------------

Title:
Robust Statistical Comparison of Random Variables with Locally Varying  Scale of Measurement

Abstract: Spaces with locally varying scale of measurement, like multidimensional
structures with differently scaled dimensions, are pretty common in statistics
and machine learning. Nevertheless, it is still understood as an open question
how to exploit the entire information encoded in them properly. We address this
problem by considering an order based on (sets of) expectations of random
variables mapping into such non-standard spaces. This order contains stochastic
dominance and expectation order as extreme cases when no, or respectively
perfect, cardinal structure is given. We derive a (regularized) statistical
test for our proposed generalized stochastic dominance (GSD) order,
operationalize it by linear optimization, and robustify it by imprecise
probability models. Our findings are illustrated with data from
multidimensional poverty measurement, finance, and medicine.


------------------------------------------------------------------------------

Title:
Neural Spectro-polarimetric Fields

Abstract: Modeling the spatial radiance distribution of light rays in a scene has been
extensively explored for applications, including view synthesis. Spectrum and
polarization, the wave properties of light, are often neglected due to their
integration into three RGB spectral bands and their non-perceptibility to human
vision. Despite this, these properties encompass substantial material and
geometric information about a scene. In this work, we propose to model
spectro-polarimetric fields, the spatial Stokes-vector distribution of any
light ray at an arbitrary wavelength. We present Neural Spectro-polarimetric
Fields (NeSpoF), a neural representation that models the physically-valid
Stokes vector at given continuous variables of position, direction, and
wavelength. NeSpoF manages inherently noisy raw measurements, showcases memory
efficiency, and preserves physically vital signals, factors that are crucial
for representing the high-dimensional signal of a spectro-polarimetric field.
To validate NeSpoF, we introduce the first multi-view
hyperspectral-polarimetric image dataset, comprised of both synthetic and
real-world scenes. These were captured using our compact
hyperspectral-polarimetric imaging system, which has been calibrated for
robustness against system imperfections. We demonstrate the capabilities of
NeSpoF on diverse scenes.


------------------------------------------------------------------------------

Title:
On Exploring Node-feature and Graph-structure Diversities for Node Drop  Graph Pooling

Abstract: A pooling operation is essential for effective graph-level representation
learning, where the node drop pooling has become one mainstream graph pooling
technology. However, current node drop pooling methods usually keep the top-k
nodes according to their significance scores, which ignore the graph diversity
in terms of the node features and the graph structures, thus resulting in
suboptimal graph-level representations. To address the aforementioned issue, we
propose a novel plug-and-play score scheme and refer to it as MID, which
consists of a \textbf{M}ultidimensional score space with two operations,
\textit{i.e.}, fl\textbf{I}pscore and \textbf{D}ropscore. Specifically, the
multidimensional score space depicts the significance of nodes through multiple
criteria; the flipscore encourages the maintenance of dissimilar node features;
and the dropscore forces the model to notice diverse graph structures instead
of being stuck in significant local structures. To evaluate the effectiveness
of our proposed MID, we perform extensive experiments by applying it to a wide
variety of recent node drop pooling methods, including TopKPool, SAGPool,
GSAPool, and ASAP. Specifically, the proposed MID can efficiently and
consistently achieve about 2.8\% average improvements over the above four
methods on seventeen real-world graph classification datasets, including four
social datasets (IMDB-BINARY, IMDB-MULTI, REDDIT-BINARY, and COLLAB), and
thirteen biochemical datasets (D\&D, PROTEINS, NCI1, MUTAG, PTC-MR, NCI109,
ENZYMES, MUTAGENICITY, FRANKENSTEIN, HIV, BBBP, TOXCAST, and TOX21). Code is
available at~\url{this https URL}.


------------------------------------------------------------------------------

Title:
Triggering Dark Showers with Conditional Dual Auto-Encoders

Abstract: Auto-encoders (AEs) have the potential to be effective and generic tools for
new physics searches at colliders, requiring little to no model-dependent
assumptions. New hypothetical physics signals can be considered anomalies that
deviate from the well-known background processes generally expected to describe
the whole dataset. We present a search formulated as an anomaly detection (AD)
problem, using an AE to define a criterion to decide about the physics nature
of an event. In this work, we perform an AD search for manifestations of a dark
version of strong force using raw detector images, which are large and very
sparse, without leveraging any physics-based pre-processing or assumption on
the signals. We propose a dual-encoder design which can learn a compact latent
space through conditioning. In the context of multiple AD metrics, we present a
clear improvement over competitive baselines and prior approaches. It is the
first time that an AE is shown to exhibit excellent discrimination against
multiple dark shower models, illustrating the suitability of this method as a
performant, model-independent algorithm to deploy, e.g., in the trigger stage
of LHC experiments such as ATLAS and CMS.


------------------------------------------------------------------------------

Title:
Generating Synergistic Formulaic Alpha Collections via Reinforcement  Learning

Abstract: In the field of quantitative trading, it is common practice to transform raw
historical stock data into indicative signals for the market trend. Such
signals are called alpha factors. Alphas in formula forms are more
interpretable and thus favored by practitioners concerned with risk. In
practice, a set of formulaic alphas is often used together for better modeling
precision, so we need to find synergistic formulaic alpha sets that work well
together. However, most traditional alpha generators mine alphas one by one
separately, overlooking the fact that the alphas would be combined later. In
this paper, we propose a new alpha-mining framework that prioritizes mining a
synergistic set of alphas, i.e., it directly uses the performance of the
downstream combination model to optimize the alpha generator. Our framework
also leverages the strong exploratory capabilities of reinforcement
learning~(RL) to better explore the vast search space of formulaic alphas. The
contribution to the combination models' performance is assigned to be the
return used in the RL process, driving the alpha generator to find better
alphas that improve upon the current set. Experimental evaluations on
real-world stock market data demonstrate both the effectiveness and the
efficiency of our framework for stock trend forecasting. The investment
simulation results show that our framework is able to achieve higher returns
compared to previous approaches.


------------------------------------------------------------------------------

Title:
Stress-induced Artificial neuron spiking in Diffusive memristors

Abstract: Diffusive memristors owing to their ability to emulate neurons spiking
behavior are competitive candidates for next-generation neuromorphic elements.
Current studies on diffusive memristors are largely based on memristors
deposited on rigid substrates that require the application of a constant
electrical field for their neuron-like spiking behavior. We report here Ag
nanoparticle-based diffusive memristor developed on a flexible PET film. The
flexibility of the memristor combined with an external impact results in the
deformation of the memristor which induces a change in capacitance in the
device. By changing the impact magnitude and time interval between consecutive
impacts, we are able to regulate the spiking frequency of our diffusive
memristor. We have proposed a mathematical model which demonstrates a good
qualitative agreement with the experiments and helps to explain the spiking
behavior of our diffusive memristors. These results indicate a potential
strategy to realize flexible and deformable memristive devices for the
development of next-generation in-memory computing sensors and AI technologies.


------------------------------------------------------------------------------

Title:
Lightweight learning from label proportions on satellite imagery

Abstract: This work addresses the challenge of producing chip level predictions on
satellite imagery when only label proportions at a coarser spatial geometry are
available, typically from statistical or aggregated data from administrative
divisions (such as municipalities or communes). This kind of tabular data is
usually widely available in many regions of the world and application areas
and, thus, its exploitation may contribute to leverage the endemic scarcity of
fine grained labelled data in Earth Observation (EO). This can be framed as a
Learning from Label Proportions (LLP) problem setup. LLP applied to EO data is
still an emerging field and performing comparative studies in applied scenarios
remains a challenge due to the lack of standardized datasets. In this work,
first, we show how simple deep learning and probabilistic methods generally
perform better than standard more complex ones, providing a surprising level of
finer grained spatial detail when trained with much coarser label proportions.
Second, we provide a set of benchmarking datasets enabling comparative LLP
applied to EO, providing both fine grained labels and aggregated data according
to existing administrative divisions. Finally, we argue how this approach might
be valuable when considering on-orbit inference and training. Source code is
available at this https URL


------------------------------------------------------------------------------

Title:
Hand Pose Estimation with Mems-Ultrasonic Sensors

Abstract: Hand tracking is an important aspect of human-computer interaction and has a
wide range of applications in extended reality devices. However, current hand
motion capture methods suffer from various limitations. For instance,
visual-based hand pose estimation is susceptible to self-occlusion and changes
in lighting conditions, while IMU-based tracking gloves experience significant
drift and are not resistant to external magnetic field interference. To address
these issues, we propose a novel and low-cost hand-tracking glove that utilizes
several MEMS-ultrasonic sensors attached to the fingers, to measure the
distance matrix among the sensors. Our lightweight deep network then
reconstructs the hand pose from the distance matrix. Our experimental results
demonstrate that this approach is both accurate, size-agnostic, and robust to
external interference. We also show the design logic for the sensor selection,
sensor configurations, circuit diagram, as well as model architecture.


------------------------------------------------------------------------------

Title:
NoRefER: a Referenceless Quality Metric for Automatic Speech Recognition  via Semi-Supervised Language Model Fine-Tuning with Contrastive Learning

Abstract: This paper introduces NoRefER, a novel referenceless quality metric for
automatic speech recognition (ASR) systems. Traditional reference-based metrics
for evaluating ASR systems require costly ground-truth transcripts. NoRefER
overcomes this limitation by fine-tuning a multilingual language model for
pair-wise ranking ASR hypotheses using contrastive learning with Siamese
network architecture. The self-supervised NoRefER exploits the known quality
relationships between hypotheses from multiple compression levels of an ASR for
learning to rank intra-sample hypotheses by quality, which is essential for
model comparisons. The semi-supervised version also uses a referenced dataset
to improve its inter-sample quality ranking, which is crucial for selecting
potentially erroneous samples. The results indicate that NoRefER correlates
highly with reference-based metrics and their intra-sample ranks, indicating a
high potential for referenceless ASR evaluation or a/b testing.


------------------------------------------------------------------------------

Title:
Investigating Poor Performance Regions of Black Boxes: LIME-based  Exploration in Sepsis Detection

Abstract: Interpreting machine learning models remains a challenge, hindering their
adoption in clinical settings. This paper proposes leveraging Local
Interpretable Model-Agnostic Explanations (LIME) to provide interpretable
descriptions of black box classification models in high-stakes sepsis
detection. By analyzing misclassified instances, significant features
contributing to suboptimal performance are identified. The analysis reveals
regions where the classifier performs poorly, allowing the calculation of error
rates within these regions. This knowledge is crucial for cautious
decision-making in sepsis detection and other critical applications. The
proposed approach is demonstrated using the eICU dataset, effectively
identifying and visualizing regions where the classifier underperforms. By
enhancing interpretability, our method promotes the adoption of machine
learning models in clinical practice, empowering informed decision-making and
mitigating risks in critical scenarios.


------------------------------------------------------------------------------

Title:
Uniqueness of Iris Pattern Based on AR Model

Abstract: The assessment of iris uniqueness plays a crucial role in analyzing the
capabilities and limitations of iris recognition systems. Among the various
methodologies proposed, Daugman's approach to iris uniqueness stands out as one
of the most widely accepted. According to Daugman, uniqueness refers to the
iris recognition system's ability to enroll an increasing number of classes
while maintaining a near-zero probability of collision between new and enrolled
classes. Daugman's approach involves creating distinct IrisCode templates for
each iris class within the system and evaluating the sustainable population
under a fixed Hamming distance between codewords. In our previous work [23], we
utilized Rate-Distortion Theory (as it pertains to the limits of
error-correction codes) to establish boundaries for the maximum possible
population of iris classes supported by Daugman's IrisCode, given the
constraint of a fixed Hamming distance between codewords. Building upon that
research, we propose a novel methodology to evaluate the scalability of an iris
recognition system, while also measuring iris quality. We achieve this by
employing a sphere-packing bound for Gaussian codewords and adopting a approach
similar to Daugman's, which utilizes relative entropy as a distance measure
between iris classes. To demonstrate the efficacy of our methodology, we
illustrate its application on two small datasets of iris images. We determine
the sustainable maximum population for each dataset based on the quality of the
images. By providing these illustrations, we aim to assist researchers in
comprehending the limitations inherent in their recognition systems, depending
on the quality of their iris databases.


------------------------------------------------------------------------------

Title:
Machine-Learning-Assisted and Real-Time-Feedback-Controlled Growth of  InAs/GaAs Quantum Dots

Abstract: Self-assembled InAs/GaAs quantum dots (QDs) have properties highly valuable
for developing various optoelectronic devices such as QD lasers and single
photon sources. The applications strongly rely on the density and quality of
these dots, which has motivated studies of the growth process control to
realize high-quality epi-wafers and devices. Establishing the process
parameters in molecular beam epitaxy (MBE) for a specific density of QDs is a
multidimensional optimization challenge, usually addressed through
time-consuming and iterative trial-and-error. Meanwhile, reflective high-energy
electron diffraction (RHEED) has been widely used to capture a wealth of growth
information in situ. However, it still faces the challenges of extracting
information from noisy and overlapping images. Here, based on 3D ResNet, we
developed a machine learning (ML) model specially designed for training RHEED
videos instead of static images and providing real-time feedback on surface
morphologies for process control. We demonstrated that ML from previous growth
could predict the post-growth density of QDs, by successfully tuning the QD
densities in near-real time from 1.5E10 cm-2 down to 3.8E8 cm-2 or up to 1.4
E11 cm-2. Compared to traditional methods, our approach, with in-situ tuning
capabilities and excellent reliability, can dramatically expedite the material
optimization process and improve the reproducibility of MBE growth,
constituting significant progress for thin film growth techniques. The concepts
and methodologies proved feasible in this work are promising to be applied to a
variety of material growth processes, which will revolutionize semiconductor
manufacturing for microelectronic and optoelectronic industries.


------------------------------------------------------------------------------

Title:
Efficient Deep Spiking Multi-Layer Perceptrons with Multiplication-Free  Inference

Abstract: Advancements in adapting deep convolution architectures for Spiking Neural
Networks (SNNs) have significantly enhanced image classification performance
and reduced computational burdens. However, the inability of
Multiplication-Free Inference (MFI) to harmonize with attention and transformer
mechanisms, which are critical to superior performance on high-resolution
vision tasks, imposes limitations on these gains. To address this, our research
explores a new pathway, drawing inspiration from the progress made in
Multi-Layer Perceptrons (MLPs). We propose an innovative spiking MLP
architecture that uses batch normalization to retain MFI compatibility and
introduces a spiking patch encoding layer to reinforce local feature extraction
capabilities. As a result, we establish an efficient multi-stage spiking MLP
network that effectively blends global receptive fields with local feature
extraction for comprehensive spike-based computation. Without relying on
pre-training or sophisticated SNN training techniques, our network secures a
top-1 accuracy of 66.39% on the ImageNet-1K dataset, surpassing the directly
trained spiking ResNet-34 by 2.67%. Furthermore, we curtail computational
costs, model capacity, and simulation steps. An expanded version of our network
challenges the performance of the spiking VGG-16 network with a 71.64% top-1
accuracy, all while operating with a model capacity 2.1 times smaller. Our
findings accentuate the potential of our deep SNN architecture in seamlessly
integrating global and local learning abilities. Interestingly, the trained
receptive field in our network mirrors the activity patterns of cortical cells.


------------------------------------------------------------------------------

Title:
FLAG: Finding Line Anomalies (in code) with Generative AI

Abstract: Code contains security and functional bugs. The process of identifying and
localizing them is difficult and relies on human labor. In this work, we
present a novel approach (FLAG) to assist human debuggers. FLAG is based on the
lexical capabilities of generative AI, specifically, Large Language Models
(LLMs). Here, we input a code file then extract and regenerate each line within
that file for self-comparison. By comparing the original code with an
LLM-generated alternative, we can flag notable differences as anomalies for
further inspection, with features such as distance from comments and LLM
confidence also aiding this classification. This reduces the inspection search
space for the designer. Unlike other automated approaches in this area, FLAG is
language-agnostic, can work on incomplete (and even non-compiling) code and
requires no creation of security properties, functional tests or definition of
rules. In this work, we explore the features that help LLMs in this
classification and evaluate the performance of FLAG on known bugs. We use 121
benchmarks across C, Python and Verilog; with each benchmark containing a known
security or functional weakness. We conduct the experiments using two state of
the art LLMs in OpenAI's code-davinci-002 and gpt-3.5-turbo, but our approach
may be used by other models. FLAG can identify 101 of the defects and helps
reduce the search space to 12-17% of source code.


------------------------------------------------------------------------------

Title:
Local 3D Editing via 3D Distillation of CLIP Knowledge

Abstract: 3D content manipulation is an important computer vision task with many
real-world applications (e.g., product design, cartoon generation, and 3D
Avatar editing). Recently proposed 3D GANs can generate diverse photorealistic
3D-aware contents using Neural Radiance fields (NeRF). However, manipulation of
NeRF still remains a challenging problem since the visual quality tends to
degrade after manipulation and suboptimal control handles such as 2D semantic
maps are used for manipulations. While text-guided manipulations have shown
potential in 3D editing, such approaches often lack locality. To overcome these
problems, we propose Local Editing NeRF (LENeRF), which only requires text
inputs for fine-grained and localized manipulation. Specifically, we present
three add-on modules of LENeRF, the Latent Residual Mapper, the Attention Field
Network, and the Deformation Network, which are jointly used for local
manipulations of 3D features by estimating a 3D attention field. The 3D
attention field is learned in an unsupervised way, by distilling the zero-shot
mask generation capability of CLIP to the 3D space with multi-view guidance. We
conduct diverse experiments and thorough evaluations both quantitatively and
qualitatively.


------------------------------------------------------------------------------

Title:
The Cost of Informing Decision-Makers in Multi-Agent Maximum Coverage  Problems with Random Resource Values

Abstract: The emergent behavior of a distributed system is conditioned by the
information available to the local decision-makers. Therefore, one may expect
that providing decision-makers with more information will improve system
performance; in this work, we find that this is not necessarily the case. In
multi-agent maximum coverage problems, we find that even when agents'
objectives are aligned with the global welfare, informing agents about the
realization of the resource's random values can reduce equilibrium performance
by a factor of 1/2. This affirms an important aspect of designing distributed
systems: information need be shared carefully. We further this understanding by
providing lower and upper bounds on the ratio of system welfare when
information is (fully or partially) revealed and when it is not, termed the
value-of-informing. We then identify a trade-off that emerges when optimizing
the performance of the best-case and worst-case equilibrium.


------------------------------------------------------------------------------

Title:
SEAL: Simultaneous Exploration and Localization in Multi-Robot Systems

Abstract: The availability of accurate localization is critical for multi-robot
exploration strategies; noisy or inconsistent localization causes failure in
meeting exploration objectives. We aim to achieve high localization accuracy
with contemporary exploration map belief and vice versa without needing global
localization information. This paper proposes a novel simultaneous exploration
and localization (SEAL) approach, which uses Gaussian Processes (GP)-based
information fusion for maximum exploration while performing communication graph
optimization for relative localization. Both these cross-dependent objectives
were integrated through the Rao-Blackwellization technique. Distributed
linearized convex hull optimization is used to select the next-best unexplored
region for distributed exploration. SEAL outperformed cutting-edge methods on
exploration and localization performance in extensive ROS-Gazebo simulations,
illustrating the practicality of the approach in real-world applications.


------------------------------------------------------------------------------

Title:
StrainNet: Predicting crystal structure elastic properties using  SE(3)-equivariant graph neural networks

Abstract: Accurately predicting the elastic properties of crystalline solids is vital
for computational materials science. However, traditional atomistic scale ab
initio approaches are computationally intensive, especially for studying
complex materials with a large number of atoms in a unit cell. We introduce a
novel data-driven approach to efficiently predict the elastic properties of
crystal structures using SE(3)-equivariant graph neural networks (GNNs). This
approach yields important scalar elastic moduli with the accuracy comparable to
recent data-driven studies. Importantly, our symmetry-aware GNNs model also
enables the prediction of the strain energy density (SED) and the associated
elastic constants, the fundamental tensorial quantities that are significantly
influenced by a material's crystallographic group. The model consistently
distinguishes independent elements of SED tensors, in accordance with the
symmetry of the crystal structures. Finally, our deep learning model possesses
meaningful latent features, offering an interpretable prediction of the elastic
properties.


------------------------------------------------------------------------------

Title:
An efficient and straightforward online quantization method for a data  stream through remove-birth updating

Abstract: The growth of network-connected devices is creating an explosion of data,
known as big data, and posing significant challenges to efficient data
analysis. This data is generated continuously, creating a dynamic flow known as
a data stream. The characteristics of a data stream may change dynamically, and
this change is known as concept drift. Consequently, a method for handling data
streams must efficiently reduce their volume while dynamically adapting to
these changing characteristics. This paper proposes a simple online vector
quantization method for concept drift. The proposed method identifies and
replaces units with low win probability through remove-birth updating, thus
achieving a rapid adaptation to concept drift. Furthermore, the results of this
study show that the proposed method can generate minimal dead units even in the
presence of concept drift. This study also suggests that some metrics
calculated from the proposed method will be helpful for drift detection.


------------------------------------------------------------------------------

Title:
Conditional Generators for Limit Order Book Environments:  Explainability, Challenges, and Robustness

Abstract: Limit order books are a fundamental and widespread market mechanism. This
paper investigates the use of conditional generative models for order book
simulation. For developing a trading agent, this approach has drawn recent
attention as an alternative to traditional backtesting due to its ability to
react to the presence of the trading agent. Using a state-of-the-art CGAN (from
Coletta et al. (2022)), we explore its dependence upon input features, which
highlights both strengths and weaknesses. To do this, we use "adversarial
attacks" on the model's features and its mechanism. We then show how these
insights can be used to improve the CGAN, both in terms of its realism and
robustness. We finish by laying out a roadmap for future work.


------------------------------------------------------------------------------

Title:
Improving Long-Horizon Imitation Through Instruction Prediction

Abstract: Complex, long-horizon planning and its combinatorial nature pose steep
challenges for learning-based agents. Difficulties in such settings are
exacerbated in low data regimes where over-fitting stifles generalization and
compounding errors hurt accuracy. In this work, we explore the use of an often
unused source of auxiliary supervision: language. Inspired by recent advances
in transformer-based models, we train agents with an instruction prediction
loss that encourages learning temporally extended representations that operate
at a high level of abstraction. Concretely, we demonstrate that instruction
modeling significantly improves performance in planning environments when
training with a limited number of demonstrations on the BabyAI and Crafter
benchmarks. In further analysis we find that instruction modeling is most
important for tasks that require complex reasoning, while understandably
offering smaller gains in environments that require simple plans. More details
and code can be found at this https URL


------------------------------------------------------------------------------

Title:
FuXi: A cascade machine learning forecasting system for 15-day global  weather forecast

Abstract: Over the past few years, due to the rapid development of machine learning
(ML) models for weather forecasting, state-of-the-art ML models have shown
superior performance compared to the European Centre for Medium-Range Weather
Forecasts (ECMWF)'s high-resolution forecast (HRES) in 10-day forecasts at a
spatial resolution of 0.25 degree. However, the challenge remains to perform
comparably to the ECMWF ensemble mean (EM) in 15-day forecasts. Previous
studies have demonstrated the importance of mitigating the accumulation of
forecast errors for effective long-term forecasts. Despite numerous efforts to
reduce accumulation errors, including autoregressive multi-time step loss,
using a single model is found to be insufficient to achieve optimal performance
in both short and long lead times. Therefore, we present FuXi, a cascaded ML
weather forecasting system that provides 15-day global forecasts with a
temporal resolution of 6 hours and a spatial resolution of 0.25 degree. FuXi is
developed using 39 years of the ECMWF ERA5 reanalysis dataset. The performance
evaluation, based on latitude-weighted root mean square error (RMSE) and
anomaly correlation coefficient (ACC), demonstrates that FuXi has comparable
forecast performance to ECMWF EM in 15-day forecasts, making FuXi the first
ML-based weather forecasting system to accomplish this achievement.


------------------------------------------------------------------------------

Title:
Learning Unseen Modality Interaction

Abstract: Multimodal learning assumes all modality combinations of interest are
available during training to learn cross-modal correspondences. In this paper,
we challenge this modality-complete assumption for multimodal learning and
instead strive for generalization to unseen modality combinations during
inference. We pose the problem of unseen modality interaction and introduce a
first solution. It exploits a feature projection module to project the
multidimensional features of different modalities into a common space with rich
information reserved. This allows the information to be accumulated with a
simple summation operation across available modalities. To reduce overfitting
to unreliable modality combinations during training, we further improve the
model learning with pseudo-supervision indicating the reliability of a
modality's prediction. We demonstrate that our approach is effective for
diverse tasks and modalities by evaluating it for multimodal video
classification, robot state regression, and multimedia retrieval.


------------------------------------------------------------------------------

Title:
Constant Memory Attention Block

Abstract: Modern foundation model architectures rely on attention mechanisms to
effectively capture context. However, these methods require linear or quadratic
memory in terms of the number of inputs/datapoints, limiting their
applicability in low-compute domains. In this work, we propose Constant Memory
Attention Block (CMAB), a novel general-purpose attention block that computes
its output in constant memory and performs updates in constant computation.
Highlighting CMABs efficacy, we introduce methods for Neural Processes and
Temporal Point Processes. Empirically, we show our proposed methods achieve
results competitive with state-of-the-art while being significantly more memory
efficient.


------------------------------------------------------------------------------

Title:
Improved Financial Forecasting via Quantum Machine Learning

Abstract: Quantum algorithms have the potential to enhance machine learning across a
variety of domains and applications. In this work, we show how quantum machine
learning can be used to improve financial forecasting. First, we use classical
and quantum Determinantal Point Processes to enhance Random Forest models for
churn prediction, improving precision by almost 6%. Second, we design quantum
neural network architectures with orthogonal and compound layers for credit
risk assessment, which match classical performance with significantly fewer
parameters. Our results demonstrate that leveraging quantum ideas can
effectively enhance the performance of machine learning, both today as
quantum-inspired classical ML solutions, and even more in the future, with the
advent of better quantum hardware.


------------------------------------------------------------------------------

Title:
Provable Routing Analysis of Programmable Photonics

Abstract: Programmable photonic integrated circuits (PPICs) are an emerging technology
recently proposed as an alternative to custom-designed application-specific
integrated photonics. Light routing is one of the most important functions that
need to be realized on a PPIC. Previous literature has investigated the light
routing problem from an algorithmic or experimental perspective, e.g., adopting
graph theory to route an optical signal. In this paper, we also focus on the
light routing problem, but from a complementary and theoretical perspective, to
answer questions about what is possible to be routed. Specifically, we
demonstrate that not all path lengths (defined as the number of tunable basic
units that an optical signal traverses) can be realized on a square-mesh PPIC,
and a rigorous realizability condition is proposed and proved. We further
consider multi-path routing, where we provide an analytical expression on path
length sum, upper bounds on path length mean/variance, and the maximum number
of realizable paths. All of our conclusions are proven mathematically.
Illustrative potential optical applications using our observations are also
presented.


------------------------------------------------------------------------------

Title:
Pushing the Limits of Machine Design: Automated CPU Design with AI

Abstract: Design activity -- constructing an artifact description satisfying given
goals and constraints -- distinguishes humanity from other animals and
traditional machines, and endowing machines with design abilities at the human
level or beyond has been a long-term pursuit. Though machines have already
demonstrated their abilities in designing new materials, proteins, and computer
programs with advanced artificial intelligence (AI) techniques, the search
space for designing such objects is relatively small, and thus, "Can machines
design like humans?" remains an open question. To explore the boundary of
machine design, here we present a new AI approach to automatically design a
central processing unit (CPU), the brain of a computer, and one of the world's
most intricate devices humanity have ever designed. This approach generates the
circuit logic, which is represented by a graph structure called Binary
Speculation Diagram (BSD), of the CPU design from only external input-output
observations instead of formal program code. During the generation of BSD,
Monte Carlo-based expansion and the distance of Boolean functions are used to
guarantee accuracy and efficiency, respectively. By efficiently exploring a
search space of unprecedented size 10^{10^{540}}, which is the largest one of
all machine-designed objects to our best knowledge, and thus pushing the limits
of machine design, our approach generates an industrial-scale RISC-V CPU within
only 5 hours. The taped-out CPU successfully runs the Linux operating system
and performs comparably against the human-designed Intel 80486SX CPU. In
addition to learning the world's first CPU only from input-output observations,
which may reform the semiconductor industry by significantly reducing the
design cycle, our approach even autonomously discovers human knowledge of the
von Neumann architecture.


------------------------------------------------------------------------------

Title:
MPSTAN: Metapopulation-based Spatio-Temporal Attention Network for  Epidemic Forecasting

Abstract: Accurate epidemic forecasting plays a vital role for governments in
developing effective prevention measures for suppressing epidemics. Most of the
present spatio-temporal models cannot provide a general framework for stable,
and accurate forecasting of epidemics with diverse evolution trends.
Incorporating epidemiological domain knowledge ranging from single-patch to
multi-patch into neural networks is expected to improve forecasting accuracy.
However, relying solely on single-patch knowledge neglects inter-patch
interactions, while constructing multi-patch knowledge is challenging without
population mobility data. To address the aforementioned problems, we propose a
novel hybrid model called Metapopulation-based Spatio-Temporal Attention
Network (MPSTAN). This model aims to improve the accuracy of epidemic
forecasting by incorporating multi-patch epidemiological knowledge into a
spatio-temporal model and adaptively defining inter-patch interactions.
Moreover, we incorporate inter-patch epidemiological knowledge into both the
model construction and loss function to help the model learn epidemic
transmission dynamics. Extensive experiments conducted on two representative
datasets with different epidemiological evolution trends demonstrate that our
proposed model outperforms the baselines and provides more accurate and stable
short- and long-term forecasting. We confirm the effectiveness of domain
knowledge in the learning model and investigate the impact of different ways of
integrating domain knowledge on forecasting. We observe that using domain
knowledge in both model construction and loss functions leads to more efficient
forecasting, and selecting appropriate domain knowledge can improve accuracy
further.


------------------------------------------------------------------------------

Title:
SoftGPT: Learn Goal-oriented Soft Object Manipulation Skills by  Generative Pre-trained Heterogeneous Graph Transformer

Abstract: Soft object manipulation tasks in domestic scenes pose a significant
challenge for existing robotic skill learning techniques due to their complex
dynamics and variable shape characteristics. Since learning new manipulation
skills from human demonstration is an effective way for robot applications,
developing prior knowledge of the representation and dynamics of soft objects
is necessary. In this regard, we propose a pre-trained soft object manipulation
skill learning model, namely SoftGPT, that is trained using large amounts of
exploration data, consisting of a three-dimensional heterogeneous graph
representation and a GPT-based dynamics model. For each downstream task, a
goal-oriented policy agent is trained to predict the subsequent actions, and
SoftGPT generates the consequences of these actions. Integrating these two
approaches establishes a thinking process in the robot's mind that provides
rollout for facilitating policy learning. Our results demonstrate that
leveraging prior knowledge through this thinking process can efficiently learn
various soft object manipulation skills, with the potential for direct learning
from human demonstrations.


------------------------------------------------------------------------------

Title:
Knowledge Distillation via Token-level Relationship Graph

Abstract: Knowledge distillation is a powerful technique for transferring knowledge
from a pre-trained teacher model to a student model. However, the true
potential of knowledge transfer has not been fully explored. Existing
approaches primarily focus on distilling individual information or
instance-level relationships, overlooking the valuable information embedded in
token-level relationships, which may be particularly affected by the long-tail
effects. To address the above limitations, we propose a novel method called
Knowledge Distillation with Token-level Relationship Graph (TRG) that leverages
the token-wise relational knowledge to enhance the performance of knowledge
distillation. By employing TRG, the student model can effectively emulate
higher-level semantic information from the teacher model, resulting in improved
distillation results. To further enhance the learning process, we introduce a
token-wise contextual loss called contextual loss, which encourages the student
model to capture the inner-instance semantic contextual of the teacher model.
We conduct experiments to evaluate the effectiveness of the proposed method
against several state-of-the-art approaches. Empirical results demonstrate the
superiority of TRG across various visual classification tasks, including those
involving imbalanced data. Our method consistently outperforms the existing
baselines, establishing a new state-of-the-art performance in the field of
knowledge distillation.


------------------------------------------------------------------------------

Title:
Towards Regulatable AI Systems: Technical Gaps and Policy Opportunities

Abstract: There is increasing attention being given to how to regulate AI systems. As
governing bodies grapple with what values to encapsulate into regulation, we
consider the technical half of the question: To what extent can AI experts vet
an AI system for adherence to regulatory requirements? We investigate this
question through two public sector procurement checklists, identifying what we
can do now, what we should be able to do with technical innovation in AI, and
what requirements necessitate a more interdisciplinary approach.


------------------------------------------------------------------------------

Title:
Stability analysis of an implicit and explicit numerical method for  Volterra integro-differential equations with kernel K(x,y(t),t)

Abstract: We present implicit and explicit versions of a numerical algorithm for
solving a Volterra integro-differential equation. These algorithms are an
extension of our previous work, and cater for a kernel of general form. We use
an appropriate test equation to study the stability of both algorithms,,
numerically deriving stability regions. The region for the implicit method
appears to be unbounded, while the explicit has a bounded region close to the
origin. We perform a few calculations to demonstrate our results.


------------------------------------------------------------------------------

Title:
Implicit spoken language diarization

Abstract: Spoken language diarization (LD) and related tasks are mostly explored using
the phonotactic approach. Phonotactic approaches mostly use explicit way of
language modeling, hence requiring intermediate phoneme modeling and
transcribed data. Alternatively, the ability of deep learning approaches to
model temporal dynamics may help for the implicit modeling of language
information through deep embedding vectors. Hence this work initially explores
the available speaker diarization frameworks that capture speaker information
implicitly to perform LD tasks. The performance of the LD system on synthetic
code-switch data using the end-to-end x-vector approach is 6.78% and 7.06%, and
for practical data is 22.50% and 60.38%, in terms of diarization error rate and
Jaccard error rate (JER), respectively. The performance degradation is due to
the data imbalance and resolved to some extent by using pre-trained wave2vec
embeddings that provide a relative improvement of 30.74% in terms of JER.


------------------------------------------------------------------------------

Title:
OptIForest: Optimal Isolation Forest for Anomaly Detection

Abstract: Anomaly detection plays an increasingly important role in various fields for
critical tasks such as intrusion detection in cybersecurity, financial risk
detection, and human health monitoring. A variety of anomaly detection methods
have been proposed, and a category based on the isolation forest mechanism
stands out due to its simplicity, effectiveness, and efficiency, e.g., iForest
is often employed as a state-of-the-art detector for real deployment. While the
majority of isolation forests use the binary structure, a framework LSHiForest
has demonstrated that the multi-fork isolation tree structure can lead to
better detection performance. However, there is no theoretical work answering
the fundamentally and practically important question on the optimal tree
structure for an isolation forest with respect to the branching factor. In this
paper, we establish a theory on isolation efficiency to answer the question and
determine the optimal branching factor for an isolation tree. Based on the
theoretical underpinning, we design a practical optimal isolation forest
OptIForest incorporating clustering based learning to hash which enables more
information to be learned from data for better isolation quality. The rationale
of our approach relies on a better bias-variance trade-off achieved by bias
reduction in OptIForest. Extensive experiments on a series of benchmarking
datasets for comparative and ablation studies demonstrate that our approach can
efficiently and robustly achieve better detection performance in general than
the state-of-the-arts including the deep learning based methods.


------------------------------------------------------------------------------

Title:
FFCV: Accelerating Training by Removing Data Bottlenecks

Abstract: We present FFCV, a library for easy and fast machine learning model training.
FFCV speeds up model training by eliminating (often subtle) data bottlenecks
from the training process. In particular, we combine techniques such as an
efficient file storage format, caching, data pre-loading, asynchronous data
transfer, and just-in-time compilation to (a) make data loading and transfer
significantly more efficient, ensuring that GPUs can reach full utilization;
and (b) offload as much data processing as possible to the CPU asynchronously,
freeing GPU cycles for training. Using FFCV, we train ResNet-18 and ResNet-50
on the ImageNet dataset with competitive tradeoff between accuracy and training
time. For example, we are able to train an ImageNet ResNet-50 model to 75\% in
only 20 mins on a single machine. We demonstrate FFCV's performance,
ease-of-use, extensibility, and ability to adapt to resource constraints
through several case studies. Detailed installation instructions,
documentation, and Slack support channel are available at this https URL .


------------------------------------------------------------------------------

Title:
Verifying Global Neural Network Specifications using Hyperproperties

Abstract: Current approaches to neural network verification focus on specifications
that target small regions around known input data points, such as local
robustness. Thus, using these approaches, we can not obtain guarantees for
inputs that are not close to known inputs. Yet, it is highly likely that a
neural network will encounter such truly unseen inputs during its application.
We study global specifications that - when satisfied - provide guarantees for
all potential inputs. We introduce a hyperproperty formalism that allows for
expressing global specifications such as monotonicity, Lipschitz continuity,
global robustness, and dependency fairness. Our formalism enables verifying
global specifications using existing neural network verification approaches by
leveraging capabilities for verifying general computational graphs. Thereby, we
extend the scope of guarantees that can be provided using existing methods.
Recent success in verifying specific global specifications shows that attaining
strong guarantees for all potential data points is feasible.


------------------------------------------------------------------------------

Title:
Toward Automated Detection of Microbleeds with Anatomical Scale  Localization: A Complete Clinical Diagnosis Support Using Deep Learning

Abstract: Cerebral Microbleeds (CMBs) are chronic deposits of small blood products in
the brain tissues, which have explicit relation to various cerebrovascular
diseases depending on their anatomical location, including cognitive decline,
intracerebral hemorrhage, and cerebral infarction. However, manual detection of
CMBs is a time-consuming and error-prone process because of their sparse and
tiny structural properties. The detection of CMBs is commonly affected by the
presence of many CMB mimics that cause a high false-positive rate (FPR), such
as calcification and pial vessels. This paper proposes a novel 3D deep learning
framework that does not only detect CMBs but also inform their anatomical
location in the brain (i.e., lobar, deep, and infratentorial regions). For the
CMB detection task, we propose a single end-to-end model by leveraging the
U-Net as a backbone with Region Proposal Network (RPN). To significantly reduce
the FPs within the same single model, we develop a new scheme, containing
Feature Fusion Module (FFM) that detects small candidates utilizing contextual
information and Hard Sample Prototype Learning (HSPL) that mines CMB mimics and
generates additional loss term called concentration loss using Convolutional
Prototype Learning (CPL). The anatomical localization task does not only tell
to which region the CMBs belong but also eliminate some FPs from the detection
task by utilizing anatomical information. The results show that the proposed
RPN that utilizes the FFM and HSPL outperforms the vanilla RPN and achieves a
sensitivity of 94.66% vs. 93.33% and an average number of false positives per
subject (FPavg) of 0.86 vs. 14.73. Also, the anatomical localization task
further improves the detection performance by reducing the FPavg to 0.56 while
maintaining the sensitivity of 94.66%.


------------------------------------------------------------------------------

Title:
On Evaluation of Document Classification using RVL-CDIP

Abstract: The RVL-CDIP benchmark is widely used for measuring performance on the task
of document classification. Despite its widespread use, we reveal several
undesirable characteristics of the RVL-CDIP benchmark. These include (1)
substantial amounts of label noise, which we estimate to be 8.1% (ranging
between 1.6% to 16.9% per document category); (2) presence of many ambiguous or
multi-label documents; (3) a large overlap between test and train splits, which
can inflate model performance metrics; and (4) presence of sensitive
personally-identifiable information like US Social Security numbers (SSNs). We
argue that there is a risk in using RVL-CDIP for benchmarking document
classifiers, as its limited scope, presence of errors (state-of-the-art models
now achieve accuracy error rates that are within our estimated label error
rate), and lack of diversity make it less than ideal for benchmarking. We
further advocate for the creation of a new document classification benchmark,
and provide recommendations for what characteristics such a resource should
include.


------------------------------------------------------------------------------

Title:
High order entropy stable discontinuous Galerkin spectral element  methods through subcell limiting

Abstract: Subcell limiting strategies for discontinuous Galerkin spectral element
methods do not provably satisfy a semi-discrete cell entropy inequality. In
this work, we introduce an extension to the subcell limiting strategy that
satisfies the semi-discrete cell entropy inequality by formulating the limiting
factors as solutions to an optimization problem. The optimization problem is
efficiently solved using a deterministic greedy algorithm. We also discuss the
extension of the proposed subcell limiting strategy to preserve general convex
constraints. Numerical experiments confirm that the proposed limiting strategy
preserves high-order accuracy for smooth solutions and satisfies the cell
entropy inequality.


------------------------------------------------------------------------------

Title:
Mixed-model Sequencing with Stochastic Failures: A Case Study for  Automobile Industry

Abstract: In the automotive industry, the sequence of vehicles to be produced is
determined ahead of the production day. However, there are some vehicles,
failed vehicles, that cannot be produced due to some reasons such as material
shortage or paint failure. These vehicles are pulled out of the sequence, and
the vehicles in the succeeding positions are moved forward, potentially
resulting in challenges for logistics or other scheduling concerns.
This paper proposes a two-stage stochastic program for the mixed-model
sequencing (MMS) problem with stochastic product failures, and provides
improvements to the second-stage problem. To tackle the exponential number of
scenarios, we employ the sample average approximation approach and two solution
methodologies. On one hand, we develop an L-shaped decomposition-based
algorithm, where the computational experiments show its superiority over
solving the deterministic equivalent formulation with an off-the-shelf solver.
Moreover, we provide a tabu search algorithm in addition to a greedy heuristic
to tackle case study instances inspired by our car manufacturer partner.
Numerical experiments show that the proposed solution methodologies generate
high quality solutions by utilizing a sample of scenarios. Particularly, a
robust sequence that is generated by considering car failures can decrease the
expected work overload by more than 20\% for both small- and large-sized
instances.


------------------------------------------------------------------------------

Title:
Inferring the finest pattern of mutual independence from data

Abstract: For a random variable $X$, we are interested in the blind extraction of its
finest mutual independence pattern $\mu ( X )$. We introduce a specific kind of
independence that we call dichotomic. If $\Delta ( X )$ stands for the set of
all patterns of dichotomic independence that hold for $X$, we show that $\mu (
X )$ can be obtained as the intersection of all elements of $\Delta ( X )$. We
then propose a method to estimate $\Delta ( X )$ when the data are independent
and identically (i.i.d.) realizations of a multivariate normal distribution. If
$\hat{\Delta} ( X )$ is the estimated set of valid patterns of dichotomic
independence, we estimate $\mu ( X )$ as the intersection of all patterns of
$\hat{\Delta} ( X )$. The method is tested on simulated data, showing its
advantages and limits. We also consider an application to a toy example as well
as to experimental data.


------------------------------------------------------------------------------

Title:
Mitigating Discrimination in Insurance with Wasserstein Barycenters

Abstract: The insurance industry is heavily reliant on predictions of risks based on
characteristics of potential customers. Although the use of said models is
common, researchers have long pointed out that such practices perpetuate
discrimination based on sensitive features such as gender or race. Given that
such discrimination can often be attributed to historical data biases, an
elimination or at least mitigation is desirable. With the shift from more
traditional models to machine-learning based predictions, calls for greater
mitigation have grown anew, as simply excluding sensitive variables in the
pricing process can be shown to be ineffective. In this article, we first
investigate why predictions are a necessity within the industry and why
correcting biases is not as straightforward as simply identifying a sensitive
variable. We then propose to ease the biases through the use of Wasserstein
barycenters instead of simple scaling. To demonstrate the effects and
effectiveness of the approach we employ it on real data and discuss its
implications.


------------------------------------------------------------------------------

Title:
Morphological Inflection with Phonological Features

Abstract: Recent years have brought great advances into solving morphological tasks,
mostly due to powerful neural models applied to various tasks as (re)inflection
and analysis. Yet, such morphological tasks cannot be considered solved,
especially when little training data is available or when generalizing to
previously unseen lemmas. This work explores effects on performance obtained
through various ways in which morphological models get access to subcharacter
phonological features that are the targets of morphological processes. We
design two methods to achieve this goal: one that leaves models as is but
manipulates the data to include features instead of characters, and another
that manipulates models to take phonological features into account when
building representations for phonemes. We elicit phonemic data from standard
graphemic data using language-specific grammars for languages with shallow
grapheme-to-phoneme mapping, and we experiment with two reinflection models
over eight languages. Our results show that our methods yield comparable
results to the grapheme-based baseline overall, with minor improvements in some
of the languages. All in all, we conclude that patterns in character
distributions are likely to allow models to infer the underlying phonological
characteristics, even when phonemes are not explicitly represented.


------------------------------------------------------------------------------

Title:
Semi-Implicit Denoising Diffusion Models (SIDDMs)

Abstract: Despite the proliferation of generative models, achieving fast sampling
during inference without compromising sample diversity and quality remains
challenging. Existing models such as Denoising Diffusion Probabilistic Models
(DDPM) deliver high-quality, diverse samples but are slowed by an inherently
high number of iterative steps. The Denoising Diffusion Generative Adversarial
Networks (DDGAN) attempted to circumvent this limitation by integrating a GAN
model for larger jumps in the diffusion process. However, DDGAN encountered
scalability limitations when applied to large datasets. To address these
limitations, we introduce a novel approach that tackles the problem by matching
implicit and explicit factors. More specifically, our approach involves
utilizing an implicit model to match the marginal distributions of noisy data
and the explicit conditional distribution of the forward diffusion. This
combination allows us to effectively match the joint denoising distributions.
Unlike DDPM but similar to DDGAN, we do not enforce a parametric distribution
for the reverse step, enabling us to take large steps during inference. Similar
to the DDPM but unlike DDGAN, we take advantage of the exact form of the
diffusion process. We demonstrate that our proposed method obtains comparable
generative performance to diffusion-based models and vastly superior results to
models with a small number of sampling steps.


------------------------------------------------------------------------------

Title:
MQ-Coder inspired arithmetic coder for synthetic DNA data storage

Abstract: Over the past years, the ever-growing trend on data storage demand, more
specifically for "cold" data (i.e. rarely accessed), has motivated research for
alternative systems of data storage. Because of its biochemical
characteristics, synthetic DNA molecules are now considered as serious
candidates for this new kind of storage. This paper introduces a novel
arithmetic coder for DNA data storage, and presents some results on a lossy
JPEG 2000 based image compression method adapted for DNA data storage that uses
this novel coder.
The DNA coding algorithms presented here have been designed to efficiently
compress images, encode them into a quaternary code, and finally store them
into synthetic DNA molecules. This work also aims at making the compression
models better fit the problematic that we encounter when storing data into DNA,
namely the fact that the DNA writing, storing and reading methods are error
prone processes.
The main take away of this work is our arithmetic coder and it's integration
into a performant image codec.


------------------------------------------------------------------------------

Title:
Carbon nanotube neurotransistors with ambipolar memory and learning  functions

Abstract: In recent years, neuromorphic computing has gained attention as a promising
approach to enhance computing efficiency. Among existing approaches,
neurotransistors have emerged as a particularly promising option as they
accurately represent neuron structure, integrating the plasticity of synapses
along with that of the neuronal membrane. An ambipolar character could offer
designers more flexibility in customizing the charge flow to construct circuits
of higher complexity. We propose a novel design for an ambipolar neuromorphic
transistor, utilizing carbon nanotubes as the semiconducting channel and an
ion-doped sol-gel as the polarizable gate dielectric. Due to its tunability and
high dielectric constant, the sol-gel effectively modulates the conductivity of
nanotubes, leading to efficient and controllable short-term potentiation and
depression. Experimental results indicate that the proposed design achieves
reliable and tunable synaptic responses with low power consumption. Our
findings suggest that the method can potentially provide an efficient solution
for realizing more adaptable cognitive computing systems.


------------------------------------------------------------------------------

Title:
Don't be so Monotone: Relaxing Stochastic Line Search in  Over-Parameterized Models

Abstract: Recent works have shown that line search methods can speed up Stochastic
Gradient Descent (SGD) and Adam in modern over-parameterized settings. However,
existing line searches may take steps that are smaller than necessary since
they require a monotone decrease of the (mini-)batch objective function. We
explore nonmonotone line search methods to relax this condition and possibly
accept larger step sizes. Despite the lack of a monotonic decrease, we prove
the same fast rates of convergence as in the monotone case. Our experiments
show that nonmonotone methods improve the speed of convergence and
generalization properties of SGD/Adam even beyond the previous monotone line
searches. We propose a POlyak NOnmonotone Stochastic (PoNoS) method, obtained
by combining a nonmonotone line search with a Polyak initial step size.
Furthermore, we develop a new resetting technique that in the majority of the
iterations reduces the amount of backtracks to zero while still maintaining a
large initial step size. To the best of our knowledge, a first runtime
comparison shows that the epoch-wise advantage of line-search-based methods
gets reflected in the overall computational time.


------------------------------------------------------------------------------

Title:
Super-Resolution of BVOC Emission Maps Via Domain Adaptation

Abstract: Enhancing the resolution of Biogenic Volatile Organic Compound (BVOC)
emission maps is a critical task in remote sensing. Recently, some
Super-Resolution (SR) methods based on Deep Learning (DL) have been proposed,
leveraging data from numerical simulations for their training process. However,
when dealing with data derived from satellite observations, the reconstruction
is particularly challenging due to the scarcity of measurements to train SR
algorithms with. In our work, we aim at super-resolving low resolution emission
maps derived from satellite observations by leveraging the information of
emission maps obtained through numerical simulations. To do this, we combine a
SR method based on DL with Domain Adaptation (DA) techniques, harmonizing the
different aggregation strategies and spatial information used in simulated and
observed domains to ensure compatibility. We investigate the effectiveness of
DA strategies at different stages by systematically varying the number of
simulated and observed emissions used, exploring the implications of data
scarcity on the adaptation strategies. To the best of our knowledge, there are
no prior investigations of DA in satellite-derived BVOC maps enhancement. Our
work represents a first step toward the development of robust strategies for
the reconstruction of observed BVOC emissions.


------------------------------------------------------------------------------

Title:
Counting occurrences of patterns in permutations

Abstract: We develop a new, powerful method for counting elements in a {\em multiset.}
As a first application, we use this algorithm to study the number of
occurrences of patterns in a permutation. For patterns of length 3 there are
two Wilf classes, and the general behaviour of these is reasonably well-known.
We slightly extend some of the known results in that case, and exhaustively
study the case of patterns of length 4, about which there is little previous
knowledge. For such patterns, there are seven Wilf classes, and based on
extensive enumerations and careful series analysis, we have conjectured the
asymptotic behaviour for all classes.
Finally, we investigate a proposal of Blitvi\'c and Steingr\'imsson as to the
range of a parameter for which a particular generating function formed from the
occurrence sequences is itself a Stieltjes moment sequence.


------------------------------------------------------------------------------

Title:
Restoration of the JPEG Maximum Lossy Compressed Face Images with  Hourglass Block based on Early Stopping Discriminator

Abstract: When a JPEG image is compressed using the loss compression method with a high
compression rate, a blocking phenomenon can occur in the image, making it
necessary to restore the image to its original quality. In particular,
restoring compressed images that are unrecognizable presents an innovative
challenge. Therefore, this paper aims to address the restoration of JPEG images
that have suffered significant loss due to maximum compression using a
GAN-based net-work method. The generator in this network is based on the U-Net
architecture and features a newly presented hourglass structure that can
preserve the charac-teristics of deep layers. Additionally, the network
incorporates two loss functions, LF Loss and HF Loss, to generate natural and
high-performance images. HF Loss uses a pretrained VGG-16 network and is
configured using a specific layer that best represents features, which can
enhance performance for the high-frequency region. LF Loss, on the other hand,
is used to handle the low-frequency region. These two loss functions facilitate
the generation of images by the generator that can deceive the discriminator
while accurately generating both high and low-frequency regions. The results
show that the blocking phe-nomenon in lost compressed images was removed, and
recognizable identities were generated. This study represents a significant
improvement over previous research in terms of image restoration performance.


------------------------------------------------------------------------------

Title:
A prior regularized full waveform inversion using generative diffusion  models

Abstract: Full waveform inversion (FWI) has the potential to provide high-resolution
subsurface model estimations. However, due to limitations in observation, e.g.,
regional noise, limited shots or receivers, and band-limited data, it is hard
to obtain the desired high-resolution model with FWI. To address this
challenge, we propose a new paradigm for FWI regularized by generative
diffusion models. Specifically, we pre-train a diffusion model in a fully
unsupervised manner on a prior velocity model distribution that represents our
expectations of the subsurface and then adapt it to the seismic observations by
incorporating the FWI into the sampling process of the generative diffusion
models. What makes diffusion models uniquely appropriate for such an
implementation is that the generative process retains the form and dimensions
of the velocity model. Numerical examples demonstrate that our method can
outperform the conventional FWI with only negligible additional computational
cost. Even in cases of very sparse observations or observations with strong
noise, the proposed method could still reconstruct a high-quality subsurface
model. Thus, we can incorporate our prior expectations of the solutions in an
efficient manner. We further test this approach on field data, which
demonstrates the effectiveness of the proposed method.


------------------------------------------------------------------------------

Title:
Physical informed neural networks with soft and hard boundary  constraints for solving advection-diffusion equations using Fourier  expansions

Abstract: Deep learning methods have gained considerable interest in the numerical
solution of various partial differential equations (PDEs). One particular focus
is on physics-informed neural networks (PINNs), which integrate physical
principles into neural networks. This transforms the process of solving PDEs
into optimization problems for neural networks. In order to address a
collection of advection-diffusion equations (ADE) in a range of difficult
circumstances, this paper proposes a novel network structure. This architecture
integrates the solver, which is a multi-scale deep neural network (MscaleDNN)
utilized in the PINN method, with a hard constraint technique known as HCPINN.
This method introduces a revised formulation of the desired solution for
advection-diffusion equations (ADE) by utilizing a loss function that
incorporates the residuals of the governing equation and penalizes any
deviations from the specified boundary and initial constraints. By surpassing
the boundary constraints automatically, this method improves the accuracy and
efficiency of the PINN technique. To address the ``spectral bias'' phenomenon
in neural networks, a subnetwork structure of MscaleDNN and a Fourier-induced
activation function are incorporated into the HCPINN, resulting in a hybrid
approach called SFHCPINN. The effectiveness of SFHCPINN is demonstrated through
various numerical experiments involving advection-diffusion equations (ADE) in
different dimensions. The numerical results indicate that SFHCPINN outperforms
both standard PINN and its subnetwork version with Fourier feature embedding.
It achieves remarkable accuracy and efficiency while effectively handling
complex boundary conditions and high-frequency scenarios in ADE.


------------------------------------------------------------------------------

Title:
Modeling T1 Resting-State MRI Variants Using Convolutional Neural  Networks in Diagnosis of OCD

Abstract: Obsessive-compulsive disorder (OCD) presents itself as a highly debilitating
disorder. The disorder has common associations with the prefrontal cortex and
the glutamate receptor known as Metabotropic Glutamate Receptor 5 (mGluR5).
This receptor has been observed to demonstrate higher levels of signaling from
positron emission tomography scans measured by its distribution volume ratios
in mice. Despite this evidence, studies are unable to fully verify the
involvement of mGluR5 as more empirical data is needed. Computational modeling
methods were used as a means of validation for previous hypotheses involving
mGluR5. The inadequacies in relation to the causal factor of OCD were answered
by utilizing T1 resting-state magnetic resonance imaging (TRS-MRI) scans of
patients suffering from schizophrenia, major depressive disorder, and
obsessive-compulsive disorder. Because comorbid cases often occur within these
disorders, cross-comparative abilities become necessary to find distinctive
characteristics. Two-dimensional convolutional neural networks alongside
ResNet50 and MobileNet models were constructed and evaluated for efficiency.
Activation heatmaps of TRS-MRI scans were outputted, allowing for
transcriptomics analysis. Though, a lack of ability to predict OCD cases
prevented gene expression analysis. Across all models, there was an 88.75\%
validation accuracy for MDD, and 82.08\% validation accuracy for SZD under the
framework of ResNet50 as well as novel computation. OCD yielded an accuracy
rate of ~54.4\%. These results provided further evidence for the p factor
theorem regarding mental disorders. Future work involves the application of
transfer learning to bolster accuracy rates.


------------------------------------------------------------------------------

Title:
Interferometric lensless imaging: rank-one projections of image  frequencies with speckle illuminations

Abstract: Lensless illumination single-pixel imaging with a multicore fiber (MCF) is a
computational imaging technique that enables potential endoscopic observations
of biological samples at cellular scale. In this work, we show that this
technique is tantamount to collecting multiple symmetric rank-one projections
(SROP) of an interferometric matrix--a matrix encoding the spectral content of
the sample image. In this model, each SROP is induced by the complex sketching
vector shaping the incident light wavefront with a spatial light modulator
(SLM), while the projected interferometric matrix collects up to $O(Q^2)$ image
frequencies for a $Q$-core MCF. While this scheme subsumes previous sensing
modalities, such as raster scanning (RS) imaging with beamformed illumination,
we demonstrate that collecting the measurements of $M$ random SLM
configurations--and thus acquiring $M$ SROPs--allows us to estimate an image of
interest if $M$ and $Q$ scale log-linearly with the image sparsity level This
demonstration is achieved both theoretically, with a specific restricted
isometry analysis of the sensing scheme, and with extensive Monte Carlo
experiments. On a practical side, we perform a single calibration of the
sensing system robust to certain deviations to the theoretical model and
independent of the sketching vectors used during the imaging phase.
Experimental results made on an actual MCF system demonstrate the effectiveness
of this imaging procedure on a benchmark image.


------------------------------------------------------------------------------

Title:
Towards quantum enhanced adversarial robustness in machine learning

Abstract: Machine learning algorithms are powerful tools for data driven tasks such as
image classification and feature detection, however their vulnerability to
adversarial examples - input samples manipulated to fool the algorithm -
remains a serious challenge. The integration of machine learning with quantum
computing has the potential to yield tools offering not only better accuracy
and computational efficiency, but also superior robustness against adversarial
attacks. Indeed, recent work has employed quantum mechanical phenomena to
defend against adversarial attacks, spurring the rapid development of the field
of quantum adversarial machine learning (QAML) and potentially yielding a new
source of quantum advantage. Despite promising early results, there remain
challenges towards building robust real-world QAML tools. In this review we
discuss recent progress in QAML and identify key challenges. We also suggest
future research directions which could determine the route to practicality for
QAML approaches as quantum computing hardware scales up and noise levels are
reduced.


------------------------------------------------------------------------------

Title:
Fitted Value Iteration Methods for Bicausal Optimal Transport

Abstract: We develop a fitted value iteration (FVI) method to compute bicausal optimal
transport (OT) where couplings have an adapted structure. Based on the dynamic
programming formulation, FVI adopts a function class to approximate the value
functions in bicausal OT. Under the concentrability condition and approximate
completeness assumption, we prove the sample complexity using (local)
Rademacher complexity. Furthermore, we demonstrate that multilayer neural
networks with appropriate structures satisfy the crucial assumptions required
in sample complexity proofs. Numerical experiments reveal that FVI outperforms
linear programming and adapted Sinkhorn methods in scalability as the time
horizon increases, while still maintaining acceptable accuracy.


------------------------------------------------------------------------------

Title:
Efficient quantum image representation and compression circuit using  zero-discarded state preparation approach

Abstract: Quantum image computing draws a lot of attention due to storing and
processing image data faster than classical. With increasing the image size,
the number of connections also increases, leading to the circuit complex.
Therefore, efficient quantum image representation and compression issues are
still challenging. The encoding of images for representation and compression in
quantum systems is different from classical ones. In quantum, encoding of
position is more concerned which is the major difference from the classical. In
this paper, a novel zero-discarded state connection novel enhance quantum
representation (ZSCNEQR) approach is introduced to reduce complexity further by
discarding '0' in the location representation information. In the control
operational gate, only input '1' contribute to its output thus, discarding zero
makes the proposed ZSCNEQR circuit more efficient. The proposed ZSCNEQR
approach significantly reduced the required bit for both representation and
compression. The proposed method requires 11.76\% less qubits compared to the
recent existing method. The results show that the proposed approach is highly
effective for representing and compressing images compared to the two relevant
existing methods in terms of rate-distortion performance.


------------------------------------------------------------------------------

Title:
Neural Multigrid Memory For Computational Fluid Dynamics

Abstract: Turbulent flow simulation plays a crucial role in various applications,
including aircraft and ship design, industrial process optimization, and
weather prediction. In this paper, we propose an advanced data-driven method
for simulating turbulent flow, representing a significant improvement over
existing approaches.
Our methodology combines the strengths of Video Prediction Transformer (VPTR)
(Ye & Bilodeau, 2022) and Multigrid Architecture (MgConv, MgResnet) (Ke et al.,
2017). VPTR excels in capturing complex spatiotemporal dependencies and
handling large input data, making it a promising choice for turbulent flow
prediction. Meanwhile, Multigrid Architecture utilizes multiple grids with
different resolutions to capture the multiscale nature of turbulent flows,
resulting in more accurate and efficient simulations.
Through our experiments, we demonstrate the effectiveness of our proposed
approach, named MGxTransformer, in accurately predicting velocity, temperature,
and turbulence intensity for incompressible turbulent flows across various
geometries and flow conditions. Our results exhibit superior accuracy compared
to other baselines, while maintaining computational efficiency.


------------------------------------------------------------------------------

Title:
VisoGender: A dataset for benchmarking gender bias in image-text pronoun  resolution

Abstract: We introduce VisoGender, a novel dataset for benchmarking gender bias in
vision-language models. We focus on occupation-related gender biases, inspired
by Winograd and Winogender schemas, where each image is associated with a
caption containing a pronoun relationship of subjects and objects in the scene.
VisoGender is balanced by gender representation in professional roles,
supporting bias evaluation in two ways: i) resolution bias, where we evaluate
the difference between gender resolution accuracies for men and women and ii)
retrieval bias, where we compare ratios of male and female professionals
retrieved for a gender-neutral search query. We benchmark several
state-of-the-art vision-language models and find that they lack the reasoning
abilities to correctly resolve gender in complex scenes. While the direction
and magnitude of gender bias depends on the task and the model being evaluated,
captioning models generally are more accurate and less biased than CLIP-like
models. Dataset and code are available at this https URL


------------------------------------------------------------------------------

Title:
Hierarchical Neural Simulation-Based Inference Over Event Ensembles

Abstract: When analyzing real-world data it is common to work with event ensembles,
which comprise sets of observations that collectively constrain the parameters
of an underlying model of interest. Such models often have a hierarchical
structure, where "local" parameters impact individual events and "global"
parameters influence the entire dataset. We introduce practical approaches for
optimal dataset-wide probabilistic inference in cases where the likelihood is
intractable, but simulations can be realized via forward modeling. We construct
neural estimators for the likelihood(-ratio) or posterior and show that
explicitly accounting for the model's hierarchical structure can lead to
tighter parameter constraints. We ground our discussion using case studies from
the physical sciences, focusing on examples from particle physics (particle
collider data) and astrophysics (strong gravitational lensing observations).


------------------------------------------------------------------------------

Title:
Trotter error bounds and dynamic multi-product formulas for Hamiltonian  simulation

Abstract: Multi-product formulas are linear combinations of Trotter circuits offering
high-quality simulation of Hamiltonian time evolution with fewer Trotter steps.
Here we report two contributions aimed at making multi-product formulas more
viable for near-term quantum simulations. First, we extend the theory of
Trotter error with commutator scaling developed by Childs, Su, Tran et al. to
multi-product formulas. Our result implies that multi-product formulas can
achieve a quadratic reduction of Trotter error on arbitrary time intervals
compared with the regular product formulas without increasing the required
circuit depth or qubit connectivity. The number of circuit repetitions grows
only by a constant factor. Secondly, we introduce dynamic multi-product
formulas with time-dependent coefficients chosen to minimize a certain
efficiently computable proxy for the Trotter error. Numerical simulations
suggest that the error achieved by the dynamic multi-product formulas is close
to the optimal one.


------------------------------------------------------------------------------

Title:
Optimistic Active Exploration of Dynamical Systems

Abstract: Reinforcement learning algorithms commonly seek to optimize policies for
solving one particular task. How should we explore an unknown dynamical system
such that the estimated model allows us to solve multiple downstream tasks in a
zero-shot manner? In this paper, we address this challenge, by developing an
algorithm -- OPAX -- for active exploration. OPAX uses well-calibrated
probabilistic models to quantify the epistemic uncertainty about the unknown
dynamics. It optimistically -- w.r.t. to plausible dynamics -- maximizes the
information gain between the unknown dynamics and state observations. We show
how the resulting optimization problem can be reduced to an optimal control
problem that can be solved at each episode using standard approaches. We
analyze our algorithm for general models, and, in the case of Gaussian process
dynamics, we give a sample complexity bound and show that the epistemic
uncertainty converges to zero. In our experiments, we compare OPAX with other
heuristic active exploration approaches on several environments. Our
experiments show that OPAX is not only theoretically sound but also performs
well for zero-shot planning on novel downstream tasks.


------------------------------------------------------------------------------

Title:
Generalized Low-Rank Update: Model Parameter Bounds for Low-Rank  Training Data Modifications

Abstract: In this study, we have developed an incremental machine learning (ML) method
that efficiently obtains the optimal model when a small number of instances or
features are added or removed. This problem holds practical importance in model
selection, such as cross-validation (CV) and feature selection. Among the class
of ML methods known as linear estimators, there exists an efficient model
update framework called the low-rank update that can effectively handle changes
in a small number of rows and columns within the data matrix. However, for ML
methods beyond linear estimators, there is currently no comprehensive framework
available to obtain knowledge about the updated solution within a specific
computational complexity. In light of this, our study introduces a method
called the Generalized Low-Rank Update (GLRU) which extends the low-rank update
framework of linear estimators to ML methods formulated as a certain class of
regularized empirical risk minimization, including commonly used methods such
as SVM and logistic regression. The proposed GLRU method not only expands the
range of its applicability but also provides information about the updated
solutions with a computational complexity proportional to the amount of dataset
changes. To demonstrate the effectiveness of the GLRU method, we conduct
experiments showcasing its efficiency in performing cross-validation and
feature selection compared to other baseline methods.


------------------------------------------------------------------------------

Title:
On Error-detecting Open-locating-dominating sets

Abstract: An open-dominating set S for a graph G is a subset of vertices where every
vertex has a neighbor in S. An open-locating-dominating set S for a graph G is
an open-dominating set such that each pair of distinct vertices in G have
distinct set of open-neighbors in S. We consider a type of a fault-tolerant
open-locating dominating set called error-detecting open-locating-dominating
sets. We present more results on the topic including its NP-completeness proof,
extremal graphs, and a characterization of cubic graphs that permit an
error-detecting open-locating-dominating set.


------------------------------------------------------------------------------

Title:
Linear and Non-Linear Barrier Coverage in Deterministic and Uncertain  environment in WSNs: A New Classification

Abstract: This paper Various studies cited in the literature deal with the classic
problem of obstacle coverage, where the deployment environment, sensor nodes,
and base stations have characteristics that are considered perfect but suffer
from various flaws in the real world. This paper presents other barrier
coverage types ranked in a new classification based on linear and nonlinear
barrier coverage according to deterministic and insecure environments, and
enumerates some of the different current and future challenges of these
coverage types and connectivity in WSNs.


------------------------------------------------------------------------------

Title:
Finite-time Lyapunov exponents of deep neural networks

Abstract: We compute how small input perturbations affect the output of deep neural
networks, exploring an analogy between deep networks and dynamical systems,
where the growth or decay of local perturbations is characterised by
finite-time Lyapunov exponents. We show that the maximal exponent forms
geometrical structures in input space, akin to coherent structures in dynamical
systems. Ridges of large positive exponents divide input space into different
regions that the network associates with different classes. These ridges
visualise the geometry that deep networks construct in input space, shedding
light on the fundamental mechanisms underlying their learning capabilities.


------------------------------------------------------------------------------

Title:
Joint Chance-constrained Game for Coordinating Microgrids in Energy and  Reserve Markets: A Bayesian Optimization Approach

Abstract: Microgrids incorporate distributed energy resources (DERs) and flexible
loads, which can provide energy and reserve services for the main grid.
However, due to uncertain renewable generations such as solar power, microgrids
might under-deliver reserve services and breach day-ahead contracts in
real-time. If multiple microgrids breach their reserve contracts
simultaneously, this could lead to a severe grid contingency. This paper
designs a distributionally robust joint chance-constrained (DRJCC)
game-theoretical framework considering uncertain real-time reserve provisions
and the value of lost load (VoLL). Leveraging historical error samples, the
reserve bidding strategy of each microgrid is formulated into a two-stage
Wasserstein-metrics distribution robust optimization (DRO) model. A JCC is
employed to regulate the under-delivered reserve capacity of all microgrids in
a non-cooperative game. Considering the unknown correlation among players, a
novel Bayesian optimization method approximates the optimal individual
violation rates of microgrids and market equilibrium. The proposed game
framework with the optimal rates is simulated with up to 14 players in a 30-bus
network. Case studies are conducted using the California power market data. The
proposed Bayesian method can effectively regulate the joint violation rate of
the under-delivered reserve and secure the profit of microgrids in the reserve
market.


------------------------------------------------------------------------------

Title:
Factors Affecting the Performance of Automated Speaker Verification in  Alzheimer's Disease Clinical Trials

Abstract: Detecting duplicate patient participation in clinical trials is a major
challenge because repeated patients can undermine the credibility and accuracy
of the trial's findings and result in significant health and financial risks.
Developing accurate automated speaker verification (ASV) models is crucial to
verify the identity of enrolled individuals and remove duplicates, but the size
and quality of data influence ASV performance. However, there has been limited
investigation into the factors that can affect ASV capabilities in clinical
environments. In this paper, we bridge the gap by conducting analysis of how
participant demographic characteristics, audio quality criteria, and severity
level of Alzheimer's disease (AD) impact the performance of ASV utilizing a
dataset of speech recordings from 659 participants with varying levels of AD,
obtained through multiple speech tasks. Our results indicate that ASV
performance: 1) is slightly better on male speakers than on female speakers; 2)
degrades for individuals who are above 70 years old; 3) is comparatively better
for non-native English speakers than for native English speakers; 4) is
negatively affected by clinician interference, noisy background, and unclear
participant speech; 5) tends to decrease with an increase in the severity level
of AD. Our study finds that voice biometrics raise fairness concerns as certain
subgroups exhibit different ASV performances owing to their inherent voice
characteristics. Moreover, the performance of ASV is influenced by the quality
of speech recordings, which underscores the importance of improving the data
collection settings in clinical trials.


------------------------------------------------------------------------------

Title:
Aligning Synthetic Medical Images with Clinical Knowledge using Human  Feedback

Abstract: Generative models capable of capturing nuanced clinical features in medical
images hold great promise for facilitating clinical data sharing, enhancing
rare disease datasets, and efficiently synthesizing annotated medical images at
scale. Despite their potential, assessing the quality of synthetic medical
images remains a challenge. While modern generative models can synthesize
visually-realistic medical images, the clinical validity of these images may be
called into question. Domain-agnostic scores, such as FID score, precision, and
recall, cannot incorporate clinical knowledge and are, therefore, not suitable
for assessing clinical sensibility. Additionally, there are numerous
unpredictable ways in which generative models may fail to synthesize clinically
plausible images, making it challenging to anticipate potential failures and
manually design scores for their detection. To address these challenges, this
paper introduces a pathologist-in-the-loop framework for generating
clinically-plausible synthetic medical images. Starting with a diffusion model
pretrained using real images, our framework comprises three steps: (1)
evaluating the generated images by expert pathologists to assess whether they
satisfy clinical desiderata, (2) training a reward model that predicts the
pathologist feedback on new samples, and (3) incorporating expert knowledge
into the diffusion model by using the reward model to inform a finetuning
objective. We show that human feedback significantly improves the quality of
synthetic images in terms of fidelity, diversity, utility in downstream
applications, and plausibility as evaluated by experts.


------------------------------------------------------------------------------

Title:
Multi-Task Consistency for Active Learning

Abstract: Learning-based solutions for vision tasks require a large amount of labeled
training data to ensure their performance and reliability. In single-task
vision-based settings, inconsistency-based active learning has proven to be
effective in selecting informative samples for annotation. However, there is a
lack of research exploiting the inconsistency between multiple tasks in
multi-task networks. To address this gap, we propose a novel multi-task active
learning strategy for two coupled vision tasks: object detection and semantic
segmentation. Our approach leverages the inconsistency between them to identify
informative samples across both tasks. We propose three constraints that
specify how the tasks are coupled and introduce a method for determining the
pixels belonging to the object detected by a bounding box, to later quantify
the constraints as inconsistency scores. To evaluate the effectiveness of our
approach, we establish multiple baselines for multi-task active learning and
introduce a new metric, mean Detection Segmentation Quality (mDSQ), tailored
for the multi-task active learning comparison that addresses the performance of
both tasks. We conduct extensive experiments on the nuImages and A9 datasets,
demonstrating that our approach outperforms existing state-of-the-art methods
by up to 3.4% mDSQ on nuImages. Our approach achieves 95% of the fully-trained
performance using only 67% of the available data, corresponding to 20% fewer
labels compared to random selection and 5% fewer labels compared to
state-of-the-art selection strategy. Our code will be made publicly available
after the review process.


------------------------------------------------------------------------------

Title:
Sigma-point Kalman Filter with Nonlinear Unknown Input Estimation via  Optimization and Data-driven Approach for Dynamic Systems

Abstract: Most works on joint state and unknown input (UI) estimation require the
assumption that the UIs are linear; this is potentially restrictive as it does
not hold in many intelligent autonomous systems. To overcome this restriction
and circumvent the need to linearize the system, we propose a derivative-free
Unknown Input Sigma-point Kalman Filter (SPKF-nUI) where the SPKF is
interconnected with a general nonlinear UI estimator that can be implemented
via nonlinear optimization and data-driven approaches. The nonlinear UI
estimator uses the posterior state estimate which is less susceptible to state
prediction error. In addition, we introduce a joint sigma-point transformation
scheme to incorporate both the state and UI uncertainties in the estimation of
SPKF-nUI. An in-depth stochastic stability analysis proves that the proposed
SPKF-nUI yields exponentially converging estimation error bounds under
reasonable assumptions. Finally, two case studies are carried out on a
simulation-based rigid robot and a physical soft robot, i.e., robots made of
soft materials with complex dynamics to validate effectiveness of the proposed
filter on nonlinear dynamic systems. Our results demonstrate that the proposed
SPKF-nUI achieves the lowest state and UI estimation errors when compared to
the existing nonlinear state-UI filters.


------------------------------------------------------------------------------

Title:
Solving Dialogue Grounding Embodied Task in a Simulated Environment  using Further Masked Language Modeling

Abstract: Enhancing AI systems with efficient communication skills that align with
human understanding is crucial for their effective assistance to human users.
Proactive initiatives from the system side are needed to discern specific
circumstances and interact aptly with users to solve these scenarios. In this
research, we opt for a collective building assignment taken from the Minecraft
dataset. Our proposed method employs language modeling to enhance task
understanding through state-of-the-art (SOTA) methods using language models.
These models focus on grounding multi-modal understandinging and task-oriented
dialogue comprehension tasks. This focus aids in gaining insights into how well
these models interpret and respond to a variety of inputs and tasks. Our
experimental results provide compelling evidence of the superiority of our
proposed method. This showcases a substantial improvement and points towards a
promising direction for future research in this domain.


------------------------------------------------------------------------------

Title:
Memristive Reservoirs Learn to Learn

Abstract: Memristive reservoirs draw inspiration from a novel class of neuromorphic
hardware known as nanowire networks. These systems display emergent brain-like
dynamics, with optimal performance demonstrated at dynamical phase transitions.
In these networks, a limited number of electrodes are available to modulate
system dynamics, in contrast to the global controllability offered by
neuromorphic hardware through random access memories. We demonstrate that the
learn-to-learn framework can effectively address this challenge in the context
of optimization. Using the framework, we successfully identify the optimal
hyperparameters for the reservoir. This finding aligns with previous research,
which suggests that the optimal performance of a memristive reservoir occurs at
the `edge of formation' of a conductive pathway. Furthermore, our results show
that these systems can mimic membrane potential behavior observed in spiking
neurons, and may serve as an interface between spike-based and continuous
processes.


------------------------------------------------------------------------------

Title:
Provably Efficient Representation Learning with Tractable Planning in  Low-Rank POMDP

Abstract: In this paper, we study representation learning in partially observable
Markov Decision Processes (POMDPs), where the agent learns a decoder function
that maps a series of high-dimensional raw observations to a compact
representation and uses it for more efficient exploration and planning.
We focus our attention on the sub-classes of \textit{$\gamma$-observable} and
\textit{decodable POMDPs}, for which it has been shown that statistically
tractable learning is possible, but there has not been any computationally
efficient algorithm. We first present an algorithm for decodable POMDPs that
combines maximum likelihood estimation (MLE) and optimism in the face of
uncertainty (OFU) to perform representation learning and achieve efficient
sample complexity, while only calling supervised learning computational
oracles. We then show how to adapt this algorithm to also work in the broader
class of $\gamma$-observable POMDPs.


------------------------------------------------------------------------------

Title:
Learning topological defects formation with neural networks in a quantum  phase transition

Abstract: Neural networks possess formidable representational power, rendering them
invaluable in solving complex quantum many-body systems. While they excel at
analyzing static solutions, nonequilibrium processes, including critical
dynamics during a quantum phase transition, pose a greater challenge for neural
networks. To address this, we utilize neural networks and machine learning
algorithms to investigate the time evolutions, universal statistics, and
correlations of topological defects in a one-dimensional transverse-field
quantum Ising model. Specifically, our analysis involves computing the energy
of the system during a quantum phase transition following a linear quench of
the transverse magnetic field strength. The excitation energies satisfy a
power-law relation to the quench rate, indicating a proportional relationship
between the excitation energy and the kink numbers. Moreover, we establish a
universal power-law relationship between the first three cumulants of the kink
numbers and the quench rate, indicating a binomial distribution of the kinks.
Finally, the normalized kink-kink correlations are also investigated and it is
found that the numerical values are consistent with the analytic formula.


------------------------------------------------------------------------------

Title:
Interpretation of immunofluorescence slides by deep learning techniques:  anti-nuclear antibodies case study

Abstract: Nowadays, diseases are increasing in numbers and severity by the hour.
Immunity diseases, affecting 8\% of the world population in 2017 according to
the World Health Organization (WHO), is a field in medicine worth attention due
to the high rate of disease occurrence classified under this category. This
work presents an up-to-date review of state-of-the-art immune diseases
healthcare solutions. We focus on tackling the issue with modern solutions such
as Deep Learning to detect anomalies in the early stages hence providing health
practitioners with efficient tools. We rely on advanced deep learning
techniques such as Convolutional Neural Networks (CNN) to fulfill our objective
of providing an efficient tool while providing a proficient analysis of this
solution. The proposed solution was tested and evaluated by the immunology
department in the Principal Military Hospital of Instruction of Tunis, which
considered it a very helpful tool.


------------------------------------------------------------------------------

Title:
PriorBand: Practical Hyperparameter Optimization in the Age of Deep  Learning

Abstract: Hyperparameters of Deep Learning (DL) pipelines are crucial for their
downstream performance. While a large number of methods for Hyperparameter
Optimization (HPO) have been developed, their incurred costs are often
untenable for modern DL. Consequently, manual experimentation is still the most
prevalent approach to optimize hyperparameters, relying on the researcher's
intuition, domain knowledge, and cheap preliminary explorations. To resolve
this misalignment between HPO algorithms and DL researchers, we propose
PriorBand, an HPO algorithm tailored to DL, able to utilize both expert beliefs
and cheap proxy tasks. Empirically, we demonstrate PriorBand's efficiency
across a range of DL benchmarks and show its gains under informative expert
input and robustness against poor expert beliefs


------------------------------------------------------------------------------

Title:
LMFlow: An Extensible Toolkit for Finetuning and Inference of Large  Foundation Models

Abstract: Large foundation models have demonstrated a great ability to achieve general
human-level intelligence far beyond traditional approaches. As the technique
keeps attracting attention from the AI community, more and more large
foundation models have become publically available. However, most of those
models exhibit a major deficiency in specialized-task applications, where the
step of finetuning is still required for obtaining satisfactory performance. As
the number of available models and specialized tasks keeps growing, the job of
general finetuning becomes highly nontrivial. In this paper, we take the first
step to address this issue. We introduce an extensible and lightweight toolkit,
LMFlow, which aims to simplify the finetuning and inference of general large
foundation models. LMFlow offers a complete finetuning workflow for a large
foundation model to support personalized training with limited computing
resources. Furthermore, it supports continuous pretraining, instruction tuning,
parameter-efficient finetuning, alignment tuning, and large model inference,
along with carefully designed and extensible APIs. This toolkit has been
thoroughly tested and is available at this https URL


------------------------------------------------------------------------------

Title:
Coqlex: Generating Formally Verified Lexers

Abstract: A compiler consists of a sequence of phases going from lexical analysis to
code generation. Ideally, the formal verification of a compiler should include
the formal verification of each component of the tool-chain. An example is the
CompCert project, a formally verified C compiler, that comes with associated
tools and proofs that allow to formally verify most of those components.
However, some components, in particular the lexer, remain unverified. In fact,
the lexer of Compcert is generated using OCamllex, a lex-like OCaml lexer
generator that produces lexers from a set of regular expressions with
associated semantic actions. Even though there exist various approaches, like
CakeML or Verbatim++, to write verified lexers, they all have only limited
practical applicability. In order to contribute to the end-to-end verification
of compilers, we implemented a generator of verified lexers whose usage is
similar to OCamllex. Our software, called Coqlex, reads a lexer specification
and generates a lexer equipped with a Coq proof of its correctness. It provides
a formally verified implementation of most features of standard, unverified
lexer generators.
The conclusions of our work are two-fold: Firstly, verified lexers gain to
follow a user experience similar to lex/flex or OCamllex, with a
domain-specific syntax to write lexers comfortably. This introduces a small gap
between the written artifact and the verified lexer, but our design minimizes
this gap and makes it practical to review the generated lexer. The user remains
able to prove further properties of their lexer. Secondly, it is possible to
combine simplicity and decent performance. Our implementation approach that
uses Brzozowski derivatives is noticeably simpler than the previous work in
Verbatim++ that tries to generate a deterministic finite automaton (DFA) ahead
of time, and it is also noticeably faster thanks to careful design choices.
We wrote several example lexers that suggest that the convenience of using
Coqlex is close to that of standard verified generators, in particular,
OCamllex. We used Coqlex in an industrial project to implement a verified lexer
of Ada. This lexer is part of a tool to optimize safety-critical programs, some
of which are very large. This experience confirmed that Coqlex is usable in
practice, and in particular that its performance is good enough. Finally, we
performed detailed performance comparisons between Coqlex, OCamllex, and
Verbatim++. Verbatim++ is the state-of-the-art tool for verified lexers in Coq,
and the performance of its lexer was carefully optimized in previous work by
Egolf and al. (2022). Our results suggest that Coqlex is two orders of
magnitude slower than OCamllex, but two orders of magnitude faster than
Verbatim++. Verified compilers and other language-processing tools are becoming
important tools for safety-critical or security-critical applications. They
provide trust and replace more costly approaches to certification, such as
manually reading the generated code. Verified lexers are a missing piece in
several Coq-based verified compilers today. Coqlex comes with safety
guarantees, and thus shows that it is possible to build formally verified
front-ends.


------------------------------------------------------------------------------

Title:
A VM-Agnostic and Backwards Compatible Protected Modifier for  Dynamically-Typed Languages

Abstract: In object-oriented languages, method visibility modifiers hold a key role in
separating internal methods from the public API. Protected visibility modifiers
offer a way to hide methods from external objects while authorizing internal
use and overriding in subclasses. While present in main statically-typed
languages, visibility modifiers are not as common or mature in
dynamically-typed languages. In this article, we present ProtDyn, a
self-send-based visibility model calculated at compile time for
dynamically-typed languages relying on name-mangling and syntactic
differentiation of self vs non self sends. We present #Pharo, a ProtDyn
implementation of this model that is backwards compatible with existing
programs, and its port to Python. Using these implementations we study the
performance impact of ProtDyn on the method lookup, in the presence of global
lookup caches and polymorphic inline caches. We show that our name mangling and
double method registration technique has a very low impact on performance and
keeps the benefits from the global lookup cache and polymorphic inline cache.
We also show that the memory overhead on a real use case is between 2% and 13%
in the worst-case scenario. Protected modifier semantics enforces encapsulation
such as private but allow developers to still extend the class in subclasses.
ProtDyn offers a VM-agnostic and backwards-compatible design to introduce
protected semantics in dynamically-typed languages.


------------------------------------------------------------------------------

Title:
Sample Complexity for Quadratic Bandits: Hessian Dependent Bounds and  Optimal Algorithms

Abstract: In stochastic zeroth-order optimization, a problem of practical relevance is
understanding how to fully exploit the local geometry of the underlying
objective function. We consider a fundamental setting in which the objective
function is quadratic, and provide the first tight characterization of the
optimal Hessian-dependent sample complexity. Our contribution is twofold.
First, from an information-theoretic point of view, we prove tight lower bounds
on Hessian-dependent complexities by introducing a concept called energy
allocation, which captures the interaction between the searching algorithm and
the geometry of objective functions. A matching upper bound is obtained by
solving the optimal energy spectrum. Then, algorithmically, we show the
existence of a Hessian-independent algorithm that universally achieves the
asymptotic optimal sample complexities for all Hessian instances. The optimal
sample complexities achieved by our algorithm remain valid for heavy-tailed
noise distributions, which are enabled by a truncation method.


------------------------------------------------------------------------------

Title:
Precision psychiatry: predicting predictability

Abstract: Precision psychiatry is an ermerging field that aims to provide
individualized approaches to mental health care. Multivariate analysis and
machine learning are used to create outcome prediction models based on clinical
data such as demographics, symptom assessments, genetic information, and brain
imaging. While much emphasis has been placed on technical innovation, the
complex and varied nature of mental health presents significant challenges to
the successful implementation of these models. From this perspective, I review
ten challenges in the field of precision psychiatry, including the need for
studies on real-world populations and realistic clinical outcome definitions,
consideration of treatment-related factors such as placebo effects and
non-adherence to prescriptions. Fairness, prospective validation in comparison
to current practice and implementation studies of prediction models are other
key issues that are currently understudied. A shift is proposed from
retrospective studies based on linear and static concepts of disease towards
prospective research that considers the importance of contextual factors and
the dynamic and complex nature of mental health.


------------------------------------------------------------------------------

Title:
Using Internal Bar Strength as a Key Indicator for Trading Country ETFs

Abstract: This report aims to investigate the effectiveness of using internal bar
strength (IBS) as a key indicator for trading country exchange-traded funds
(ETFs). The study uses a quantitative approach to analyze historical price data
for a bucket of country ETFs over a period of 10 years and uses the idea of
Mean Reversion to create a profitable trading strategy. Our findings suggest
that IBS can be a useful technical indicator for predicting short-term price
movements in this basket of ETFs.


------------------------------------------------------------------------------

Title:
Adversarial Training with Generated Data in High-Dimensional Regression:  An Asymptotic Study

Abstract: In recent years, studies such as
\cite{carmon2019unlabeled,gowal2021improving,xing2022artificial} have
demonstrated that incorporating additional real or generated data with
pseudo-labels can enhance adversarial training through a two-stage training
approach. In this paper, we perform a theoretical analysis of the asymptotic
behavior of this method in high-dimensional linear regression. While a
double-descent phenomenon can be observed in ridgeless training, with an
appropriate $\mathcal{L}_2$ regularization, the two-stage adversarial training
achieves a better performance. Finally, we derive a shortcut cross-validation
formula specifically tailored for the two-stage training method.


------------------------------------------------------------------------------

Title:
DEPAC: a Corpus for Depression and Anxiety Detection from Speech

Abstract: Mental distress like depression and anxiety contribute to the largest
proportion of the global burden of diseases. Automated diagnosis systems of
such disorders, empowered by recent innovations in Artificial Intelligence, can
pave the way to reduce the sufferings of the affected individuals. Development
of such systems requires information-rich and balanced corpora. In this work,
we introduce a novel mental distress analysis audio dataset DEPAC, labeled
based on established thresholds on depression and anxiety standard screening
tools. This large dataset comprises multiple speech tasks per individual, as
well as relevant demographic information. Alongside, we present a feature set
consisting of hand-curated acoustic and linguistic features, which were found
effective in identifying signs of mental illnesses in human speech. Finally, we
justify the quality and effectiveness of our proposed audio corpus and feature
set in predicting depression severity by comparing the performance of baseline
machine learning models built on this dataset with baseline models trained on
other well-known depression corpora.


------------------------------------------------------------------------------

Title:
An efficient, provably exact algorithm for the 0-1 loss linear  classification problem

Abstract: Algorithms for solving the linear classification problem have a long history,
dating back at least to 1936 with linear discriminant analysis. For linearly
separable data, many algorithms can obtain the exact solution to the
corresponding 0-1 loss classification problem efficiently, but for data which
is not linearly separable, it has been shown that this problem, in full
generality, is NP-hard. Alternative approaches all involve approximations of
some kind, including the use of surrogates for the 0-1 loss (for example, the
hinge or logistic loss) or approximate combinatorial search, none of which can
be guaranteed to solve the problem exactly. Finding efficient algorithms to
obtain an exact i.e. globally optimal solution for the 0-1 loss linear
classification problem with fixed dimension, remains an open problem. In
research we report here, we detail the construction of a new algorithm,
incremental cell enumeration (ICE), that can solve the 0-1 loss classification
problem exactly in polynomial time. To our knowledge, this is the first,
rigorously-proven polynomial time algorithm for this long-standing problem.


------------------------------------------------------------------------------

Title:
Comparative Analysis of Segment Anything Model and U-Net for Breast  Tumor Detection in Ultrasound and Mammography Images

Abstract: In this study, the main objective is to develop an algorithm capable of
identifying and delineating tumor regions in breast ultrasound (BUS) and
mammographic images. The technique employs two advanced deep learning
architectures, namely U-Net and pretrained SAM, for tumor segmentation. The
U-Net model is specifically designed for medical image segmentation and
leverages its deep convolutional neural network framework to extract meaningful
features from input images. On the other hand, the pretrained SAM architecture
incorporates a mechanism to capture spatial dependencies and generate
segmentation results. Evaluation is conducted on a diverse dataset containing
annotated tumor regions in BUS and mammographic images, covering both benign
and malignant tumors. This dataset enables a comprehensive assessment of the
algorithm's performance across different tumor types. Results demonstrate that
the U-Net model outperforms the pretrained SAM architecture in accurately
identifying and segmenting tumor regions in both BUS and mammographic images.
The U-Net exhibits superior performance in challenging cases involving
irregular shapes, indistinct boundaries, and high tumor heterogeneity. In
contrast, the pretrained SAM architecture exhibits limitations in accurately
identifying tumor areas, particularly for malignant tumors and objects with
weak boundaries or complex shapes. These findings highlight the importance of
selecting appropriate deep learning architectures tailored for medical image
segmentation. The U-Net model showcases its potential as a robust and accurate
tool for tumor detection, while the pretrained SAM architecture suggests the
need for further improvements to enhance segmentation performance.


------------------------------------------------------------------------------

Title:
Separability of State Trajectories and its Applications to Security of  Cyber-Physical Systems

Abstract: This article studies a fundamental problem of security of cyber-physical
systems (CPSs). We focus on a class of attacks where some of the actuators
could be malicious while all the sensors are considered to be honest. We
introduce a novel idea of separability of state trajectories that are generated
by the honest and corrupt actuators, and establish its connection to the
security of CPSs in the context of detecting the presence of malicious
actuators (if any,) in the system. As a defense strategy to guard the CPS
against malicious attacks, we focus on the mechanism of perturbing the
pre-determined control action by injecting a certain class of random process by
the honest actuators called private excitation, which is assumed to have a
known distribution. As primary contributions we give sufficient conditions for
the existence and non-existence of a separator for linear time-invariant
stochastic systems, under the assumption that the policies are
randomized-Markovian and randomized history dependent. Several technical
aspects of the established results are discussed extensively.


------------------------------------------------------------------------------

Title:
Adversarial guesswork with quantum side information

Abstract: The guesswork of a classical-quantum channel quantifies the cost incurred in
guessing the state transmitted by the channel when only one state can be
queried at a time, maximized over any classical pre-processing and minimized
over any quantum post-processing. For arbitrary-dimensional covariant
classical-quantum channels, we prove the invariance of the optimal
pre-processing and the covariance of the optimal post-processing. In the qubit
case, we compute the optimal guesswork for the class of so-called highly
symmetric informationally complete classical-quantum channels.


------------------------------------------------------------------------------

Title:
A formalization of the CHSH inequality and Tsirelson's upper-bound in  Isabelle/HOL

Abstract: We present a formalization of several fundamental notions and results from
Quantum Information theory, including density matrices and projective
measurements, along with the proof that the local hidden-variable hypothesis
advocated by Einstein to model quantum mechanics cannot hold. The proof of the
latter result is based on the so-called CHSH inequality, and it is the
violation of this inequality that was experimentally evidenced by Aspect who
earned the Nobel Prize in 2022 for his work. We also formalize various results
related to the violation of the CHSH inequality, such as Tsirelson's bound
which permits to obtain the maximum violation of this inequality in a quantum
setting.


------------------------------------------------------------------------------

Title:
Accelerated Griffin-Lim algorithm: A fast and provably converging  numerical method for phase retrieval

Abstract: The recovery of a signal from the magnitudes of its transformation, like the
Fourier transform, is known as the phase retrieval problem and is of big
relevance in various fields of engineering and applied physics. In this paper,
we present a fast inertial/momentum based algorithm for the phase retrieval
problem and we prove a convergence guarantee for the new algorithm and for the
Fast Griffin-Lim algorithm, whose convergence remained unproven in the past
decade. In the final chapter, we compare the algorithm for the Short Time
Fourier transform phase retrieval with the Griffin-Lim algorithm and FGLA and
to other iterative algorithms typically used for this type of problem.


------------------------------------------------------------------------------

Title:
Online Resource Allocation with Convex-set Machine-Learned Advice

Abstract: Decision-makers often have access to a machine-learned prediction about
demand, referred to as advice, which can potentially be utilized in online
decision-making processes for resource allocation. However, exploiting such
advice poses challenges due to its potential inaccuracy. To address this issue,
we propose a framework that enhances online resource allocation decisions with
potentially unreliable machine-learned (ML) advice. We assume here that this
advice is represented by a general convex uncertainty set for the demand
vector.
We introduce a parameterized class of Pareto optimal online resource
allocation algorithms that strike a balance between consistent and robust
ratios. The consistent ratio measures the algorithm's performance (compared to
the optimal hindsight solution) when the ML advice is accurate, while the
robust ratio captures performance under an adversarial demand process when the
advice is inaccurate. Specifically, in a C-Pareto optimal setting, we maximize
the robust ratio while ensuring that the consistent ratio is at least C. Our
proposed C-Pareto optimal algorithm is an adaptive protection level algorithm,
which extends the classical fixed protection level algorithm introduced in
Littlewood (2005) and Ball and Queyranne (2009). Solving a complex non-convex
continuous optimization problem characterizes the adaptive protection level
algorithm. To complement our algorithms, we present a simple method for
computing the maximum achievable consistent ratio, which serves as an estimate
for the maximum value of the ML advice. Additionally, we present numerical
studies to evaluate the performance of our algorithm in comparison to benchmark
algorithms. The results demonstrate that by adjusting the parameter C, our
algorithms effectively strike a balance between worst-case and average
performance, outperforming the benchmark algorithms.


------------------------------------------------------------------------------

Title:
Solving and Generating NPR Sunday Puzzles with Large Language Models

Abstract: We explore the ability of large language models to solve and generate puzzles
from the NPR Sunday Puzzle game show using PUZZLEQA, a dataset comprising 15
years of on-air puzzles. We evaluate four large language models using PUZZLEQA,
in both multiple choice and free response formats, and explore two prompt
engineering techniques to improve free response performance: chain-of-thought
reasoning and prompt summarization. We find that state-of-the-art large
language models can solve many PUZZLEQA puzzles: the best model, GPT-3.5,
achieves 50.2% loose accuracy. However, in our few-shot puzzle generation
experiment, we find no evidence that models can generate puzzles: GPT-3.5
generates puzzles with answers that do not conform to the generated rules.
Puzzle generation remains a challenging task for future work.


------------------------------------------------------------------------------

Title:
Single-shot decoding of good quantum LDPC codes

Abstract: Quantum Tanner codes constitute a family of quantum low-density parity-check
(LDPC) codes with good parameters, i.e., constant encoding rate and relative
distance. In this article, we prove that quantum Tanner codes also facilitate
single-shot quantum error correction (QEC) of adversarial noise, where one
measurement round (consisting of constant-weight parity checks) suffices to
perform reliable QEC even in the presence of measurement errors. We establish
this result for both the sequential and parallel decoding algorithms introduced
by Leverrier and Z\'emor. Furthermore, we show that in order to suppress errors
over multiple repeated rounds of QEC, it suffices to run the parallel decoding
algorithm for constant time in each round. Combined with good code parameters,
the resulting constant-time overhead of QEC and robustness to (possibly
time-correlated) adversarial noise make quantum Tanner codes alluring from the
perspective of quantum fault-tolerant protocols.


------------------------------------------------------------------------------

Title:
Opportunities of Renewable Energy Powered DNN Inference

Abstract: With the proliferation of the adoption of renewable energy in powering data
centers, addressing the challenges of such energy sources has attracted
researchers from academia and industry. One of the challenging characteristics
of data centers with renewable energy is the intrinsic power fluctuation.
Fluctuation in renewable power supply inevitably requires adjusting
applications' power consumption, which can lead to undesirable performance
degradation. This paper investigates the possible control knobs to manage the
power and performance of a popular cloud workload, i.e., deep neural network
inference, under the fluctuating power supply. Through empirical profiling and
trace-driven simulations, we observe the different impact levels associated
with inference control knobs on throughput, under varying power supplies. Based
on our observations, we provide a list of future research directions to
leverage the control knobs to achieve high throughput.


------------------------------------------------------------------------------

Title:
Improving Software Requirements Prioritization through the Lens of  Constraint Solving

Abstract: Requirements prioritization is a critical activity during the early software
development process, which produces a set of key requirements to implement. The
prioritization process offers a parity among the requirements based on multiple
characteristics, including end-users' preferences, cost to implement, and
technical dependencies. This paper presents an interactive method to
requirements prioritization that leverages the pairwise comparisons and a
constraint solver. Our method employs an interactive accumulation of knowledge
from the requirements analyst when the relative priority among the requirements
cannot be determined based on the existing knowledge from the requirements
documents. The final ranking of the requirements is produced via the constraint
solver and interactive pairwise comparisons. We evaluate the proposed method
using the requirements from a real healthcare project. The proposed
prioritization method relying on a constraint solver outperforms
state-of-the-art interactive prioritization methods in terms of effectiveness
and robustness to analyst's errors.


------------------------------------------------------------------------------

Title:
Geometric Algorithms for $k$-NN Poisoning

Abstract: We propose a label poisoning attack on geometric data sets against
$k$-nearest neighbor classification. We provide an algorithm that can compute
an $\varepsilon n$-additive approximation of the optimal poisoning in $n\cdot
2^{2^{O(d+k/\varepsilon)}}$ time for a given data set $X \in \mathbb{R}^d$,
where $|X| = n$. Our algorithm achieves its objectives through the application
of multi-scale random partitions.


------------------------------------------------------------------------------

Title:
Tailstorm: A Secure and Fair Blockchain for Cash Transactions

Abstract: Proof-of-work (PoW) cryptocurrencies rely on a balance of security and
fairness in order to maintain a sustainable ecosystem of miners and users.
Users demand fast and consistent transaction confirmation, and in exchange
drive the adoption and valuation of the cryptocurrency. Miners provide the
confirmations, however, they primarily seek rewards. In unfair systems, miners
can amplify their rewards by consolidating mining power. Centralization
however, undermines the security guarantees of the system and might discourage
users.
In this paper we present Tailstorm, a cryptocurrency that strikes this
balance. Tailstorm merges multiple recent protocol improvements addressing
security, confirmation latency, and throughput with a novel incentive mechanism
improving fairness. We implement a parallel proof-of-work consensus mechanism
with $k$ PoWs per block to obtain state-of-the-art consistency guarantees.
Inspired by Bobtail and Storm, we structure the individual PoWs in a tree
which, by including a list of transactions with each PoW, reduces confirmation
latency and improves throughput. Our proposed incentive mechanism discounts
rewards based on the depth of this tree. Thereby, it effectively punishes
information withholding, the core attack strategy used to reap an unfair share
of rewards.
We back our claims with a comprehensive analysis. We present a generic system
model which allows us to specify Bitcoin, $B_k$, and Tailstorm from a joint set
of assumptions. We provide an analytical bound for the fairness of Tailstorm
and Bitcoin in honest networks and we confirm the results through simulation.
We evaluate the effectiveness of dishonest behaviour through reinforcement
learning. Our attack search reproduces known optimal strategies against
Bitcoin, uncovers new ones against $B_k$, and confirms that Tailstorm's reward
discounting makes it more resilient to incentive layer attacks.


------------------------------------------------------------------------------

Title:
Probing the limit of hydrologic predictability with the Transformer  network

Abstract: For a number of years since its introduction to hydrology, recurrent neural
networks like long short-term memory (LSTM) have proven remarkably difficult to
surpass in terms of daily hydrograph metrics on known, comparable benchmarks.
Outside of hydrology, Transformers have now become the model of choice for
sequential prediction tasks, making it a curious architecture to investigate.
Here, we first show that a vanilla Transformer architecture is not competitive
against LSTM on the widely benchmarked CAMELS dataset, and lagged especially
for the high-flow metrics due to short-term processes. However, a
recurrence-free variant of Transformer can obtain mixed comparisons with LSTM,
producing the same Kling-Gupta efficiency coefficient (KGE), along with other
metrics. The lack of advantages for the Transformer is linked to the Markovian
nature of the hydrologic prediction problem. Similar to LSTM, the Transformer
can also merge multiple forcing dataset to improve model performance. While the
Transformer results are not higher than current state-of-the-art, we still
learned some valuable lessons: (1) the vanilla Transformer architecture is not
suitable for hydrologic modeling; (2) the proposed recurrence-free modification
can improve Transformer performance so future work can continue to test more of
such modifications; and (3) the prediction limits on the dataset should be
close to the current state-of-the-art model. As a non-recurrent model, the
Transformer may bear scale advantages for learning from bigger datasets and
storing knowledge. This work serves as a reference point for future
modifications of the model.


------------------------------------------------------------------------------

Title:
Fantastic Weights and How to Find Them: Where to Prune in Dynamic Sparse  Training

Abstract: Dynamic Sparse Training (DST) is a rapidly evolving area of research that
seeks to optimize the sparse initialization of a neural network by adapting its
topology during training. It has been shown that under specific conditions, DST
is able to outperform dense models. The key components of this framework are
the pruning and growing criteria, which are repeatedly applied during the
training process to adjust the network's sparse connectivity. While the growing
criterion's impact on DST performance is relatively well studied, the influence
of the pruning criterion remains overlooked. To address this issue, we design
and perform an extensive empirical analysis of various pruning criteria to
better understand their effect on the dynamics of DST solutions. Surprisingly,
we find that most of the studied methods yield similar results. The differences
become more significant in the low-density regime, where the best performance
is predominantly given by the simplest technique: magnitude-based pruning. The
code is provided at this https URL


------------------------------------------------------------------------------

Title:
Decoupled Boundary Handling in SPH

Abstract: Particle-based boundary representations are frequently used in Smoothed
Particle Hydrodynamics (SPH) due to their simple integration into fluid
solvers. Commonly, incompressible fluid solvers estimate the current density
and corresponding forces in case the current density exceeds the rest density
to push fluid particles apart. Close to the boundary, the calculation of the
fluid particles' density involves both, neighboring fluid and neighboring
boundary particles, yielding an overestimation of density, and, subsequently,
wrong pressure forces and wrong velocities leading to the disturbed fluid
particles' behavior in the vicinity of the boundary. In this paper, we present
a detailed explanation of this disturbed fluid particle behavior, which is
mainly due to the combined or coupled handling of the fluid-fluid particle and
the fluid-boundary particle interaction. We propose the decoupled handling of
both interaction types, leading to two densities for a given fluid particle,
i.e., fluid-induced density and boundary-induced density. In our approach, we
alternately apply the corresponding fluid-induced and boundary-induced forces
during pressure estimation. This separation avoids force overestimation and
reduces unintended fluid dynamics near the boundary, as well as a consistent
fluid-boundary distance across different fluid amounts and different
particle-based boundary handling methods. We compare our method with two
regular state-of-the-art methods in different experiments and show how our
method handles detailed boundary shapes.


------------------------------------------------------------------------------

Title:
ICAR, a categorical framework to connect vulnerability, threat and asset  managements

Abstract: We present ICAR, a mathematical framework derived from category theory for
representing cybersecurity NIST and MITRE's ontologies. Designed for
cybersecurity, ICAR is a category whose objects are cybersecurity knowledge
(weakness, vulnerability, impacted product, attack technique, etc.) and whose
morphisms are relations between this knowledge, that make sense for
cybersecurity. Within this rigorous and unified framework, we obtain a
knowledge graph capable of identifying the attack and weakness structures of an
IS, at the interface between description logics, database theory and
cybersecurity. We then define ten cybersecurity queries to help understand the
risks incurred by IS and organise their defence.


------------------------------------------------------------------------------

Title:
StarVQA+: Co-training Space-Time Attention for Video Quality Assessment

Abstract: Self-attention based Transformer has achieved great success in many computer
vision tasks. However, its application to video quality assessment (VQA) has
not been satisfactory so far. Evaluating the quality of in-the-wild videos is
challenging due to the unknown of pristine reference and shooting distortion.
This paper presents a co-trained Space-Time Attention network for the VQA
problem, termed StarVQA+. Specifically, we first build StarVQA+ by alternately
concatenating the divided space-time attention. Then, to facilitate the
training of StarVQA+, we design a vectorized regression loss by encoding the
mean opinion score (MOS) to the probability vector and embedding a special
token as the learnable variable of MOS, leading to better fitting of human's
rating process. Finally, to solve the data hungry problem with Transformer, we
propose to co-train the spatial and temporal attention weights using both
images and videos. Various experiments are conducted on the de-facto
in-the-wild video datasets, including LIVE-Qualcomm, LIVE-VQC, KoNViD-1k,
YouTube-UGC, LSVQ, LSVQ-1080p, and DVL2021. Experimental results demonstrate
the superiority of the proposed StarVQA+ over the state-of-the-art.


------------------------------------------------------------------------------

Title:
The Effect of Noise on the Emergence of Continuous Norms and its  Evolutionary Dynamics

Abstract: We examine the effect of noise on societies of agents using an agent-based
model of evolutionary norm emergence. Generally, we see that noisy societies
are more selfish, smaller and discontent, and are caught in rounds of perpetual
punishment preventing them from flourishing. Surprisingly, despite the effect
of noise on the population, it does not seem to evolve away. In fact, in some
cases it seems that the level of noise increases. We carry out further analysis
and provide reasons for why this may be the case. Furthermore, we claim that
our framework that evolves the noise/ambiguity of norms may be a new way to
model the tight/loose framework of norms, suggesting that despite ambiguous
norms detrimental effect on society, evolution does not favour clarity.


------------------------------------------------------------------------------

Title:
Decentralized Aerial Transportation and Manipulation of a Cable-Slung  Payload With Swarm of Agents

Abstract: With the advent of Unmanned Aerial Vehicles (UAV) and Micro Aerial Vehicles
(MAV) in commercial sectors, their application for transporting and
manipulating payloads has attracted many research work. A swarm of agents,
cooperatively working to transport and manipulate a payload can overcome the
physical limitations of a single agent, adding redundancy and tolerance against
failures. In this paper, the dynamics of a swarm connected to a payload via
flexible cables are modeled, and a decentralized control is designed using
Artificial Potential Field (APF). The swarm is able to transport the payload
through an unknown environment to a goal position while avoiding obstacles from
the local information received from the onboard sensors. The key contributions
are (a) the cables are modelled more accurately using lumped mass model instead
of geometric constraints, (b) a decentralized swarm control is designed using
potential field approach to ensure hover stability of system without payload
state information, (c) the manipulation of payload elevation and azimuth angles
are controlled by APF, and (d) the trajectory of the payload for transportation
is governed by potential fields generated by goal point and obstacles. The
efficacy of the method proposed in this work are evaluated through numerical
simulations under the influence of external disturbances and failure of agents.


------------------------------------------------------------------------------

Title:
SIFTER: A Task-specific Alignment Strategy for Enhancing Sentence  Embeddings

Abstract: The paradigm of pre-training followed by fine-tuning on downstream tasks has
become the mainstream method in natural language processing tasks. Although
pre-trained models have the advantage of generalization, their performance may
still vary significantly across different domain tasks. This is because the
data distribution in different domains varies. For example, the different parts
of the sentence 'He married Smt. Dipali Ghosh in 1947 and led a very happy
married life' may have different impact for downstream tasks. For similarity
calculations, words such as 'led' and 'life' are more important. On the other
hand, for sentiment analysis, the word 'happy' is crucial. This indicates that
different downstream tasks have different levels of sensitivity to sentence
components. Our starting point is to scale information of the model and data
according to the specifics of downstream tasks, enhancing domain information of
relevant parts for these tasks and reducing irrelevant elements for different
domain tasks, called SIFTER. In the experimental part, we use the SIFTER to
improve SimCSE by constructing positive sample pairs based on enhancing the
sentence stem and reducing the unimportant components in the sentence, and
maximize the similarity between three sentences. Similarly, SIFTER can improve
the gate mechanism of the LSTM model by short-circuiting the input gate of
important words so that the LSTM model remembers the important parts of the
sentence. Our experiments demonstrate that SIFTER outperforms the SimCSE and
LSTM baselines.


------------------------------------------------------------------------------

Title:
Geometric Pooling: maintaining more useful information

Abstract: Graph Pooling technology plays an important role in graph node classification
tasks. Sorting pooling technologies maintain large-value units for pooling
graphs of varying sizes. However, by analyzing the statistical characteristic
of activated units after pooling, we found that a large number of units dropped
by sorting pooling are negative-value units that contain useful information and
can contribute considerably to the final decision. To maintain more useful
information, a novel pooling technology, called Geometric Pooling (GP), was
proposed to contain the unique node features with negative values by measuring
the similarity of all node features. We reveal the effectiveness of GP from the
entropy reduction view. The experiments were conducted on TUdatasets to show
the effectiveness of GP. The results showed that the proposed GP outperforms
the SOTA graph pooling technologies by 1%\sim5% with fewer parameters.


------------------------------------------------------------------------------

Title:
Computationally efficient human body modelling for real time motion  comfort assessment

Abstract: Due to the complexity of the human body and its neuromuscular stabilization,
it has been challenging to efficiently and accurately predict human motion and
capture posture while being driven. Existing simple models of the seated human
body are mostly two-dimensional and developed in the mid-sagittal plane
ex-posed to in-plane excitation. Such models capture fore-aft and vertical
motion but not the more complex 3D motions due to lateral loading. Advanced 3D
full-body active human models (AHMs), such as in MADYMO, can be used for
comfort analysis and to investigate how vibrations influence the human body
while being driven. However, such AHMs are very time-consuming due to their
complexity. To effectively analyze motion comfort, a computationally efficient
and accurate three dimensional (3D) human model, which runs faster than
real-time, is presented. The model's postural stabilization parameters are
tuned using available 3D vibration data for head, trunk and pelvis translation
and rotation. A comparison between AHM and EHM is conducted regarding human
body kinematics. According to the results, the EHM model configuration with two
neck joints, two torso bending joints, and a spinal compression joint
accurately predicts body kinematics.


------------------------------------------------------------------------------

Title:
Persuading Farsighted Receivers in MDPs: the Power of Honesty

Abstract: Bayesian persuasion studies the problem faced by an informed sender who
strategically discloses information to influence the behavior of an uninformed
receiver. Recently, a growing attention has been devoted to settings where the
sender and the receiver interact sequentially, in which the receiver's
decision-making problem is usually modeled as a Markov decision process (MDP).
However, previous works focused on computing optimal information-revelation
policies (a.k.a. signaling schemes) under the restrictive assumption that the
receiver acts myopically, selecting actions to maximize the one-step utility
and disregarding future rewards. This is justified by the fact that, when the
receiver is farsighted and thus considers future rewards, finding an optimal
Markovian signaling scheme is NP-hard. In this paper, we show that Markovian
signaling schemes do not constitute the "right" class of policies. Indeed,
differently from most of the MDPs settings, we prove that Markovian signaling
schemes are not optimal, and general history-dependent signaling schemes should
be considered. Moreover, we also show that history-dependent signaling schemes
circumvent the negative complexity results affecting Markovian signaling
schemes. Formally, we design an algorithm that computes an optimal and
{\epsilon}-persuasive history-dependent signaling scheme in time polynomial in
1/{\epsilon} and in the instance size. The crucial challenge is that general
history-dependent signaling schemes cannot be represented in polynomial space.
Nevertheless, we introduce a convenient subclass of history-dependent signaling
schemes, called promise-form, which are as powerful as general
history-dependent ones and efficiently representable. Intuitively, promise-form
signaling schemes compactly encode histories in the form of honest promises on
future receiver's rewards.


------------------------------------------------------------------------------

Title:
Medical ministrations through web scraping

Abstract: Web scraping is a technique that allows us to extract data from websites
automatically. in the field of medicine, web scraping can be used to collect
information about medical procedures, treatments, and healthcare providers.
this information can be used to improve patient care, monitor the quality of
healthcare services, and identify areas for improvement. one area where web
scraping can be particularly useful is in medical ministrations. medical
ministrations are the actions taken to provide medical care to patients, and
web scraping can help healthcare providers identify the most effective
ministrations for their patients. for example, healthcare providers can use web
scraping to collect data about the symptoms and medical histories of their
patients, and then use this information to determine the most appropriate
ministrations. they can also use web scraping to gather information about the
latest medical research and clinical trials, which can help them stay
up-to-date with the latest treatments and procedures.


------------------------------------------------------------------------------

Title:
Supporting Construction and Architectural Visualization through BIM and  AR/VR: A Systematic Literature Review

Abstract: The Architecture, Engineering, Construction, and Facility Management (AEC/FM)
industry deals with the design, construction, and operation of complex
buildings. Today, Building Information Modeling (BIM) is used to represent
information about a building in a single, non-redundant representation. Here,
Augmented Reality (AR) and Virtual Reality (VR) can improve the visualization
and interaction with the resulting model by augmenting the real world with
information from the BIM model or allowing a user to immerse in a virtual world
generated from the BIM model. This can improve the design, construction, and
operation of buildings. While an increasing number of studies in HCI,
construction, or engineering have shown the potential of using AR and VR
technology together with BIM, often research remains focused on individual
explorations and key design strategies. In addition to that, a systematic
overview and discussion of recent works combining AR/VR with BIM are not yet
fully covered. Therefore, this paper systematically reviews recent approaches
combining AR/VR with BIM and categorizes the literature by the building's
lifecycle phase while systematically describing relevant use cases. In total,
32 out of 447 papers between 2017 and 2022 were categorized. The categorization
shows that most approaches focus on the construction phase and the use case of
review and quality assurance. In the design phase, most approaches use VR,
while in the construction and operation phases, AR is prevalent.


------------------------------------------------------------------------------

Title:
Beyond Deep Ensembles: A Large-Scale Evaluation of Bayesian Deep  Learning under Distribution Shift

Abstract: Bayesian deep learning (BDL) is a promising approach to achieve
well-calibrated predictions on distribution-shifted data. Nevertheless, there
exists no large-scale survey that evaluates recent SOTA methods on diverse,
realistic, and challenging benchmark tasks in a systematic manner. To provide a
clear picture of the current state of BDL research, we evaluate modern BDL
algorithms on real-world datasets from the WILDS collection containing
challenging classification and regression tasks, with a focus on generalization
capability and calibration under distribution shift. We compare the algorithms
on a wide range of large, convolutional and transformer-based neural network
architectures. In particular, we investigate a signed version of the expected
calibration error that reveals whether the methods are over- or
under-confident, providing further insight into the behavior of the methods.
Further, we provide the first systematic evaluation of BDL for fine-tuning
large pre-trained models, where training from scratch is prohibitively
expensive. Finally, given the recent success of Deep Ensembles, we extend
popular single-mode posterior approximations to multiple modes by the use of
ensembles. While we find that ensembling single-mode approximations generally
improves the generalization capability and calibration of the models by a
significant margin, we also identify a failure mode of ensembles when
finetuning large transformer-based language models. In this setting,
variational inference based approaches such as last-layer Bayes By Backprop
outperform other methods in terms of accuracy by a large margin, while modern
approximate inference algorithms such as SWAG achieve the best calibration.


------------------------------------------------------------------------------

Title:
GADBench: Revisiting and Benchmarking Supervised Graph Anomaly Detection

Abstract: With a long history of traditional Graph Anomaly Detection (GAD) algorithms
and recently popular Graph Neural Networks (GNNs), it is still not clear (1)
how they perform under a standard comprehensive setting, (2) whether GNNs
outperform traditional algorithms such as tree ensembles, and (3) their
efficiency on large-scale graphs. In response, we present GADBench -- a
comprehensive benchmark for supervised anomalous node detection on static
graphs. GADBench provides a thorough comparison across 23 distinct models on
ten real-world GAD datasets ranging from thousands to millions of nodes
($\sim$6M). Our main finding is that tree ensembles with simple neighborhood
aggregation outperform all other baselines, including the latest GNNs tailored
for the GAD task. By making GADBench available as an open-source tool, we offer
pivotal insights into the current advancements of GAD and establish a solid
foundation for future research. Our code is available at
this https URL


------------------------------------------------------------------------------

Title:
PalmGazer: Unimanual Eye-hand Menus in Augmented Reality

Abstract: How can we design the user interfaces for augmented reality (AR) so that we
can interact as simple, flexible and expressive as we can with smartphones in
one hand? To explore this question, we propose PalmGazer as an interaction
concept integrating eye-hand interaction to establish a singlehandedly operable
menu system. In particular, PalmGazer is designed to support quick and
spontaneous digital commands -- such as to play a music track, check
notifications or browse visual media -- through our devised three-way
interaction model: hand opening to summon the menu UI, eye-hand input for
selection of items, and dragging gesture for navigation. A key aspect is that
it remains always-accessible and movable to the user, as the menu supports
meaningful hand and head based reference frames. We demonstrate the concept in
practice through a prototypical personal UI with application probes, and
describe technique designs specifically-tailored to the application UI. A
qualitative evaluation highlights the system's design benefits and drawbacks,
e.g., that common 2D scroll and selection tasks are simple to operate, but
higher degrees of freedom may be reserved for two hands. Our work contributes
interaction techniques and design insights to expand AR's uni-manual
capabilities.


------------------------------------------------------------------------------

Title:
Timely Asynchronous Hierarchical Federated Learning: Age of Convergence

Abstract: We consider an asynchronous hierarchical federated learning (AHFL) setting
with a client-edge-cloud framework. The clients exchange the trained parameters
with their corresponding edge servers, which update the locally aggregated
model. This model is then transmitted to all the clients in the local cluster.
The edge servers communicate to the central cloud server for global model
aggregation. The goal of each client is to converge to the global model, while
maintaining timeliness of the clients, i.e., having optimum training iteration
time. We investigate the convergence criteria for such a system with dense
clusters. Our analysis shows that for a system of $n$ clients with fixed
average timeliness, the convergence in finite time is probabilistically
guaranteed, if the nodes are divided into $O(1)$ number of clusters, that is,
if the system is built as a sparse set of edge servers with dense client bases
each.


------------------------------------------------------------------------------

Title:
Predicting protein variants with equivariant graph neural networks

Abstract: Pre-trained models have been successful in many protein engineering tasks.
Most notably, sequence-based models have achieved state-of-the-art performance
on protein fitness prediction while structure-based models have been used
experimentally to develop proteins with enhanced functions. However, there is a
research gap in comparing structure- and sequence-based methods for predicting
protein variants that are better than the wildtype protein. This paper aims to
address this gap by conducting a comparative study between the abilities of
equivariant graph neural networks (EGNNs) and sequence-based approaches to
identify promising amino-acid mutations. The results show that our proposed
structural approach achieves a competitive performance to sequence-based
methods while being trained on significantly fewer molecules. Additionally, we
find that combining assay labelled data with structure pre-trained models
yields similar trends as with sequence pre-trained models.


------------------------------------------------------------------------------

Title:
On the Validation of Gibbs Algorithms: Training Datasets, Test Datasets  and their Aggregation

Abstract: The dependence on training data of the Gibbs algorithm (GA) is analytically
characterized. By adopting the expected empirical risk as the performance
metric, the sensitivity of the GA is obtained in closed form. In this case,
sensitivity is the performance difference with respect to an arbitrary
alternative algorithm. This description enables the development of explicit
expressions involving the training errors and test errors of GAs trained with
different datasets. Using these tools, dataset aggregation is studied and
different figures of merit to evaluate the generalization capabilities of GAs
are introduced. For particular sizes of such datasets and parameters of the
GAs, a connection between Jeffrey's divergence, training and test errors is
established.


------------------------------------------------------------------------------

Title:
Python Framework for Modular and Parametric SPICE Netlists Generation

Abstract: Due to the complex specifications of current electronic systems, design
decisions need to be explored automatically. However, the exploration process
is a complex task given the plethora of design choices such as the selection of
components, number of components, operating modes of each of the components,
connections between the components and variety of ways in which the same
functionality can be implemented. To tackle these issues, scripts are used
generate designs based on high abstract constructions. Still, this approach is
usually ad-hoc and platform dependent, making the whole procedure hardly
reusable, scalable and versatile. We propose a generic, open-source framework
tackling rapid design exploration for the generation of modular and parametric
electronic designs that is able to work on any major simulator.


------------------------------------------------------------------------------

Title:
Empirical Risk Minimization with Shuffled SGD: A Primal-Dual Perspective  and Improved Bounds

Abstract: Stochastic gradient descent (SGD) is perhaps the most prevalent optimization
method in modern machine learning. Contrary to the empirical practice of
sampling from the datasets without replacement and with (possible) reshuffling
at each epoch, the theoretical counterpart of SGD usually relies on the
assumption of sampling with replacement. It is only very recently that SGD with
sampling without replacement -- shuffled SGD -- has been analyzed. For convex
finite sum problems with $n$ components and under the $L$-smoothness assumption
for each component function, there are matching upper and lower bounds, under
sufficiently small -- $\mathcal{O}(\frac{1}{nL})$ -- step sizes. Yet those
bounds appear too pessimistic -- in fact, the predicted performance is
generally no better than for full gradient descent -- and do not agree with the
empirical observations. In this work, to narrow the gap between the theory and
practice of shuffled SGD, we sharpen the focus from general finite sum problems
to empirical risk minimization with linear predictors. This allows us to take a
primal-dual perspective and interpret shuffled SGD as a primal-dual method with
cyclic coordinate updates on the dual side. Leveraging this perspective, we
prove a fine-grained complexity bound that depends on the data matrix and is
never worse than what is predicted by the existing bounds. Notably, our bound
can predict much faster convergence than the existing analyses -- by a factor
of the order of $\sqrt{n}$ in some cases. We empirically demonstrate that on
common machine learning datasets our bound is indeed much tighter. We further
show how to extend our analysis to convex nonsmooth problems, with similar
improvements.


------------------------------------------------------------------------------

Title:
Randomized algorithms for low-rank matrix approximation: Design,  analysis, and applications

Abstract: This survey explores modern approaches for computing low-rank approximations
of high-dimensional matrices by means of the randomized SVD, randomized
subspace iteration, and randomized block Krylov iteration. The paper compares
the procedures via theoretical analyses and numerical studies to highlight how
the best choice of algorithm depends on spectral properties of the matrix and
the computational resources available.
Despite superior performance for many problems, randomized block Krylov
iteration has not been widely adopted in computational science. This paper
strengthens the case for this method in three ways. First, it presents new
pseudocode that can significantly reduce computational costs. Second, it
provides a new analysis that yields simple, precise, and informative error
bounds. Last, it showcases applications to challenging scientific problems,
including principal component analysis for genetic data and spectral clustering
for molecular dynamics data.


------------------------------------------------------------------------------

Title:
One Policy to Dress Them All: Learning to Dress People with Diverse  Poses and Garments

Abstract: Robot-assisted dressing could benefit the lives of many people such as older
adults and individuals with disabilities. Despite such potential,
robot-assisted dressing remains a challenging task for robotics as it involves
complex manipulation of deformable cloth in 3D space. Many prior works aim to
solve the robot-assisted dressing task, but they make certain assumptions such
as a fixed garment and a fixed arm pose that limit their ability to generalize.
In this work, we develop a robot-assisted dressing system that is able to dress
different garments on people with diverse poses from partial point cloud
observations, based on a learned policy. We show that with proper design of the
policy architecture and Q function, reinforcement learning (RL) can be used to
learn effective policies with partial point cloud observations that work well
for dressing diverse garments. We further leverage policy distillation to
combine multiple policies trained on different ranges of human arm poses into a
single policy that works over a wide range of different arm poses. We conduct
comprehensive real-world evaluations of our system with 510 dressing trials in
a human study with 17 participants with different arm poses and dressed
garments. Our system is able to dress 86% of the length of the participants'
arms on average. Videos can be found on our project webpage:
this https URL


------------------------------------------------------------------------------

Title:
ScenarioNet: Open-Source Platform for Large-Scale Traffic Scenario  Simulation and Modeling

Abstract: Large-scale driving datasets such as Waymo Open Dataset and nuScenes
substantially accelerate autonomous driving research, especially for perception
tasks such as 3D detection and trajectory forecasting. Since the driving logs
in these datasets contain HD maps and detailed object annotations which
accurately reflect the real-world complexity of traffic behaviors, we can
harvest a massive number of complex traffic scenarios and recreate their
digital twins in simulation. Compared to the hand-crafted scenarios often used
in existing simulators, data-driven scenarios collected from the real world can
facilitate many research opportunities in machine learning and autonomous
driving. In this work, we present ScenarioNet, an open-source platform for
large-scale traffic scenario modeling and simulation. ScenarioNet defines a
unified scenario description format and collects a large-scale repository of
real-world traffic scenarios from the heterogeneous data in various driving
datasets including Waymo, nuScenes, Lyft L5, and nuPlan datasets. These
scenarios can be further replayed and interacted with in multiple views from
Bird-Eye-View layout to realistic 3D rendering in MetaDrive simulator. This
provides a benchmark for evaluating the safety of autonomous driving stacks in
simulation before their real-world deployment. We further demonstrate the
strengths of ScenarioNet on large-scale scenario generation, imitation
learning, and reinforcement learning in both single-agent and multi-agent
settings. Code, demo videos, and website are available at
this https URL


------------------------------------------------------------------------------

Title:
Introspective Action Advising for Interpretable Transfer Learning

Abstract: Transfer learning can be applied in deep reinforcement learning to accelerate
the training of a policy in a target task by transferring knowledge from a
policy learned in a related source task. This is commonly achieved by copying
pretrained weights from the source policy to the target policy prior to
training, under the constraint that they use the same model architecture.
However, not only does this require a robust representation learned over a wide
distribution of states -- often failing to transfer between specialist models
trained over single tasks -- but it is largely uninterpretable and provides
little indication of what knowledge is transferred. In this work, we propose an
alternative approach to transfer learning between tasks based on action
advising, in which a teacher trained in a source task actively guides a
student's exploration in a target task. Through introspection, the teacher is
capable of identifying when advice is beneficial to the student and should be
given, and when it is not. Our approach allows knowledge transfer between
policies agnostic of the underlying representations, and we empirically show
that this leads to improved convergence rates in Gridworld and Atari
environments while providing insight into what knowledge is transferred.


------------------------------------------------------------------------------

Title:
Discovering Intrinsic Spatial-Temporal Logic Rules to Explain Human  Actions

Abstract: We propose a logic-informed knowledge-driven modeling framework for human
movements by analyzing their trajectories. Our approach is inspired by the fact
that human actions are usually driven by their intentions or desires, and are
influenced by environmental factors such as the spatial relationships with
surrounding objects. In this paper, we introduce a set of spatial-temporal
logic rules as knowledge to explain human actions. These rules will be
automatically discovered from observational data. To learn the model parameters
and the rule content, we design an expectation-maximization (EM) algorithm,
which treats the rule content as latent variables. The EM algorithm alternates
between the E-step and M-step: in the E-step, the posterior distribution over
the latent rule content is evaluated; in the M-step, the rule generator and
model parameters are jointly optimized by maximizing the current expected
log-likelihood. Our model may have a wide range of applications in areas such
as sports analytics, robotics, and autonomous cars, where understanding human
movements are essential. We demonstrate the model's superior interpretability
and prediction performance on pedestrian and NBA basketball player datasets,
both achieving promising results.


------------------------------------------------------------------------------

Title:
Inter-Instance Similarity Modeling for Contrastive Learning

Abstract: The existing contrastive learning methods widely adopt one-hot instance
discrimination as pretext task for self-supervised learning, which inevitably
neglects rich inter-instance similarities among natural images, then leading to
potential representation degeneration. In this paper, we propose a novel image
mix method, PatchMix, for contrastive learning in Vision Transformer (ViT), to
model inter-instance similarities among images. Following the nature of ViT, we
randomly mix multiple images from mini-batch in patch level to construct mixed
image patch sequences for ViT. Compared to the existing sample mix methods, our
PatchMix can flexibly and efficiently mix more than two images and simulate
more complicated similarity relations among natural images. In this manner, our
contrastive framework can significantly reduce the gap between contrastive
objective and ground truth in reality. Experimental results demonstrate that
our proposed method significantly outperforms the previous state-of-the-art on
both ImageNet-1K and CIFAR datasets, e.g., 3.0% linear accuracy improvement on
ImageNet-1K and 8.7% kNN accuracy improvement on CIFAR100. Moreover, our method
achieves the leading transfer performance on downstream tasks, object detection
and instance segmentation on COCO dataset. The code is available at
this https URL


------------------------------------------------------------------------------

Title:
ProtoGate: Prototype-based Neural Networks with Local Feature Selection  for Tabular Biomedical Data

Abstract: Tabular biomedical data poses challenges in machine learning because it is
often high-dimensional and typically low-sample-size. Previous research has
attempted to address these challenges via feature selection approaches, which
can lead to unstable performance on real-world data. This suggests that current
methods lack appropriate inductive biases that capture patterns common to
different samples. In this paper, we propose ProtoGate, a prototype-based
neural model that introduces an inductive bias by attending to both homogeneity
and heterogeneity across samples. ProtoGate selects features in a
global-to-local manner and leverages them to produce explainable predictions
via an interpretable prototype-based model. We conduct comprehensive
experiments to evaluate the performance of ProtoGate on synthetic and
real-world datasets. Our results show that exploiting the homogeneous and
heterogeneous patterns in the data can improve prediction accuracy while
prototypes imbue interpretability.


------------------------------------------------------------------------------

Title:
A Finite Expression Method for Solving High-Dimensional Committor  Problems

Abstract: Transition path theory (TPT) is a mathematical framework for quantifying rare
transition events between a pair of selected metastable states $A$ and $B$.
Central to TPT is the committor function, which describes the probability to
hit the metastable state $B$ prior to $A$ from any given starting point of the
phase space. Once the committor is computed, the transition channels and the
transition rate can be readily found. The committor is the solution to the
backward Kolmogorov equation with appropriate boundary conditions. However,
solving it is a challenging task in high dimensions due to the need to mesh a
whole region of the ambient space. In this work, we explore the finite
expression method (FEX, Liang and Yang (2022)) as a tool for computing the
committor. FEX approximates the committor by an algebraic expression involving
a fixed finite number of nonlinear functions and binary arithmetic operations.
The optimal nonlinear functions, the binary operations, and the numerical
coefficients in the expression template are found via reinforcement learning.
The FEX-based committor solver is tested on several high-dimensional benchmark
problems. It gives comparable or better results than neural network-based
solvers. Most importantly, FEX is capable of correctly identifying the
algebraic structure of the solution which allows one to reduce the committor
problem to a low-dimensional one and find the committor with any desired
accuracy.


------------------------------------------------------------------------------

Title:
Do you still need a manual smart contract audit?

Abstract: We investigate the feasibility of employing large language models (LLMs) for
conducting the security audit of smart contracts, a traditionally
time-consuming and costly process. Our research focuses on the optimization of
prompt engineering for enhanced security analysis, and we evaluate the
performance and accuracy of LLMs using a benchmark dataset comprising 52
Decentralized Finance (DeFi) smart contracts that have previously been
compromised.
Our findings reveal that, when applied to vulnerable contracts, both GPT-4
and Claude models correctly identify the vulnerability type in 40% of the
cases. However, these models also demonstrate a high false positive rate,
necessitating continued involvement from manual auditors. The LLMs tested
outperform a random model by 20% in terms of F1-score.
To ensure the integrity of our study, we conduct mutation testing on five
newly developed and ostensibly secure smart contracts, into which we manually
insert two and 15 vulnerabilities each. This testing yielded a remarkable
best-case 78.7% true positive rate for the GPT-4-32k model. We tested both,
asking the models to perform a binary classification on whether a contract is
vulnerable, and a non-binary prompt. We also examined the influence of model
temperature variations and context length on the LLM's performance.
Despite the potential for many further enhancements, this work lays the
groundwork for a more efficient and economical approach to smart contract
security audits.


------------------------------------------------------------------------------

Title:
Adaptive DNN Surgery for Selfish Inference Acceleration with On-demand  Edge Resource

Abstract: Deep Neural Networks (DNNs) have significantly improved the accuracy of
intelligent applications on mobile devices. DNN surgery, which partitions DNN
processing between mobile devices and multi-access edge computing (MEC)
servers, can enable real-time inference despite the computational limitations
of mobile devices. However, DNN surgery faces a critical challenge: determining
the optimal computing resource demand from the server and the corresponding
partition strategy, while considering both inference latency and MEC server
usage costs. This problem is compounded by two factors: (1) the finite
computing capacity of the MEC server, which is shared among multiple devices,
leading to inter-dependent demands, and (2) the shift in modern DNN
architecture from chains to directed acyclic graphs (DAGs), which complicates
potential solutions.
In this paper, we introduce a novel Decentralized DNN Surgery (DDS)
framework. We formulate the partition strategy as a min-cut and propose a
resource allocation game to adaptively schedule the demands of mobile devices
in an MEC environment. We prove the existence of a Nash Equilibrium (NE), and
develop an iterative algorithm to efficiently reach the NE for each device. Our
extensive experiments demonstrate that DDS can effectively handle varying MEC
scenarios, achieving up to 1.25$\times$ acceleration compared to the
state-of-the-art algorithm.


------------------------------------------------------------------------------

Title:
Automatic Speech Disentanglement for Voice Conversion using Rank Module  and Speech Augmentation

Abstract: Voice Conversion (VC) converts the voice of a source speech to that of a
target while maintaining the source's content. Speech can be mainly decomposed
into four components: content, timbre, rhythm and pitch. Unfortunately, most
related works only take into account content and timbre, which results in less
natural speech. Some recent works are able to disentangle speech into several
components, but they require laborious bottleneck tuning or various
hand-crafted features, each assumed to contain disentangled speech information.
In this paper, we propose a VC model that can automatically disentangle speech
into four components using only two augmentation functions, without the
requirement of multiple hand-crafted features or laborious bottleneck tuning.
The proposed model is straightforward yet efficient, and the empirical results
demonstrate that our model can achieve a better performance than the baseline,
regarding disentanglement effectiveness and speech naturalness.


------------------------------------------------------------------------------

Title:
Robotic Navigation with Convergence Guarantees in Complex Dynamic  Environments

Abstract: This article addresses the obstacle avoidance problem for setpoint
stabilization and path-following tasks in complex dynamic 2-D environments that
go beyond conventional scenes with isolated convex obstacles. A combined motion
planner and controller is proposed for setpoint stabilization that integrates
the favorable convergence characteristics of closed-form motion planning
techniques with the intuitive representation of system constraints through
Model Predictive Control (MPC). The method is analytically proven to accomplish
collision avoidance and convergence under soft conditions, and it is extended
to path-following control. Various simulation scenarios using a non-holonomic
unicycle robot are provided to showcase the efficacy of the control scheme and
its improved convergence results compared to standard path-following MPC
approaches with obstacle avoidance.


------------------------------------------------------------------------------

Title:
One-shot Imitation Learning via Interaction Warping

Abstract: Imitation learning of robot policies from few demonstrations is crucial in
open-ended applications. We propose a new method, Interaction Warping, for
learning SE(3) robotic manipulation policies from a single demonstration. We
infer the 3D mesh of each object in the environment using shape warping, a
technique for aligning point clouds across object instances. Then, we represent
manipulation actions as keypoints on objects, which can be warped with the
shape of the object. We show successful one-shot imitation learning on three
simulated and real-world object re-arrangement tasks. We also demonstrate the
ability of our method to predict object meshes and robot grasps in the wild.


------------------------------------------------------------------------------

Title:
Iterated Piecewise Affine (IPA) Approximation for Language Modeling

Abstract: In this work, we demonstrate the application of a simple first-order Taylor
expansion to approximate a generic function $F: R^{n \times m} \to R^{n \times
m}$ and utilize it in language modeling. To enhance the basic Taylor expansion,
we introduce iteration and piecewise modeling, leading us to name the algorithm
the Iterative Piecewise Affine (IPA) approximation. The final algorithm
exhibits interesting resemblances to the Transformers decoder architecture. By
comparing parameter arrangements in IPA and Transformers, we observe a
strikingly similar performance, with IPA outperforming Transformers by 1.5\% in
the next token prediction task with cross-entropy loss for smaller sequence
lengths.


------------------------------------------------------------------------------

Title:
Benchmarking and Analyzing 3D-aware Image Synthesis with a Modularized  Codebase

Abstract: Despite the rapid advance of 3D-aware image synthesis, existing studies
usually adopt a mixture of techniques and tricks, leaving it unclear how each
part contributes to the final performance in terms of generality. Following the
most popular and effective paradigm in this field, which incorporates a neural
radiance field (NeRF) into the generator of a generative adversarial network
(GAN), we build a well-structured codebase, dubbed Carver, through modularizing
the generation process. Such a design allows researchers to develop and replace
each module independently, and hence offers an opportunity to fairly compare
various approaches and recognize their contributions from the module
perspective. The reproduction of a range of cutting-edge algorithms
demonstrates the availability of our modularized codebase. We also perform a
variety of in-depth analyses, such as the comparison across different types of
point feature, the necessity of the tailing upsampler in the generator, the
reliance on the camera pose prior, etc., which deepen our understanding of
existing methods and point out some further directions of the research work. We
release code and models at this https URL to facilitate the
development and evaluation of this field.


------------------------------------------------------------------------------

Title:
STAN: Stage-Adaptive Network for Multi-Task Recommendation by Learning  User Lifecycle-Based Representation

Abstract: Recommendation systems play a vital role in many online platforms, with their
primary objective being to satisfy and retain users. As directly optimizing
user retention is challenging, multiple evaluation metrics are often employed.
Existing methods generally formulate the optimization of these evaluation
metrics as a multitask learning problem, but often overlook the fact that user
preferences for different tasks are personalized and change over time.
Identifying and tracking the evolution of user preferences can lead to better
user retention. To address this issue, we introduce the concept of "user
lifecycle", consisting of multiple stages characterized by users' varying
preferences for different tasks. We propose a novel Stage-Adaptive Network
(STAN) framework for modeling user lifecycle stages. STAN first identifies
latent user lifecycle stages based on learned user preferences, and then
employs the stage representation to enhance multi-task learning performance.
Our experimental results using both public and industrial datasets demonstrate
that the proposed model significantly improves multi-task prediction
performance compared to state-of-the-art methods, highlighting the importance
of considering user lifecycle stages in recommendation systems. Furthermore,
online A/B testing reveals that our model outperforms the existing model,
achieving a significant improvement of 3.05% in staytime per user and 0.88% in
CVR. These results indicate that our approach effectively improves the overall
efficiency of the multi-task recommendation system.


------------------------------------------------------------------------------

Title:
Knowledge-based Multimodal Music Similarity

Abstract: Music similarity is an essential aspect of music retrieval, recommendation
systems, and music analysis. Moreover, similarity is of vital interest for
music experts, as it allows studying analogies and influences among composers
and historical periods. Current approaches to musical similarity rely mainly on
symbolic content, which can be expensive to produce and is not always readily
available. Conversely, approaches using audio signals typically fail to provide
any insight about the reasons behind the observed similarity. This research
addresses the limitations of current approaches by focusing on the study of
musical similarity using both symbolic and audio content. The aim of this
research is to develop a fully explainable and interpretable system that can
provide end-users with more control and understanding of music similarity and
classification systems.


------------------------------------------------------------------------------

Title:
Addressing Discontinuous Root-Finding for Subsequent Differentiability  in Machine Learning, Inverse Problems, and Control

Abstract: There are many physical processes that have inherent discontinuities in their
mathematical formulations. This paper is motivated by the specific case of
collisions between two rigid or deformable bodies and the intrinsic nature of
that discontinuity. The impulse response to a collision is discontinuous with
the lack of any response when no collision occurs, which causes difficulties
for numerical approaches that require differentiability which are typical in
machine learning, inverse problems, and control. We theoretically and
numerically demonstrate that the derivative of the collision time with respect
to the parameters becomes infinite as one approaches the barrier separating
colliding from not colliding, and use lifting to complexify the solution space
so that solutions on the other side of the barrier are directly attainable as
precise values. Subsequently, we mollify the barrier posed by the unbounded
derivatives, so that one can tunnel back and forth in a smooth and reliable
fashion facilitating the use of standard numerical approaches. Moreover, we
illustrate that standard approaches fail in numerous ways mostly due to a lack
of understanding of the mathematical nature of the problem (e.g. typical
backpropagation utilizes many rules of differentiation, but ignores L'Hopital's
rule).


------------------------------------------------------------------------------

Title:
An efficient mass lumping scheme for isogeometric analysis based on  approximate dual basis functions

Abstract: In this contribution, we provide a new mass lumping scheme for explicit
dynamics in isogeometric analysis (IGA). To this end, an element formulation
based on the idea of dual functionals is developed. Non-Uniform Rational
B-splines (NURBS) are applied as shape functions and their corresponding dual
basis functions are applied as test functions in the variational form, where
two kinds of dual basis functions are compared. The first type are approximate
dual basis functions (AD) with varying degree of reproduction, resulting in
banded mass matrices. Dual basis functions derived from the inversion of the
Gram matrix (IG) are the second type and already yield diagonal mass matrices.
We will show that it is possible to apply the dual scheme as a transformation
of the resulting system of equations based on NURBS as shape and test
functions. Hence, it can be easily implemented into existing IGA routines.
Treating the application of dual test functions as preconditioner reduces the
additional computational effort, but it cannot entirely erase it and the
density of the stiffness matrix still remains higher than in standard
Bubnov-Galerkin formulations. In return applying additional row-sum lumping to
the mass matrices is either not necessary for IG or the caused loss of accuracy
is lowered to a reasonable magnitude in the case of AD. Numerical examples show
a significantly better approximation of the dynamic behavior for the dual
lumping scheme compared to standard NURBS approaches making use of row-sum
lumping. Applying IG yields accurate numerical results without additional
lumping. But as result of the global support of the IG dual basis functions,
fully populated stiffness matrices occur, which are entirely unsuitable for
explicit dynamic simulations. Combining AD and row-sum lumping leads to an
efficient computation regarding effort and accuracy.


------------------------------------------------------------------------------

Title:
DreamTime: An Improved Optimization Strategy for Text-to-3D Content  Creation

Abstract: Text-to-image diffusion models pre-trained on billions of image-text pairs
have recently enabled text-to-3D content creation by optimizing a randomly
initialized Neural Radiance Fields (NeRF) with score distillation. However, the
resultant 3D models exhibit two limitations: (a) quality concerns such as
saturated color and the Janus problem; (b) extremely low diversity comparing to
text-guided image synthesis. In this paper, we show that the conflict between
NeRF optimization process and uniform timestep sampling in score distillation
is the main reason for these limitations. To resolve this conflict, we propose
to prioritize timestep sampling with monotonically non-increasing functions,
which aligns NeRF optimization with the sampling process of diffusion model.
Extensive experiments show that our simple redesign significantly improves
text-to-3D content creation with higher quality and diversity.


------------------------------------------------------------------------------

Title:
CompMix: A Benchmark for Heterogeneous Question Answering

Abstract: Fact-centric question answering (QA) often requires access to multiple,
heterogeneous, information sources. By jointly considering several sources like
a knowledge base (KB), a text collection, and tables from the web, QA systems
can enhance their answer coverage and confidence. However, existing QA
benchmarks are mostly constructed with a single source of knowledge in mind.
This limits capabilities of these benchmarks to fairly evaluate QA systems that
can tap into more than one information repository. To bridge this gap, we
release CompMix, a crowdsourced QA benchmark which naturally demands the
integration of a mixture of input sources. CompMix has a total of 9,410
questions, and features several complex intents like joins and temporal
conditions. Evaluation of a range of QA systems on CompMix highlights the need
for further research on leveraging information from heterogeneous sources.


------------------------------------------------------------------------------

Title:
Resilient Sparse Array Radar with the Aid of Deep Learning

Abstract: In this paper, we address the problem of direction of arrival (DOA)
estimation for multiple targets in the presence of sensor failures in a sparse
array. Generally, sparse arrays are known with very high-resolution
capabilities, where N physical sensors can resolve up to $\mathcal{O}(N^2)$
uncorrelated sources. However, among the many configurations introduced in the
literature, the arrays that provide the largest hole-free co-array are the most
susceptible to sensor failures. We propose here two machine learning (ML)
methods to mitigate the effect of sensor failures and maintain the DOA
estimation performance and resolution. The first method enhances the
conventional spatial smoothing using deep neural network (DNN), while the
second one is an end-to-end data-driven method. Numerical results show that
both approaches can significantly improve the performance of MRA with two
failed sensors. The data-driven method can maintain the performance of the
array with no failures at high signal-tonoise ratio (SNR). Moreover, both
approaches can even perform better than the original array at low SNR thanks to
the denoising effect of the proposed DNN


------------------------------------------------------------------------------

Title:
Dynamic Implicit Image Function for Efficient Arbitrary-Scale Image  Representation

Abstract: Recent years have witnessed the remarkable success of implicit neural
representation methods. The recent work Local Implicit Image Function (LIIF)
has achieved satisfactory performance for continuous image representation,
where pixel values are inferred from a neural network in a continuous spatial
domain. However, the computational cost of such implicit arbitrary-scale
super-resolution (SR) methods increases rapidly as the scale factor increases,
which makes arbitrary-scale SR time-consuming. In this paper, we propose
Dynamic Implicit Image Function (DIIF), which is a fast and efficient method to
represent images with arbitrary resolution. Instead of taking an image
coordinate and the nearest 2D deep features as inputs to predict its pixel
value, we propose a coordinate grouping and slicing strategy, which enables the
neural network to perform decoding from coordinate slices to pixel value
slices. We further propose a Coarse-to-Fine Multilayer Perceptron (C2F-MLP) to
perform decoding with dynamic coordinate slicing, where the number of
coordinates in each slice varies as the scale factor varies. With dynamic
coordinate slicing, DIIF significantly reduces the computational cost when
encountering arbitrary-scale SR. Experimental results demonstrate that DIIF can
be integrated with implicit arbitrary-scale SR methods and achieves SOTA SR
performance with significantly superior computational efficiency, thereby
opening a path for real-time arbitrary-scale image representation. Our code can
be found at this https URL


------------------------------------------------------------------------------

Title:
Tackling the Awkward Squad for Reactive Programming: The Actor-Reactor  Model

Abstract: Reactive programming is a programming paradigm whereby programs are
internally represented by a dependency graph, which is used to automatically
(re)compute parts of a program whenever its input changes. In practice reactive
programming can only be used for some parts of an application: a reactive
program is usually embedded in an application that is still written in ordinary
imperative languages such as JavaScript or Scala. In this paper we investigate
this embedding and we distill "the awkward squad for reactive programming" as 3
concerns that are essential for real-world software development, but that do
not fit within reactive programming. They are related to long lasting
computations, side-effects, and the coordination between imperative and
reactive code. To solve these issues we design a new programming model called
the Actor-Reactor Model in which programs are split up in a number of actors
and reactors. Actors and reactors enforce a strict separation of imperative and
reactive code, and they can be composed via a number of composition operators
that make use of data streams. We demonstrate the model via our own
implementation in a language called Stella.


------------------------------------------------------------------------------

Title:
Towards Efficient MPPI Trajectory Generation with Unscented Guidance:  U-MPPI Control Strategy

Abstract: The classical Model Predictive Path Integral (MPPI) control framework lacks
reliable safety guarantees since it relies on a risk-neutral trajectory
evaluation technique, which can present challenges for safety-critical
applications such as autonomous driving. Additionally, if the majority of MPPI
sampled trajectories concentrate in high-cost regions, it may generate an
infeasible control sequence. To address this challenge, we propose the U-MPPI
control strategy, a novel methodology that can effectively manage system
uncertainties while integrating a more efficient trajectory sampling strategy.
The core concept is to leverage the Unscented Transform (UT) to propagate not
only the mean but also the covariance of the system dynamics, going beyond the
traditional MPPI method. As a result, it introduces a novel and more efficient
trajectory sampling strategy, significantly enhancing state-space exploration
and ultimately reducing the risk of being trapped in local minima. Furthermore,
by leveraging the uncertainty information provided by UT, we incorporate a
risk-sensitive cost function that explicitly accounts for risk or uncertainty
throughout the trajectory evaluation process, resulting in a more resilient
control system capable of handling uncertain conditions. By conducting
extensive simulations of 2D aggressive autonomous navigation in both known and
unknown cluttered environments, we verify the efficiency and robustness of our
proposed U-MPPI control strategy compared to the baseline MPPI. We further
validate the practicality of U-MPPI through real-world demonstrations in
unknown cluttered environments, showcasing its superior ability to incorporate
both the UT and local costmap into the optimization problem without introducing
additional complexity.


------------------------------------------------------------------------------

Title:
Wildfire Detection Via Transfer Learning: A Survey

Abstract: This paper surveys different publicly available neural network models used
for detecting wildfires using regular visible-range cameras which are placed on
hilltops or forest lookout towers. The neural network models are pre-trained on
ImageNet-1K and fine-tuned on a custom wildfire dataset. The performance of
these models is evaluated on a diverse set of wildfire images, and the survey
provides useful information for those interested in using transfer learning for
wildfire detection. Swin Transformer-tiny has the highest AUC value but
ConvNext-tiny detects all the wildfire events and has the lowest false alarm
rate in our dataset.


------------------------------------------------------------------------------

Title:
MimiC: Combating Client Dropouts in Federated Learning by Mimicking  Central Updates

Abstract: Federated learning (FL) is a promising framework for privacy-preserving
collaborative learning. In FL, the model training tasks are distributed to
clients and only the model updates need to be collected at a central server.
However, when being deployed at the mobile edge network, clients (e.g.,
smartphones and wearables) may have unpredictable availability and randomly
drop out of any training iteration, which hinders FL from achieving the
convergence. This paper tackles such a critical challenge of FL. In particular,
we first investigate the convergence of the classical FedAvg algorithm with
arbitrary client dropouts. We find that with the common choice of a decaying
learning rate, FedAvg can only oscillate within the neighborhood of a
stationary point of the global loss function, which is caused by the divergence
between the aggregated update and the desired central update. Motivated by this
new observation, we then design a novel training algorithm named MimiC, where
the server modifies each received model update based on the previous ones. The
proposed modification of the received model updates is able to mimic the
imaginary central update irrespective of the dropout clients. The theoretical
analysis of MimiC shows that the divergence between the aggregated update and
the central update diminishes with a proper choice of the learning rates,
leading to its convergence. Simulation results further demonstrate that MimiC
maintains stable convergence performance in the presence of client dropouts and
learns better models than the baseline methods.


------------------------------------------------------------------------------

Title:
Coverage Performance of UAV-powered Sensors for Energy-neutral Networks  with Recharging Stations

Abstract: The projected number of Internet of Things (IoT) sensors makes battery
maintenance a challenging task. Although battery-less IoT is technologically
viable, the sensors should be somehow energized, either locally or remotely.
Unmanned aerial vehicles (UAVs) can respond to this quest via wireless power
transfer (WPT). However, to achieve energy neutrality across the IoT networks
and thus mitigate the maintenance issues, the UAVs providing energy and
connectivity to IoT sensors must be supplied by recharging stations having
multi-source energy harvesting (EH) capability. Yet, as these sensors rely
solely on UAV-transferred power, the absence of UAVs causes sensor outages and
hence loss of coverage when they visit recharging stations for battery
replenishment. Hence, besides the UAV parameters (e.g., battery size and
velocity), recharging duration and station density must be carefully determined
to avoid these outages. To address that, this paper uses stochastic geometry to
derive the coverage probability of UAV-powered sensors. Our analysis sheds
light on the fundamental trade-offs and design guidelines for energy-neutral
IoT networks with recharging stations in regard to the regulatory organization
limitations, practical rectenna and UAV models, and the minimum power
requirements of sensors.


------------------------------------------------------------------------------

Title:
Opening the Black Box: Analyzing Attention Weights and Hidden States in  Pre-trained Language Models for Non-language Tasks

Abstract: Investigating deep learning language models has always been a significant
research area due to the ``black box" nature of most advanced models. With the
recent advancements in pre-trained language models based on transformers and
their increasing integration into daily life, addressing this issue has become
more pressing. In order to achieve an explainable AI model, it is essential to
comprehend the procedural steps involved and compare them with human thought
processes. Thus, in this paper, we use simple, well-understood non-language
tasks to explore these models' inner workings. Specifically, we apply a
pre-trained language model to constrained arithmetic problems with hierarchical
structure, to analyze their attention weight scores and hidden states. The
investigation reveals promising results, with the model addressing hierarchical
problems in a moderately structured manner, similar to human problem-solving
strategies. Additionally, by inspecting the attention weights layer by layer,
we uncover an unconventional finding that layer 10, rather than the model's
final layer, is the optimal layer to unfreeze for the least parameter-intensive
approach to fine-tune the model. We support these findings with entropy
analysis and token embeddings similarity analysis. The attention analysis
allows us to hypothesize that the model can generalize to longer sequences in
ListOps dataset, a conclusion later confirmed through testing on sequences
longer than those in the training set. Lastly, by utilizing a straightforward
task in which the model predicts the winner of a Tic Tac Toe game, we identify
limitations in attention analysis, particularly its inability to capture 2D
patterns.


------------------------------------------------------------------------------

Title:
Bidirectional End-to-End Learning of Retriever-Reader Paradigm for  Entity Linking

Abstract: Entity Linking (EL) is a fundamental task for Information Extraction and
Knowledge Graphs. The general form of EL (i.e., end-to-end EL) aims to first
find mentions in the given input document and then link the mentions to
corresponding entities in a specific knowledge base. Recently, the paradigm of
retriever-reader promotes the progress of end-to-end EL, benefiting from the
advantages of dense entity retrieval and machine reading comprehension.
However, the existing study only trains the retriever and the reader separately
in a pipeline manner, which ignores the benefit that the interaction between
the retriever and the reader can bring to the task. To advance the
retriever-reader paradigm to perform more perfectly on end-to-end EL, we
propose BEER$^2$, a Bidirectional End-to-End training framework for Retriever
and Reader. Through our designed bidirectional end-to-end training, BEER$^2$
guides the retriever and the reader to learn from each other, make progress
together, and ultimately improve EL performance. Extensive experiments on
benchmarks of multiple domains demonstrate the effectiveness of our proposed
BEER$^2$.


------------------------------------------------------------------------------

Title:
Analysis of a Crank-Nicolson finite difference scheme for (2+1)D  perturbed nonlinear Schrödinger equations with saturable nonlinearity

Abstract: We analyze a Crank-Nicolson finite difference discretization for the
perturbed (2+1)D nonlinear Schr\"odinger equation with saturable nonlinearity
and a perturbation of cubic loss. We show the boundedness, the existence and
uniqueness of a numerical solution. We establish the error bound to prove the
convergence of the numerical solution. Moreover, we find that the convergence
rate is at the second order in both time step and spatial mesh size under a
mild assumption. The numerical scheme is validated by the extensive simulations
of the (2+1)D saturable nonlinear Schr\"odinger model with cubic loss. The
simulations for travelling solitons are implemented by using an accelerated
imaginary-time evolution scheme and the Crank-Nicolson finite difference
method.


------------------------------------------------------------------------------

Title:
Probabilistic estimation of the algebraic degree of Boolean functions

Abstract: The algebraic degree is an important parameter of Boolean functions used in
cryptography. When a function in a large number of variables is not given
explicitly in algebraic normal form, it might not be feasible to compute its
degree. Instead, one can try to estimate the degree using probabilistic tests.
We propose a probabilistic test for deciding whether the algebraic degree of
a Boolean function $f$ is below a certain value $k$. The test involves picking
an affine space of dimension $k$ and testing whether the values on $f$ on that
space sum up to zero. If $deg(f)<k$, then $f$ will always pass the test,
otherwise it will sometimes pass and sometimes fail the test, depending on
which affine space was chosen. The probability of failing the proposed test is
closely related to the number of monomials of degree $k$ in a polynomial $g$,
averaged over all the polynomials $g$ which are affine equivalent to $f$.
We initiate the study of the probability of failing the proposed
``$deg(f)<k$'' test. We show that in the particular case when the degree of $f$
is actually equal to $k$, the probability will be in the interval $(0.288788,
0.5]$, and therefore a small number of runs of the test is sufficient to give,
with very high probability, the correct answer. Exact values of this
probability for all the polynomials in 8 variables were computed using the
representatives listed by Hou and by Langevin and Leander.


------------------------------------------------------------------------------

Title:
Quantifying lottery tickets under label noise: accuracy, calibration,  and complexity

Abstract: Pruning deep neural networks is a widely used strategy to alleviate the
computational burden in machine learning. Overwhelming empirical evidence
suggests that pruned models retain very high accuracy even with a tiny fraction
of parameters. However, relatively little work has gone into characterising the
small pruned networks obtained, beyond a measure of their accuracy. In this
paper, we use the sparse double descent approach to identify univocally and
characterise pruned models associated with classification tasks. We observe
empirically that, for a given task, iterative magnitude pruning (IMP) tends to
converge to networks of comparable sizes even when starting from full networks
with sizes ranging over orders of magnitude. We analyse the best pruned models
in a controlled experimental setup and show that their number of parameters
reflects task difficulty and that they are much better than full networks at
capturing the true conditional probability distribution of the labels. On real
data, we similarly observe that pruned models are less prone to overconfident
predictions. Our results suggest that pruned models obtained via IMP not only
have advantageous computational properties but also provide a better
representation of uncertainty in learning.


------------------------------------------------------------------------------

Title:
A Practical Overview of Quantum Computing: Is Exascale Possible?

Abstract: Despite numerous advances in the field and a seemingly ever-increasing amount
of investment, we are still some years away from seeing a production quantum
computer in action. However, it is possible to make some educated guesses about
the operational difficulties and challenges that may be encountered in
practice. We can be reasonably confident that the early machines will be
hybrid, with the quantum devices used in an apparently similar way to current
accelerators such as FPGAs or GPUs. Compilers, libraries and the other tools
relied upon currently for development of software will have to evolve/be
reinvented to support the new technology, and training courses will have to be
rethought completely rather than ``just'' updated alongside them.
The workloads we are likely to see making best use of these hybrid machines
will initially be few, before rapidly increasing in diversity as we saw with
the uptake of GPUs and other new technologies in the past. This will again be
helped by the increase in the number of supporting libraries and development
tools, and by the gradual re-development of existing software, to make use of
the new quantum devices.
Unfortunately, at present the problem of error correction is still largely
unsolved, although there have been many advances. Quantum computation is very
sensitive to noise, leading to frequent errors during execution.
Quantum calculations, although asymptotically faster than their equivalents
in ``traditional'' HPC, still take time, and while the profiling tools and
programming approaches will have to change drastically, many of the skills
honed in the current HPC industry will not suddenly become obsolete, but
continue to be useful in the quantum era.


------------------------------------------------------------------------------

Title:
A Multimodal Prototypical Approach for Unsupervised Sound Classification

Abstract: In the context of environmental sound classification, the adaptability of
systems is key: which sound classes are interesting depends on the context and
the user's needs. Recent advances in text-to-audio retrieval allow for
zero-shot audio classification, but performance compared to supervised models
remains limited. This work proposes a multimodal prototypical approach that
exploits local audio-text embeddings to provide more relevant answers to audio
queries, augmenting the adaptability of sound detection in the wild. We do this
by first using text to query a nearby community of audio embeddings that best
characterize each query sound, and select the group's centroids as our
prototypes. Second, we compare unseen audio to these prototypes for
classification. We perform multiple ablation studies to understand the impact
of the embedding models and prompts. Our unsupervised approach improves upon
the zero-shot state-of-the-art in three sound recognition benchmarks by an
average of 12%.


------------------------------------------------------------------------------

Title:
Comparing deep learning models for volatility prediction using  multivariate data

Abstract: This study aims at comparing several deep learning-based forecasters in the
task of volatility prediction using multivariate data, proceeding from simpler
or shallower to deeper and more complex models and compare them to the naive
prediction and variations of classical GARCH models. Specifically, the
volatility of five assets (i.e., S\&P500, NASDAQ100, gold, silver, and oil) was
predicted with the GARCH models, Multi-Layer Perceptrons, recurrent neural
networks, Temporal Convolutional Networks, and the Temporal Fusion Transformer.
In most cases the Temporal Fusion Transformer followed by variants of Temporal
Convolutional Network outperformed classical approaches and shallow networks.
These experiments were repeated, and the difference between competing models
was shown to be statistically significant, therefore encouraging their use in
practice.


------------------------------------------------------------------------------

Title:
Post-hoc Selection of Pareto-Optimal Solutions in Search and  Recommendation

Abstract: Information Retrieval (IR) and Recommender Systems (RS) tasks are moving from
computing a ranking of final results based on a single metric to
multi-objective problems. Solving these problems leads to a set of
Pareto-optimal solutions, known as Pareto frontier, in which no objective can
be further improved without hurting the others. In principle, all the points on
the Pareto frontier are potential candidates to represent the best model
selected with respect to the combination of two, or more, metrics. To our
knowledge, there are no well-recognized strategies to decide which point should
be selected on the frontier. In this paper, we propose a novel, post-hoc,
theoretically-justified technique, named "Population Distance from Utopia"
(PDU), to identify and select the one-best Pareto-optimal solution from the
frontier. In detail, PDU analyzes the distribution of the points by
investigating how far each point is from its utopia point (the ideal
performance for the objectives). The possibility of considering fine-grained
utopia points allows PDU to select solutions tailored to individual user
preferences, a novel feature we call "calibration". We compare PDU against
existing state-of-the-art strategies through extensive experiments on tasks
from both IR and RS. Experimental results show that PDU and combined with
calibration notably impact the solution selection. Furthermore, the results
show that the proposed framework selects a solution in a principled way,
irrespective of its position on the frontier, thus overcoming the limits of
other strategies.


------------------------------------------------------------------------------

Title:
Inverse Constraint Learning and Generalization by Transferable Reward  Decomposition

Abstract: We present the problem of inverse constraint learning (ICL), which recovers
constraints from demonstrations to autonomously reproduce constrained skills in
new scenarios. However, ICL suffers from an ill-posed nature, leading to
inaccurate inference of constraints from demonstrations. To figure it out, we
introduce a transferable constraint learning (TCL) algorithm that jointly
infers a task-oriented reward and a task-agnostic constraint, enabling the
generalization of learned skills. Our method TCL additively decomposes the
overall reward into a task reward and its residual as soft constraints,
maximizing policy divergence between task- and constraint-oriented policies to
obtain a transferable constraint. Evaluating our method and four baselines in
three simulated environments, we show TCL outperforms state-of-the-art IRL and
ICL algorithms, achieving up to a $72\%$ higher task-success rates with
accurate decomposition compared to the next best approach in novel scenarios.
Further, we demonstrate the robustness of TCL on a real-world robotic
tray-carrying task.


------------------------------------------------------------------------------

Title:
Feature Interactions Reveal Linguistic Structure in Language Models

Abstract: We study feature interactions in the context of feature attribution methods
for post-hoc interpretability. In interpretability research, getting to grips
with feature interactions is increasingly recognised as an important challenge,
because interacting features are key to the success of neural networks. Feature
interactions allow a model to build up hierarchical representations for its
input, and might provide an ideal starting point for the investigation into
linguistic structure in language models. However, uncovering the exact role
that these interactions play is also difficult, and a diverse range of
interaction attribution methods has been proposed. In this paper, we focus on
the question which of these methods most faithfully reflects the inner workings
of the target models. We work out a grey box methodology, in which we train
models to perfection on a formal language classification task, using PCFGs. We
show that under specific configurations, some methods are indeed able to
uncover the grammatical rules acquired by a model. Based on these findings we
extend our evaluation to a case study on language models, providing novel
insights into the linguistic structure that these models have acquired.


------------------------------------------------------------------------------

Title:
Split Learning in 6G Edge Networks

Abstract: With the proliferation of distributed edge computing resources, the 6G mobile
network will evolve into a network for connected intelligence. Along this line,
the proposal to incorporate federated learning into the mobile edge has gained
considerable interest in recent years. However, the deployment of federated
learning faces substantial challenges as massive resource-limited IoT devices
can hardly support on-device model training. This leads to the emergence of
split learning (SL) which enables servers to handle the major training workload
while still enhancing data privacy. In this article, we offer a brief overview
of key advancements in SL and articulate its seamless integration with wireless
edge networks. We begin by illustrating the tailored 6G architecture to support
edge SL. Then, we examine the critical design issues for edge SL, including
innovative resource-efficient learning frameworks and resource management
strategies under a single edge server. Additionally, we expand the scope to
multi-edge scenarios, exploring multi-edge collaboration and mobility
management from a networking perspective. Finally, we discuss open problems for
edge SL, including convergence analysis, asynchronous SL and U-shaped SL.


------------------------------------------------------------------------------

Title:
Predicting the popularity of information on social platforms without  underlying network structure

Abstract: The ability to predict the size of information cascades in online social
networks is crucial for various applications, including decision-making and
viral marketing. However, traditional methods either rely on complicated
time-varying features that are challenging to extract from multilingual and
cross-platform content, or on network structures and properties that are often
difficult to obtain. To address these issues, we conducted empirical research
using data from two well-known social networking platforms, WeChat and Weibo.
Our findings suggest that the information-cascading process is best described
as an activate-decay dynamical process. Building on these insights, we
developed an Activate-Decay (AD)-based algorithm that can accurately predict
the long-term popularity of online content based solely on its early repost
amount. We tested our algorithm using data from WeChat and Weibo, demonstrating
that we could fit the evolution trend of content propagation and predict the
longer-term dynamics of message forwarding from earlier data. We also
discovered a close correlation between the peak forwarding amount of
information and the total amount of dissemination. Finding the peak of the
amount of information dissemination can significantly improve the prediction
accuracy of our model. Our method also outperformed existing baseline methods
for predicting the popularity of information.


------------------------------------------------------------------------------

Title:
PrivSketch: A Private Sketch-based Frequency Estimation Protocol for  Data Streams

Abstract: Local differential privacy (LDP) has recently become a popular
privacy-preserving data collection technique protecting users' privacy. The
main problem of data stream collection under LDP is the poor utility due to
multi-item collection from a very large domain. This paper proposes PrivSketch,
a high-utility frequency estimation protocol taking advantage of sketches,
suitable for private data stream collection. Combining the proposed background
information and a decode-first collection-side workflow, PrivSketch improves
the utility by reducing the errors introduced by the sketching algorithm and
the privacy budget utilization when collecting multiple items. We analytically
prove the superior accuracy and privacy characteristics of PrivSketch, and also
evaluate them experimentally. Our evaluation, with several diverse synthetic
and real datasets, demonstrates that PrivSketch is 1-3 orders of magnitude
better than the competitors in terms of utility in both frequency estimation
and frequent item estimation, while being up to ~100x faster.


------------------------------------------------------------------------------

Title:
Static-Equilibrium Oriented Interaction Force Modeling and Control of  Aerial Manipulation with Uni-Directional Thrust Multirotors

Abstract: This paper presents a static-equilibrium oriented interaction force modeling
and control approach of aerial manipulation employing uni-directional thrust
(UDT) multirotors interacting with variously defined environments. First, a
simplified system model for a quadrotor-based aerial manipulator is introduced
considering parameterized work surfaces under assumptions, and then a range of
meaningful manipulation tasks are utilized to explore the system properties in
a quasi-static equilibrium state. An explicit interaction force model in
relation with the aerial manipulator pose configuration and the environment
parameter is derived from the static equilibrium analysis, based on which
singularity is pointed out. Then a hybrid attitude/force interaction control
strategy is presented to verify the proposed interaction force model, which
involves high gain attitude control and feedforward plus feedback force
control. This paper represents preliminary results. We study the properties of
UDT-based aerial manipulators via specific tasks, and propose a novel framework
for interaction force modeling and control aiming at maximizing the commercial
values of UDT platforms for aerial manipulation purpose.


------------------------------------------------------------------------------

Title:
HSR-Diff:Hyperspectral Image Super-Resolution via Conditional Diffusion  Models

Abstract: Despite the proven significance of hyperspectral images (HSIs) in performing
various computer vision tasks, its potential is adversely affected by the
low-resolution (LR) property in the spatial domain, resulting from multiple
physical factors. Inspired by recent advancements in deep generative models, we
propose an HSI Super-resolution (SR) approach with Conditional Diffusion Models
(HSR-Diff) that merges a high-resolution (HR) multispectral image (MSI) with
the corresponding LR-HSI. HSR-Diff generates an HR-HSI via repeated refinement,
in which the HR-HSI is initialized with pure Gaussian noise and iteratively
refined. At each iteration, the noise is removed with a Conditional Denoising
Transformer (CDF ormer) that is trained on denoising at different noise levels,
conditioned on the hierarchical feature maps of HR-MSI and LR-HSI. In addition,
a progressive learning strategy is employed to exploit the global information
of full-resolution images. Systematic experiments have been conducted on four
public datasets, demonstrating that HSR-Diff outperforms state-of-the-art
methods.


------------------------------------------------------------------------------

Title:
Which Spurious Correlations Impact Reasoning in NLI Models? A Visual  Interactive Diagnosis through Data-Constrained Counterfactuals

Abstract: We present a human-in-the-loop dashboard tailored to diagnosing potential
spurious features that NLI models rely on for predictions. The dashboard
enables users to generate diverse and challenging examples by drawing
inspiration from GPT-3 suggestions. Additionally, users can receive feedback
from a trained NLI model on how challenging the newly created example is and
make refinements based on the feedback. Through our investigation, we discover
several categories of spurious correlations that impact the reasoning of NLI
models, which we group into three categories: Semantic Relevance, Logical
Fallacies, and Bias. Based on our findings, we identify and describe various
research opportunities, including diversifying training data and assessing NLI
models' robustness by creating adversarial test suites.


------------------------------------------------------------------------------

Title:
Condition numbers for the Moore-Penrose inverse and the least squares  problem involving rank-structured matrices

Abstract: Perturbation theory plays a crucial role in sensitivity analysis, which is
extensively used to assess the robustness of numerical techniques. To quantify
the relative sensitivity of any problem, it becomes essential to investigate
structured condition numbers (CNs) via componentwise perturbation theory. This
paper address and analyze structured mixed condition number (MCN) and
componentwise condition number (CCN) for the Moore-Penrose (M-P) inverse and
the minimum norm least squares (MNLS) solution involving rank-structured
matrices, which include the Cauchy-Vandermonde (CV) matrices and
$\{1,1\}$-quasiseparable (QS) matrices. A general framework has been developed
first to compute the upper bounds for MCN and CCN of rank deficient
parameterized matrices. This framework leads to faster computation of upper
bounds of structured CNs for CV and $\{1,1\}$-QS matrices. Furthermore,
comparisons of obtained upper bounds are investigated theoretically and
experimentally. In addition, the structured effective CNs for the M-P inverse
and the MNLS solution of $\{1,1\}$-QS matrices are presented. Numerical tests
reveal the reliability of the proposed upper bounds as well as demonstrate that
the effective CNs are computationally less expensive and can be substantially
smaller compared to the unstructured CNs.


------------------------------------------------------------------------------

Title:
Facial Expression Re-targeting from a Single Character

Abstract: Video retargeting for digital face animation is used in virtual reality,
social media, gaming, movies, and video conference, aiming to animate avatars'
facial expressions based on videos of human faces. The standard method to
represent facial expressions for 3D characters is by blendshapes, a vector of
weights representing the avatar's neutral shape and its variations under facial
expressions, e.g., smile, puff, blinking. Datasets of paired frames with
blendshape vectors are rare, and labeling can be laborious, time-consuming, and
subjective. In this work, we developed an approach that handles the lack of
appropriate datasets. Instead, we used a synthetic dataset of only one
character. To generalize various characters, we re-represented each frame to
face landmarks. We developed a unique deep-learning architecture that groups
landmarks for each facial organ and connects them to relevant blendshape
weights. Additionally, we incorporated complementary methods for facial
expressions that landmarks did not represent well and gave special attention to
eye expressions. We have demonstrated the superiority of our approach to
previous research in qualitative and quantitative metrics. Our approach
achieved a higher MOS of 68% and a lower MSE of 44.2% when tested on videos
with various users and expressions.


------------------------------------------------------------------------------

Title:
OphGLM: Training an Ophthalmology Large Language-and-Vision Assistant  based on Instructions and Dialogue

Abstract: Large multimodal language models (LMMs) have achieved significant success in
general domains. However, due to the significant differences between medical
images and text and general web content, the performance of LMMs in medical
scenarios is limited. In ophthalmology, clinical diagnosis relies on multiple
modalities of medical images, but unfortunately, multimodal ophthalmic large
language models have not been explored to date. In this paper, we study and
construct an ophthalmic large multimodal model. Firstly, we use fundus images
as an entry point to build a disease assessment and diagnosis pipeline to
achieve common ophthalmic disease diagnosis and lesion segmentation. Then, we
establish a new ophthalmic multimodal instruction-following and dialogue
fine-tuning dataset based on disease-related knowledge data and publicly
available real-world medical dialogue. We introduce visual ability into the
large language model to complete the ophthalmic large language and vision
assistant (OphGLM). Our experimental results demonstrate that the OphGLM model
performs exceptionally well, and it has the potential to revolutionize clinical
applications in ophthalmology. The dataset, code, and models will be made
publicly available at this https URL


------------------------------------------------------------------------------

Title:
Mixture Encoder for Joint Speech Separation and Recognition

Abstract: Multi-speaker automatic speech recognition (ASR) is crucial for many
real-world applications, but it requires dedicated modeling techniques.
Existing approaches can be divided into modular and end-to-end methods. Modular
approaches separate speakers and recognize each of them with a single-speaker
ASR system. End-to-end models process overlapped speech directly in a single,
powerful neural network. This work proposes a middle-ground approach that
leverages explicit speech separation similarly to the modular approach but also
incorporates mixture speech information directly into the ASR module in order
to mitigate the propagation of errors made by the speech separator. We also
explore a way to exchange cross-speaker context information through a layer
that combines information of the individual speakers. Our system is optimized
through separate and joint training stages and achieves a relative improvement
of 7% in word error rate over a purely modular setup on the SMS-WSJ task.


------------------------------------------------------------------------------

Title:
Leveraging User-Wise SVD for Accelerated Convergence in Iterative  ELAA-MIMO Detections

Abstract: Numerous low-complexity iterative algorithms have been proposed to offer the
performance of linear multiple-input multiple-output (MIMO) detectors bypassing
the channel matrix inverse. These algorithms exhibit fast convergence in
well-conditioned MIMO channels. However, in the emerging MIMO paradigm
utilizing extremely large aperture arrays (ELAA), the wireless channel may
become ill-conditioned because of spatial non-stationarity, which results in a
considerably slower convergence rate for these algorithms. In this paper, we
propose a novel ELAA-MIMO detection scheme that leverages user-wise singular
value decomposition (UW-SVD) to accelerate the convergence of these iterative
algorithms. By applying UW-SVD, the MIMO signal model can be converted into an
equivalent form featuring a better-conditioned transfer function. Then,
existing iterative algorithms can be utilized to recover the transmitted signal
from the converted signal model with accelerated convergence towards
zero-forcing performance. Our simulation results indicate that proposed UW-SVD
scheme can significantly accelerate the convergence of the iterative algorithms
in spatially non-stationary ELAA channels. Moreover, the computational
complexity of the UW-SVD is comparatively minor in relation to the inherent
complexity of the iterative algorithms.


------------------------------------------------------------------------------

Title:
Automated Machine Learning for Remaining Useful Life Predictions

Abstract: Being able to predict the remaining useful life (RUL) of an engineering
system is an important task in prognostics and health management. Recently,
data-driven approaches to RUL predictions are becoming prevalent over
model-based approaches since no underlying physical knowledge of the
engineering system is required. Yet, this just replaces required expertise of
the underlying physics with machine learning (ML) expertise, which is often
also not available. Automated machine learning (AutoML) promises to build
end-to-end ML pipelines automatically enabling domain experts without ML
expertise to create their own models. This paper introduces AutoRUL, an
AutoML-driven end-to-end approach for automatic RUL predictions. AutoRUL
combines fine-tuned standard regression methods to an ensemble with high
predictive power. By evaluating the proposed method on eight real-world and
synthetic datasets against state-of-the-art hand-crafted models, we show that
AutoML provides a viable alternative to hand-crafted data-driven RUL
predictions. Consequently, creating RUL predictions can be made more accessible
for domain experts using AutoML by eliminating ML expertise from data-driven
model construction.


------------------------------------------------------------------------------

Title:
Optimal (degree+1)-Coloring in Congested Clique

Abstract: We consider the distributed complexity of the (degree+1)-list coloring
problem, in which each node $u$ of degree $d(u)$ is assigned a palette of
$d(u)+1$ colors, and the goal is to find a proper coloring using these color
palettes. The (degree+1)-list coloring problem is a natural generalization of
the classical $(\Delta+1)$-coloring and $(\Delta+1)$-list coloring problems,
both being benchmark problems extensively studied in distributed and parallel
computing. In this paper we settle the complexity of the (degree+1)-list
coloring problem in the Congested Clique model by showing that it can be solved
deterministically in a constant number of rounds.


------------------------------------------------------------------------------

Title:
Understanding human mobility patterns in Chicago: an analysis of taxi  data using clustering techniques

Abstract: Understanding human mobility patterns is important in applications as diverse
as urban planning, public health, and political organizing. One rich source of
data on human mobility is taxi ride data. Using the city of Chicago as a case
study, we examine data from taxi rides in 2016 with the goal of understanding
how neighborhoods are interconnected. This analysis will provide a sense of
which neighborhoods individuals are using taxis to travel between, suggesting
regions to focus new public transit development efforts. Additionally, this
analysis will map traffic circulation patterns and provide an understanding of
where in the city people are traveling from and where they are heading to -
perhaps informing traffic or road pollution mitigation efforts. For the first
application, representing the data as an undirected graph will suffice. Transit
lines run in both directions so simply a knowledge of which neighborhoods have
high rates of taxi travel between them provides an argument for placing public
transit along those routes. However, in order to understand the flow of people
throughout a city, we must make a distinction between the neighborhood from
which people are departing and the areas to which they are arriving - this
requires methods that can deal with directed graphs. All developed codes can be
found at this https URL


------------------------------------------------------------------------------

Title:
Limits for Learning with Language Models

Abstract: With the advent of large language models (LLMs), the trend in NLP has been to
train LLMs on vast amounts of data to solve diverse language understanding and
generation tasks. The list of LLM successes is long and varied. Nevertheless,
several recent papers provide empirical evidence that LLMs fail to capture
important aspects of linguistic meaning. Focusing on universal quantification,
we provide a theoretical foundation for these empirical findings by proving
that LLMs cannot learn certain fundamental semantic properties including
semantic entailment and consistency as they are defined in formal semantics.
More generally, we show that LLMs are unable to learn concepts beyond the first
level of the Borel Hierarchy, which imposes severe limits on the ability of
LMs, both large and small, to capture many aspects of linguistic meaning. This
means that LLMs will continue to operate without formal guarantees on tasks
that require entailments and deep linguistic understanding.


------------------------------------------------------------------------------

Title:
Investigating Pre-trained Language Models on Cross-Domain Datasets, a  Step Closer to General AI

Abstract: Pre-trained language models have recently emerged as a powerful tool for
fine-tuning a variety of language tasks. Ideally, when models are pre-trained
on large amount of data, they are expected to gain implicit knowledge. In this
paper, we investigate the ability of pre-trained language models to generalize
to different non-language tasks. In particular, we test them on tasks from
different domains such as computer vision, reasoning on hierarchical data, and
protein fold prediction. The four pre-trained models that we used, T5, BART,
BERT, and GPT-2 achieve outstanding results. They all have similar performance
and they outperform transformers that are trained from scratch by a large
margin. For instance, pre-trained language models perform better on the Listops
dataset, with an average accuracy of 58.7\%, compared to transformers trained
from scratch, which have an average accuracy of 29.0\%. The significant
improvement demonstrated across three types of datasets suggests that
pre-training on language helps the models to acquire general knowledge,
bringing us a step closer to general AI. We also showed that reducing the
number of parameters in pre-trained language models does not have a great
impact as the performance drops slightly when using T5-Small instead of
T5-Base. In fact, when using only 2\% of the parameters, we achieved a great
improvement compared to training from scratch. Finally, in contrast to prior
work, we find out that using pre-trained embeddings for the input layer is
necessary to achieve the desired results.


------------------------------------------------------------------------------

Title:
Decisions & Disruptions 2: Decide Harder

Abstract: Cyber incident response is critical to business continuity -- we describe a
new exercise that challenges professionals to play the role of Chief
Information Security Officer (CISO) for a major financial organisation. Teams
must decide how organisational team and budget resources should be deployed
across Enterprise Architecture (EA) upgrades and cyber incidents. Every choice
made has an impact -- some prevent whilst others may trigger new or continue
current attacks. We explain how the underlying platform supports these
interactions through a reactionary event mechanism that introduces events based
on the current attack surface of the organisation. We explore how our platform
manages to introduce randomness on top of triggered events to ensure that the
exercise is not deterministic and better matches incidents in the real world.
We conclude by describing next steps for the exercise and how we plan to use it
in the future to better understand risk decision making.


------------------------------------------------------------------------------

Title:
Benchmark data to study the influence of pre-training on explanation  performance in MR image classification

Abstract: Convolutional Neural Networks (CNNs) are frequently and successfully used in
medical prediction tasks. They are often used in combination with transfer
learning, leading to improved performance when training data for the task are
scarce. The resulting models are highly complex and typically do not provide
any insight into their predictive mechanisms, motivating the field of
'explainable' artificial intelligence (XAI). However, previous studies have
rarely quantitatively evaluated the 'explanation performance' of XAI methods
against ground-truth data, and transfer learning and its influence on objective
measures of explanation performance has not been investigated. Here, we propose
a benchmark dataset that allows for quantifying explanation performance in a
realistic magnetic resonance imaging (MRI) classification task. We employ this
benchmark to understand the influence of transfer learning on the quality of
explanations. Experimental results show that popular XAI methods applied to the
same underlying model differ vastly in performance, even when considering only
correctly classified examples. We further observe that explanation performance
strongly depends on the task used for pre-training and the number of CNN layers
pre-trained. These results hold after correcting for a substantial correlation
between explanation and classification performance.


------------------------------------------------------------------------------

Title:
Exploiting Multimodal Synthetic Data for Egocentric Human-Object  Interaction Detection in an Industrial Scenario

Abstract: In this paper, we tackle the problem of Egocentric Human-Object Interaction
(EHOI) detection in an industrial setting. To overcome the lack of public
datasets in this context, we propose a pipeline and a tool for generating
synthetic images of EHOIs paired with several annotations and data signals
(e.g., depth maps or instance segmentation masks). Using the proposed pipeline,
we present EgoISM-HOI a new multimodal dataset composed of synthetic EHOI
images in an industrial environment with rich annotations of hands and objects.
To demonstrate the utility and effectiveness of synthetic EHOI data produced by
the proposed tool, we designed a new method that predicts and combines
different multimodal signals to detect EHOIs in RGB images. Our study shows
that exploiting synthetic data to pre-train the proposed method significantly
improves performance when tested on real-world data. Moreover, the proposed
approach outperforms state-of-the-art class-agnostic methods. To support
research in this field, we publicly release the datasets, source code, and
pre-trained models at this https URL


------------------------------------------------------------------------------

Title:
A Comprehensive Study on the Robustness of Image Classification and  Object Detection in Remote Sensing: Surveying and Benchmarking

Abstract: Deep neural networks (DNNs) have found widespread applications in
interpreting remote sensing (RS) imagery. However, it has been demonstrated in
previous works that DNNs are vulnerable to different types of noises,
particularly adversarial noises. Surprisingly, there has been a lack of
comprehensive studies on the robustness of RS tasks, prompting us to undertake
a thorough survey and benchmark on the robustness of image classification and
object detection in RS. To our best knowledge, this study represents the first
comprehensive examination of both natural robustness and adversarial robustness
in RS tasks. Specifically, we have curated and made publicly available datasets
that contain natural and adversarial noises. These datasets serve as valuable
resources for evaluating the robustness of DNNs-based models. To provide a
comprehensive assessment of model robustness, we conducted meticulous
experiments with numerous different classifiers and detectors, encompassing a
wide range of mainstream methods. Through rigorous evaluation, we have
uncovered insightful and intriguing findings, which shed light on the
relationship between adversarial noise crafting and model training, yielding a
deeper understanding of the susceptibility and limitations of various models,
and providing guidance for the development of more resilient and robust models


------------------------------------------------------------------------------

Title:
Visualizing Relation Between (De)Motivating Topics and Public Stance  toward COVID-19 Vaccine

Abstract: While social media plays a vital role in communication nowadays,
misinformation and trolls can easily take over the conversation and steer
public opinion on these platforms. We saw the effect of misinformation during
the {COVID-19} pandemic when public health officials faced significant
push-back while trying to motivate the public to vaccinate. To tackle the
current and any future threats in emergencies and motivate the public towards a
common goal, it is essential to understand how public motivation shifts and
which topics resonate among the general population. In this study, we proposed
an interactive visualization tool to inspect and analyze the topics that
resonated among Twitter-sphere during the {COVID-19} pandemic and understand
the key factors that shifted public stance for vaccination. This tool can
easily be generalized for any scenario for visual analysis and to increase the
transparency of social media data for researchers and the general population
alike.


------------------------------------------------------------------------------

Title:
A Reliable and Interpretable Framework of Multi-view Learning for Liver  Fibrosis Staging

Abstract: Staging of liver fibrosis is important in the diagnosis and treatment
planning of patients suffering from liver diseases. Current deep learning-based
methods using abdominal magnetic resonance imaging (MRI) usually take a
sub-region of the liver as an input, which nevertheless could miss critical
information. To explore richer representations, we formulate this task as a
multi-view learning problem and employ multiple sub-regions of the liver.
Previously, features or predictions are usually combined in an implicit manner,
and uncertainty-aware methods have been proposed. However, these methods could
be challenged to capture cross-view representations, which can be important in
the accurate prediction of staging. Therefore, we propose a reliable multi-view
learning method with interpretable combination rules, which can model global
representations to improve the accuracy of predictions. Specifically, the
proposed method estimates uncertainties based on subjective logic to improve
reliability, and an explicit combination rule is applied based on
Dempster-Shafer's evidence theory with good power of interpretability.
Moreover, a data-efficient transformer is introduced to capture representations
in the global view. Results evaluated on enhanced MRI data show that our method
delivers superior performance over existing multi-view learning methods.


------------------------------------------------------------------------------

Title:
What Constitutes Good Contrastive Learning in Time-Series Forecasting?

Abstract: In recent years, the introduction of self-supervised contrastive learning
(SSCL) has demonstrated remarkable improvements in representation learning
across various domains, including natural language processing and computer
vision. By leveraging the inherent benefits of self-supervision, SSCL enables
the pre-training of representation models using vast amounts of unlabeled data.
Despite these advances, there remains a significant gap in understanding the
impact of different SSCL strategies on time series forecasting performance, as
well as the specific benefits that SSCL can bring. This paper aims to address
these gaps by conducting a comprehensive analysis of the effectiveness of
various training variables, including different SSCL algorithms, learning
strategies, model architectures, and their interplay. Additionally, to gain
deeper insights into the improvements brought about by SSCL in the context of
time-series forecasting, a qualitative analysis of the empirical receptive
field is performed. Through our experiments, we demonstrate that the end-to-end
training of a Transformer model using the Mean Squared Error (MSE) loss and
SSCL emerges as the most effective approach in time series forecasting.
Notably, the incorporation of the contrastive objective enables the model to
prioritize more pertinent information for forecasting, such as scale and
periodic relationships. These findings contribute to a better understanding of
the benefits of SSCL in time series forecasting and provide valuable insights
for future research in this area.


------------------------------------------------------------------------------

Title:
Corrector Operator to Enhance Accuracy and Reliability of Neural  Operator Surrogates of Nonlinear Variational Boundary-Value Problems

Abstract: This work focuses on developing methods for approximating the solution
operators of a class of parametric partial differential equations via neural
operators. Neural operators have several challenges, including the issue of
generating appropriate training data, cost-accuracy trade-offs, and nontrivial
hyperparameter tuning. The unpredictability of the accuracy of neural operators
impacts their applications in downstream problems of inference, optimization,
and control. A framework is proposed based on the linear variational problem
that gives the correction to the prediction furnished by neural operators. The
operator associated with the corrector problem is referred to as the corrector
operator. Numerical results involving a nonlinear diffusion model in two
dimensions with PCANet-type neural operators show almost two orders of increase
in the accuracy of approximations when neural operators are corrected using the
proposed scheme. Further, topology optimization involving a nonlinear diffusion
model is considered to highlight the limitations of neural operators and the
efficacy of the correction scheme. Optimizers with neural operator surrogates
are seen to make significant errors (as high as 80 percent). However, the
errors are much lower (below 7 percent) when neural operators are corrected
following the proposed method.


------------------------------------------------------------------------------

Title:
Annotating Ambiguous Images: General Annotation Strategy for Image  Classification with Real-World Biomedical Validation on Vertebral Fracture  Diagnosis

Abstract: While numerous methods exist to solve classification problems within curated
datasets, these solutions often fall short in biomedical applications due to
the biased or ambiguous nature of the data. These difficulties are particularly
evident when inferring height reduction from vertebral data, a key component of
the clinically-recognized Genant score. Although strategies such as
semi-supervised learning, proposal usage, and class blending may provide some
resolution, a clear and superior solution remains elusive. This paper
introduces a flowchart of general strategy to address these issues. We
demonstrate the application of this strategy by constructing a vertebral
fracture dataset with over 300,000 annotations. This work facilitates the
transition of the classification problem into clinically meaningful scores and
enriches our understanding of vertebral height reduction.


------------------------------------------------------------------------------

Title:
$χ$iplot: web-first visualisation platform for multidimensional data

Abstract: $\chi$iplot is an HTML5-based system for interactive exploration of data and
machine learning models. A key aspect is interaction, not only for the
interactive plots but also between plots. Even though $\chi$iplot is not
restricted to any single application domain, we have developed and tested it
with domain experts in quantum chemistry to study molecular interactions and
regression models. $\chi$iplot can be run both locally and online in a web
browser (keeping the data local). The plots and data can also easily be
exported and shared. A modular structure also makes $\chi$iplot optimal for
developing machine learning and new interaction methods.


------------------------------------------------------------------------------

Title:
ViTEraser: Harnessing the Power of Vision Transformers for Scene Text  Removal with SegMIM Pretraining

Abstract: Scene text removal (STR) aims at replacing text strokes in natural scenes
with visually coherent backgrounds. Recent STR approaches rely on iterative
refinements or explicit text masks, resulting in higher complexity and
sensitivity to the accuracy of text localization. Moreover, most existing STR
methods utilize convolutional neural networks (CNNs) for feature representation
while the potential of vision Transformers (ViTs) remains largely unexplored.
In this paper, we propose a simple-yet-effective ViT-based text eraser, dubbed
ViTEraser. Following a concise encoder-decoder framework, different types of
ViTs can be easily integrated into ViTEraser to enhance the long-range
dependencies and global reasoning. Specifically, the encoder hierarchically
maps the input image into the hidden space through ViT blocks and patch
embedding layers, while the decoder gradually upsamples the hidden features to
the text-erased image with ViT blocks and patch splitting layers. As ViTEraser
implicitly integrates text localization and inpainting, we propose a novel
end-to-end pretraining method, termed SegMIM, which focuses the encoder and
decoder on the text box segmentation and masked image modeling tasks,
respectively. To verify the effectiveness of the proposed methods, we
comprehensively explore the architecture, pretraining, and scalability of the
ViT-based encoder-decoder for STR, which provides deep insights into the
application of ViT to STR. Experimental results demonstrate that ViTEraser with
SegMIM achieves state-of-the-art performance on STR by a substantial margin.
Furthermore, the extended experiment on tampered scene text detection
demonstrates the generality of ViTEraser to other tasks. We believe this paper
can inspire more research on ViT-based STR approaches. Code will be available
at this https URL


------------------------------------------------------------------------------

Title:
Prompt Sapper: A LLM-Empowered Production Tool for Building AI Chains

Abstract: The emergence of foundation models, such as large language models (LLMs)
GPT-4 and text-to-image models DALL-E, has opened up numerous possibilities
across various domains. People can now use natural language (i.e. prompts) to
communicate with AI to perform tasks. While people can use foundation models
through chatbots (e.g., ChatGPT), chat, regardless of the capabilities of the
underlying models, is not a production tool for building reusable AI services.
APIs like LangChain allow for LLM-based application development but require
substantial programming knowledge, thus posing a barrier. To mitigate this, we
propose the concept of AI chain and introduce the best principles and practices
that have been accumulated in software engineering for decades into AI chain
engineering, to systematise AI chain engineering methodology. We also develop a
no-code integrated development environment, Prompt Sapper, which embodies these
AI chain engineering principles and patterns naturally in the process of
building AI chains, thereby improving the performance and quality of AI chains.
With Prompt Sapper, AI chain engineers can compose prompt-based AI services on
top of foundation models through chat-based requirement analysis and visual
programming. Our user study evaluated and demonstrated the efficiency and
correctness of Prompt Sapper.


------------------------------------------------------------------------------

Title:
Evaluating Adversarial Robustness of Convolution-based Human Motion  Prediction

Abstract: Human motion prediction has achieved a brilliant performance with the help of
CNNs, which facilitates human-machine cooperation. However, currently, there is
no work evaluating the potential risk in human motion prediction when facing
adversarial attacks, which may cause danger in real applications. The
adversarial attack will face two problems against human motion prediction: 1.
For naturalness, pose data is highly related to the physical dynamics of human
skeletons where Lp norm constraints cannot constrain the adversarial example
well; 2. Unlike the pixel value in images, pose data is diverse at scale
because of the different acquisition equipment and the data processing, which
makes it hard to set fixed parameters to perform attacks. To solve the problems
above, we propose a new adversarial attack method that perturbs the input human
motion sequence by maximizing the prediction error with physical constraints.
Specifically, we introduce a novel adaptable scheme that facilitates the attack
to suit the scale of the target pose and two physical constraints to enhance
the imperceptibility of the adversarial example. The evaluating experiments on
three datasets show that the prediction errors of all target models are
enlarged significantly, which means current convolution-based human motion
prediction models can be easily disturbed under the proposed attack. The
quantitative analysis shows that prior knowledge and semantic information
modeling can be the key to the adversarial robustness of human motion
predictors. The qualitative results indicate that the adversarial sample is
hard to be noticed when compared frame by frame but is relatively easy to be
detected when the sample is animated.


------------------------------------------------------------------------------

Title:
Adversarial Attacks Neutralization via Data Set Randomization

Abstract: Adversarial attacks on deep-learning models pose a serious threat to their
reliability and security. Existing defense mechanisms are narrow addressing a
specific type of attack or being vulnerable to sophisticated attacks. We
propose a new defense mechanism that, while being focused on image-based
classifiers, is general with respect to the cited category. It is rooted on
hyperspace projection. In particular, our solution provides a pseudo-random
projection of the original dataset into a new dataset. The proposed defense
mechanism creates a set of diverse projected datasets, where each projected
dataset is used to train a specific classifier, resulting in different trained
classifiers with different decision boundaries. During testing, it randomly
selects a classifier to test the input. Our approach does not sacrifice
accuracy over legitimate input. Other than detailing and providing a thorough
characterization of our defense mechanism, we also provide a proof of concept
of using four optimization-based adversarial attacks (PGD, FGSM, IGSM, and
C\&W) and a generative adversarial attack testing them on the MNIST dataset.
Our experimental results show that our solution increases the robustness of
deep learning models against adversarial attacks and significantly reduces the
attack success rate by at least 89% for optimization attacks and 78% for
generative attacks. We also analyze the relationship between the number of used
hyperspaces and the efficacy of the defense mechanism. As expected, the two are
positively correlated, offering an easy-to-tune parameter to enforce the
desired level of security. The generality and scalability of our solution and
adaptability to different attack scenarios, combined with the excellent
achieved results, other than providing a robust defense against adversarial
attacks on deep learning networks, also lay the groundwork for future research
in the field.


------------------------------------------------------------------------------

Title:
Analyzing Font Style Usage and Contextual Factors in Real Images

Abstract: There are various font styles in the world. Different styles give different
impressions and readability. This paper analyzes the relationship between font
styles and contextual factors that might affect font style selection with
large-scale datasets. For example, we will analyze the relationship between
font style and its surrounding object (such as ``bus'') by using about 800,000
words in the Open Images dataset. We also use a book cover dataset to analyze
the relationship between font styles with book genres. Moreover, the meaning of
the word is assumed as another contextual factor. For these numeric analyses,
we utilize our own font-style feature extraction model and word2vec. As a
result of co-occurrence-based relationship analysis, we found several instances
of specific font styles being used for specific contextual factors.


------------------------------------------------------------------------------

Title:
Fast Segment Anything

Abstract: The recently proposed segment anything model (SAM) has made a significant
influence in many computer vision tasks. It is becoming a foundation step for
many high-level tasks, like image segmentation, image caption, and image
editing. However, its huge computation costs prevent it from wider applications
in industry scenarios. The computation mainly comes from the Transformer
architecture at high-resolution inputs. In this paper, we propose a speed-up
alternative method for this fundamental task with comparable performance. By
reformulating the task as segments-generation and prompting, we find that a
regular CNN detector with an instance segmentation branch can also accomplish
this task well. Specifically, we convert this task to the well-studied instance
segmentation task and directly train the existing instance segmentation method
using only 1/50 of the SA-1B dataset published by SAM authors. With our method,
we achieve a comparable performance with the SAM method at 50 times higher
run-time speed. We give sufficient experimental results to demonstrate its
effectiveness. The codes and demos will be released at
this https URL


------------------------------------------------------------------------------

Title:
Edge Devices Inference Performance Comparison

Abstract: In this work, we investigate the inference time of the MobileNet family,
EfficientNet V1 and V2 family, VGG models, Resnet family, and InceptionV3 on
four edge platforms. Specifically NVIDIA Jetson Nano, Intel Neural Stick,
Google Coral USB Dongle, and Google Coral PCIe. Our main contribution is a
thorough analysis of the aforementioned models in multiple settings, especially
as a function of input size, the presence of the classification head, its size,
and the scale of the model. Since throughout the industry, those architectures
are mainly utilized as feature extractors we put our main focus on analyzing
them as such. We show that Google platforms offer the fastest average inference
time, especially for newer models like MobileNet or EfficientNet family, while
Intel Neural Stick is the most universal accelerator allowing to run most
architectures. These results should provide guidance for engineers in the early
stages of AI edge systems development. All of them are accessible at
this https URL


------------------------------------------------------------------------------

Title:
Mean square exponential stability of numerical methods for stochastic  differential delay equations

Abstract: Mean square exponential stability of $\theta$-EM and modified truncated
Euler-Maruyama (MTEM) methods for stochastic differential delay equations
(SDDEs) are investigated in this paper. We present new criterion of mean square
exponential stability of the $\theta$-EM and MTEM methods for SDDEs, which are
different from most existing results under Khasminskii-type conditions. Two
examples are provided to support our conclusions.


------------------------------------------------------------------------------

Title:
DAT: Data Architecture Modeling Tool for Data-Driven Applications

Abstract: Data is the key to success for any Data-Driven Organization, and managing it
is considered the most challenging task. Data Architecture (DA) focuses on
describing, collecting, storing, processing, and analyzing the data to meet
business needs. In this tool demo paper, we present the DAT, a model-driven
engineering tool enabling data architects, data engineers, and other
stakeholders to describe how data flows through the system and provides a
blueprint for managing data that saves time and effort dedicated to Data
Architectures for IoT applications. We evaluated this work by modeling five
case studies, receiving expressiveness and ease of use feedback from two
companies, more than six researchers, and eighteen undergraduate students from
the software architecture course


------------------------------------------------------------------------------

Title:
Synaptic metaplasticity with multi-level memristive devices

Abstract: Deep learning has made remarkable progress in various tasks, surpassing human
performance in some cases. However, one drawback of neural networks is
catastrophic forgetting, where a network trained on one task forgets the
solution when learning a new one. To address this issue, recent works have
proposed solutions based on Binarized Neural Networks (BNNs) incorporating
metaplasticity. In this work, we extend this solution to quantized neural
networks (QNNs) and present a memristor-based hardware solution for
implementing metaplasticity during both inference and training. We propose a
hardware architecture that integrates quantized weights in memristor devices
programmed in an analog multi-level fashion with a digital processing unit for
high-precision metaplastic storage. We validated our approach using a combined
software framework and memristor based crossbar array for in-memory computing
fabricated in 130 nm CMOS technology. Our experimental results show that a
two-layer perceptron achieves 97% and 86% accuracy on consecutive training of
MNIST and Fashion-MNIST, equal to software baseline. This result demonstrates
immunity to catastrophic forgetting and the resilience to analog device
imperfections of the proposed solution. Moreover, our architecture is
compatible with the memristor limited endurance and has a 15x reduction in
memory


------------------------------------------------------------------------------

Title:
Spatial Heterophily Aware Graph Neural Networks

Abstract: Graph Neural Networks (GNNs) have been broadly applied in many urban
applications upon formulating a city as an urban graph whose nodes are urban
objects like regions or points of interest. Recently, a few enhanced GNN
architectures have been developed to tackle heterophily graphs where connected
nodes are dissimilar. However, urban graphs usually can be observed to possess
a unique spatial heterophily property; that is, the dissimilarity of neighbors
at different spatial distances can exhibit great diversity. This property has
not been explored, while it often exists. To this end, in this paper, we
propose a metric, named Spatial Diversity Score, to quantitatively measure the
spatial heterophily and show how it can influence the performance of GNNs.
Indeed, our experimental investigation clearly shows that existing heterophilic
GNNs are still deficient in handling the urban graph with high spatial
diversity score. This, in turn, may degrade their effectiveness in urban
applications. Along this line, we propose a Spatial Heterophily Aware Graph
Neural Network (SHGNN), to tackle the spatial diversity of heterophily of urban
graphs. Based on the key observation that spatially close neighbors on the
urban graph present a more similar mode of difference to the central node, we
first design a rotation-scaling spatial aggregation module, whose core idea is
to properly group the spatially close neighbors and separately process each
group with less diversity inside. Then, a heterophily-sensitive spatial
interaction module is designed to adaptively capture the commonality and
diverse dissimilarity in different spatial groups. Extensive experiments on
three real-world urban datasets demonstrate the superiority of our SHGNN over
several its competitors.


------------------------------------------------------------------------------

Title:
Polygon Detection for Room Layout Estimation using Heterogeneous Graphs  and Wireframes

Abstract: This paper presents a neural network based semantic plane detection method
utilizing polygon representations. The method can for example be used to solve
room layout estimations tasks. The method is built on, combines and further
develops several different modules from previous research. The network takes an
RGB image and estimates a wireframe as well as a feature space using an
hourglass backbone. From these, line and junction features are sampled. The
lines and junctions are then represented as an undirected graph, from which
polygon representations of the sought planes are obtained. Two different
methods for this last step are investigated, where the most promising method is
built on a heterogeneous graph transformer. The final output is in all cases a
projection of the semantic planes in 2D. The methods are evaluated on the
Structured 3D dataset and we investigate the performance both using sampled and
estimated wireframes. The experiments show the potential of the graph-based
method by outperforming state of the art methods in Room Layout estimation in
the 2D metrics using synthetic wireframe detections.


------------------------------------------------------------------------------

Title:
Complex accident, clear responsibility

Abstract: The problem of allocating accident responsibility for autonomous driving is a
difficult issue in the field of autonomous driving. Due to the complexity of
autonomous driving technology, most of the research on the responsibility of
autonomous driving accidents has remained at the theoretical level. When
encountering actual autonomous driving accidents, a proven and fair solution is
needed. To address this problem, this study proposes a multi-subject
responsibility allocation optimization method based on the RCModel (Risk Chain
Model), which analyzes the responsibility of each actor from a technical
perspective and promotes a more reasonable and fair allocation of
responsibility.


------------------------------------------------------------------------------

Title:
Structure-Aware DropEdge Towards Deep Graph Convolutional Networks

Abstract: It has been discovered that Graph Convolutional Networks (GCNs) encounter a
remarkable drop in performance when multiple layers are piled up. The main
factor that accounts for why deep GCNs fail lies in over-smoothing, which
isolates the network output from the input with the increase of network depth,
weakening expressivity and trainability. In this paper, we start by
investigating refined measures upon DropEdge -- an existing simple yet
effective technique to relieve over-smoothing. We term our method as DropEdge++
for its two structure-aware samplers in contrast to DropEdge: layer-dependent
sampler and feature-dependent sampler. Regarding the layer-dependent sampler,
we interestingly find that increasingly sampling edges from the bottom layer
yields superior performance than the decreasing counterpart as well as
DropEdge. We theoretically reveal this phenomenon with Mean-Edge-Number (MEN),
a metric closely related to over-smoothing. For the feature-dependent sampler,
we associate the edge sampling probability with the feature similarity of node
pairs, and prove that it further correlates the convergence subspace of the
output layer with the input features. Extensive experiments on several node
classification benchmarks, including both full- and semi- supervised tasks,
illustrate the efficacy of DropEdge++ and its compatibility with a variety of
backbones by achieving generally better performance over DropEdge and the
no-drop version.


------------------------------------------------------------------------------

Title:
Design of Energy Harvesting based Hardware for IoT Applications

Abstract: Internet of Things (IoT) devices are rapidly expanding in many areas,
including deep mines, space, industrial environments, and health monitoring
systems. Most of the sensors and actuators are battery-powered, and these
batteries have a finite lifespan. Maintaining and replacing these many
batteries increases the maintenance cost of IoT systems and causes massive
environmental damage. Energy-harvesting devices (EHDs) are the alternative and
promising solution for these battery-operated IoT devices. These EHDs collect
energy from the environment and use it for daily computations, like collecting
and processing data from the sensors and actuators. Using EHDs in IoT reduces
overall maintenance costs and makes the IoT system energy-sufficient. However,
energy availability from these EHDs is unpredictable, resulting in frequent
power failures.
Most of these devices use volatile memories as storage elements, implying
that all collected data and decisions made by the IoT devices are lost during
frequent power failures, resulting in two possible overheads. First, the IoT
device must execute the application from the beginning whenever power comes
back. Second, IoT devices may make wrong decisions by considering incomplete
data, i.e., data-inconsistency issues. To address these two challenges, a
computing model is required that backs up the collected data during power
failures and restores it for later computations; this type of computing is
defined as intermittent computing. However, this computing model doesn't work
with conventional processors or memories. Non-volatile memory and processors
are required to design a battery-less IoT device that supports intermittent
computing.


------------------------------------------------------------------------------

Title:
ChatGPT as a tool for User Story Quality Evaluation: Trustworthy Out of  the Box?

Abstract: In Agile software development, user stories play a vital role in capturing
and conveying end-user needs, prioritizing features, and facilitating
communication and collaboration within development teams. However, automated
methods for evaluating user stories require training in NLP tools and can be
time-consuming to develop and integrate. This study explores using ChatGPT for
user story quality evaluation and compares its performance with an existing
benchmark. Our study shows that ChatGPT's evaluation aligns well with human
evaluation, and we propose a ``best of three'' strategy to improve its output
stability. We also discuss the concept of trustworthiness in AI and its
implications for non-experts using ChatGPT's unprocessed outputs. Our research
contributes to understanding the reliability and applicability of AI in user
story evaluation and offers recommendations for future research.


------------------------------------------------------------------------------

Title:
Chili Pepper Disease Diagnosis via Image Reconstruction Using GrabCut  and Generative Adversarial Serial Autoencoder

Abstract: With the recent development of smart farms, researchers are very interested
in such fields. In particular, the field of disease diagnosis is the most
important factor. Disease diagnosis belongs to the field of anomaly detection
and aims to distinguish whether plants or fruits are normal or abnormal. The
problem can be solved by binary or multi-classification based on CNN, but it
can also be solved by image reconstruction. However, due to the limitation of
the performance of image generation, SOTA's methods propose a score calculation
method using a latent vector error. In this paper, we propose a network that
focuses on chili peppers and proceeds with background removal through Grabcut.
It shows high performance through image-based score calculation method. Due to
the difficulty of reconstructing the input image, the difference between the
input and output images is large. However, the serial autoencoder proposed in
this paper uses the difference between the two fake images except for the
actual input as a score. We propose a method of generating meaningful images
using the GAN structure and classifying three results simultaneously by one
discriminator. The proposed method showed higher performance than previous
researches, and image-based scores showed the best performanc


------------------------------------------------------------------------------

Title:
A Semi-Autoregressive Graph Generative Model for Dependency Graph  Parsing

Abstract: Recent years have witnessed the impressive progress in Neural Dependency
Parsing. According to the different factorization approaches to the graph joint
probabilities, existing parsers can be roughly divided into autoregressive and
non-autoregressive patterns. The former means that the graph should be
factorized into multiple sequentially dependent components, then it can be
built up component by component. And the latter assumes these components to be
independent so that they can be outputted in a one-shot manner. However, when
treating the directed edge as an explicit dependency relationship, we discover
that there is a mixture of independent and interdependent components in the
dependency graph, signifying that both aforementioned models fail to precisely
capture the explicit dependencies among nodes and edges. Based on this
property, we design a Semi-Autoregressive Dependency Parser to generate
dependency graphs via adding node groups and edge groups autoregressively while
pouring out all group elements in parallel. The model gains a trade-off between
non-autoregression and autoregression, which respectively suffer from the lack
of target inter-dependencies and the uncertainty of graph generation orders.
The experiments show the proposed parser outperforms strong baselines on
Enhanced Universal Dependencies of multiple languages, especially achieving
$4\%$ average promotion at graph-level accuracy. Also, the performances of
model variations show the importance of specific parts.


------------------------------------------------------------------------------

Title:
DGEMM on Integer Matrix Multiplication Unit

Abstract: Deep learning hardware achieves high throughput and low power consumption by
reducing computing precision and specializing in matrix multiplication. For
machine learning inference, fixed-point value computation is commonplace, where
the input and output values and the model parameters are quantized. Thus, many
processors are now equipped with fast integer matrix multiplication units
(IMMU). It is of significant interest to find a way to harness these IMMUs to
improve the performance of HPC applications while maintaining accuracy. We
focus on the Ozaki scheme, which computes a high-precision matrix
multiplication by using lower-precision computing units, and show the
advantages and disadvantages of using IMMU. The experiment using integer Tensor
Cores shows that we can compute double-precision matrix multiplication faster
than cuBLAS and an existing Ozaki scheme implementation on FP16 Tensor Cores on
NVIDIA consumer GPUs. Furthermore, we demonstrate accelerating a quantum
circuit simulation by up to 4.33 while maintaining the FP64 accuracy.


------------------------------------------------------------------------------

Title:
The Importance of Education for Technological Development and the Role  of Internet-Based Learning in Education

Abstract: In today's world, many technologically advanced countries have realized that
real power lies not in physical strength but in educated minds. As a result,
every country has embarked on restructuring its education system to meet the
demands of technology. As a country in the midst of these developments, we
cannot remain indifferent to this transformation in education. In the
Information Age of the 21st century, rapid access to information is crucial for
the development of individuals and societies. To take our place among the
knowledge societies in a world moving rapidly towards globalization, we must
closely follow technological innovations and meet the requirements of
technology. This can be achieved by providing learning opportunities to anyone
interested in acquiring education in their area of interest. This study focuses
on the advantages and disadvantages of internet-based learning compared to
traditional teaching methods, the importance of computer usage in
internet-based learning, negative factors affecting internet-based learning,
and the necessary recommendations for addressing these issues. In today's
world, it is impossible to talk about education without technology or
technology without education.


------------------------------------------------------------------------------

Title:
HumanDiffusion: diffusion model using perceptual gradients

Abstract: We propose {\it HumanDiffusion,} a diffusion model trained from humans'
perceptual gradients to learn an acceptable range of data for humans (i.e.,
human-acceptable distribution). Conventional HumanGAN aims to model the
human-acceptable distribution wider than the real-data distribution by training
a neural network-based generator with human-based discriminators. However,
HumanGAN training tends to converge in a meaningless distribution due to the
gradient vanishing or mode collapse and requires careful heuristics. In
contrast, our HumanDiffusion learns the human-acceptable distribution through
Langevin dynamics based on gradients of human perceptual evaluations. Our
training iterates a process to diffuse real data to cover a wider
human-acceptable distribution and can avoid the issues in the HumanGAN
training. The evaluation results demonstrate that our HumanDiffusion can
successfully represent the human-acceptable distribution without any heuristics
for the training.


------------------------------------------------------------------------------

Title:
Recoil: Parallel rANS Decoding with Decoder-Adaptive Scalability

Abstract: Entropy coding is essential to data compression, image and video coding, etc.
The Range variant of Asymmetric Numeral Systems (rANS) is a modern entropy
coder, featuring superior speed and compression rate. As rANS is not designed
for parallel execution, the conventional approach to parallel rANS partitions
the input symbol sequence and encodes partitions with independent codecs, and
more partitions bring extra overhead. This approach is found in
state-of-the-art implementations such as DietGPU. It is unsuitable for
content-delivery applications, as the parallelism is wasted if the decoder
cannot decode all the partitions in parallel, but all the overhead is still
transferred.
To solve this, we propose Recoil, a parallel rANS decoding approach with
decoder-adaptive scalability. We discover that a single rANS-encoded bitstream
can be decoded from any arbitrary position if the intermediate states are
known. After renormalization, these states also have a smaller upper bound,
which can be stored efficiently. We then split the encoded bitstream using a
heuristic to evenly distribute the workload, and store the intermediate states
and corresponding symbol indices as metadata. The splits can then be combined
simply by eliminating extra metadata entries.
The main contribution of Recoil is reducing unnecessary data transfer by
adaptively scaling parallelism overhead to match the decoder capability. The
experiments show that Recoil decoding throughput is comparable to the
conventional approach, scaling massively on CPUs and GPUs and greatly
outperforming various other ANS-based codecs.


------------------------------------------------------------------------------

Title:
Self-Distilled Masked Auto-Encoders are Efficient Video Anomaly  Detectors

Abstract: We propose an efficient abnormal event detection model based on a lightweight
masked auto-encoder (AE) applied at the video frame level. The novelty of the
proposed model is threefold. First, we introduce an approach to weight tokens
based on motion gradients, thus avoiding learning to reconstruct the static
background scene. Second, we integrate a teacher decoder and a student decoder
into our architecture, leveraging the discrepancy between the outputs given by
the two decoders to improve anomaly detection. Third, we generate synthetic
abnormal events to augment the training videos, and task the masked AE model to
jointly reconstruct the original frames (without anomalies) and the
corresponding pixel-level anomaly maps. Our design leads to an efficient and
effective model, as demonstrated by the extensive experiments carried out on
three benchmarks: Avenue, ShanghaiTech and UCSD Ped2. The empirical results
show that our model achieves an excellent trade-off between speed and accuracy,
obtaining competitive AUC scores, while processing 1670 FPS. Hence, our model
is between 8 and 70 times faster than competing methods. We also conduct an
ablation study to justify our design.


------------------------------------------------------------------------------

Title:
Strategies in Transfer Learning for Low-Resource Speech Synthesis: Phone  Mapping, Features Input, and Source Language Selection

Abstract: We compare using a PHOIBLE-based phone mapping method and using phonological
features input in transfer learning for TTS in low-resource languages. We use
diverse source languages (English, Finnish, Hindi, Japanese, and Russian) and
target languages (Bulgarian, Georgian, Kazakh, Swahili, Urdu, and Uzbek) to
test the language-independence of the methods and enhance the findings'
applicability. We use Character Error Rates from automatic speech recognition
and predicted Mean Opinion Scores for evaluation. Results show that both phone
mapping and features input improve the output quality and the latter performs
better, but these effects also depend on the specific language combination. We
also compare the recently-proposed Angular Similarity of Phone Frequencies
(ASPF) with a family tree-based distance measure as a criterion to select
source languages in transfer learning. ASPF proves effective if label-based
phone input is used, while the language distance does not have expected
effects.


------------------------------------------------------------------------------

Title:
Spiking Neural Network for Ultra-low-latency and High-accurate Object  Detection

Abstract: Spiking Neural Networks (SNNs) have garnered widespread interest for their
energy efficiency and brain-inspired event-driven properties. While recent
methods like Spiking-YOLO have expanded the SNNs to more challenging object
detection tasks, they often suffer from high latency and low detection
accuracy, making them difficult to deploy on latency sensitive mobile
platforms. Furthermore, the conversion method from Artificial Neural Networks
(ANNs) to SNNs is hard to maintain the complete structure of the ANNs,
resulting in poor feature representation and high conversion errors. To address
these challenges, we propose two methods: timesteps compression and
spike-time-dependent integrated (STDI) coding. The former reduces the timesteps
required in ANN-SNN conversion by compressing information, while the latter
sets a time-varying threshold to expand the information holding capacity. We
also present a SNN-based ultra-low latency and high accurate object detection
model (SUHD) that achieves state-of-the-art performance on nontrivial datasets
like PASCAL VOC and MS COCO, with about remarkable 750x fewer timesteps and 30%
mean average precision (mAP) improvement, compared to the Spiking-YOLO on MS
COCO datasets. To the best of our knowledge, SUHD is the deepest spike-based
object detection model to date that achieves ultra low timesteps to complete
the lossless conversion.


------------------------------------------------------------------------------

Title:
3HAN: A Deep Neural Network for Fake News Detection

Abstract: The rapid spread of fake news is a serious problem calling for AI solutions.
We employ a deep learning based automated detector through a three level
hierarchical attention network (3HAN) for fast, accurate detection of fake
news. 3HAN has three levels, one each for words, sentences, and the headline,
and constructs a news vector: an effective representation of an input news
article, by processing an article in an hierarchical bottom-up manner. The
headline is known to be a distinguishing feature of fake news, and furthermore,
relatively few words and sentences in an article are more important than the
rest. 3HAN gives a differential importance to parts of an article, on account
of its three layers of attention. By experiments on a large real-world data
set, we observe the effectiveness of 3HAN with an accuracy of 96.77%. Unlike
some other deep learning models, 3HAN provides an understandable output through
the attention weights given to different parts of an article, which can be
visualized through a heatmap to enable further manual fact checking.


------------------------------------------------------------------------------

Title:
Towards Accurate Translation via Semantically Appropriate Application of  Lexical Constraints

Abstract: Lexically-constrained NMT (LNMT) aims to incorporate user-provided
terminology into translations. Despite its practical advantages, existing work
has not evaluated LNMT models under challenging real-world conditions. In this
paper, we focus on two important but under-studied issues that lie in the
current evaluation process of LNMT studies. The model needs to cope with
challenging lexical constraints that are "homographs" or "unseen" during
training. To this end, we first design a homograph disambiguation module to
differentiate the meanings of homographs. Moreover, we propose PLUMCOT, which
integrates contextually rich information about unseen lexical constraints from
pre-trained language models and strengthens a copy mechanism of the pointer
network via direct supervision of a copying score. We also release HOLLY, an
evaluation benchmark for assessing the ability of a model to cope with
"homographic" and "unseen" lexical constraints. Experiments on HOLLY and the
previous test setup show the effectiveness of our method. The effects of
PLUMCOT are shown to be remarkable in "unseen" constraints. Our dataset is
available at this https URL


------------------------------------------------------------------------------

Title:
Learning Homogenization for Elliptic Operators

Abstract: Multiscale partial differential equations (PDEs) arise in various
applications, and several schemes have been developed to solve them
efficiently. Homogenization theory is a powerful methodology that eliminates
the small-scale dependence, resulting in simplified equations that are
computationally tractable. In the field of continuum mechanics, homogenization
is crucial for deriving constitutive laws that incorporate microscale physics
in order to formulate balance laws for the macroscopic quantities of interest.
However, obtaining homogenized constitutive laws is often challenging as they
do not in general have an analytic form and can exhibit phenomena not present
on the microscale. In response, data-driven learning of the constitutive law
has been proposed as appropriate for this task. However, a major challenge in
data-driven learning approaches for this problem has remained unexplored: the
impact of discontinuities and corner interfaces in the underlying material.
These discontinuities in the coefficients affect the smoothness of the
solutions of the underlying equations. Given the prevalence of discontinuous
materials in continuum mechanics applications, it is important to address the
challenge of learning in this context; in particular to develop underpinning
theory to establish the reliability of data-driven methods in this scientific
domain. The paper addresses this unexplored challenge by investigating the
learnability of homogenized constitutive laws for elliptic operators in the
presence of such complexities. Approximation theory is presented, and numerical
experiments are performed which validate the theory for the solution operator
defined by the cell-problem arising in homogenization for elliptic PDEs.


------------------------------------------------------------------------------

Title:
Learning Latent Dynamics via Invariant Decomposition and  (Spatio-)Temporal Transformers

Abstract: We propose a method for learning dynamical systems from high-dimensional
empirical data that combines variational autoencoders and (spatio-)temporal
attention within a framework designed to enforce certain
scientifically-motivated invariances. We focus on the setting in which data are
available from multiple different instances of a system whose underlying
dynamical model is entirely unknown at the outset. The approach rests on a
separation into an instance-specific encoding (capturing initial conditions,
constants etc.) and a latent dynamics model that is itself universal across all
instances/realizations of the system. The separation is achieved in an
automated, data-driven manner and only empirical data are required as inputs to
the model. The approach allows effective inference of system behaviour at any
continuous time but does not require an explicit neural ODE formulation, which
makes it efficient and highly scalable. We study behaviour through simple
theoretical analyses and extensive experiments on synthetic and real-world
datasets. The latter investigate learning the dynamics of complex systems based
on finite data and show that the proposed approach can outperform
state-of-the-art neural-dynamical models. We study also more general inductive
bias in the context of transfer to data obtained under entirely novel system
interventions. Overall, our results provide a promising new framework for
efficiently learning dynamical models from heterogeneous data with potential
applications in a wide range of fields including physics, medicine, biology and
engineering.


------------------------------------------------------------------------------

Title:
Lightweight wood panel defect detection method incorporating attention  mechanism and feature fusion network

Abstract: In recent years, deep learning has made significant progress in wood panel
defect detection. However, there are still challenges such as low detection ,
slow detection speed, and difficulties in deploying embedded devices on wood
panel surfaces. To overcome these issues, we propose a lightweight wood panel
defect detection method called YOLOv5-LW, which incorporates attention
mechanisms and a feature fusion network.Firstly, to enhance the detection
capability of acceptable defects, we introduce the Multi-scale Bi-directional
Feature Pyramid Network (MBiFPN) as a feature fusion network. The MBiFPN
reduces feature loss, enriches local and detailed features, and improves the
model's detection capability for acceptable defects.Secondly, to achieve a
lightweight design, we reconstruct the ShuffleNetv2 network model as the
backbone network. This reconstruction reduces the number of parameters and
computational requirements while maintaining performance. We also introduce the
Stem Block and Spatial Pyramid Pooling Fast (SPPF) models to compensate for any
accuracy loss resulting from the lightweight design, ensuring the model's
detection capabilities remain intact while being computationally
efficient.Thirdly, we enhance the backbone network by incorporating Efficient
Channel Attention (ECA), which improves the network's focus on key information
relevant to defect detection. By attending to essential features, the model
becomes more proficient in accurately identifying and localizing defects.We
validate the proposed method using a self-developed wood panel defect
dataset.The experimental results demonstrate the effectiveness of the improved
YOLOv5-LW method. Compared to the original model, our approach achieves a
92.8\% accuracy rate, reduces the number of parameters by 27.78\%, compresses
computational volume by 41.25\%, improves detection inference speed by 10.16\%


------------------------------------------------------------------------------

Title:
Automatic Inference of Resource Leak Specifications

Abstract: A resource leak occurs when a program fails to free some finite resource
after it is no longer needed. Such leaks are a significant cause of real-world
crashes and performance problems. Recent work proposed an approach to prevent
resource leaks based on checking resource management specifications. These
specifications are written in the form of program annotations that track the
ownership relationship between objects and resources. While this
specify-and-check approach has several advantages compared to prior techniques,
the need to manually write annotations presents a significant barrier to its
practical adoption.
This paper presents a novel technique to automatically infer a resource
management specification for a program, broadening the applicability of
specify-and-check verification for resource leaks. Inference in this domain
poses significant challenges because resource management specifications differ
significantly in nature from the types that most inference techniques target.
Further, for practical effectiveness, we desire a technique that can infer the
resource management specification intended by the developer, even in cases when
the code does not fully adhere to this specification. We address these
challenges through a set of inference rules carefully designed to capture
real-world coding patterns, yielding an effective fixed-point-based inference
algorithm.
We have implemented our inference algorithm in two different systems,
targeting programs in the Java and C# programming languages. An experimental
evaluation showed that our technique could infer 87% of the annotations written
manually for the benchmarks. Further, the true positive rate for resource leak
bugs achieved by the verifier when run after our fully-automatic inference
nearly matched the rate achieved after manual annotation of the code.


------------------------------------------------------------------------------

Title:
Continual Learners are Incremental Model Generalizers

Abstract: Motivated by the efficiency and rapid convergence of pre-trained models for
solving downstream tasks, this paper extensively studies the impact of
Continual Learning (CL) models as pre-trainers. In both supervised and
unsupervised CL, we find that the transfer quality of the representation often
increases gradually without noticeable degradation in fine-tuning performance.
This is because CL models can learn improved task-general features when easily
forgetting task-specific knowledge. Based on this observation, we suggest a new
unsupervised CL framework with masked modeling, which aims to capture fluent
task-generic representation during training. Furthermore, we propose a new
fine-tuning scheme, GLobal Attention Discretization (GLAD), that preserves rich
task-generic representation during solving downstream tasks. The model
fine-tuned with GLAD achieves competitive performance and can also be used as a
good pre-trained model itself. We believe this paper breaks the barriers
between pre-training and fine-tuning steps and leads to a sustainable learning
framework in which the continual learner incrementally improves model
generalization, yielding better transfer to unseen tasks.


------------------------------------------------------------------------------

Title:
Addressing the Rank Degeneration in Sequential Recommendation via  Singular Spectrum Smoothing

Abstract: Sequential recommendation (SR) investigates the dynamic user preferences
modeling and generates the next-item prediction. The next item preference is
typically generated by the affinity between the sequence and item
representations. However, both sequence and item representations suffer from
the rank degeneration issue due to the data sparsity problem. The rank
degeneration issue significantly impairs the representations for SR. This
motivates us to measure how severe is the rank degeneration issue and alleviate
the sequence and item representation rank degeneration issues simultaneously
for SR.
In this work, we theoretically connect the sequence representation
degeneration issue with the item rank degeneration, particularly for short
sequences and cold items. We also identify the connection between the fast
singular value decay phenomenon and the rank collapse issue in transformer
sequence output and item embeddings. We propose the area under the singular
value curve metric to evaluate the severity of the singular value decay
phenomenon and use it as an indicator of rank degeneration. We further
introduce a novel singular spectrum smoothing regularization to alleviate the
rank degeneration on both sequence and item sides, which is the Singular
sPectrum sMoothing for sequential Recommendation (SPMRec). We also establish a
correlation between the ranks of sequence and item embeddings and the rank of
the user-item preference prediction matrix, which can affect recommendation
diversity. We conduct experiments on four benchmark datasets to demonstrate the
superiority of SPMRec over the state-of-the-art recommendation methods,
especially in short sequences. The experiments also demonstrate a strong
connection between our proposed singular spectrum smoothing and recommendation
diversity.


------------------------------------------------------------------------------

Title:
Joint Dense-Point Representation for Contour-Aware Graph Segmentation

Abstract: We present a novel methodology that combines graph and dense segmentation
techniques by jointly learning both point and pixel contour representations,
thereby leveraging the benefits of each approach. This addresses deficiencies
in typical graph segmentation methods where misaligned objectives restrict the
network from learning discriminative vertex and contour features. Our joint
learning strategy allows for rich and diverse semantic features to be encoded,
while alleviating common contour stability issues in dense-based approaches,
where pixel-level objectives can lead to anatomically implausible topologies.
In addition, we identify scenarios where correct predictions that fall on the
contour boundary are penalised and address this with a novel hybrid contour
distance loss. Our approach is validated on several Chest X-ray datasets,
demonstrating clear improvements in segmentation stability and accuracy against
a variety of dense- and point-based methods. Our source code is freely
available at: www.github.com/kitbransby/Joint_Graph_Segmentation


------------------------------------------------------------------------------

Title:
Winter Wheat Crop Yield Prediction on Multiple Heterogeneous Datasets  using Machine Learning

Abstract: Winter wheat is one of the most important crops in the United Kingdom, and
crop yield prediction is essential for the nation's food security. Several
studies have employed machine learning (ML) techniques to predict crop yield on
a county or farm-based level. The main objective of this study is to predict
winter wheat crop yield using ML models on multiple heterogeneous datasets,
i.e., soil and weather on a zone-based level. Experimental results demonstrated
their impact when used alone and in combination. In addition, we employ
numerous ML algorithms to emphasize the significance of data quality in any
machine-learning strategy.


------------------------------------------------------------------------------

Title:
Comparative analysis of various web crawler algorithms

Abstract: This presentation focuses on the importance of web crawling and page ranking
algorithms in dealing with the massive amount of data present on the World Wide
Web. As the web continues to grow exponentially, efficient search and retrieval
methods become crucial. Web crawling is a process that converts unstructured
data into structured data, enabling effective information retrieval.
Additionally, page ranking algorithms play a significant role in assessing the
quality and popularity of web pages. The presentation explores the background
of these algorithms and evaluates five different crawling algorithms: Shark
Search, Priority-Based Queue, Naive Bayes, Breadth-First, and Depth-First. The
goal is to identify the most effective algorithm for crawling web pages. By
understanding these algorithms, we can enhance our ability to navigate the web
and extract valuable information efficiently.


------------------------------------------------------------------------------

Title:
EquiformerV2: Improved Equivariant Transformer for Scaling to  Higher-Degree Representations

Abstract: Equivariant Transformers such as Equiformer have demonstrated the efficacy of
applying Transformers to the domain of 3D atomistic systems. However, they are
still limited to small degrees of equivariant representations due to their
computational complexity. In this paper, we investigate whether these
architectures can scale well to higher degrees. Starting from Equiformer, we
first replace $SO(3)$ convolutions with eSCN convolutions to efficiently
incorporate higher-degree tensors. Then, to better leverage the power of higher
degrees, we propose three architectural improvements -- attention
re-normalization, separable $S^2$ activation and separable layer normalization.
Putting this all together, we propose EquiformerV2, which outperforms previous
state-of-the-art methods on the large-scale OC20 dataset by up to $12\%$ on
forces, $4\%$ on energies, offers better speed-accuracy trade-offs, and
$2\times$ reduction in DFT calculations needed for computing adsorption
energies.


------------------------------------------------------------------------------

Title:
End-to-End Augmentation Hyperparameter Tuning for Self-Supervised  Anomaly Detection

Abstract: Self-supervised learning (SSL) has emerged as a promising paradigm that
presents self-generated supervisory signals to real-world problems, bypassing
the extensive manual labeling burden. SSL is especially attractive for
unsupervised tasks such as anomaly detection, where labeled anomalies are often
nonexistent and costly to obtain. While self-supervised anomaly detection
(SSAD) has seen a recent surge of interest, the literature has failed to treat
data augmentation as a hyperparameter. Meanwhile, recent works have reported
that the choice of augmentation has significant impact on detection
performance. In this paper, we introduce ST-SSAD (Self-Tuning Self-Supervised
Anomaly Detection), the first systematic approach to SSAD in regards to
rigorously tuning augmentation. To this end, our work presents two key
contributions. The first is a new unsupervised validation loss that quantifies
the alignment between the augmented training data and the (unlabeled) test
data. In principle we adopt transduction, quantifying the extent to which
augmentation mimics the true anomaly-generating mechanism, in contrast to
augmenting data with arbitrary pseudo anomalies without regard to test data.
Second, we present new differentiable augmentation functions, allowing data
augmentation hyperparameter(s) to be tuned end-to-end via our proposed
validation loss. Experiments on two testbeds with semantic class anomalies and
subtle industrial defects show that systematically tuning augmentation offers
significant performance gains over current practices.


------------------------------------------------------------------------------

Title:
A Parameterized Algorithm for Flat Folding

Abstract: We prove that testing the flat foldability of an origami crease pattern
(either labeled with mountain and valley folds, or unlabeled) is
fixed-parameter tractable when parameterized by the ply of the flat-folded
state and by the treewidth of an associated planar graph, the cell adjacency
graph of an arrangement of polygons formed by the flat-folded state. For flat
foldings of bounded ply, our algorithm is single-exponential in the treewidth;
this dependence on treewidth is necessary under the exponential time
hypothesis.


------------------------------------------------------------------------------

Title:
Block-Wise Index Modulation and Receiver Design for High-Mobility OTFS  Communications

Abstract: As a promising technique for high-mobility wireless communications,
orthogonal time frequency space (OTFS) has been proved to enjoy excellent
advantages with respect to traditional orthogonal frequency division
multiplexing (OFDM). Although multiple studies have considered index modulation
(IM) based OTFS (IM-OTFS) schemes to further improve system performance, a
challenging and open problem is the development of effective IM schemes and
efficient receivers for practical OTFS systems that must operate in the
presence of channel delays and Doppler shifts. In this paper, we propose two
novel block-wise IM schemes for OTFS systems, named delay-IM with OTFS
(DeIM-OTFS) and Doppler-IM with OTFS (DoIM-OTFS), where a block of
delay/Doppler resource bins are activated simultaneously. Based on a maximum
likelihood (ML) detector, we analyze upper bounds on the average bit error
rates for the proposed DeIM-OTFS and DoIM-OTFS schemes, and verify their
performance advantages over the existing IM-OTFS systems. We also develop a
multi-layer joint symbol and activation pattern detection (MLJSAPD) algorithm
and a customized message passing detection (CMPD) algorithm for our proposed
DeIMOTFS and DoIM-OTFS systems with low complexity. Simulation results
demonstrate that our proposed MLJSAPD and CMPD algorithms can achieve desired
performance with robustness to the imperfect channel state information (CSI).


------------------------------------------------------------------------------

Title:
Multimodality Fusion for Smart Healthcare: a Journey from Data,  Information, Knowledge to Wisdom

Abstract: Multimodal medical data fusion has emerged as a transformative approach in
smart healthcare, enabling a comprehensive understanding of patient health and
personalized treatment plans. In this paper, a journey from data, information,
and knowledge to wisdom (DIKW) is explored through multimodal fusion for smart
healthcare. A comprehensive review of multimodal medical data fusion focuses on
the integration of various data modalities are presented. It explores different
approaches such as Feature selection, Rule-based systems, Machine learning,
Deep learning, and Natural Language Processing for fusing and analyzing
multimodal data. The paper also highlights the challenges associated with
multimodal fusion in healthcare. By synthesizing the reviewed frameworks and
insights, a generic framework for multimodal medical data fusion is proposed
while aligning with the DIKW mechanism. Moreover, it discusses future
directions aligned with the four pillars of healthcare: Predictive, Preventive,
Personalized, and Participatory approaches based on the DIKW and the generic
framework. The components from this comprehensive survey form the foundation
for the successful implementation of multimodal fusion in smart healthcare. The
findings of this survey can guide researchers and practitioners in leveraging
the power of multimodal fusion with the approaches to revolutionize healthcare
and improve patient outcomes.


------------------------------------------------------------------------------

Title:
NeuroCLIP: Neuromorphic Data Understanding by CLIP and SNN

Abstract: Recently, the neuromorphic vision sensor has received more and more interest.
However, the neuromorphic data consists of asynchronous event spikes, which is
not natural and difficult to construct a benchmark, thus limiting the
neuromorphic data understanding for "unseen" objects by deep learning.
Zero-shot and few-shot learning via Contrastive Vision-Language Pre-training
(CLIP) have shown inspirational performance in 2D frame image recognition. To
handle "unseen" recognition for the neuromorphic data, in this paper, we
propose NeuroCLIP, which transfers the CLIP's 2D pre-trained knowledge to event
spikes. To improve the few-shot performance, we also provide an inter-timestep
adapter based on a spiking neural network. Our code is open-sourced at
this https URL


------------------------------------------------------------------------------

Title:
Mitigating Communication Costs in Neural Networks: The Role of Dendritic  Nonlinearity

Abstract: Our comprehension of biological neuronal networks has profoundly influenced
the evolution of artificial neural networks (ANNs). However, the neurons
employed in ANNs exhibit remarkable deviations from their biological analogs,
mainly due to the absence of complex dendritic trees encompassing local
nonlinearity. Despite such disparities, previous investigations have
demonstrated that point neurons can functionally substitute dendritic neurons
in executing computational tasks. In this study, we scrutinized the importance
of nonlinear dendrites within neural networks. By employing machine-learning
methodologies, we assessed the impact of dendritic structure nonlinearity on
neural network performance. Our findings reveal that integrating dendritic
structures can substantially enhance model capacity and performance while
keeping signal communication costs effectively restrained. This investigation
offers pivotal insights that hold considerable implications for the development
of future neural network accelerators.


------------------------------------------------------------------------------

Title:
Quantifying Node-based Core Resilience

Abstract: Core decomposition is an efficient building block for various graph analysis
tasks such as dense subgraph discovery and identifying influential nodes. One
crucial weakness of the core decomposition is its sensitivity to changes in the
graph: inserting or removing a few edges can drastically change the core
structure of a graph. Hence, it is essential to characterize, quantify, and, if
possible, improve the resilience of the core structure of a given graph in
global and local levels. Previous works mostly considered the core resilience
of the entire graph or important subgraphs in it. In this work, we study
node-based core resilience measures upon edge removals and insertions. We first
show that a previously proposed measure, Core Strength, does not correctly
capture the core resilience of a node upon edge removals. Next, we introduce
the concept of dependency graph to capture the impact of neighbor nodes (for
edge removal) and probable future neighbor nodes (for edge insertion) on the
core number of a given node. Accordingly, we define Removal Strength and
Insertion Strength measures to capture the resilience of an individual node
upon removing and inserting an edge, respectively. As naive computation of
those measures is costly, we provide efficient heuristics built on key
observations about the core structure. We consider two key applications,
finding critical edges and identifying influential spreaders, to demonstrate
the usefulness of our new measures on various real-world networks and against
several baselines. We also show that our heuristic algorithms are more
efficient than the naive approaches.


------------------------------------------------------------------------------

Title:
Machine Learning Based Compensation for Inconsistencies in Knitted Force  Sensors

Abstract: Knitted sensors frequently suffer from inconsistencies due to innate effects
such as offset, relaxation, and drift. These properties, in combination, make
it challenging to reliably map from sensor data to physical actuation. In this
paper, we demonstrate a method for counteracting this by applying processing
using a minimal artificial neural network (ANN) in combination with
straightforward pre-processing. We apply a number of exponential smoothing
filters on a re-sampled sensor signal, to produce features that preserve
different levels of historical sensor data and, in combination, represent an
adequate state of previous sensor actuation. By training a three-layer ANN with
a total of 8 neurons, we manage to significantly improve the mapping between
sensor reading and actuation force. Our findings also show that our technique
translates to sensors of reasonably different composition in terms of material
and structure, and it can furthermore be applied to related physical features
such as strain.


------------------------------------------------------------------------------

Title:
Training Transformers with 4-bit Integers

Abstract: Quantizing the activation, weight, and gradient to 4-bit is promising to
accelerate neural network training. However, existing 4-bit training methods
require custom numerical formats which are not supported by contemporary
hardware. In this work, we propose a training method for transformers with all
matrix multiplications implemented with the INT4 arithmetic. Training with an
ultra-low INT4 precision is challenging. To achieve this, we carefully analyze
the specific structures of activation and gradients in transformers to propose
dedicated quantizers for them. For forward propagation, we identify the
challenge of outliers and propose a Hadamard quantizer to suppress the
outliers. For backpropagation, we leverage the structural sparsity of gradients
by proposing bit splitting and leverage score sampling techniques to quantize
gradients accurately. Our algorithm achieves competitive accuracy on a wide
range of tasks including natural language understanding, machine translation,
and image classification. Unlike previous 4-bit training methods, our algorithm
can be implemented on the current generation of GPUs. Our prototypical linear
operator implementation is up to 2.2 times faster than the FP16 counterparts
and speeds up the training by up to 35.1%.


------------------------------------------------------------------------------

Title:
Adaptive Ensemble Q-learning: Minimizing Estimation Bias via Error  Feedback

Abstract: The ensemble method is a promising way to mitigate the overestimation issue
in Q-learning, where multiple function approximators are used to estimate the
action values. It is known that the estimation bias hinges heavily on the
ensemble size (i.e., the number of Q-function approximators used in the
target), and that determining the `right' ensemble size is highly nontrivial,
because of the time-varying nature of the function approximation errors during
the learning process. To tackle this challenge, we first derive an upper bound
and a lower bound on the estimation bias, based on which the ensemble size is
adapted to drive the bias to be nearly zero, thereby coping with the impact of
the time-varying approximation errors accordingly. Motivated by the theoretic
findings, we advocate that the ensemble method can be combined with Model
Identification Adaptive Control (MIAC) for effective ensemble size adaptation.
Specifically, we devise Adaptive Ensemble Q-learning (AdaEQ), a generalized
ensemble method with two key steps: (a) approximation error characterization
which serves as the feedback for flexibly controlling the ensemble size, and
(b) ensemble size adaptation tailored towards minimizing the estimation bias.
Extensive experiments are carried out to show that AdaEQ can improve the
learning performance than the existing methods for the MuJoCo benchmark.


------------------------------------------------------------------------------

Title:
A new color image secret sharing protocol

Abstract: Visual cryptography aims to protect images against their possible
illegitimate use. Thus, one can cipher, hash, or add watermarks for protecting
copyright, among others. In this paper we provide a new solution to the problem
of secret sharing for the case when the secret is an image. Our method combines
the Shamir scheme for secret sharing using finite fields of characteristic 2
with the CBC mode of operation of a secure symmetric cryptographic scheme like
AES, so that the security relies on that of the mentioned techniques. The
resulting shares have the same resolution as that of the original image. The
idea of the method could be generalized to other multimedia formats like audio
or video, adapting the method to the corresponding encoded information.


------------------------------------------------------------------------------

Title:
Opportunities and Risks of LLMs for Scalable Deliberation with Polis

Abstract: Polis is a platform that leverages machine intelligence to scale up
deliberative processes. In this paper, we explore the opportunities and risks
associated with applying Large Language Models (LLMs) towards challenges with
facilitating, moderating and summarizing the results of Polis engagements. In
particular, we demonstrate with pilot experiments using Anthropic's Claude that
LLMs can indeed augment human intelligence to help more efficiently run Polis
conversations. In particular, we find that summarization capabilities enable
categorically new methods with immense promise to empower the public in
collective meaning-making exercises. And notably, LLM context limitations have
a significant impact on insight and quality of these results.
However, these opportunities come with risks. We discuss some of these risks,
as well as principles and techniques for characterizing and mitigating them,
and the implications for other deliberative or political systems that may
employ LLMs. Finally, we conclude with several open future research directions
for augmenting tools like Polis with LLMs.


------------------------------------------------------------------------------

Title:
Task-Robust Pre-Training for Worst-Case Downstream Adaptation

Abstract: Pre-training has achieved remarkable success when transferred to downstream
tasks. In machine learning, we care about not only the good performance of a
model but also its behavior under reasonable shifts of condition. The same
philosophy holds when pre-training a foundation model. However, the foundation
model may not uniformly behave well for a series of related downstream tasks.
This happens, for example, when conducting mask recovery regression where the
recovery ability or the training instances diverge like pattern features are
extracted dominantly on pre-training, but semantic features are also required
on a downstream task. This paper considers pre-training a model that guarantees
a uniformly good performance over the downstream tasks. We call this goal as
$\textit{downstream-task robustness}$. Our method first separates the upstream
task into several representative ones and applies a simple minimax loss for
pre-training. We then design an efficient algorithm to solve the minimax loss
and prove its convergence in the convex setting. In the experiments, we show
both on large-scale natural language processing and computer vision datasets
our method increases the metrics on worse-case downstream tasks. Additionally,
some theoretical explanations for why our loss is beneficial are provided.
Specifically, we show fewer samples are inherently required for the most
challenging downstream task in some cases.


------------------------------------------------------------------------------

Title:
Efficient ResNets: Residual Network Design

Abstract: ResNets (or Residual Networks) are one of the most commonly used models for
image classification tasks. In this project, we design and train a modified
ResNet model for CIFAR-10 image classification. In particular, we aimed at
maximizing the test accuracy on the CIFAR-10 benchmark while keeping the size
of our ResNet model under the specified fixed budget of 5 million trainable
parameters. Model size, typically measured as the number of trainable
parameters, is important when models need to be stored on devices with limited
storage capacity (e.g. IoT/edge devices). In this article, we present our
residual network design which has less than 5 million parameters. We show that
our ResNet achieves a test accuracy of 96.04% on CIFAR-10 which is much higher
than ResNet18 (which has greater than 11 million trainable parameters) when
equipped with a number of training strategies and suitable ResNet
hyperparameters. Models and code are available at
this https URL


------------------------------------------------------------------------------

Title:
Exploring New Frontiers in Agricultural NLP: Investigating the Potential  of Large Language Models for Food Applications

Abstract: This paper explores new frontiers in agricultural natural language processing
by investigating the effectiveness of using food-related text corpora for
pretraining transformer-based language models. In particular, we focus on the
task of semantic matching, which involves establishing mappings between food
descriptions and nutrition data. To accomplish this, we fine-tune a pre-trained
transformer-based language model, AgriBERT, on this task, utilizing an external
source of knowledge, such as the FoodOn ontology. To advance the field of
agricultural NLP, we propose two new avenues of exploration: (1) utilizing
GPT-based models as a baseline and (2) leveraging ChatGPT as an external source
of knowledge. ChatGPT has shown to be a strong baseline in many NLP tasks, and
we believe it has the potential to improve our model in the task of semantic
matching and enhance our model's understanding of food-related concepts and
relationships. Additionally, we experiment with other applications, such as
cuisine prediction based on food ingredients, and expand the scope of our
research to include other NLP tasks beyond semantic matching. Overall, this
paper provides promising avenues for future research in this field, with
potential implications for improving the performance of agricultural NLP
applications.


------------------------------------------------------------------------------

Title:
Generalizable Metric Network for Cross-domain Person Re-identification

Abstract: Person Re-identification (Re-ID) is a crucial technique for public security
and has made significant progress in supervised settings. However, the
cross-domain (i.e., domain generalization) scene presents a challenge in Re-ID
tasks due to unseen test domains and domain-shift between the training and test
sets. To tackle this challenge, most existing methods aim to learn
domain-invariant or robust features for all domains. In this paper, we observe
that the data-distribution gap between the training and test sets is smaller in
the sample-pair space than in the sample-instance space. Based on this
observation, we propose a Generalizable Metric Network (GMN) to further explore
sample similarity in the sample-pair space. Specifically, we add a Metric
Network (M-Net) after the main network and train it on positive and negative
sample-pair features, which is then employed during the test stage.
Additionally, we introduce the Dropout-based Perturbation (DP) module to
enhance the generalization capability of the metric network by enriching the
sample-pair diversity. Moreover, we develop a Pair-Identity Center (PIC) loss
to enhance the model's discrimination by ensuring that sample-pair features
with the same pair-identity are consistent. We validate the effectiveness of
our proposed method through a lot of experiments on multiple benchmark datasets
and confirm the value of each module in our GMN.


------------------------------------------------------------------------------

Title:
Copula-Based Deep Survival Models for Dependent Censoring

Abstract: A survival dataset describes a set of instances (e.g. patients) and provides,
for each, either the time until an event (e.g. death), or the censoring time
(e.g. when lost to follow-up - which is a lower bound on the time until the
event). We consider the challenge of survival prediction: learning, from such
data, a predictive model that can produce an individual survival distribution
for a novel instance. Many contemporary methods of survival prediction
implicitly assume that the event and censoring distributions are independent
conditional on the instance's covariates - a strong assumption that is
difficult to verify (as we observe only one outcome for each instance) and
which can induce significant bias when it does not hold. This paper presents a
parametric model of survival that extends modern non-linear survival analysis
by relaxing the assumption of conditional independence. On synthetic and
semi-synthetic data, our approach significantly improves estimates of survival
distributions compared to the standard that assumes conditional independence in
the data.


------------------------------------------------------------------------------

Title:
Unexplainable Explanations: Towards Interpreting tSNE and UMAP  Embeddings

Abstract: It has become standard to explain neural network latent spaces with
attraction/repulsion dimensionality reduction (ARDR) methods like tSNE and
UMAP. This relies on the premise that structure in the 2D representation is
consistent with the structure in the model's latent space. However, this is an
unproven assumption -- we are unaware of any convergence guarantees for ARDR
algorithms. We work on closing this question by relating ARDR methods to
classical dimensionality reduction techniques. Specifically, we show that one
can fully recover a PCA embedding by applying attractions and repulsions onto a
randomly initialized dataset. We also show that, with a small change, Locally
Linear Embeddings (LLE) can reproduce ARDR embeddings. Finally, we formalize a
series of conjectures that, if true, would allow one to attribute structure in
the 2D embedding back to the input distribution.


------------------------------------------------------------------------------

Title:
An Overview of Catastrophic AI Risks

Abstract: Rapid advancements in artificial intelligence (AI) have sparked growing
concerns among experts, policymakers, and world leaders regarding the potential
for increasingly advanced AI systems to pose catastrophic risks. Although
numerous risks have been detailed separately, there is a pressing need for a
systematic discussion and illustration of the potential dangers to better
inform efforts to mitigate them. This paper provides an overview of the main
sources of catastrophic AI risks, which we organize into four categories:
malicious use, in which individuals or groups intentionally use AIs to cause
harm; AI race, in which competitive environments compel actors to deploy unsafe
AIs or cede control to AIs; organizational risks, highlighting how human
factors and complex systems can increase the chances of catastrophic accidents;
and rogue AIs, describing the inherent difficulty in controlling agents far
more intelligent than humans. For each category of risk, we describe specific
hazards, present illustrative stories, envision ideal scenarios, and propose
practical suggestions for mitigating these dangers. Our goal is to foster a
comprehensive understanding of these risks and inspire collective and proactive
efforts to ensure that AIs are developed and deployed in a safe manner.
Ultimately, we hope this will allow us to realize the benefits of this powerful
technology while minimizing the potential for catastrophic outcomes.


------------------------------------------------------------------------------

Title:
A Chain of AI-based Solutions for Resolving FQNs and Fixing Syntax  Errors in Partial Code

Abstract: API documentation, technical blogs and programming Q&A sites contain numerous
partial code that can be reused in programming tasks, but often these code are
uncompilable due to unresolved names and syntax errors. To facilitate partial
code reuse, we propose the Partial Code Reuse Chain (PCR-Chain) for resolving
fully-qualified names (FQNs) and fixing last-mile syntax errors in partial code
based on a giant large language model (LLM) like ChatGPT. Methodologically,
PCR-Chain is backed up by the underlying global-level prompt architecture
(which combines three design ideas: hierarchical task breakdown, prompt
composition, and a mix of prompt-based AI and non-AI units) and the local-level
prompt design. Technically, we propose PCR-Chain, which employs in-context
learning rather than symbolic, costly training methods. Experimental results
demonstrate that in dynamically-typed languages (Python), PCR-Chain outperforms
current state-of-the-art (SOTA) 5% accuracy like RING. For statically-type
languages (Java), our approach achieves high accuracy of 80.5% in resolving
both non-FQNs and last-mile syntax errors, surpassing SOTA methods (RING) that
can only address last-mile syntax errors. The correct execution of the unit,
module, and PCR-Chain demonstrates the effectiveness of the prompt design,
composition, and architecture and opens up possibilities for building software
engineering tools based on LLMs, replacing traditional program analysis
methods.


------------------------------------------------------------------------------

Title:
FLGo: A Fully Customizable Federated Learning Platform

Abstract: Federated learning (FL) has found numerous applications in healthcare,
finance, and IoT scenarios. Many existing FL frameworks offer a range of
benchmarks to evaluate the performance of FL under realistic conditions.
However, the process of customizing simulations to accommodate
application-specific settings, data heterogeneity, and system heterogeneity
typically remains unnecessarily complicated. This creates significant hurdles
for traditional ML researchers in exploring the usage of FL, while also
compromising the shareability of codes across FL frameworks. To address this
issue, we propose a novel lightweight FL platform called FLGo, to facilitate
cross-application FL studies with a high degree of shareability. Our platform
offers 40+ benchmarks, 20+ algorithms, and 2 system simulators as
out-of-the-box plugins. We also provide user-friendly APIs for quickly
customizing new plugins that can be readily shared and reused for improved
reproducibility. Finally, we develop a range of experimental tools, including
parallel acceleration, experiment tracker and analyzer, and parameters
auto-tuning. FLGo is maintained at \url{flgo-xmu.github.io}.


------------------------------------------------------------------------------

Title:
SPRINT: Scalable Policy Pre-Training via Language Instruction Relabeling

Abstract: Pre-training robot policies with a rich set of skills can substantially
accelerate the learning of downstream tasks. Prior works have defined
pre-training tasks via natural language instructions, but doing so requires
tedious human annotation of hundreds of thousands of instructions. Thus, we
propose SPRINT, a scalable offline policy pre-training approach which
substantially reduces the human effort needed for pre-training a diverse set of
skills. Our method uses two core ideas to automatically expand a base set of
pre-training tasks: instruction relabeling via large language models and
cross-trajectory skill chaining through offline reinforcement learning. As a
result, SPRINT pre-training equips robots with a much richer repertoire of
skills. Experimental results in a household simulator and on a real robot
kitchen manipulation task show that SPRINT leads to substantially faster
learning of new long-horizon tasks than previous pre-training approaches.
Website at this https URL


------------------------------------------------------------------------------

Title:
Sample Attackability in Natural Language Adversarial Attacks

Abstract: Adversarial attack research in natural language processing (NLP) has made
significant progress in designing powerful attack methods and defence
approaches. However, few efforts have sought to identify which source samples
are the most attackable or robust, i.e. can we determine for an unseen target
model, which samples are the most vulnerable to an adversarial attack. This
work formally extends the definition of sample attackability/robustness for NLP
attacks. Experiments on two popular NLP datasets, four state of the art models
and four different NLP adversarial attack methods, demonstrate that sample
uncertainty is insufficient for describing characteristics of attackable/robust
samples and hence a deep learning based detector can perform much better at
identifying the most attackable and robust samples for an unseen target model.
Nevertheless, further analysis finds that there is little agreement in which
samples are considered the most attackable/robust across different NLP attack
methods, explaining a lack of portability of attackability detection methods
across attack methods.


------------------------------------------------------------------------------

Title:
Outside the Sandbox: A Study of Input/Output Methods in Java

Abstract: Programming languages often demarcate the internal sandbox, consisting of
entities such as objects and variables, from the outside world, e.g., files or
network. Although communication with the external world poses fundamental
challenges for live programming, reversible debugging, testing, and program
analysis in general, studies about this phenomenon are rare. In this paper, we
present a preliminary empirical study about the prevalence of input/output
(I/O) method usage in Java. We manually categorized 1435 native methods in a
Java Standard Edition distribution into non-I/O and I/O-related methods, which
were further classified into areas such as desktop or file-related ones.
According to the static analysis of a call graph for 798 projects, about 57% of
methods potentially call I/O natives. The results of dynamic analysis on 16
benchmarks showed that 21% of the executed methods directly or indirectly
called an I/O native. We conclude that neglecting I/O is not a viable option
for tool designers and suggest the integration of I/O-related metadata with
source code to facilitate their querying.


------------------------------------------------------------------------------

Title:
Stability analysis of admittance control using asymmetric stiffness  matrix

Abstract: In contact-rich tasks, setting the stiffness of the control system is a
critical factor in its performance. Although the setting range can be extended
by making the stiffness matrix asymmetric, its stability has not been proven.
This study focuses on the stability of compliance control in a robot arm that
deals with an asymmetric stiffness matrix. It discusses the convergence
stability of the admittance control. The paper explains how to derive an
asymmetric stiffness matrix and how to incorporate it into the admittance
model. The authors also present simulation and experimental results that
demonstrate the effectiveness of their proposed method.


------------------------------------------------------------------------------

Title:
Efficient Dynamics Modeling in Interactive Environments with Koopman  Theory

Abstract: The accurate modeling of dynamics in interactive environments is critical for
successful long-range prediction. Such a capability could advance Reinforcement
Learning (RL) and Planning algorithms, but achieving it is challenging.
Inaccuracies in model estimates can compound, resulting in increased errors
over long horizons. We approach this problem from the lens of Koopman theory,
where the nonlinear dynamics of the environment can be linearized in a
high-dimensional latent space. This allows us to efficiently parallelize the
sequential problem of long-range prediction using convolution, while accounting
for the agent's action at every time step. Our approach also enables stability
analysis and better control over gradients through time. Taken together, these
advantages result in significant improvement over the existing approaches, both
in the efficiency and the accuracy of modeling dynamics over extended horizons.
We also report promising experimental results in dynamics modeling for the
scenarios of both model-based planning and model-free RL.


------------------------------------------------------------------------------

Title:
Multiverse Transformer: 1st Place Solution for Waymo Open Sim Agents  Challenge 2023

Abstract: This technical report presents our 1st place solution for the Waymo Open Sim
Agents Challenge (WOSAC) 2023. Our proposed MultiVerse Transformer for Agent
simulation (MVTA) effectively leverages transformer-based motion prediction
approaches, and is tailored for closed-loop simulation of agents. In order to
produce simulations with a high degree of realism, we design novel training and
sampling methods, and implement a receding horizon prediction mechanism. In
addition, we introduce a variable-length history aggregation method to mitigate
the compounding error that can arise during closed-loop autoregressive
execution. On the WOSAC, our MVTA and its enhanced version MVTE reach a realism
meta-metric of 0.5091 and 0.5168, respectively, outperforming all the other
methods on the leaderboard.


------------------------------------------------------------------------------

Title:
Deep perceptual hashing algorithms with hidden dual purpose: when  client-side scanning does facial recognition

Abstract: End-to-end encryption (E2EE) provides strong technical protections to
individuals from interferences. Governments and law enforcement agencies around
the world have however raised concerns that E2EE also allows illegal content to
be shared undetected. Client-side scanning (CSS), using perceptual hashing (PH)
to detect known illegal content before it is shared, is seen as a promising
solution to prevent the diffusion of illegal content while preserving
encryption. While these proposals raise strong privacy concerns, proponents of
the solutions have argued that the risk is limited as the technology has a
limited scope: detecting known illegal content. In this paper, we show that
modern perceptual hashing algorithms are actually fairly flexible pieces of
technology and that this flexibility could be used by an adversary to add a
secondary hidden feature to a client-side scanning system. More specifically,
we show that an adversary providing the PH algorithm can ``hide" a secondary
purpose of face recognition of a target individual alongside its primary
purpose of image copy detection. We first propose a procedure to train a
dual-purpose deep perceptual hashing model by jointly optimizing for both the
image copy detection and the targeted facial recognition task. Second, we
extensively evaluate our dual-purpose model and show it to be able to reliably
identify a target individual 67% of the time while not impacting its
performance at detecting illegal content. We also show that our model is
neither a general face detection nor a facial recognition model, allowing its
secondary purpose to be hidden. Finally, we show that the secondary purpose can
be enabled by adding a single illegal looking image to the database. Taken
together, our results raise concerns that a deep perceptual hashing-based CSS
system could turn billions of user devices into tools to locate targeted
individuals.


------------------------------------------------------------------------------

Title:
Evaluation of Popular XAI Applied to Clinical Prediction Models: Can  They be Trusted?

Abstract: The absence of transparency and explainability hinders the clinical adoption
of Machine learning (ML) algorithms. Although various methods of explainable
artificial intelligence (XAI) have been suggested, there is a lack of
literature that delves into their practicality and assesses them based on
criteria that could foster trust in clinical environments. To address this gap
this study evaluates two popular XAI methods used for explaining predictive
models in the healthcare context in terms of whether they (i) generate
domain-appropriate representation, i.e. coherent with respect to the
application task, (ii) impact clinical workflow and (iii) are consistent. To
that end, explanations generated at the cohort and patient levels were
analysed. The paper reports the first benchmarking of the XAI methods applied
to risk prediction models obtained by evaluating the concordance between
generated explanations and the trigger of a future clinical deterioration
episode recorded by the data collection system. We carried out an analysis
using two Electronic Medical Records (EMR) datasets sourced from Australian
major hospitals. The findings underscore the limitations of state-of-the-art
XAI methods in the clinical context and their potential benefits. We discuss
these limitations and contribute to the theoretical development of trustworthy
XAI solutions where clinical decision support guides the choice of intervention
by suggesting the pattern or drivers for clinical deterioration in the future.


------------------------------------------------------------------------------

Title:
Hybrid Soft-Rigid Continuum Robot Inspired by Spider Monkey Tail

Abstract: Spider monkeys (genus Ateles) have a prehensile tail that functions as a
flexible, multipurpose fifth limb, enabling them to navigate complex terrains,
grasp objects of various sizes, and swing between supports. Inspired by the
spider monkey tail, we present a life size hybrid soft-rigid continuum robot
designed to imitate the function of the tail. Our planar design has a rigid
skeleton with soft elements at its joints that achieve decreasing stiffness
along its length. Five manually-operated wires along this central structure
control the motion of the tail to form a variety of possible shapes in the 2D
plane. Our design also includes a skin-like silicone and fabric tail pad that
moves with the tail's tip and assists with object grasping. We quantify the
force required to pull various objects out of the robot's grasp and demonstrate
that this force increases with the object diameter and the number of edges in a
polygonal object. We demonstrate the robot's ability to grasp, move, and
release objects of various diameters, as well as to navigate around obstacles,
and to retrieve an object after passing under a low passageway.


------------------------------------------------------------------------------

Title:
Open-Domain Text Evaluation via Meta Distribution Modeling

Abstract: Recent advances in open-domain text generation models powered by large
pre-trained language models (LLMs) have achieved remarkable performance.
However, evaluating and controlling these models for desired attributes remains
a challenge, as traditional reference-based metrics such as BLEU, ROUGE, and
METEOR are insufficient for open-ended generation tasks. Similarly, while
trainable discriminator-based evaluation metrics show promise, obtaining
high-quality training data is a non-trivial task. In this paper, we introduce a
novel approach to evaluate open-domain generation - the Meta-Distribution
Methods (MDM). Drawing on the correlation between the rising parameter counts
and the improving performance of LLMs, MDM creates a mapping from the contrast
of two probabilistic distributions -- one known to be superior to the other --
to quality measures, which can be viewed as a distribution of distributions
i.e. Meta-Distribution. We investigate MDM for open-domain text generation
evaluation under two paradigms: 1) \emph{Generative} MDM, which leverages the
Meta-Distribution Methods to generate in-domain negative samples for training
discriminator-based metrics; 2) \emph{Discriminative} MDM, which directly uses
distribution discrepancies between two language models for evaluation. Our
experiments on multi-turn dialogue and factuality in abstractive summarization
demonstrate that MDMs correlate better with human judgment than existing
automatic evaluation metrics on both tasks, highlighting the strong performance
and generalizability of such methods.


------------------------------------------------------------------------------

Title:
Towards Mitigating Spurious Correlations in the Wild: A Benchmark & a  more Realistic Dataset

Abstract: Deep neural networks often exploit non-predictive features that are
spuriously correlated with class labels, leading to poor performance on groups
of examples without such features. Despite the growing body of recent works on
remedying spurious correlations, the lack of a standardized benchmark hinders
reproducible evaluation and comparison of the proposed solutions. To address
this, we present SpuCo, a python package with modular implementations of
state-of-the-art solutions enabling easy and reproducible evaluation of current
methods. Using SpuCo, we demonstrate the limitations of existing datasets and
evaluation schemes in validating the learning of predictive features over
spurious ones. To overcome these limitations, we propose two new vision
datasets: (1) SpuCoMNIST, a synthetic dataset that enables simulating the
effect of real world data properties e.g. difficulty of learning spurious
feature, as well as noise in the labels and features; (2) SpuCoAnimals, a
large-scale dataset curated from ImageNet that captures spurious correlations
in the wild much more closely than existing datasets. These contributions
highlight the shortcomings of current methods and provide a direction for
future research in tackling spurious correlations. SpuCo, containing the
benchmark and datasets, can be found at this https URL,
with detailed documentation available at
this https URL


------------------------------------------------------------------------------

Title:
Balanced Mixture of SuperNets for Learning the CNN Pooling Architecture

Abstract: Downsampling layers, including pooling and strided convolutions, are crucial
components of the convolutional neural network architecture that determine both
the granularity/scale of image feature analysis as well as the receptive field
size of a given layer. To fully understand this problem, we analyse the
performance of models independently trained with each pooling configurations on
CIFAR10, using a ResNet20 network, and show that the position of the
downsampling layers can highly influence the performance of a network and
predefined downsampling configurations are not optimal. Network Architecture
Search (NAS) might be used to optimize downsampling configurations as an
hyperparameter. However, we find that common one-shot NAS based on a single
SuperNet does not work for this problem. We argue that this is because a
SuperNet trained for finding the optimal pooling configuration fully shares its
parameters among all pooling configurations. This makes its training hard,
because learning some configurations can harm the performance of others.
Therefore, we propose a balanced mixture of SuperNets that automatically
associates pooling configurations to different weight models and helps to
reduce the weight-sharing and inter-influence of pooling configurations on the
SuperNet parameters. We evaluate our proposed approach on CIFAR10, CIFAR100, as
well as Food101 and show that in all cases, our model outperforms other
approaches and improves over the default pooling configurations.


------------------------------------------------------------------------------

Title:
Ambigram Generation by A Diffusion Model

Abstract: Ambigrams are graphical letter designs that can be read not only from the
original direction but also from a rotated direction (especially with 180
degrees). Designing ambigrams is difficult even for human experts because
keeping their dual readability from both directions is often difficult. This
paper proposes an ambigram generation model. As its generation module, we use a
diffusion model, which has recently been used to generate high-quality
photographic images. By specifying a pair of letter classes, such as 'A' and
'B', the proposed model generates various ambigram images which can be read as
'A' from the original direction and 'B' from a direction rotated 180 degrees.
Quantitative and qualitative analyses of experimental results show that the
proposed model can generate high-quality and diverse ambigrams. In addition, we
define ambigramability, an objective measure of how easy it is to generate
ambigrams for each letter pair. For example, the pair of 'A' and 'V' shows a
high ambigramability (that is, it is easy to generate their ambigrams), and the
pair of 'D' and 'K' shows a lower ambigramability. The ambigramability gives
various hints of the ambigram generation not only for computers but also for
human experts. The code can be found at
(this https URL).


------------------------------------------------------------------------------

Title:
AdCraft: An Advanced Reinforcement Learning Benchmark Environment for  Search Engine Marketing Optimization

Abstract: We introduce AdCraft, a novel benchmark environment for the Reinforcement
Learning (RL) community distinguished by its stochastic and non-stationary
properties. The environment simulates bidding and budgeting dynamics within
Search Engine Marketing (SEM), a digital marketing technique utilizing paid
advertising to enhance the visibility of websites on search engine results
pages (SERPs). The performance of SEM advertisement campaigns depends on
several factors, including keyword selection, ad design, bid management, budget
adjustments, and performance monitoring. Deep RL recently emerged as a
potential strategy to optimize campaign profitability within the complex and
dynamic landscape of SEM but it requires substantial data, which may be costly
or infeasible to acquire in practice. Our customizable environment enables
practitioners to assess and enhance the robustness of RL algorithms pertinent
to SEM bid and budget management without such costs. Through a series of
experiments within the environment, we demonstrate the challenges imposed by
sparsity and non-stationarity on agent convergence and performance. We hope
these challenges further encourage discourse and development around effective
strategies for managing real-world uncertainties.


------------------------------------------------------------------------------

Title:
The Conditioning of Hybrid Variational Data Assimilation

Abstract: In variational assimilation, the most probable state of a dynamical system
under Gaussian assumptions for the prior and likelihood can be found by solving
a least-squares minimization problem . In recent years, we have seen the
popularity of hybrid variational data assimilation methods for Numerical
Weather Prediction. In these methods, the prior error covariance matrix is a
weighted sum of a climatological part and a flow-dependent ensemble part, the
latter being rank deficient. The nonlinear least squares problem of variational
data assimilation is solved using iterative numerical methods, and the
condition number of the Hessian is a good proxy for the convergence behavior of
such methods. In this paper, we study the conditioning of the least squares
problem in a hybrid four-dimensional variational data assimilation (Hybrid
4D-Var) scheme by establishing bounds on the condition number of the Hessian.
In particular, we consider the effect of the ensemble component of the prior
covariance on the conditioning of the system. Numerical experiments show that
the bounds obtained can be useful in predicting the behavior of the true
condition number and the convergence speed of an iterative algorithm


------------------------------------------------------------------------------

Title:
LNL+K: Learning with Noisy Labels and Noise Source Distribution  Knowledge

Abstract: Learning with noisy labels (LNL) is challenging as the model tends to
memorize noisy labels, which can lead to overfitting. Many LNL methods detect
clean samples by maximizing the similarity between samples in each category,
which does not make any assumptions about likely noise sources. However, we
often have some knowledge about the potential source(s) of noisy labels. For
example, an image mislabeled as a cheetah is more likely a leopard than a
hippopotamus due to their visual similarity. Thus, we introduce a new task
called Learning with Noisy Labels and noise source distribution Knowledge
(LNL+K), which assumes we have some knowledge about likely source(s) of label
noise that we can take advantage of. By making this presumption, methods are
better equipped to distinguish hard negatives between categories from label
noise. In addition, this enables us to explore datasets where the noise may
represent the majority of samples, a setting that breaks a critical premise of
most methods developed for the LNL task. We explore several baseline LNL+K
approaches that integrate noise source knowledge into state-of-the-art LNL
methods across three diverse datasets and three types of noise, where we report
a 5-15% boost in performance compared with the unadapted methods. Critically,
we find that LNL methods do not generalize well in every setting, highlighting
the importance of directly exploring our LNL+K task.


------------------------------------------------------------------------------

Title:
Unsupervised Deep Unfolded PGD for Transmit Power Allocation in Wireless  Systems

Abstract: Transmit power control (TPC) is a key mechanism for managing interference,
energy utilization, and connectivity in wireless systems. In this paper, we
propose a simple low-complexity TPC algorithm based on the deep unfolding of
the iterative projected gradient descent (PGD) algorithm into layers of a deep
neural network and learning the step-size parameters. An unsupervised learning
method with either online learning or offline pretraining is applied for
optimizing the weights of the DNN. Performance evaluation in dense
device-to-device (D2D) communication scenarios showed that the proposed method
can achieve better performance than the iterative algorithm with more than a
factor of 2 lower number of iterations.


------------------------------------------------------------------------------

Title:
LLM-based Smart Reply (LSR): Enhancing Collaborative Performance with  ChatGPT-mediated Smart Reply System (ACM)(Draft) LLM-based Smart Reply (LSR):  Enhancing Collaborative Performance with ChatGPT-mediated Smart Reply System

Abstract: CSCW studies have increasingly explored AI's role in enhancing communication
efficiency and productivity in collaborative tasks. AI tools such as chatbots,
smart replies, and language models aim to optimize conversation management and
improve team performance. Early AI assistants, such as Gmail smart reply, were
limited by predefined knowledge bases and decision trees. However, the advent
of large language models (LLMs) such as ChatGPT has revolutionized AI
assistants, employing advanced deep learning architecture to generate
context-aware, coherent, and personalized responses. Consequently,
ChatGPT-based AI assistants provide a more natural and efficient user
experience across various tasks and domains. In this paper, we formalize the
concept of AI Collaborative Tools (ACT) as AI technologies in human
collaborative work and discuss how the emergence of ChatGPT has transformed the
AI landscape and increased focus on ACT for improving team performance.
Meanwhile, we present an LLM-based Smart Reply (LSR) system utilizing the
ChatGPT API to generate personalized responses in daily collaborative
scenarios, considering context, tone, and communication style. Our two-step
process involves generating a preliminary response type (e.g., Agree, Disagree)
to provide a generalized direction for message generation, thus reducing
response drafting time. We conducted an experiment in which participants
completed simulated work tasks, involving Google Calendar manipulation and a
double-back N-back test, while interacting with researchers posing as teammates
requesting scheduling changes. Our findings indicate that the AI teammate
increases perceived performance and reduces mental demand, as measured by the
NASA TLX, and improves performance in the N-back task. We also provide
qualitative feedback on participants' experiences working with the AI teammate.


------------------------------------------------------------------------------

Title:
A Deep Learning Model for Heterogeneous Dataset Analysis -- Application  to Winter Wheat Crop Yield Prediction

Abstract: Western countries rely heavily on wheat, and yield prediction is crucial.
Time-series deep learning models, such as Long Short Term Memory (LSTM), have
already been explored and applied to yield prediction. Existing literature
reported that they perform better than traditional Machine Learning (ML)
models. However, the existing LSTM cannot handle heterogeneous datasets (a
combination of data which varies and remains static with time). In this paper,
we propose an efficient deep learning model that can deal with heterogeneous
datasets. We developed the system architecture and applied it to the real-world
dataset in the digital agriculture area. We showed that it outperforms the
existing ML models.


------------------------------------------------------------------------------

Title:
Predicting Strategic Energy Storage Behaviors

Abstract: Energy storage are strategic participants in electricity markets to arbitrage
price differences. Future power system operators must understand and predict
strategic storage arbitrage behaviors for market power monitoring and capacity
adequacy planning. This paper proposes a novel data-driven approach that
incorporates prior model knowledge for predicting the strategic behaviors of
price-taker energy storage systems. We propose a gradient-descent method to
find the storage model parameters given the historical price signals and
observations. We prove that the identified model parameters will converge to
the true user parameters under a class of quadratic objective and linear
equality-constrained storage models. We demonstrate the effectiveness of our
approach through numerical experiments with synthetic and real-world storage
behavior data. The proposed approach significantly improves the accuracy of
storage model identification and behavior forecasting compared to previous
blackbox data-driven approaches.


------------------------------------------------------------------------------

Title:
Proactive Human-Robot Co-Assembly: Leveraging Human Intention Prediction  and Robust Safe Control

Abstract: Human-robot collaboration (HRC) is one key component to achieving flexible
manufacturing to meet the different needs of customers. However, it is
difficult to build intelligent robots that can proactively assist humans in a
safe and efficient way due to several challenges.First, it is challenging to
achieve efficient collaboration due to diverse human behaviors and data
scarcity. Second, it is difficult to ensure interactive safety due to
uncertainty in human behaviors. This paper presents an integrated framework for
proactive HRC. A robust intention prediction module, which leverages prior task
information and human-in-the-loop training, is learned to guide the robot for
efficient collaboration. The proposed framework also uses robust safe control
to ensure interactive safety under uncertainty. The developed framework is
applied to a co-assembly task using a Kinova Gen3 robot. The experiment
demonstrates that our solution is robust to environmental changes as well as
different human preferences and behaviors. In addition, it improves task
efficiency by approximately 15-20%. Moreover, the experiment demonstrates that
our solution can guarantee interactive safety during proactive collaboration.


------------------------------------------------------------------------------

Title:
Structure-Aware Robustness Certificates for Graph Classification

Abstract: Certifying the robustness of a graph-based machine learning model poses a
critical challenge for safety. Current robustness certificates for graph
classifiers guarantee output invariance with respect to the total number of
node pair flips (edge addition or edge deletion), which amounts to an $l_{0}$
ball centred on the adjacency matrix. Although theoretically attractive, this
type of isotropic structural noise can be too restrictive in practical
scenarios where some node pairs are more critical than others in determining
the classifier's output. The certificate, in this case, gives a pessimistic
depiction of the robustness of the graph model. To tackle this issue, we
develop a randomised smoothing method based on adding an anisotropic noise
distribution to the input graph structure. We show that our process generates
structural-aware certificates for our classifiers, whereby the magnitude of
robustness certificates can vary across different pre-defined structures of the
graph. We demonstrate the benefits of these certificates in both synthetic and
real-world experiments.


------------------------------------------------------------------------------

Title:
A Model-free Closeness-of-influence Test for Features in Supervised  Learning

Abstract: Understanding the effect of a feature vector $x \in \mathbb{R}^d$ on the
response value (label) $y \in \mathbb{R}$ is the cornerstone of many
statistical learning problems. Ideally, it is desired to understand how a set
of collected features combine together and influence the response value, but
this problem is notoriously difficult, due to the high-dimensionality of data
and limited number of labeled data points, among many others. In this work, we
take a new perspective on this problem, and we study the question of assessing
the difference of influence that the two given features have on the response
value. We first propose a notion of closeness for the influence of features,
and show that our definition recovers the familiar notion of the magnitude of
coefficients in the parametric model. We then propose a novel method to test
for the closeness of influence in general model-free supervised learning
problems. Our proposed test can be used with finite number of samples with
control on type I error rate, no matter the ground truth conditional law
$\mathcal{L}(Y |X)$. We analyze the power of our test for two general learning
problems i) linear regression, and ii) binary classification under mixture of
Gaussian models, and show that under the proper choice of score function, an
internal component of our test, with sufficient number of samples will achieve
full statistical power. We evaluate our findings through extensive numerical
simulations, specifically we adopt the datamodel framework (Ilyas, et al.,
2022) for CIFAR-10 dataset to identify pairs of training samples with different
influence on the trained model via optional black box training mechanisms.


------------------------------------------------------------------------------

Title:
Deep Fusion: Efficient Network Training via Pre-trained Initializations

Abstract: In recent years, deep learning has made remarkable progress in a wide range
of domains, with a particularly notable impact on natural language processing
tasks. One of the challenges associated with training deep neural networks is
the need for large amounts of computational resources and time. In this paper,
we present Deep Fusion, an efficient approach to network training that
leverages pre-trained initializations of smaller networks. % We show that Deep
Fusion accelerates the training process, reduces computational requirements,
and leads to improved generalization performance on a variety of NLP tasks and
T5 model sizes. % Our experiments demonstrate that Deep Fusion is a practical
and effective approach to reduce the training time and resource consumption
while maintaining, or even surpassing, the performance of traditional training
methods.


------------------------------------------------------------------------------

Title:
Sampling Individually-Fair Rankings that are Always Group Fair

Abstract: Rankings on online platforms help their end-users find the relevant
information -- people, news, media, and products -- quickly. Fair ranking
tasks, which ask to rank a set of items to maximize utility subject to
satisfying group-fairness constraints, have gained significant interest in the
Algorithmic Fairness, Information Retrieval, and Machine Learning literature.
Recent works, however, identify uncertainty in the utilities of items as a
primary cause of unfairness and propose introducing randomness in the output.
This randomness is carefully chosen to guarantee an adequate representation of
each item (while accounting for the uncertainty). However, due to this
randomness, the output rankings may violate group fairness constraints. We give
an efficient algorithm that samples rankings from an individually-fair
distribution while ensuring that every output ranking is group fair. The
expected utility of the output ranking is at least $\alpha$ times the utility
of the optimal fair solution. Here, $\alpha$ depends on the utilities,
position-discounts, and constraints -- it approaches 1 as the range of
utilities or the position-discounts shrinks, or when utilities satisfy
distributional assumptions. Empirically, we observe that our algorithm achieves
individual and group fairness and that Pareto dominates the state-of-the-art
baselines.


------------------------------------------------------------------------------

Title:
An Efficient Virtual Data Generation Method for Reducing Communication  in Federated Learning

Abstract: Communication overhead is one of the major challenges in Federated
Learning(FL). A few classical schemes assume the server can extract the
auxiliary information about training data of the participants from the local
models to construct a central dummy dataset. The server uses the dummy dataset
to finetune aggregated global model to achieve the target test accuracy in
fewer communication rounds. In this paper, we summarize the above solutions
into a data-based communication-efficient FL framework. The key of the proposed
framework is to design an efficient extraction module(EM) which ensures the
dummy dataset has a positive effect on finetuning aggregated global model.
Different from the existing methods that use generator to design EM, our
proposed method, FedINIBoost borrows the idea of gradient match to construct
EM. Specifically, FedINIBoost builds a proxy dataset of the real dataset in two
steps for each participant at each communication round. Then the server
aggregates all the proxy datasets to form a central dummy dataset, which is
used to finetune aggregated global model. Extensive experiments verify the
superiority of our method compared with the existing classical method, FedAVG,
FedProx, Moon and FedFTG. Moreover, FedINIBoost plays a significant role in
finetuning the performance of aggregated global model at the initial stage of
FL.


------------------------------------------------------------------------------

Title:
Interactive Molecular Discovery with Natural Language

Abstract: Natural language is expected to be a key medium for various human-machine
interactions in the era of large language models. When it comes to the
biochemistry field, a series of tasks around molecules (e.g., property
prediction, molecule mining, etc.) are of great significance while having a
high technical threshold. Bridging the molecule expressions in natural language
and chemical language can not only hugely improve the interpretability and
reduce the operation difficulty of these tasks, but also fuse the chemical
knowledge scattered in complementary materials for a deeper comprehension of
molecules. Based on these benefits, we propose the conversational molecular
design, a novel task adopting natural language for describing and editing
target molecules. To better accomplish this task, we design ChatMol, a
knowledgeable and versatile generative pre-trained model, enhanced by injecting
experimental property information, molecular spatial knowledge, and the
associations between natural and chemical languages into it. Several typical
solutions including large language models (e.g., ChatGPT) are evaluated,
proving the challenge of conversational molecular design and the effectiveness
of our knowledge enhancement method. Case observations and analysis are
conducted to provide directions for further exploration of natural-language
interaction in molecular discovery.


------------------------------------------------------------------------------

Title:
RSMT: Real-time Stylized Motion Transition for Characters

Abstract: Styled online in-between motion generation has important application
scenarios in computer animation and games. Its core challenge lies in the need
to satisfy four critical requirements simultaneously: generation speed, motion
quality, style diversity, and synthesis controllability. While the first two
challenges demand a delicate balance between simple fast models and learning
capacity for generation quality, the latter two are rarely investigated
together in existing methods, which largely focus on either control without
style or uncontrolled stylized motions. To this end, we propose a Real-time
Stylized Motion Transition method (RSMT) to achieve all aforementioned goals.
Our method consists of two critical, independent components: a general motion
manifold model and a style motion sampler. The former acts as a high-quality
motion source and the latter synthesizes styled motions on the fly under
control signals. Since both components can be trained separately on different
datasets, our method provides great flexibility, requires less data, and
generalizes well when no/few samples are available for unseen styles. Through
exhaustive evaluation, our method proves to be fast, high-quality, versatile,
and controllable. The code and data are available at
{this https URL}


------------------------------------------------------------------------------

Title:
$λ$FS: A Scalable and Elastic Distributed File System Metadata  Service using Serverless Functions

Abstract: The metadata service (MDS) sits on the critical path for distributed file
system (DFS) operations, and therefore it is key to the overall performance of
a large-scale DFS. Common "serverful" MDS architectures, such as a single
server or cluster of servers, have a significant shortcoming: either they are
not scalable, or they make it difficult to achieve an optimal balance of
performance, resource utilization, and cost. A modern MDS requires a novel
architecture that addresses this shortcoming.
To this end, we design and implement $\lambda$FS, an elastic,
high-performance metadata service for large-scale DFSes. $\lambda$FS scales a
DFS metadata cache elastically on a FaaS (Function-as-a-Service) platform and
synthesizes a series of techniques to overcome the obstacles that are
encountered when building large, stateful, and performance-sensitive
applications on FaaS platforms. $\lambda$FS takes full advantage of the unique
benefits offered by FaaS $\unicode{x2013}$ elastic scaling and massive
parallelism $\unicode{x2013}$ to realize a highly-optimized metadata service
capable of sustaining up to 4.13$\times$ higher throughput, 90.40% lower
latency, 85.99% lower cost, 3.33$\times$ better performance-per-cost, and
better resource utilization and efficiency than a state-of-the-art DFS for an
industrial workload.


------------------------------------------------------------------------------

Title:
Complementary Learning Subnetworks for Parameter-Efficient  Class-Incremental Learning

Abstract: In the scenario of class-incremental learning (CIL), deep neural networks
have to adapt their model parameters to non-stationary data distributions,
e.g., the emergence of new classes over time. However, CIL models are
challenged by the well-known catastrophic forgetting phenomenon. Typical
methods such as rehearsal-based ones rely on storing exemplars of old classes
to mitigate catastrophic forgetting, which limits real-world applications
considering memory resources and privacy issues. In this paper, we propose a
novel rehearsal-free CIL approach that learns continually via the synergy
between two Complementary Learning Subnetworks. Our approach involves jointly
optimizing a plastic CNN feature extractor and an analytical feed-forward
classifier. The inaccessibility of historical data is tackled by holistically
controlling the parameters of a well-trained model, ensuring that the decision
boundary learned fits new classes while retaining recognition of previously
learned classes. Specifically, the trainable CNN feature extractor provides
task-dependent knowledge separately without interference; and the final
classifier integrates task-specific knowledge incrementally for decision-making
without forgetting. In each CIL session, it accommodates new tasks by attaching
a tiny set of declarative parameters to its backbone, in which only one matrix
per task or one vector per class is kept for knowledge retention. Extensive
experiments on a variety of task sequences show that our method achieves
competitive results against state-of-the-art methods, especially in accuracy
gain, memory cost, training efficiency, and task-order robustness. Furthermore,
to make the non-growing backbone (i.e., a model with limited network capacity)
suffice to train on more incoming tasks, a graceful forgetting implementation
on previously learned trivial tasks is empirically investigated.


------------------------------------------------------------------------------

Title:
Out of Distribution Generalization via Interventional Style Transfer in  Single-Cell Microscopy

Abstract: Real-world deployment of computer vision systems, including in the discovery
processes of biomedical research, requires causal representations that are
invariant to contextual nuisances and generalize to new data. Leveraging the
internal replicate structure of two novel single-cell fluorescent microscopy
datasets, we propose generally applicable tests to assess the extent to which
models learn causal representations across increasingly challenging levels of
OOD-generalization. We show that despite seemingly strong performance, as
assessed by other established metrics, both naive and contemporary baselines
designed to ward against confounding, collapse on these tests. We introduce a
new method, Interventional Style Transfer (IST), that substantially improves
OOD generalization by generating interventional training distributions in which
spurious correlations between biological causes and nuisances are mitigated. We
publish our code and datasets.


------------------------------------------------------------------------------

Title:
Personalized Federated Learning with Feature Alignment and Classifier  Collaboration

Abstract: Data heterogeneity is one of the most challenging issues in federated
learning, which motivates a variety of approaches to learn personalized models
for participating clients. One such approach in deep neural networks based
tasks is employing a shared feature representation and learning a customized
classifier head for each client. However, previous works do not utilize the
global knowledge during local representation learning and also neglect the
fine-grained collaboration between local classifier heads, which limit the
model generalization ability. In this work, we conduct explicit local-global
feature alignment by leveraging global semantic knowledge for learning a better
representation. Moreover, we quantify the benefit of classifier combination for
each client as a function of the combining weights and derive an optimization
problem for estimating optimal weights. Finally, extensive evaluation results
on benchmark datasets with various heterogeneous data scenarios demonstrate the
effectiveness of our proposed method. Code is available at
this https URL


------------------------------------------------------------------------------

Title:
Modeling Hierarchical Reasoning Chains by Linking Discourse Units and  Key Phrases for Reading Comprehension

Abstract: Machine reading comprehension (MRC) poses new challenges over logical
reasoning, which aims to understand the implicit logical relations entailed in
the given contexts and perform inference over them. Due to the complexity of
logic, logical relations exist at different granularity levels. However, most
existing methods of logical reasoning individually focus on either entity-aware
or discourse-based information but ignore the hierarchical relations that may
even have mutual effects. In this paper, we propose a holistic graph network
(HGN) which deals with context at both discourse level and word level, as the
basis for logical reasoning, to provide a more fine-grained relation
extraction. Specifically, node-level and type-level relations, which can be
interpreted as bridges in the reasoning process, are modeled by a hierarchical
interaction mechanism to improve the interpretation of MRC systems.
Experimental results on logical reasoning QA datasets (ReClor and LogiQA) and
natural language inference datasets (SNLI and ANLI) show the effectiveness and
generalization of our method, and in-depth analysis verifies its capability to
understand complex logical relations.


------------------------------------------------------------------------------

Title:
LVM-Med: Learning Large-Scale Self-Supervised Vision Models for Medical  Imaging via Second-order Graph Matching

Abstract: Obtaining large pre-trained models that can be fine-tuned to new tasks with
limited annotated samples has remained an open challenge for medical imaging
data. While pre-trained deep networks on ImageNet and vision-language
foundation models trained on web-scale data are prevailing approaches, their
effectiveness on medical tasks is limited due to the significant domain shift
between natural and medical images. To bridge this gap, we introduce LVM-Med,
the first family of deep networks trained on large-scale medical datasets. We
have collected approximately 1.3 million medical images from 55 publicly
available datasets, covering a large number of organs and modalities such as
CT, MRI, X-ray, and Ultrasound. We benchmark several state-of-the-art
self-supervised algorithms on this dataset and propose a novel self-supervised
contrastive learning algorithm using a graph-matching formulation. The proposed
approach makes three contributions: (i) it integrates prior pair-wise image
similarity metrics based on local and global information; (ii) it captures the
structural constraints of feature embeddings through a loss function
constructed via a combinatorial graph-matching objective; and (iii) it can be
trained efficiently end-to-end using modern gradient-estimation techniques for
black-box solvers. We thoroughly evaluate the proposed LVM-Med on 15 downstream
medical tasks ranging from segmentation and classification to object detection,
and both for the in and out-of-distribution settings. LVM-Med empirically
outperforms a number of state-of-the-art supervised, self-supervised, and
foundation models. For challenging tasks such as Brain Tumor Classification or
Diabetic Retinopathy Grading, LVM-Med improves previous vision-language models
trained on 1 billion masks by 6-7% while using only a ResNet-50.


------------------------------------------------------------------------------

Title:
Reinforcement Learning-based Virtual Fixtures for Teleoperation of  Hydraulic Construction Machine

Abstract: The utilization of teleoperation is a crucial aspect of the construction
industry, as it enables operators to control machines safely from a distance.
However, remote operation of these machines at a joint level using individual
joysticks necessitates extensive training for operators to achieve proficiency
due to their multiple degrees of freedom. Additionally, verifying the machine
resulting motion is only possible after execution, making optimal control
challenging. In addressing this issue, this study proposes a reinforcement
learning-based approach to optimize task performance. The control policy
acquired through learning is used to provide instructions on efficiently
controlling and coordinating multiple joints. To evaluate the effectiveness of
the proposed framework, a user study is conducted with a Brokk 170 construction
machine by assessing its performance in a typical construction task involving
inserting a chisel into a borehole. The effectiveness of the proposed framework
is evaluated by comparing the performance of participants in the presence and
absence of virtual fixtures. This study results demonstrate the proposed
framework potential in enhancing the teleoperation process in the construction
industry.


------------------------------------------------------------------------------

Title:
NILUT: Conditional Neural Implicit 3D Lookup Tables for Image  Enhancement

Abstract: 3D lookup tables (3D LUTs) are a key component for image enhancement. Modern
image signal processors (ISPs) have dedicated support for these as part of the
camera rendering pipeline. Cameras typically provide multiple options for
picture styles, where each style is usually obtained by applying a unique
handcrafted 3D LUT. Current approaches for learning and applying 3D LUTs are
notably fast, yet not so memory-efficient, as storing multiple 3D LUTs is
required. For this reason and other implementation limitations, their use on
mobile devices is less popular. In this work, we propose a Neural Implicit LUT
(NILUT), an implicitly defined continuous 3D color transformation parameterized
by a neural network. We show that NILUTs are capable of accurately emulating
real 3D LUTs. Moreover, a NILUT can be extended to incorporate multiple styles
into a single network with the ability to blend styles implicitly. Our novel
approach is memory-efficient, controllable and can complement previous methods,
including learned ISPs. Code, models and dataset available at:
this https URL


------------------------------------------------------------------------------

Title:
High Throughput Open-Source Implementation of Wi-Fi 6 and WiMAX LDPC  Encoder and Decoder

Abstract: This paper describes the design and C99 implementation of a free and
open-source Low-Density Parity-Check (LDPC) codes encoder and decoder focused
primarily on the Quasi-Cyclic LDPC (QCLDPC) codes utilized in the IEEE
802.11ax-2021 (Wi-Fi 6) and IEEE 802.16-2017 (WiMAX) standards. The encoder is
designed in two variants: the first one universal, the other a minimal memory
usage design. The decoder provides a single- and multi- threaded implementation
of the layered singlescan min-sum LDPC decoding algorithm both for floating
point and fixed-point arithmetic. Both encoder and decoder are directly
callable from MATLAB using the provided MEX wrappers but are designed to be
simply used in any C project. A comparison of throughput and error performance
with the recent commercial closed-source MEX implementation of an LDPC encoder
and decoder introduced in MATLAB R2021b Communications Toolbox is provided.
Source code portability to alternative nonx86 architectures is facilitated by
using only the standard C99 constructs, GNU tools, and POSIX libraries. The
implementation maintains low-memory requirements, enabling its deployment in a
constrained-architecture in the context of Internet of Things. All source codes
are freely available on GitHub under a permissive BSD license.


------------------------------------------------------------------------------

Title:
Evaluation of Chinese-English Machine Translation of Emotion-Loaded  Microblog Texts: A Human Annotated Dataset for the Quality Assessment of  Emotion Translation

Abstract: In this paper, we focus on how current Machine Translation (MT) tools perform
on the translation of emotion-loaded texts by evaluating outputs from Google
Translate according to a framework proposed in this paper. We propose this
evaluation framework based on the Multidimensional Quality Metrics (MQM) and
perform a detailed error analysis of the MT outputs. From our analysis, we
observe that about 50% of the MT outputs fail to preserve the original emotion.
After further analysis of the errors, we find that emotion carrying words and
linguistic phenomena such as polysemous words, negation, abbreviation etc., are
common causes for these translation errors.


------------------------------------------------------------------------------

Title:
Cryptographic ransomware encryption detection: Survey

Abstract: The ransomware threat has loomed over our digital life since 1989. Criminals
use this type of cyber attack to lock or encrypt victims' data, often coercing
them to pay exorbitant amounts in ransom. The damage ransomware causes ranges
from monetary losses paid for ransom at best to endangering human lives.
Cryptographic ransomware, where attackers encrypt the victim's data, stands as
the predominant ransomware variant. The primary characteristics of these
attacks have remained the same since the first ransomware attack. For this
reason, we consider this a key factor differentiating ransomware from other
cyber attacks, making it vital in tackling the threat of cryptographic
ransomware. This paper proposes a cyber kill chain that describes the modern
crypto-ransomware attack. The survey focuses on the Encryption phase as
described in our proposed cyber kill chain and its detection techniques. We
identify three main methods used in detecting encryption-related activities by
ransomware, namely API and System calls, I/O monitoring, and file system
activities monitoring. Machine learning (ML) is a tool used in all three
identified methodologies, and some of the issues within the ML domain related
to this survey are also covered as part of their respective methodologies. The
survey of selected proposals is conducted through the prism of those three
methodologies, showcasing the importance of detecting ransomware during
pre-encryption and encryption activities and the windows of opportunity to do
so. We also examine commercial crypto-ransomware protection and detection
offerings and show the gap between academic research and commercial
applications.


------------------------------------------------------------------------------

Title:
Towards Understanding What Code Language Models Learned

Abstract: Pre-trained language models are effective in a variety of natural language
tasks, but it has been argued their capabilities fall short of fully learning
meaning or understanding language. To understand the extent to which language
models can learn some form of meaning, we investigate their ability to capture
semantics of code beyond superficial frequency and co-occurrence. In contrast
to previous research on probing models for linguistic features, we study
pre-trained models in a setting that allows for objective and straightforward
evaluation of a model's ability to learn semantics. In this paper, we examine
whether such models capture the semantics of code, which is precisely and
formally defined. Through experiments involving the manipulation of code
fragments, we show that code pre-trained models of code learn a robust
representation of the computational semantics of code that goes beyond
superficial features of form alone


------------------------------------------------------------------------------

Title:
Mass-Producing Failures of Multimodal Systems with Language Models

Abstract: Deployed multimodal systems can fail in ways that evaluators did not
anticipate. In order to find these failures before deployment, we introduce
MultiMon, a system that automatically identifies systematic failures --
generalizable, natural-language descriptions of patterns of model failures. To
uncover systematic failures, MultiMon scrapes a corpus for examples of
erroneous agreement: inputs that produce the same output, but should not. It
then prompts a language model (e.g., GPT-4) to find systematic patterns of
failure and describe them in natural language. We use MultiMon to find 14
systematic failures (e.g., "ignores quantifiers") of the CLIP text-encoder,
each comprising hundreds of distinct inputs (e.g., "a shelf with a few/many
books"). Because CLIP is the backbone for most state-of-the-art multimodal
systems, these inputs produce failures in Midjourney 5.1, DALL-E, VideoFusion,
and others. MultiMon can also steer towards failures relevant to specific use
cases, such as self-driving cars. We see MultiMon as a step towards evaluation
that autonomously explores the long tail of potential system failures. Code for
MULTIMON is available at this https URL


------------------------------------------------------------------------------

Title:
Heterogeneous Coalition Formation and Scheduling with Multi-Skilled  Robots

Abstract: We present an approach to task scheduling in heterogeneous multi-robot
systems. In our setting, the tasks to complete require diverse skills. We
assume that each robot is multi-skilled, i.e., each robot offers a subset of
the possible skills. This makes the formation of heterogeneous teams
(\emph{coalitions}) a requirement for task completion. We present two
centralized algorithms to schedule robots across tasks and to form suitable
coalitions, assuming stochastic travel times across tasks. The coalitions are
dynamic, in that the robots form and disband coalitions as the schedule is
executed. The first algorithm we propose guarantees optimality, but its
run-time is acceptable only for small problem instances. The second algorithm
we propose can tackle large problems with short run-times, and is based on a
heuristic approach that typically reaches 1x-2x of the optimal solution cost.


------------------------------------------------------------------------------

Title:
Moving Mesh Simulations of Pitting Corrosion

Abstract: Damages due to pitting corrosion of metals cost industry billions of dollars
per year and can put human lives at risk. The design and implementation of an
adaptive moving mesh method is provided for a moving boundary problem related
to pitting corrosion. The adaptive mesh is generated automatically by solving a
mesh PDE coupled to the nonlinear potential problem. The moving mesh approach
is shown to enable initial mesh generation, provide mesh recovery and is able
to smoothly tackle changing pit geometry. Materials with varying
crystallography are considered. Changing mesh topology due to the merging of
pits is also handled. The evolution of the pit shape, the pit depth and the pit
width are computed and compared to existing results in the literature.


------------------------------------------------------------------------------

Title:
Vital Videos: A dataset of videos with PPG and blood pressure ground  truths

Abstract: We collected a large dataset consisting of nearly 900 unique participants.
For every participant we recorded two 30 second uncompressed videos,
synchronized PPG waveforms and a single blood pressure measurement. Gender, age
and skin color were also registered for every participant. The dataset includes
roughly equal numbers of males and females, as well as participants of all
ages. While the skin color distribution could have been more balanced, the
dataset contains individuals from every skin color. The data was collected in a
diverse set of locations to ensure a wide variety of backgrounds and lighting
conditions. In an effort to assist in the research and development of remote
vital sign measurement we are now opening up access to this dataset.


------------------------------------------------------------------------------

Title:
Online Unsupervised Video Object Segmentation via Contrastive Motion  Clustering

Abstract: Online unsupervised video object segmentation (UVOS) uses the previous frames
as its input to automatically separate the primary object(s) from a streaming
video without using any further manual annotation. A major challenge is that
the model has no access to the future and must rely solely on the history,
i.e., the segmentation mask is predicted from the current frame as soon as it
is captured. In this work, a novel contrastive motion clustering algorithm with
an optical flow as its input is proposed for the online UVOS by exploiting the
common fate principle that visual elements tend to be perceived as a group if
they possess the same motion pattern. We build a simple and effective
auto-encoder to iteratively summarize non-learnable prototypical bases for the
motion pattern, while the bases in turn help learn the representation of the
embedding network. Further, a contrastive learning strategy based on a boundary
prior is developed to improve foreground and background feature discrimination
in the representation learning stage. The proposed algorithm can be optimized
on arbitrarily-scale data i.e., frame, clip, dataset) and performed in an
online fashion. Experiments on $\textit{DAVIS}_{\textit{16}}$, $\textit{FBMS}$,
and $\textit{SegTrackV2}$ datasets show that the accuracy of our method
surpasses the previous state-of-the-art (SoTA) online UVOS method by a margin
of 0.8%, 2.9%, and 1.1%, respectively. Furthermore, by using an online deep
subspace clustering to tackle the motion grouping, our method is able to
achieve higher accuracy at $3\times$ faster inference time compared to SoTA
online UVOS method, and making a good trade-off between effectiveness and
efficiency.


------------------------------------------------------------------------------

Title:
Beyond Learned Metadata-based Raw Image Reconstruction

Abstract: While raw images have distinct advantages over sRGB images, e.g., linearity
and fine-grained quantization levels, they are not widely adopted by general
users due to their substantial storage requirements. Very recent studies
propose to compress raw images by designing sampling masks within the pixel
space of the raw image. However, these approaches often leave space for
pursuing more effective image representations and compact metadata. In this
work, we propose a novel framework that learns a compact representation in the
latent space, serving as metadata, in an end-to-end manner. Compared with lossy
image compression, we analyze the intrinsic difference of the raw image
reconstruction task caused by rich information from the sRGB image. Based on
the analysis, a novel backbone design with asymmetric and hybrid spatial
feature resolutions is proposed, which significantly improves the
rate-distortion performance. Besides, we propose a novel design of the context
model, which can better predict the order masks of encoding/decoding based on
both the sRGB image and the masks of already processed features. Benefited from
the better modeling of the correlation between order masks, the already
processed information can be better utilized. Moreover, a novel sRGB-guided
adaptive quantization precision strategy, which dynamically assigns varying
levels of quantization precision to different regions, further enhances the
representation ability of the model. Finally, based on the iterative properties
of the proposed context model, we propose a novel strategy to achieve variable
bit rates using a single model. This strategy allows for the continuous
convergence of a wide range of bit rates. Extensive experimental results
demonstrate that the proposed method can achieve better reconstruction quality
with a smaller metadata size.


------------------------------------------------------------------------------

Title:
On the Optimal Bounds for Noisy Computing

Abstract: We revisit the problem of computing with noisy information considered in
Feige et al. 1994, which includes computing the OR function from noisy queries,
and computing the MAX, SEARCH and SORT functions from noisy pairwise
comparisons. For $K$ given elements, the goal is to correctly recover the
desired function with probability at least $1-\delta$ when the outcome of each
query is flipped with probability $p$. We consider both the adaptive sampling
setting where each query can be adaptively designed based on past outcomes, and
the non-adaptive sampling setting where the query cannot depend on past
outcomes. The prior work provides tight bounds on the worst-case query
complexity in terms of the dependence on $K$. However, the upper and lower
bounds do not match in terms of the dependence on $\delta$ and $p$. We improve
the lower bounds for all the four functions under both adaptive and
non-adaptive query models. Most of our lower bounds match the upper bounds up
to constant factors when either $p$ or $\delta$ is bounded away from $0$, while
the ratio between the best prior upper and lower bounds goes to infinity when
$p\rightarrow 0$ or $p\rightarrow 1/2$. On the other hand, we also provide
matching upper and lower bounds for the number of queries in expectation,
improving both the upper and lower bounds for the variable-length query model.


------------------------------------------------------------------------------

Title:
Randomized Quantization is All You Need for Differential Privacy in  Federated Learning

Abstract: Federated learning (FL) is a common and practical framework for learning a
machine model in a decentralized fashion. A primary motivation behind this
decentralized approach is data privacy, ensuring that the learner never sees
the data of each local source itself. Federated learning then comes with two
majors challenges: one is handling potentially complex model updates between a
server and a large number of data sources; the other is that de-centralization
may, in fact, be insufficient for privacy, as the local updates themselves can
reveal information about the sources' data. To address these issues, we
consider an approach to federated learning that combines quantization and
differential privacy. Absent privacy, Federated Learning often relies on
quantization to reduce communication complexity. We build upon this approach
and develop a new algorithm called the \textbf{R}andomized
\textbf{Q}uantization \textbf{M}echanism (RQM), which obtains privacy through a
two-levels of randomization. More precisely, we randomly sub-sample feasible
quantization levels, then employ a randomized rounding procedure using these
sub-sampled discrete levels. We are able to establish that our results preserve
``Renyi differential privacy'' (Renyi DP). We empirically study the performance
of our algorithm and demonstrate that compared to previous work it yields
improved privacy-accuracy trade-offs for DP federated learning. To the best of
our knowledge, this is the first study that solely relies on randomized
quantization without incorporating explicit discrete noise to achieve Renyi DP
guarantees in Federated Learning systems.


------------------------------------------------------------------------------

Title:
TADIL: Task-Agnostic Domain-Incremental Learning through Task-ID  Inference using Transformer Nearest-Centroid Embeddings

Abstract: Machine Learning (ML) models struggle with data that changes over time or
across domains due to factors such as noise, occlusion, illumination, or
frequency, unlike humans who can learn from such non independent and
identically distributed data. Consequently, a Continual Learning (CL) approach
is indispensable, particularly, Domain-Incremental Learning. In this paper, we
propose a novel pipeline for identifying tasks in domain-incremental learning
scenarios without supervision. The pipeline comprises four steps. First, we
obtain base embeddings from the raw data using an existing transformer-based
model. Second, we group the embedding densities based on their similarity to
obtain the nearest points to each cluster centroid. Third, we train an
incremental task classifier using only these few points. Finally, we leverage
the lightweight computational requirements of the pipeline to devise an
algorithm that decides in an online fashion when to learn a new task using the
task classifier and a drift detector. We conduct experiments using the SODA10M
real-world driving dataset and several CL strategies. We demonstrate that the
performance of these CL strategies with our pipeline can match the ground-truth
approach, both in classical experiments assuming task boundaries, and also in
more realistic task-agnostic scenarios that require detecting new tasks
on-the-fly


------------------------------------------------------------------------------

Title:
Reward Shaping via Diffusion Process in Reinforcement Learning

Abstract: Reinforcement Learning (RL) models have continually evolved to navigate the
exploration - exploitation trade-off in uncertain Markov Decision Processes
(MDPs). In this study, I leverage the principles of stochastic thermodynamics
and system dynamics to explore reward shaping via diffusion processes. This
provides an elegant framework as a way to think about exploration-exploitation
trade-off. This article sheds light on relationships between information
entropy, stochastic system dynamics, and their influences on entropy
production. This exploration allows us to construct a dual-pronged framework
that can be interpreted as either a maximum entropy program for deriving
efficient policies or a modified cost optimization program accounting for
informational costs and benefits. This work presents a novel perspective on the
physical nature of information and its implications for online learning in
MDPs, consequently providing a better understanding of information-oriented
formulations in RL.


------------------------------------------------------------------------------

Title:
Protecting the Decentralized Future: An Exploration of Common Blockchain  Attacks and their Countermeasures

Abstract: Blockchain technology transformed the digital sphere by providing a
transparent, secure, and decentralized platform for data security across a
range of industries, including cryptocurrencies and supply chain management.
Blockchain's integrity and dependability have been jeopardized by the rising
number of security threats, which have attracted cybercriminals as a target. By
summarizing suggested fixes, this research aims to offer a thorough analysis of
mitigating blockchain attacks. The objectives of the paper include identifying
weak blockchain attacks, evaluating various solutions, and determining how
effective and effective they are at preventing these attacks. The study also
highlights how crucial it is to take into account the particular needs of every
blockchain application. This study provides beneficial perspectives and
insights for blockchain researchers and practitioners, making it essential
reading for those interested in current and future trends in blockchain
security research.


------------------------------------------------------------------------------

Title:
No Wrong Turns: The Simple Geometry Of Neural Networks Optimization  Paths

Abstract: Understanding the optimization dynamics of neural networks is necessary for
closing the gap between theory and practice. Stochastic first-order
optimization algorithms are known to efficiently locate favorable minima in
deep neural networks. This efficiency, however, contrasts with the non-convex
and seemingly complex structure of neural loss landscapes. In this study, we
delve into the fundamental geometric properties of sampled gradients along
optimization paths. We focus on two key quantities, which appear in the
restricted secant inequality and error bound. Both hold high significance for
first-order optimization. Our analysis reveals that these quantities exhibit
predictable, consistent behavior throughout training, despite the stochasticity
induced by sampling minibatches. Our findings suggest that not only do
optimization trajectories never encounter significant obstacles, but they also
maintain stable dynamics during the majority of training. These observed
properties are sufficiently expressive to theoretically guarantee linear
convergence and prescribe learning rate schedules mirroring empirical
practices. We conduct our experiments on image classification, semantic
segmentation and language modeling across different batch sizes, network
architectures, datasets, optimizers, and initialization seeds. We discuss the
impact of each factor. Our work provides novel insights into the properties of
neural network loss functions, and opens the door to theoretical frameworks
more relevant to prevalent practice.


------------------------------------------------------------------------------

Title:
QuOTeS: Query-Oriented Technical Summarization

Abstract: Abstract. When writing an academic paper, researchers often spend
considerable time reviewing and summarizing papers to extract relevant
citations and data to compose the Introduction and Related Work sections. To
address this problem, we propose QuOTeS, an interactive system designed to
retrieve sentences related to a summary of the research from a collection of
potential references and hence assist in the composition of new papers. QuOTeS
integrates techniques from Query-Focused Extractive Summarization and
High-Recall Information Retrieval to provide Interactive Query-Focused
Summarization of scientific documents. To measure the performance of our
system, we carried out a comprehensive user study where participants uploaded
papers related to their research and evaluated the system in terms of its
usability and the quality of the summaries it produces. The results show that
QuOTeS provides a positive user experience and consistently provides
query-focused summaries that are relevant, concise, and complete. We share the
code of our system and the novel Query-Focused Summarization dataset collected
during our experiments at this https URL


------------------------------------------------------------------------------

Title:
Efficient Machine Translation Corpus Generation

Abstract: This paper proposes an efficient and semi-automated method for
human-in-the-loop post-editing for machine translation (MT) corpus generation.
The method is based on online training of a custom MT quality estimation metric
on-the-fly as linguists perform post-edits. The online estimator is used to
prioritize worse hypotheses for post-editing, and auto-close best hypotheses
without post-editing. This way, significant improvements can be achieved in the
resulting quality of post-edits at a lower cost due to reduced human
involvement. The trained estimator can also provide an online sanity check
mechanism for post-edits and remove the need for additional linguists to review
them or work on the same hypotheses. In this paper, the effect of prioritizing
with the proposed method on the resulting MT corpus quality is presented versus
scheduling hypotheses randomly. As demonstrated by experiments, the proposed
method improves the lifecycle of MT models by focusing the linguist effort on
production samples and hypotheses, which matter most for expanding MT corpora
to be used for re-training them.


------------------------------------------------------------------------------

Title:
EvolveMT: an Ensemble MT Engine Improving Itself with Usage Only

Abstract: This paper presents EvolveMT for efficiently combining multiple machine
translation (MT) engines. The proposed system selects the output from a single
engine for each segment by utilizing online learning techniques to predict the
most suitable system for every translation request. A neural quality estimation
metric supervises the method without requiring reference translations. The
online learning capability of this system allows for dynamic adaptation to
alterations in the domain or machine translation engines, thereby obviating the
necessity for additional training. EvolveMT selects a subset of translation
engines to be called based on the source sentence features. The degree of
exploration is configurable according to the desired quality-cost trade-off.
Results from custom datasets demonstrate that EvolveMT achieves similar
translation accuracy at a lower cost than selecting the best translation of
each segment from all translations using an MT quality estimator. To our
knowledge, EvolveMT is the first meta MT system that adapts itself after
deployment to incoming translation requests from the production environment
without needing costly retraining on human feedback.


------------------------------------------------------------------------------

Title:
Learning and evolution: factors influencing an effective combination

Abstract: The mutual relationship between evolution and learning is a controversial
argument among the artificial intelligence and neuro-evolution communities.
After more than three decades, there is still no common agreement on the
matter. In this paper the author investigates whether combining learning and
evolution permits to find better solutions than those discovered by evolution
alone. More specifically, the author presents a series of empirical studies
that highlight some specific conditions determining the success of such a
combination, like the introduction of noise during the learning and selection
processes. Results are obtained in two qualitatively different domains, where
agent/environment interactions are minimal or absent.


------------------------------------------------------------------------------

Title:
Retrieval-Based Transformer for Table Augmentation

Abstract: Data preparation, also called data wrangling, is considered one of the most
expensive and time-consuming steps when performing analytics or building
machine learning models. Preparing data typically involves collecting and
merging data from complex heterogeneous, and often large-scale data sources,
such as data lakes. In this paper, we introduce a novel approach toward
automatic data wrangling in an attempt to alleviate the effort of end-users,
e.g. data analysts, in structuring dynamic views from data lakes in the form of
tabular data. We aim to address table augmentation tasks, including row/column
population and data imputation. Given a corpus of tables, we propose a
retrieval augmented self-trained transformer model. Our self-learning strategy
consists in randomly ablating tables from the corpus and training the
retrieval-based model to reconstruct the original values or headers given the
partial tables as input. We adopt this strategy to first train the dense neural
retrieval model encoding table-parts to vectors, and then the end-to-end model
trained to perform table augmentation tasks. We test on EntiTables, the
standard benchmark for table augmentation, as well as introduce a new benchmark
to advance further research: WebTables. Our model consistently and
substantially outperforms both supervised statistical methods and the current
state-of-the-art transformer-based models.


------------------------------------------------------------------------------

Title:
Designing Explainable Predictive Machine Learning Artifacts: Methodology  and Practical Demonstration

Abstract: Prediction-oriented machine learning is becoming increasingly valuable to
organizations, as it may drive applications in crucial business areas. However,
decision-makers from companies across various industries are still largely
reluctant to employ applications based on modern machine learning algorithms.
We ascribe this issue to the widely held view on advanced machine learning
algorithms as "black boxes" whose complexity does not allow for uncovering the
factors that drive the output of a corresponding system. To contribute to
overcome this adoption barrier, we argue that research in information systems
should devote more attention to the design of prototypical prediction-oriented
machine learning applications (i.e., artifacts) whose predictions can be
explained to human decision-makers. However, despite the recent emergence of a
variety of tools that facilitate the development of such artifacts, there has
so far been little research on their development. We attribute this research
gap to the lack of methodological guidance to support the creation of these
artifacts. For this reason, we develop a methodology which unifies
methodological knowledge from design science research and predictive analytics
with state-of-the-art approaches to explainable artificial intelligence.
Moreover, we showcase the methodology using the example of price prediction in
the sharing economy (i.e., on Airbnb).


------------------------------------------------------------------------------

Title:
Verification and Validation of Cylinder Drag: Pressure and Stress  Approximations on Curved Boundaries

Abstract: We study a technique for verification of stress and pressure computations on
boundaries in flow simulations. We utilize existing experiments to provide
validation of the simulations. We show that this approach can reveal critical
flaws in simulation algorithms. Using the successful computational algorithms,
we examine Lamb's model for cylinder drag at low Reynolds numbers. We comment
on a discrepancy observed in an experimental paper, suggesting that the domain
size may be a contributing factor. Our simulations on suitably large domains
confirm Lamb's model. We highlight a paradox related to imposing Dirichlet
(Stokes) boundary conditions on polygonal approximations of the curved surface
using finite-element methods that are exactly divergence free. The
finite-element simulations provide very poor representations of drag when the
boundary conditions are imposed strongly. We demonstrate that relaxing the
boundary conditions using Nitsche's method restores high-order approximation.


------------------------------------------------------------------------------

Title:
Any Deep ReLU Network is Shallow

Abstract: We constructively prove that every deep ReLU network can be rewritten as a
functionally identical three-layer network with weights valued in the extended
reals. Based on this proof, we provide an algorithm that, given a deep ReLU
network, finds the explicit weights of the corresponding shallow network. The
resulting shallow network is transparent and used to generate explanations of
the model s behaviour.


------------------------------------------------------------------------------

Title:
Using super-resolution for enhancing visual perception and segmentation  performance in veterinary cytology

Abstract: The primary objective of this research was to enhance the quality of semantic
segmentation in cytology images by incorporating super-resolution (SR)
architectures. An additional contribution was the development of a novel
dataset aimed at improving imaging quality in the presence of inaccurate focus.
Our experimental results demonstrate that the integration of SR techniques into
the segmentation pipeline can lead to a significant improvement of up to 25% in
the mean average precision (mAP) segmentation metric. These findings suggest
that leveraging SR architectures holds great promise for advancing the state of
the art in cytology image analysis.


------------------------------------------------------------------------------

Title:
UMM: Unsupervised Mean-difference Maximization

Abstract: Many brain-computer interfaces make use of brain signals that are elicited in
response to a visual, auditory or tactile stimulus, so-called event-related
potentials (ERPs). In visual ERP speller applications, sets of letters shown on
a screen are flashed randomly, and the participant attends to the target letter
they want to spell. When this letter flashes, the resulting ERP is different
compared to when any other non-target letter flashes. We propose a new
unsupervised approach to detect this attended letter. In each trial, for every
available letter our approach makes the hypothesis that it is in fact the
attended letter, and calculates the ERPs based on each of these hypotheses. We
leverage the fact that only the true hypothesis produces the largest difference
between the class means. Note that this unsupervised method does not require
any changes to the underlying experimental paradigm and therefore can be
employed in almost any ERP-based setup. To deal with limited data, we use a
block-Toeplitz regularized covariance matrix that models the background
activity. We implemented the proposed novel unsupervised mean-difference
maximization (UMM) method and evaluated it in offline replays of brain-computer
interface visual speller datasets. For a dataset that used 16 flashes per
symbol per trial, UMM correctly classifies 3651 out of 3654 letters
($99.92\,\%$) across 25 participants. In another dataset with fewer and shorter
trials, 7344 out of 7383 letters ($99.47\,\%$) are classified correctly across
54 participants with two sessions each. Even in more challenging datasets
obtained from patients with amyotrophic lateral sclerosis ($77.86\,\%$) or when
using auditory ERPs ($82.52\,\%$), the obtained classification rates obtained
by UMM are competitive. In addition, UMM provides stable confidence measures
which can be used to monitor convergence.


------------------------------------------------------------------------------

Title:
Discovering Causality for Efficient Cooperation in Multi-Agent  Environments

Abstract: In cooperative Multi-Agent Reinforcement Learning (MARL) agents are required
to learn behaviours as a team to achieve a common goal. However, while learning
a task, some agents may end up learning sub-optimal policies, not contributing
to the objective of the team. Such agents are called lazy agents due to their
non-cooperative behaviours that may arise from failing to understand whether
they caused the rewards. As a consequence, we observe that the emergence of
cooperative behaviours is not necessarily a byproduct of being able to solve a
task as a team. In this paper, we investigate the applications of causality in
MARL and how it can be applied in MARL to penalise these lazy agents. We
observe that causality estimations can be used to improve the credit assignment
to the agents and show how it can be leveraged to improve independent learning
in MARL. Furthermore, we investigate how Amortized Causal Discovery can be used
to automate causality detection within MARL environments. The results
demonstrate that causality relations between individual observations and the
team reward can be used to detect and punish lazy agents, making them develop
more intelligent behaviours. This results in improvements not only in the
overall performances of the team but also in their individual capabilities. In
addition, results show that Amortized Causal Discovery can be used efficiently
to find causal relations in MARL.


------------------------------------------------------------------------------

Title:
$\mathbf{\mathbb{E}^{FWI}}$: Multi-parameter Benchmark Datasets for  Elastic Full Waveform Inversion of Geophysical Properties

Abstract: Elastic geophysical properties (such as P- and S-wave velocities) are of
great importance to various subsurface applications like CO$_2$ sequestration
and energy exploration (e.g., hydrogen and geothermal). Elastic full waveform
inversion (FWI) is widely applied for characterizing reservoir properties. In
this paper, we introduce $\mathbf{\mathbb{E}^{FWI}}$, a comprehensive benchmark
dataset that is specifically designed for elastic FWI.
$\mathbf{\mathbb{E}^{FWI}}$ encompasses 8 distinct datasets that cover diverse
subsurface geologic structures (flat, curve, faults, etc). The benchmark
results produced by three different deep learning methods are provided. In
contrast to our previously presented dataset (pressure recordings) for acoustic
FWI (referred to as OpenFWI), the seismic dataset in
$\mathbf{\mathbb{E}^{FWI}}$ has both vertical and horizontal components.
Moreover, the velocity maps in $\mathbf{\mathbb{E}^{FWI}}$ incorporate both P-
and S-wave velocities. While the multicomponent data and the added S-wave
velocity make the data more realistic, more challenges are introduced regarding
the convergence and computational cost of the inversion. We conduct
comprehensive numerical experiments to explore the relationship between P-wave
and S-wave velocities in seismic data. The relation between P- and S-wave
velocities provides crucial insights into the subsurface properties such as
lithology, porosity, fluid content, etc. We anticipate that
$\mathbf{\mathbb{E}^{FWI}}$ will facilitate future research on multiparameter
inversions and stimulate endeavors in several critical research topics of
carbon-zero and new energy exploration. All datasets, codes and relevant
information can be accessed through our website at this https URL


------------------------------------------------------------------------------

Title:
DynaQuant: Compressing Deep Learning Training Checkpoints via Dynamic  Quantization

Abstract: With the increase in the scale of Deep Learning (DL) training workloads in
terms of compute resources and time consumption, the likelihood of encountering
in-training failures rises substantially, leading to lost work and resource
wastage. Such failures are typically offset by a checkpointing mechanism, which
comes at the cost of storage and network bandwidth overhead. State-of-the-art
approaches involve lossy model compression mechanisms, which induce a tradeoff
between the resulting model quality (accuracy) and compression ratio. Delta
compression is then also used to further reduce the overhead by only storing
the difference between consecutive checkpoints. We make a key enabling
observation that the sensitivity of model weights to compression varies during
training, and different weights benefit from different quantization levels
(ranging from retaining full precision to pruning). We propose (1) a
non-uniform quantization scheme that leverages this variation, (2) an efficient
search mechanism to dynamically adjust to the best quantization configurations,
and (3) a quantization-aware delta compression mechanism that rearranges
weights to minimize checkpoint differences, thereby maximizing compression. We
instantiate these contributions in DynaQuant - a framework for DL workload
checkpoint compression. Our experiments show that DynaQuant consistently
achieves better tradeoff between accuracy and compression ratios compared to
prior works, enabling a compression ratio up to 39x and withstanding up to 10
restores with negligible accuracy impact for fault-tolerant training. DynaQuant
achieves at least an order of magnitude reduction in checkpoint storage
overhead for training failure recovery as well as transfer learning use cases
without any loss of accuracy


------------------------------------------------------------------------------

Title:
Time-Optimal Path Planning in a Constant Wind for Uncrewed Aerial  Vehicles using Dubins Set Classification

Abstract: Time-optimal path planning in high winds for a turning rate constrained UAV
is a challenging problem to solve and is important for deployment and field
operations. Previous works have used trochoidal path segments, which consist of
straight and maximum-rate turn segments, as optimal extremal paths in uniform
wind conditions. Current methods iterate over all candidate trochoidal
trajectory types and choose the time-optimal one; however, this exhaustive
search can be computationally slow. In this paper we present a method to
decrease the computation time. We achieve this via a geometric approach to
reduce the candidate trochoidal trajectory types by framing the problem in the
air-relative frame and bounding the solution within a subset of candidate
trajectories. This method reduces overall computation by 37.4% compared to
pre-existing methods in Bang-Straight-Bang trajectories, freeing up computation
for other onboard processes and can lead to significant total computational
reductions when solving many trochoidal paths. When used within the framework
of a global path planner, faster state expansions help find solutions faster or
compute higher-quality paths. We also release our open-source codebase as a C++
package.


------------------------------------------------------------------------------

Title:
Decoding Urban-health Nexus: Interpretable Machine Learning Illuminates  Cancer Prevalence based on Intertwined City Features

Abstract: This study investigates the interplay among social demographics, built
environment characteristics, and environmental hazard exposure features in
determining community level cancer prevalence. Utilizing data from five
Metropolitan Statistical Areas in the United States: Chicago, Dallas, Houston,
Los Angeles, and New York, the study implemented an XGBoost machine learning
model to predict the extent of cancer prevalence and evaluate the importance of
different features. Our model demonstrates reliable performance, with results
indicating that age, minority status, and population density are among the most
influential factors in cancer prevalence. We further explore urban development
and design strategies that could mitigate cancer prevalence, focusing on green
space, developed areas, and total emissions. Through a series of experimental
evaluations based on causal inference, the results show that increasing green
space and reducing developed areas and total emissions could alleviate cancer
prevalence. The study and findings contribute to a better understanding of the
interplay among urban features and community health and also show the value of
interpretable machine learning models for integrated urban design to promote
public health. The findings also provide actionable insights for urban planning
and design, emphasizing the need for a multifaceted approach to addressing
urban health disparities through integrated urban design strategies.


------------------------------------------------------------------------------

Title:
MultiEarth 2023 Deforestation Challenge -- Team FOREVER

Abstract: It is important problem to accurately estimate deforestation of satellite
imagery since this approach can analyse extensive area without direct human
access. However, it is not simple problem because of difficulty in observing
the clear ground surface due to extensive cloud cover during long rainy season.
In this paper, we present a multi-view learning strategy to predict
deforestation status in the Amazon rainforest area with latest deep neural
network models. Multi-modal dataset consists of three types of different
satellites imagery, Sentinel-1, Sentinel-2 and Landsat 8 is utilized to train
and predict deforestation status. MMsegmentation framework is selected to apply
comprehensive data augmentation and diverse networks. The proposed method
effectively and accurately predicts the deforestation status of new queries.


------------------------------------------------------------------------------

Title:
System Level Evaluation of Network-Controlled Repeaters: Performance  Improvement of Serving Cell and Interference Impact on Neighbor Cells

Abstract: Heterogeneous networks have been studied as one of the enablers of network
densification. These studies have been intensified to overcome some drawbacks
related to propagation in millimeter waves (mmWaves), such as severe path and
penetration losses. One of the promising heterogeneous nodes is
network-controlled repeater (NCR). It was proposed by the 3rd generation
partnership project (3GPP) in Release 18 as a candidate solution to enhance
network coverage. In this context, this work performs a system level evaluation
to analyze the performance improvement that an NCR can cause in its serving
cell as well as its interference impact on neighbor cells. Particularly, the
results show a considerable improvement on the performance of user equipments
(UEs) served by the NCR, while neighbor UEs that receive the NCR signal as
interference are negatively impacted, but not enough to suffer from outage.


------------------------------------------------------------------------------

Title:
Near-Optimal Dynamic Rounding of Fractional Matchings in Bipartite  Graphs

Abstract: We study dynamic $(1-\epsilon)$-approximate rounding of fractional matchings
-- a key ingredient in numerous breakthroughs in the dynamic graph algorithms
literature. Our first contribution is a surprisingly simple deterministic
rounding algorithm in bipartite graphs with amortized update time
$O(\epsilon^{-1} \log^2 (\epsilon^{-1} \cdot n))$, matching an (unconditional)
recourse lower bound of $\Omega(\epsilon^{-1})$ up to logarithmic factors.
Moreover, this algorithm's update time improves provided the minimum (non-zero)
weight in the fractional matching is lower bounded throughout. Combining this
algorithm with novel dynamic \emph{partial rounding} algorithms to increase
this minimum weight, we obtain several algorithms that improve this dependence
on $n$. For example, we give a high-probability randomized algorithm with
$\tilde{O}(\epsilon^{-1}\cdot (\log\log n)^2)$-update time against adaptive
adversaries. (We use Soft-Oh notation, $\tilde{O}$, to suppress polylogarithmic
factors in the argument, i.e., $\tilde{O}(f)=O(f\cdot \mathrm{poly}(\log f))$.)
Using our rounding algorithms, we also round known $(1-\epsilon)$-decremental
fractional bipartite matching algorithms with no asymptotic overhead, thus
improving on state-of-the-art algorithms for the decremental bipartite matching
problem. Further, we provide extensions of our results to general graphs and to
maintaining almost-maximal matchings.


------------------------------------------------------------------------------

Title:
A System of Monitoring and Analyzing Human Indoor Mobility and Air  Quality

Abstract: Human movements in the workspace usually have non-negligible relations with
air quality parameters (e.g., CO$_2$, PM2.5, and PM10). We establish a system
to monitor indoor human mobility with air quality and assess the
interrelationship between these two types of time series data. More
specifically, a sensor network was designed in indoor environments to observe
air quality parameters continuously. Simultaneously, another sensing module
detected participants' movements around the study areas. In this module, modern
data analysis and machine learning techniques have been applied to reconstruct
the trajectories of participants with relevant sensor information. Finally, a
further study revealed the correlation between human indoor mobility patterns
and indoor air quality parameters. Our experimental results demonstrate that
human movements in different environments can significantly impact air quality
during busy hours. With the results, we propose recommendations for future
studies.


------------------------------------------------------------------------------

Title:
Exploring the Effectiveness of Dataset Synthesis: An application of  Apple Detection in Orchards

Abstract: Deep object detection models have achieved notable successes in recent years,
but one major obstacle remains: the requirement for a large amount of training
data. Obtaining such data is a tedious process and is mainly time consuming,
leading to the exploration of new research avenues like synthetic data
generation techniques. In this study, we explore the usability of Stable
Diffusion 2.1-base for generating synthetic datasets of apple trees for object
detection and compare it to a baseline model trained on real-world data. After
creating a dataset of realistic apple trees with prompt engineering and
utilizing a previously trained Stable Diffusion model, the custom dataset was
annotated and evaluated by training a YOLOv5m object detection model to predict
apples in a real-world apple detection dataset. YOLOv5m was chosen for its
rapid inference time and minimal hardware demands. Results demonstrate that the
model trained on generated data is slightly underperforming compared to a
baseline model trained on real-world images when evaluated on a set of
real-world images. However, these findings remain highly promising, as the
average precision difference is only 0.09 and 0.06, respectively. Qualitative
results indicate that the model can accurately predict the location of apples,
except in cases of heavy shading. These findings illustrate the potential of
synthetic data generation techniques as a viable alternative to the collection
of extensive training data for object detection models.


------------------------------------------------------------------------------

Title:
Agreeing and Disagreeing in Collaborative Knowledge Graph Construction:  An Analysis of Wikidata

Abstract: In this work, we study disagreement in discussions around Wikidata, an online
knowledge community that builds the data backend of Wikipedia. Discussions are
important in collaborative work as they can increase contributor performance
and encourage the emergence of shared norms and practices. While disagreements
can play a productive role in discussions, they can also lead to conflicts and
controversies, which impact contributor well-being and their motivation to
engage. We want to understand if and when such phenomena arise in Wikidata,
using a mix of quantitative and qualitative analyses to identify the types of
topics people disagree about, the most common patterns of interaction, and
roles people play when arguing for or against an issue. We find that decisions
to create Wikidata properties are much faster than those to delete properties
and that more than half of controversial discussions do not lead to consensus.
Our analysis suggests that Wikidata is an inclusive community, considering
different opinions when making decisions, and that conflict and vandalism are
rare in discussions. At the same time, while one-fourth of the editors
participating in controversial discussions contribute with legit and insightful
opinions about Wikidata's emerging issues, they do not remain engaged in the
discussions. We hope our findings will help Wikidata support community decision
making, and improve discussion tools and practices.


------------------------------------------------------------------------------

Title:
CFL Optimized Forward-Backward Runge-Kutta Schemes for the Shallow Water  Equations

Abstract: We present the formulation and optimization of a Runge-Kutta-type
time-stepping scheme for solving the shallow water equations, aimed at
substantially increasing the effective allowable time-step over that of
comparable methods. This scheme, called FB-RK(3,2), uses weighted
forward-backward averaging of thickness data to advance the momentum equation.
The weights for this averaging are chosen with an optimization process that
employs a von Neumann-type analysis, ensuring that the weights maximize the
admittable Courant number. Through a simplified local truncation error analysis
and numerical experiments, we show that the method is at least second order in
time for any choice of weights and exhibits low dispersion and dissipation
errors for well-resolved waves. Further, we show that an optimized FB-RK(3,2)
can take time-steps up to 2.8 times as large as a popular three-stage,
third-order strong stability preserving Runge-Kutta method in a quasi-linear
test case. In fully nonlinear shallow water test cases relevant to oceanic and
atmospheric flows, FB-RK(3,2) outperforms SSPRK3 in admittable time-step by
factors between roughly between 1.6 and 2.2, making the scheme approximately
twice as computationally efficient with little to no effect on solution
quality.


------------------------------------------------------------------------------

Title:
Pre-Pruning and Gradient-Dropping Improve Differentially Private Image  Classification

Abstract: Scalability is a significant challenge when it comes to applying differential
privacy to training deep neural networks. The commonly used DP-SGD algorithm
struggles to maintain a high level of privacy protection while achieving high
accuracy on even moderately sized models. To tackle this challenge, we take
advantage of the fact that neural networks are overparameterized, which allows
us to improve neural network training with differential privacy. Specifically,
we introduce a new training paradigm that uses \textit{pre-pruning} and
\textit{gradient-dropping} to reduce the parameter space and improve
scalability. The process starts with pre-pruning the parameters of the original
network to obtain a smaller model that is then trained with DP-SGD. During
training, less important gradients are dropped, and only selected gradients are
updated. Our training paradigm introduces a tension between the rates of
pre-pruning and gradient-dropping, privacy loss, and classification accuracy.
Too much pre-pruning and gradient-dropping reduces the model's capacity and
worsens accuracy, while training a smaller model requires less privacy budget
for achieving good accuracy. We evaluate the interplay between these factors
and demonstrate the effectiveness of our training paradigm for both training
from scratch and fine-tuning pre-trained networks on several benchmark image
classification datasets. The tools can also be readily incorporated into
existing training paradigms.


------------------------------------------------------------------------------

Title:
A Graphical Modeling Language for Artificial Intelligence Applications  in Automation Systems

Abstract: Artificial Intelligence (AI) applications in automation systems are usually
distributed systems whose development and integration involve several experts.
Each expert uses its own domain-specific modeling language and tools to model
the system elements. An interdisciplinary graphical modeling language that
enables the modeling of an AI application as an overall system comprehensible
to all disciplines does not yet exist. As a result, there is often a lack of
interdisciplinary system understanding, leading to increased development,
integration, and maintenance efforts. This paper therefore presents a graphical
modeling language that enables consistent and understandable modeling of AI
applications in automation systems at system level. This makes it possible to
subdivide individual subareas into domain specific subsystems and thus reduce
the existing efforts.


------------------------------------------------------------------------------

Title:
A C++20 Interface for MPI 4.0

Abstract: We present a modern C++20 interface for MPI 4.0. The interface utilizes
recent language features to ease development of MPI applications. An aggregate
reflection system enables generation of MPI data types from user-defined
classes automatically. Immediate and persistent operations are mapped to
futures, which can be chained to describe sequential asynchronous operations
and task graphs in a concise way. This work introduces the prominent features
of the interface with examples. We further measure its performance overhead
with respect to the raw C interface.


------------------------------------------------------------------------------

Title:
Topological Parallax: A Geometric Specification for Deep Perception  Models

Abstract: For safety and robustness of AI systems, we introduce topological parallax as
a theoretical and computational tool that compares a trained model to a
reference dataset to determine whether they have similar multiscale geometric
structure. Our proofs and examples show that this geometric similarity between
dataset and model is essential to trustworthy interpolation and perturbation,
and we conjecture that this new concept will add value to the current debate
regarding the unclear relationship between overfitting and generalization in
applications of deep-learning. In typical DNN applications, an explicit
geometric description of the model is impossible, but parallax can estimate
topological features (components, cycles, voids, etc.) in the model by
examining the effect on the Rips complex of geodesic distortions using the
reference dataset. Thus, parallax indicates whether the model shares similar
multiscale geometric features with the dataset. Parallax presents theoretically
via topological data analysis [TDA] as a bi-filtered persistence module, and
the key properties of this module are stable under perturbation of the
reference dataset.


------------------------------------------------------------------------------

Title:
Learning to Generate Better Than Your LLM

Abstract: Reinforcement learning (RL) has emerged as a powerful paradigm for
fine-tuning Large Language Models (LLMs) for conditional text generation. In
particular, recent LLMs such as ChatGPT and GPT-4 can engage in fluent
conversations with users by incorporating RL and feedback from humans. Inspired
by learning-to-search algorithms and capitalizing on key properties of text
generation, we seek to investigate reinforcement learning algorithms beyond
general purpose algorithms such as Proximal policy optimization (PPO). In
particular, we extend RL algorithms to allow them to interact with a dynamic
black-box guide LLM such as GPT-3 and propose RL with guided feedback (RLGF), a
suite of RL algorithms for LLM fine-tuning. We experiment on the IMDB positive
review and CommonGen text generation task from the GRUE benchmark. We show that
our RL algorithms achieve higher performance than supervised learning (SL) and
default PPO baselines, demonstrating the benefit of interaction with the guide
LLM. On CommonGen, we not only outperform our SL baselines but also improve
beyond PPO across a variety of lexical and semantic metrics beyond the one we
optimized for. Notably, on the IMDB dataset, we show that our GPT-2 based
policy outperforms the zero-shot GPT-3 oracle, indicating that our algorithms
can learn from a powerful, black-box GPT-3 oracle with a simpler, cheaper, and
publicly available GPT-2 model while gaining performance.


------------------------------------------------------------------------------

Title:
On Identifiability of Conditional Causal Effects

Abstract: We address the problem of identifiability of an arbitrary conditional causal
effect given both the causal graph and a set of any observational and/or
interventional distributions of the form $Q[S]:=P(S|do(V\setminus S))$, where
$V$ denotes the set of all observed variables and $S\subseteq V$. We call this
problem conditional generalized identifiability (c-gID in short) and prove the
completeness of Pearl's $do$-calculus for the c-gID problem by providing sound
and complete algorithm for the c-gID problem. This work revisited the c-gID
problem in Lee et al. [2020], Correa et al. [2021] by adding explicitly the
positivity assumption which is crucial for identifiability. It extends the
results of [Lee et al., 2019, Kivva et al., 2022] on general identifiability
(gID) which studied the problem for unconditional causal effects and Shpitser
and Pearl [2006b] on identifiability of conditional causal effects given merely
the observational distribution $P(\mathbf{V})$ as our algorithm generalizes the
algorithms proposed in [Kivva et al., 2022] and [Shpitser and Pearl, 2006b].


------------------------------------------------------------------------------

Title:
Few-Shot Rotation-Invariant Aerial Image Semantic Segmentation

Abstract: Few-shot aerial image segmentation is a challenging task that involves
precisely parsing objects in query aerial images with limited annotated
support. Conventional matching methods without consideration of varying object
orientations can fail to activate same-category objects with different
orientations. Moreover, conventional algorithms can lead to false recognition
of lower-scored rotated semantic objects. In response to these challenges, the
authors propose a novel few-shot rotation-invariant aerial semantic
segmentation network (FRINet). FRINet matches each query feature
rotation-adaptively with orientation-varying yet category-consistent support
information. The segmentation predictions from different orientations are
supervised by the same label, and the backbones are pre-trained in the base
category to boost segmentation performance. Experimental results demonstrate
that FRINet achieves state-of-the-art performance in few-shot aerial semantic
segmentation benchmark.


------------------------------------------------------------------------------

Title:
DiffuseIR:Diffusion Models For Isotropic Reconstruction of 3D  Microscopic Images

Abstract: Three-dimensional microscopy is often limited by anisotropic spatial
resolution, resulting in lower axial resolution than lateral resolution.
Current State-of-The-Art (SoTA) isotropic reconstruction methods utilizing deep
neural networks can achieve impressive super-resolution performance in fixed
imaging settings. However, their generality in practical use is limited by
degraded performance caused by artifacts and blurring when facing unseen
anisotropic factors. To address these issues, we propose DiffuseIR, an
unsupervised method for isotropic reconstruction based on diffusion models.
First, we pre-train a diffusion model to learn the structural distribution of
biological tissue from lateral microscopic images, resulting in generating
naturally high-resolution images. Then we use low-axial-resolution microscopy
images to condition the generation process of the diffusion model and generate
high-axial-resolution reconstruction results. Since the diffusion model learns
the universal structural distribution of biological tissues, which is
independent of the axial resolution, DiffuseIR can reconstruct authentic images
with unseen low-axial resolutions into a high-axial resolution without
requiring re-training. The proposed DiffuseIR achieves SoTA performance in
experiments on EM data and can even compete with supervised methods.


------------------------------------------------------------------------------

Title:
A survey on deep learning approaches for data integration in autonomous  driving system

Abstract: The perception module of self-driving vehicles relies on a multi-sensor
system to understand its environment. Recent advancements in deep learning have
led to the rapid development of approaches that integrate multi-sensory
measurements to enhance perception capabilities. This paper surveys the latest
deep learning integration techniques applied to the perception module in
autonomous driving systems, categorizing integration approaches based on "what,
how, and when to integrate." A new taxonomy of integration is proposed, based
on three dimensions: multi-view, multi-modality, and multi-frame. The
integration operations and their pros and cons are summarized, providing new
insights into the properties of an "ideal" data integration approach that can
alleviate the limitations of existing methods. After reviewing hundreds of
relevant papers, this survey concludes with a discussion of the key features of
an optimal data integration approach.


------------------------------------------------------------------------------

Title:
Self-supervised Multi-task Learning Framework for Safety and  Health-Oriented Connected Driving Environment Perception using Onboard Camera

Abstract: Cutting-edge connected vehicle (CV) technologies have drawn much attention in
recent years. The real-time traffic data captured by a CV can be shared with
other CVs and data centers so as to open new possibilities for solving diverse
transportation problems. However, imagery captured by onboard cameras in a
connected environment, are not sufficiently investigated, especially for safety
and health-oriented visual perception. In this paper, a bidirectional process
of image synthesis and decomposition (BPISD) approach is proposed, and thus a
novel self-supervised multi-task learning framework, to simultaneously estimate
depth map, atmospheric visibility, airlight, and PM2.5 mass concentration, in
which depth map and visibility are considered highly associated with traffic
safety, while airlight and PM2.5 mass concentration are directly correlated
with human health. Both the training and testing phases of the proposed system
solely require a single image as input. Due to the innovative training
pipeline, the depth estimation network can manage various levels of visibility
conditions and overcome inherent problems in current image-synthesis-based
depth estimation, thereby generating high-quality depth maps even in
low-visibility situations and further benefiting accurate estimations of
visibility, airlight, and PM2.5 mass concentration. Extensive experiments on
the synthesized data from the KITTI and real-world data collected in Beijing
demonstrate that the proposed method can (1) achieve performance competitive in
depth estimation as compared with state-of-the-art methods when taking clear
images as input; (2) predict vivid depth map for images contaminated by various
levels of haze; and (3) accurately estimate visibility, airlight, and PM2.5
mass concentrations. Beneficial applications can be developed based on the
presented work to improve traffic safety, air quality, and public health.


------------------------------------------------------------------------------

Title:
Attention Hybrid Variational Net for Accelerated MRI Reconstruction

Abstract: The application of compressed sensing (CS)-enabled data reconstruction for
accelerating magnetic resonance imaging (MRI) remains a challenging problem.
This is due to the fact that the information lost in k-space from the
acceleration mask makes it difficult to reconstruct an image similar to the
quality of a fully sampled image. Multiple deep learning-based structures have
been proposed for MRI reconstruction using CS, both in the k-space and image
domains as well as using unrolled optimization methods. However, the drawback
of these structures is that they are not fully utilizing the information from
both domains (k-space and image). Herein, we propose a deep learning-based
attention hybrid variational network that performs learning in both the k-space
and image domain. We evaluate our method on a well-known open-source MRI
dataset and a clinical MRI dataset of patients diagnosed with strokes from our
institution to demonstrate the performance of our network. In addition to
quantitative evaluation, we undertook a blinded comparison of image quality
across networks performed by a subspecialty trained radiologist. Overall, we
demonstrate that our network achieves a superior performance among others under
multiple reconstruction tasks.


------------------------------------------------------------------------------

Title:
The Manipulation Problem: Conversational AI as a Threat to Epistemic  Agency

Abstract: The technology of Conversational AI has made significant advancements over
the last eighteen months. As a consequence, conversational agents are likely to
be deployed in the near future that are designed to pursue targeted influence
objectives. Sometimes referred to as the "AI Manipulation Problem," the
emerging risk is that consumers will unwittingly engage in real-time dialog
with predatory AI agents that can skillfully persuade them to buy particular
products, believe particular pieces of misinformation, or fool them into
revealing sensitive personal data. For many users, current systems like ChatGPT
and LaMDA feel safe because they are primarily text-based, but the industry is
already shifting towards real-time voice and photorealistic digital personas
that look, move, and express like real people. This will enable the deployment
of agenda-driven Virtual Spokespeople (VSPs) that will be highly persuasive
through real-time adaptive influence. This paper explores the manipulative
tactics that are likely to be deployed through conversational AI agents, the
unique threats such agents pose to the epistemic agency of human users, and the
emerging need for policymakers to protect against the most likely predatory
practices.


------------------------------------------------------------------------------

Title:
GEO-Nav: a geometric dataset of voltage-gated sodium channels

Abstract: Voltage-gated sodium (Nav) channels constitute a prime target for drug design
and discovery, given their implication in various diseases such as epilepsy,
migraine and ataxia to name a few. In this regard, performing morphological
analysis is a crucial step in comprehensively understanding their biological
function and mechanism, as well as in uncovering subtle details of their
mechanism that may be elusive to experimental observations. Despite their
tremendous therapeutic potential, drug design resources are deficient,
particularly in terms of accurate and comprehensive geometric information. This
paper presents a geometric dataset of molecular surfaces that are
representative of Nav channels in mammals. For each structure we provide three
representations and a number of geometric measures, including length, volume
and straightness of the recognized channels. To demonstrate the effective use
of GEO-Nav, we have tested it on two methods belonging to two different
categories of approaches: a sphere-based and a tessellation-based method.


------------------------------------------------------------------------------

Title:
Neural Shape Diameter Function for Efficient Mesh Segmentation

Abstract: Partitioning a polygonal mesh into meaningful parts can be challenging. Many
applications require decomposing such structures for further processing in
computer graphics. In the last decade, several methods were proposed to tackle
this problem, at the cost of intensive computational times. Recently, machine
learning has proven to be effective for the segmentation task on 3D structures.
Nevertheless, these state-of-the-art methods are often hardly generalizable and
require dividing the learned model into several specific classes of objects to
avoid overfitting. We present a data-driven approach leveraging deep learning
to encode a mapping function prior to mesh segmentation for multiple
applications. Our network reproduces a neighborhood map using our knowledge of
the \textsl{Shape Diameter Function} (SDF) method using similarities among
vertex neighborhoods. Our approach is resolution-agnostic as we downsample the
input meshes and query the full-resolution structure solely for neighborhood
contributions. Using our predicted SDF values, we can inject the resulting
structure into a graph-cut algorithm to generate an efficient and robust mesh
segmentation while considerably reducing the required computation times.


------------------------------------------------------------------------------

Title:
Extended NYUSIM-based MmWave Channel Model and Simulator for  RIS-Assisted Systems

Abstract: Spectrum scarcity has motivated the exploration of the millimeter-wave
(mmWave) band as a key technology to cope with the ever-increasing data
traffic. However, in this band, radiofrequency waves are highly susceptible to
transmission loss and blockage. Recently, reconfigurable intelligent surfaces
(RIS) have been proposed to transform the random nature of the propagation
channel into a programmable and controllable radio environment. This innovative
technique can improve mmWave coverage. However, most works consider theoretical
channel models. In order to fill the gap towards a realistic RIS channel
simulator, we extend the 3D statistical channel simulator NYUSIM based on
extensive measurements to help model RIS-assisted mmWave systems. We validate
the extended simulator analytically and via simulations. In addition, we study
the received power in different configurations. Finally, we highlight the
effectiveness of using RIS when the direct link is partially blocked or
non-existent.


------------------------------------------------------------------------------

Title:
NeRF synthesis with shading guidance

Abstract: The emerging Neural Radiance Field (NeRF) shows great potential in
representing 3D scenes, which can render photo-realistic images from novel view
with only sparse views given. However, utilizing NeRF to reconstruct real-world
scenes requires images from different viewpoints, which limits its practical
application. This problem can be even more pronounced for large scenes. In this
paper, we introduce a new task called NeRF synthesis that utilizes the
structural content of a NeRF patch exemplar to construct a new radiance field
of large size. We propose a two-phase method for synthesizing new scenes that
are continuous in geometry and appearance. We also propose a boundary
constraint method to synthesize scenes of arbitrary size without artifacts.
Specifically, we control the lighting effects of synthesized scenes using
shading guidance instead of decoupling the scene. We have demonstrated that our
method can generate high-quality results with consistent geometry and
appearance, even for scenes with complex lighting. We can also synthesize new
scenes on curved surface with arbitrary lighting effects, which enhances the
practicality of our proposed NeRF synthesis approach.


------------------------------------------------------------------------------

Title:
ARA-residual power series method for solving partial fractional  differential equations

Abstract: In this article a new approach in solving time fractional partial
differential equations is introduced, that is, the ARA-residual power series
method. The main idea of this technique, depends on applying the ARA-transform
and using Taylor's expansion to construct approximate series solutions. The
procedure of getting the approximate solutions for nonlinear time fractional
partial differential equations is a difficult mission, the ARA-residual power
series method over comes this trouble throughout expressing the solution in a
series form then obtain the series coefficients using the idea of the residual
function and the concept of the limit at infinity. This method is efficient and
applicable to solve a wide family of time fractional partial differential
equations. Four attractive applications are considered to show the speed and
the strength of the proposed method in constructing solitary series solutions
of the target equations.


------------------------------------------------------------------------------

Title:
Concurrent ischemic lesion age estimation and segmentation of CT brain  using a Transformer-based network

Abstract: The cornerstone of stroke care is expedient management that varies depending
on the time since stroke onset. Consequently, clinical decision making is
centered on accurate knowledge of timing and often requires a radiologist to
interpret Computed Tomography (CT) of the brain to confirm the occurrence and
age of an event. These tasks are particularly challenging due to the subtle
expression of acute ischemic lesions and the dynamic nature of their
appearance. Automation efforts have not yet applied deep learning to estimate
lesion age and treated these two tasks independently, so, have overlooked their
inherent complementary relationship. To leverage this, we propose a novel
end-to-end multi-task transformer-based network optimized for concurrent
segmentation and age estimation of cerebral ischemic lesions. By utilizing
gated positional self-attention and CT-specific data augmentation, the proposed
method can capture long-range spatial dependencies while maintaining its
ability to be trained from scratch under low-data regimes commonly found in
medical imaging. Furthermore, to better combine multiple predictions, we
incorporate uncertainty by utilizing quantile loss to facilitate estimating a
probability density function of lesion age. The effectiveness of our model is
then extensively evaluated on a clinical dataset consisting of 776 CT images
from two medical centers. Experimental results demonstrate that our method
obtains promising performance, with an area under the curve (AUC) of 0.933 for
classifying lesion ages <=4.5 hours compared to 0.858 using a conventional
approach, and outperforms task-specific state-of-the-art algorithms.


------------------------------------------------------------------------------

Title:
On Compositionality and Improved Training of NADO

Abstract: NeurAlly-Decomposed Oracle (NADO) is a powerful approach for controllable
generation with large language models. Differentiating from finetuning/prompt
tuning, it has the potential to avoid catastrophic forgetting of the large base
model and achieve guaranteed convergence to an entropy-maximized closed-form
solution without significantly limiting the model capacity. Despite its
success, several challenges arise when applying NADO to more complex scenarios.
First, the best practice of using NADO for the composition of multiple control
signals is under-explored. Second, vanilla NADO suffers from gradient vanishing
for low-probability control signals and is highly reliant on the
forward-consistency regularization. In this paper, we study the aforementioned
challenges when using NADO theoretically and empirically. We show we can
achieve guaranteed compositional generalization of NADO with a certain
practice, and propose a novel alternative parameterization of NADO to perfectly
guarantee the forward-consistency. We evaluate the improved training of NADO,
i.e. NADO++, on CommonGen. Results show that NADO++ improves the effectiveness
of the algorithm in multiple aspects.


------------------------------------------------------------------------------

Title:
Deep Learning Accelerator in Loop Reliability Evaluation for Autonomous  Driving

Abstract: The reliability of deep learning accelerators (DLAs) used in autonomous
driving systems has significant impact on the system safety. However, the DLA
reliability is usually evaluated with low-level metrics like mean square errors
of the output which remains rather different from the high-level metrics like
total distance traveled before failure in autonomous driving. As a result, the
high-level reliability metrics evaluated at the post-silicon stage may still
lead to DLA design revision and result in expensive reliable DLA design
iterations targeting at autonomous driving. To address the problem, we proposed
a DLA-in-loop reliability evaluation platform to enable system reliability
evaluation at the early DLA design stage.


------------------------------------------------------------------------------

Title:
Nearly Optimal Committee Selection For Bias Minimization

Abstract: We study the model of metric voting proposed by Feldman et al. [2020]. In
this model, experts and candidates are located in a metric space, and each
candidate possesses a quality that is independent of her location. An expert
evaluates each candidate as the candidate's quality less a bias term--the
distance between the candidate and the expert in the metric space. The expert
then votes for her favorite candidate. The goal is to select a voting rule and
a committee of experts to mitigate the bias. More specifically, given $m$
candidates, what is the minimum number of experts needed to ensure that the
voting rule selects a candidate whose quality is at most $\varepsilon$ worse
than the best one?
Our first main result is a new way to select the committee using
exponentially less experts compared to the method proposed in Feldman et al.
[2020]. Our second main result is a novel construction that substantially
improves the lower bound on the committee size. Indeed, our upper and lower
bounds match in terms of $m$, the number of candidates, and $\varepsilon$, the
desired accuracy, for general convex normed spaces, and differ by a
multiplicative factor that only depends on the dimension of the underlying
normed space but is independent of other parameters of the problem. We extend
the nearly matching upper and lower bounds to the setting in which each expert
returns a ranking of her top $k$ candidates and we wish to choose $\ell$
candidates with cumulative quality at most $\varepsilon$ worse than that of the
best set of $\ell$ candidates, settling an open problem of Feldman et al.
[2020]. Finally, we consider the setting where there are multiple rounds of
voting. We show that by introducing another round of voting, the number of
experts needed to guarantee the selection of an $\varepsilon$-optimal candidate
becomes independent of the number of candidates.


------------------------------------------------------------------------------

Title:
Quantum soft-covering lemma with applications to rate-distortion coding,  resolvability and identification via quantum channels

Abstract: We propose a quantum soft-covering problem for a given general quantum
channel and one of its output states, which consists in finding the minimum
rank of an input state needed to approximate the given channel output. We then
prove a one-shot quantum covering lemma in terms of smooth min-entropies by
leveraging decoupling techniques from quantum Shannon theory. This covering
result is shown to be equivalent to a coding theorem for rate distortion under
a posterior (reverse) channel distortion criterion [Atif, Sohail, Pradhan,
arXiv:2302.00625]. Both one-shot results directly yield corollaries about the
i.i.d. asymptotics, in terms of the coherent information of the channel.
The power of our quantum covering lemma is demonstrated by two additional
applications: first, we formulate a quantum channel resolvability problem, and
provide one-shot as well as asymptotic upper and lower bounds. Secondly, we
provide new upper bounds on the unrestricted and simultaneous identification
capacities of quantum channels, in particular separating for the first time the
simultaneous identification capacity from the unrestricted one, proving a
long-standing conjecture of the last author.


------------------------------------------------------------------------------

Title:
Lumbar spine segmentation in MR images: a dataset and a public benchmark

Abstract: This paper presents a large publicly available multi-center lumbar spine
magnetic resonance imaging (MRI) dataset with reference segmentations of
vertebrae, intervertebral discs (IVDs), and spinal canal. The dataset includes
447 sagittal T1 and T2 MRI series from 218 patients with a history of low back
pain. It was collected from four different hospitals and was divided into a
training (179 patients) and validation (39 patients) set. An iterative data
annotation approach was used by training a segmentation algorithm on a small
part of the dataset, enabling semi-automatic segmentation of the remaining
images. The algorithm provided an initial segmentation, which was subsequently
reviewed, manually corrected, and added to the training data. We provide
reference performance values for this baseline algorithm and nnU-Net, which
performed comparably. We set up a continuous segmentation challenge to allow
for a fair comparison of different segmentation algorithms. This study may
encourage wider collaboration in the field of spine segmentation, and improve
the diagnostic value of lumbar spine MRI.


------------------------------------------------------------------------------

Title:
Visual-Aware Text-to-Speech

Abstract: Dynamically synthesizing talking speech that actively responds to a listening
head is critical during the face-to-face interaction. For example, the speaker
could take advantage of the listener's facial expression to adjust the tones,
stressed syllables, or pauses. In this work, we present a new visual-aware
text-to-speech (VA-TTS) task to synthesize speech conditioned on both textual
inputs and sequential visual feedback (e.g., nod, smile) of the listener in
face-to-face communication. Different from traditional text-to-speech, VA-TTS
highlights the impact of visual modality. On this newly-minted task, we devise
a baseline model to fuse phoneme linguistic information and listener visual
signals for speech synthesis. Extensive experiments on multimodal conversation
dataset ViCo-X verify our proposal for generating more natural audio with
scenario-appropriate rhythm and prosody.


------------------------------------------------------------------------------

Title:
Protein Discovery with Discrete Walk-Jump Sampling

Abstract: We resolve difficulties in training and sampling from a discrete generative
model by learning a smoothed energy function, sampling from the smoothed data
manifold with Langevin Markov chain Monte Carlo (MCMC), and projecting back to
the true data manifold with one-step denoising. Our Discrete Walk-Jump Sampling
formalism combines the maximum likelihood training of an energy-based model and
improved sample quality of a score-based model, while simplifying training and
sampling by requiring only a single noise level. We evaluate the robustness of
our approach on generative modeling of antibody proteins and introduce the
distributional conformity score to benchmark protein generative models. By
optimizing and sampling from our models for the proposed distributional
conformity score, 97-100% of generated samples are successfully expressed and
purified and 35% of functional designs show equal or improved binding affinity
compared to known functional antibodies on the first attempt in a single round
of laboratory experiments. We also report the first demonstration of long-run
fast-mixing MCMC chains where diverse antibody protein classes are visited in a
single MCMC chain.


------------------------------------------------------------------------------

Title:
Diffusion Posterior Sampling for Informed Single-Channel Dereverberation

Abstract: We present in this paper an informed single-channel dereverberation method
based on conditional generation with diffusion models. With knowledge of the
room impulse response, the anechoic utterance is generated via reverse
diffusion using a measurement consistency criterion coupled with a neural
network that represents the clean speech prior. The proposed approach is
largely more robust to measurement noise compared to a state-of-the-art
informed single-channel dereverberation method, especially for non-stationary
noise. Furthermore, we compare to other blind dereverberation methods using
diffusion models and show superiority of the proposed approach for large
reverberation times. We motivate the presented algorithm by introducing an
extension for blind dereverberation allowing joint estimation of the room
impulse response and anechoic speech. Audio samples and code can be found
online (this https URL).


------------------------------------------------------------------------------

Title:
More PAC-Bayes bounds: From bounded losses, to losses with general tail  behaviors, to anytime-validity

Abstract: In this paper, we present new high-probability PAC-Bayes bounds for different
types of losses. Firstly, for losses with a bounded range, we present a
strengthened version of Catoni's bound that holds uniformly for all parameter
values. This leads to new fast rate and mixed rate bounds that are
interpretable and tighter than previous bounds in the literature. Secondly, for
losses with more general tail behaviors, we introduce two new parameter-free
bounds: a PAC-Bayes Chernoff analogue when the loss' cumulative generating
function is bounded, and a bound when the loss' second moment is bounded. These
two bounds are obtained using a new technique based on a discretization of the
space of possible events for the "in probability" parameter optimization
problem. Finally, we extend all previous results to anytime-valid bounds using
a simple technique applicable to any existing bound.


------------------------------------------------------------------------------

Title:
Focusing on Relevant Responses for Multi-modal Rumor Detection

Abstract: In the absence of an authoritative statement about a rumor, people may expose
the truth behind such rumor through their responses on social media. Most rumor
detection methods aggregate the information of all the responses and have made
great progress. However, due to the different backgrounds of users, the
responses have different relevance for discovering th suspicious points hidden
in a rumor claim. The methods that focus on all the responding tweets would
dilute the effect of the critical ones. Moreover, for a multi-modal rumor
claim, the focus of a user may be on several words in the text or an object in
the image, so the different modalities should be considered to select the
relevant responses and verify the claim. In this paper, we propose a novel
multi-modal rumor detection model, termed Focal Reasoning Model (FoRM), to
filter out the irrelevant responses and further conduct fine-grained reasoning
with the multi-modal claim and corresponding responses. Concretely, there are
two main components in our FoRM: the coarse-grained selection and the
fine-grained reasoning. The coarse-grained selection component leverages the
post-level features of the responses to verify the claim and learns a relevant
score of each response. Based on the relevant scores, the most relevant
responses are reserved as the critical ones to the further reasoning. In the
fine-grained reasoning component, we design a relation attention module to
explore the fine-grained relations, i.e., token-to-token and token-to-object
relations, between the reserved responses and the multi-modal claim for finding
out the valuable clues. Extensive experiments have been conducted on two
real-world datasets, and the results demonstrate that our proposed model
outperforms all the baselines.


------------------------------------------------------------------------------

Title:
Optimal Algorithms for Stochastic Bilevel Optimization under Relaxed  Smoothness Conditions

Abstract: Stochastic Bilevel optimization usually involves minimizing an upper-level
(UL) function that is dependent on the arg-min of a strongly-convex lower-level
(LL) function. Several algorithms utilize Neumann series to approximate certain
matrix inverses involved in estimating the implicit gradient of the UL function
(hypergradient). The state-of-the-art StOchastic Bilevel Algorithm (SOBA) [16]
instead uses stochastic gradient descent steps to solve the linear system
associated with the explicit matrix inversion. This modification enables SOBA
to match the lower bound of sample complexity for the single-level counterpart
in non-convex settings. Unfortunately, the current analysis of SOBA relies on
the assumption of higher-order smoothness for the UL and LL functions to
achieve optimality. In this paper, we introduce a novel fully single-loop and
Hessian-inversion-free algorithmic framework for stochastic bilevel
optimization and present a tighter analysis under standard smoothness
assumptions (first-order Lipschitzness of the UL function and second-order
Lipschitzness of the LL function). Furthermore, we show that by a slight
modification of our approach, our algorithm can handle a more general
multi-objective robust bilevel optimization problem. For this case, we obtain
the state-of-the-art oracle complexity results demonstrating the generality of
both the proposed algorithmic and analytic frameworks. Numerical experiments
demonstrate the performance gain of the proposed algorithms over existing ones.


------------------------------------------------------------------------------

Title:
Multi-view 3D Object Reconstruction and Uncertainty Modelling with  Neural Shape Prior

Abstract: 3D object reconstruction is important for semantic scene understanding. It is
challenging to reconstruct detailed 3D shapes from monocular images directly
due to a lack of depth information, occlusion and noise. Most current methods
generate deterministic object models without any awareness of the uncertainty
of the reconstruction. We tackle this problem by leveraging a neural object
representation which learns an object shape distribution from large dataset of
3d object models and maps it into a latent space. We propose a method to model
uncertainty as part of the representation and define an uncertainty-aware
encoder which generates latent codes with uncertainty directly from individual
input images. Further, we propose a method to propagate the uncertainty in the
latent code to SDF values and generate a 3d object mesh with local uncertainty
for each mesh component. Finally, we propose an incremental fusion method under
a Bayesian framework to fuse the latent codes from multi-view observations. We
evaluate the system in both synthetic and real datasets to demonstrate the
effectiveness of uncertainty-based fusion to improve 3D object reconstruction
accuracy.


------------------------------------------------------------------------------

Title:
Towards a Definition of Complex Software System

Abstract: Complex Systems were identified and studied in different fields, such as
physics, biology, and economics. These systems exhibit exciting properties such
as self-organization, robust order, and emergence. In recent years, software
systems displaying behaviors associated with Complex Systems are starting to
appear, and these behaviors are showing previously unknown potential (e.g.,
GPT-based applications). Yet, there is no commonly shared definition of a
Complex Software System that can serve as a key reference for academia to
support research in the area. In this paper, we adopt the theory-to-research
strategy to extract properties of Complex Systems from research in other
fields, mapping them to software systems to create a formal definition of a
Complex Software System. We support the evolution of the properties through
future validation, and we provide examples of the application of the
definition. Overall, the definition will allow for a more precise, consistent,
and rigorous frame of reference for conducting scientific research on software
systems.


------------------------------------------------------------------------------

Title:
M-VAAL: Multimodal Variational Adversarial Active Learning for  Downstream Medical Image Analysis Tasks

Abstract: Acquiring properly annotated data is expensive in the medical field as it
requires experts, time-consuming protocols, and rigorous validation. Active
learning attempts to minimize the need for large annotated samples by actively
sampling the most informative examples for annotation. These examples
contribute significantly to improving the performance of supervised machine
learning models, and thus, active learning can play an essential role in
selecting the most appropriate information in deep learning-based diagnosis,
clinical assessments, and treatment planning. Although some existing works have
proposed methods for sampling the best examples for annotation in medical image
analysis, they are not task-agnostic and do not use multimodal auxiliary
information in the sampler, which has the potential to increase robustness.
Therefore, in this work, we propose a Multimodal Variational Adversarial Active
Learning (M-VAAL) method that uses auxiliary information from additional
modalities to enhance the active sampling. We applied our method to two
datasets: i) brain tumor segmentation and multi-label classification using the
BraTS2018 dataset, and ii) chest X-ray image classification using the
COVID-QU-Ex dataset. Our results show a promising direction toward
data-efficient learning under limited annotations.


------------------------------------------------------------------------------

Title:
Explaining human body responses in random vibration: Effect of motion  direction, sitting posture, and anthropometry

Abstract: This study investigates the effects of anthropometric attributes, biological
sex, and posture on translational body kinematic responses in translational
vibrations. In total, 35 participants were recruited. Perturbations were
applied on a standard car seat using a motion-based platform with 0.1 to 12.0
Hz random noise signals, with 0.3 m/s2 rms acceleration, for 60 seconds.
Multiple linear regression models (three basic models and one advanced model,
including interactions between predictors) were created to determine the most
influential predictors of peak translational gains in the frequency domain per
body segment (pelvis, trunk, and head). The models introduced experimentally
manipulated factors (motion direction, posture, measured anthropometric
attributes, and biological sex) as predictors. Effects of included predictors
on the model fit were estimated. Basic linear regression models could explain
over 70% of peak body segments' kinematic body response (where the R2 adjusted
was 0.728). The inclusion of additional predictors (posture, body height and
weight, and biological sex) did enhance the model fit, but not significantly
(R2 adjusted was 0.730). The multiple stepwise linear regression, including
interactions between predictors, accounted for the data well with an adjusted
R2 of 0.907. The present study shows that perturbation direction and body
segment kinematics are crucial factors influencing peak translational gains.
Besides the body segments' response, perturbation direction was the strongest
predictor. Adopted postures and biological sex do not significantly affect
kinematic responses.


------------------------------------------------------------------------------

Title:
Recent Advances in Direct Speech-to-text Translation

Abstract: Recently, speech-to-text translation has attracted more and more attention
and many studies have emerged rapidly. In this paper, we present a
comprehensive survey on direct speech translation aiming to summarize the
current state-of-the-art techniques. First, we categorize the existing research
work into three directions based on the main challenges -- modeling burden,
data scarcity, and application issues. To tackle the problem of modeling
burden, two main structures have been proposed, encoder-decoder framework
(Transformer and the variants) and multitask frameworks. For the challenge of
data scarcity, recent work resorts to many sophisticated techniques, such as
data augmentation, pre-training, knowledge distillation, and multilingual
modeling. We analyze and summarize the application issues, which include
real-time, segmentation, named entity, gender bias, and code-switching.
Finally, we discuss some promising directions for future work.


------------------------------------------------------------------------------

Title:
SkyGPT: Probabilistic Short-term Solar Forecasting Using Synthetic Sky  Videos from Physics-constrained VideoGPT

Abstract: In recent years, deep learning-based solar forecasting using all-sky images
has emerged as a promising approach for alleviating uncertainty in PV power
generation. However, the stochastic nature of cloud movement remains a major
challenge for accurate and reliable solar forecasting. With the recent advances
in generative artificial intelligence, the synthesis of visually plausible yet
diversified sky videos has potential for aiding in forecasts. In this study, we
introduce \emph{SkyGPT}, a physics-informed stochastic video prediction model
that is able to generate multiple possible future images of the sky with
diverse cloud motion patterns, by using past sky image sequences as input.
Extensive experiments and comparison with benchmark video prediction models
demonstrate the effectiveness of the proposed model in capturing cloud dynamics
and generating future sky images with high realism and diversity. Furthermore,
we feed the generated future sky images from the video prediction models for
15-minute-ahead probabilistic solar forecasting for a 30-kW roof-top PV system,
and compare it with an end-to-end deep learning baseline model SUNSET and a
smart persistence model. Better PV output prediction reliability and sharpness
is observed by using the predicted sky images generated with SkyGPT compared
with other benchmark models, achieving a continuous ranked probability score
(CRPS) of 2.81 (13\% better than SUNSET and 23\% better than smart persistence)
and a Winkler score of 26.70 for the test set. Although an arbitrary number of
futures can be generated from a historical sky image sequence, the results
suggest that 10 future scenarios is a good choice that balances probabilistic
solar forecasting performance and computational cost.


------------------------------------------------------------------------------

Title:
Universal adversarial perturbations for multiple classification tasks  with quantum classifiers

Abstract: Quantum adversarial machine learning is an emerging field that studies the
vulnerability of quantum learning systems against adversarial perturbations and
develops possible defense strategies. Quantum universal adversarial
perturbations are small perturbations, which can make different input samples
into adversarial examples that may deceive a given quantum classifier. This is
a field that was rarely looked into but worthwhile investigating because
universal perturbations might simplify malicious attacks to a large extent,
causing unexpected devastation to quantum machine learning models. In this
paper, we take a step forward and explore the quantum universal perturbations
in the context of heterogeneous classification tasks. In particular, we find
that quantum classifiers that achieve almost state-of-the-art accuracy on two
different classification tasks can be both conclusively deceived by one
carefully-crafted universal perturbation. This result is explicitly
demonstrated with well-designed quantum continual learning models with elastic
weight consolidation method to avoid catastrophic forgetting, as well as
real-life heterogeneous datasets from hand-written digits and medical MRI
images. Our results provide a simple and efficient way to generate universal
perturbations on heterogeneous classification tasks and thus would provide
valuable guidance for future quantum learning technologies.


------------------------------------------------------------------------------

Title:
DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT  Models

Abstract: Generative Pre-trained Transformer (GPT) models have exhibited exciting
progress in capabilities, capturing the interest of practitioners and the
public alike. Yet, while the literature on the trustworthiness of GPT models
remains limited, practitioners have proposed employing capable GPT models for
sensitive applications to healthcare and finance - where mistakes can be
costly. To this end, this work proposes a comprehensive trustworthiness
evaluation for large language models with a focus on GPT-4 and GPT-3.5,
considering diverse perspectives - including toxicity, stereotype bias,
adversarial robustness, out-of-distribution robustness, robustness on
adversarial demonstrations, privacy, machine ethics, and fairness. Based on our
evaluations, we discover previously unpublished vulnerabilities to
trustworthiness threats. For instance, we find that GPT models can be easily
misled to generate toxic and biased outputs and leak private information in
both training data and conversation history. We also find that although GPT-4
is usually more trustworthy than GPT-3.5 on standard benchmarks, GPT-4 is more
vulnerable given jailbreaking system or user prompts, potentially due to the
reason that GPT-4 follows the (misleading) instructions more precisely. Our
work illustrates a comprehensive trustworthiness evaluation of GPT models and
sheds light on the trustworthiness gaps. Our benchmark is publicly available at
this https URL


------------------------------------------------------------------------------

Title:
Entropic characterization of optimal rates for learning Gaussian  mixtures

Abstract: We consider the question of estimating multi-dimensional Gaussian mixtures
(GM) with compactly supported or subgaussian mixing distributions. Minimax
estimation rate for this class (under Hellinger, TV and KL divergences) is a
long-standing open question, even in one dimension. In this paper we
characterize this rate (for all constant dimensions) in terms of the metric
entropy of the class. Such characterizations originate from seminal works of Le
Cam (1973); Birge (1983); Haussler and Opper (1997); Yang and Barron (1999).
However, for GMs a key ingredient missing from earlier work (and widely
sought-after) is a comparison result showing that the KL and the squared
Hellinger distance are within a constant multiple of each other uniformly over
the class. Our main technical contribution is in showing this fact, from which
we derive entropy characterization for estimation rate under Hellinger and KL.
Interestingly, the sequential (online learning) estimation rate is
characterized by the global entropy, while the single-step (batch) rate
corresponds to local entropy, paralleling a similar result for the Gaussian
sequence model recently discovered by Neykov (2022) and Mourtada (2023).
Additionally, since Hellinger is a proper metric, our comparison shows that GMs
under KL satisfy the triangle inequality within multiplicative constants,
implying that proper and improper estimation rates coincide.


------------------------------------------------------------------------------

Title:
Fundamental Performance Bounds for Carrier Phase Positioning in Cellular  Networks

Abstract: The carrier phase of cellular signals can be utilized for highly accurate
positioning, with the potential for orders-of-magnitude performance
improvements compared to standard time-difference-of-arrival positioning. Due
to the integer ambiguities, standard performance evaluation tools such as the
Cram\'er-Rao bound (CRB) are overly optimistic. In this paper, a new
performance bound, called the mixed-integer CRB (MICRB) is introduced that
explicitly accounts for this integer ambiguity. While computationally more
complex than the standard CRB, the MICRB can accurately predict positioning
performance, as verified by numerical simulations.


------------------------------------------------------------------------------

Title:
Optical Coherence Tomography Image Enhancement via Block Hankelization  and Low Rank Tensor Network Approximation

Abstract: In this paper, the problem of image super-resolution for Optical Coherence
Tomography (OCT) has been addressed. Due to the motion artifacts, OCT imaging
is usually done with a low sampling rate and the resulting images are often
noisy and have low resolution. Therefore, reconstruction of high resolution OCT
images from the low resolution versions is an essential step for better OCT
based diagnosis. In this paper, we propose a novel OCT super-resolution
technique using Tensor Ring decomposition in the embedded space. A new
tensorization method based on a block Hankelization approach with overlapped
patches, called overlapped patch Hankelization, has been proposed which allows
us to employ Tensor Ring decomposition. The Hankelization method enables us to
better exploit the inter connection of pixels and consequently achieve better
super-resolution of images. The low resolution image was first patch Hankelized
and then its Tensor Ring decomposition with rank incremental has been computed.
Simulation results confirm that the proposed approach is effective in OCT
super-resolution.


------------------------------------------------------------------------------

Title:
Accelerating Generalized Random Forests with Fixed-Point Trees

Abstract: Generalized random forests arXiv:1610.01271 build upon the well-established
success of conventional forests (Breiman, 2001) to offer a flexible and
powerful non-parametric method for estimating local solutions of heterogeneous
estimating equations. Estimators are constructed by leveraging random forests
as an adaptive kernel weighting algorithm and implemented through a
gradient-based tree-growing procedure. By expressing this gradient-based
approximation as being induced from a single Newton-Raphson root-finding
iteration, and drawing upon the connection between estimating equations and
fixed-point problems arXiv:2110.11074, we propose a new tree-growing rule for
generalized random forests induced from a fixed-point iteration type of
approximation, enabling gradient-free optimization, and yielding substantial
time savings for tasks involving even modest dimensionality of the target
quantity (e.g. multiple/multi-level treatment effects). We develop an
asymptotic theory for estimators obtained from forests whose trees are grown
through the fixed-point splitting rule, and provide numerical simulations
demonstrating that the estimators obtained from such forests are comparable to
those obtained from the more costly gradient-based rule.


------------------------------------------------------------------------------

Title:
Combining multi-spectral data with statistical and deep-learning models  for improved exoplanet detection in direct imaging at high contrast

Abstract: Exoplanet detection by direct imaging is a difficult task: the faint signals
from the objects of interest are buried under a spatially structured nuisance
component induced by the host star. The exoplanet signals can only be
identified when combining several observations with dedicated detection
algorithms. In contrast to most of existing methods, we propose to learn a
model of the spatial, temporal and spectral characteristics of the nuisance,
directly from the observations. In a pre-processing step, a statistical model
of their correlations is built locally, and the data are centered and whitened
to improve both their stationarity and signal-to-noise ratio (SNR). A
convolutional neural network (CNN) is then trained in a supervised fashion to
detect the residual signature of synthetic sources in the pre-processed images.
Our method leads to a better trade-off between precision and recall than
standard approaches in the field. It also outperforms a state-of-the-art
algorithm based solely on a statistical framework. Besides, the exploitation of
the spectral diversity improves the performance compared to a similar model
built solely from spatio-temporal data.


------------------------------------------------------------------------------

Title:
Open Problem: Learning with Variational Objectives on Measures

Abstract: The theory of statistical learning has focused on variational objectives
expressed on functions. In this note, we discuss motivations to write similar
objectives on measures, in particular to discuss out-of-distribution
generalization and weakly-supervised learning. It raises a natural question:
can one cast usual statistical learning results to objectives expressed on
measures? Does the resulting construction lead to new algorithms of practical
interest?


------------------------------------------------------------------------------

Title:
A Systematic Survey in Geometric Deep Learning for Structure-based Drug  Design

Abstract: Structure-based drug design (SBDD), which utilizes the three-dimensional
geometry of proteins to identify potential drug candidates, is becoming
increasingly vital in drug discovery. However, traditional methods based on
physiochemical modeling and experts' domain knowledge are time-consuming and
laborious. The recent advancements in geometric deep learning, which integrates
and processes 3D geometric data, coupled with the availability of accurate
protein 3D structure predictions from tools like AlphaFold, have significantly
propelled progress in structure-based drug design. In this paper, we
systematically review the recent progress of geometric deep learning for
structure-based drug design. We start with a brief discussion of the mainstream
tasks in structure-based drug design, commonly used 3D protein representations
and representative predictive/generative models. Then we delve into detailed
reviews for each task (binding site prediction, binding pose generation,
\emph{de novo} molecule generation, linker design, and binding affinity
prediction), including the problem setup, representative methods, datasets, and
evaluation metrics. Finally, we conclude this survey with the current
challenges and highlight potential opportunities of geometric deep learning for
structure-based drug design.


------------------------------------------------------------------------------

Title:
Data-Driven but Privacy-Conscious: Pedestrian Dataset De-identification  via Full-Body Person Synthesis

Abstract: The advent of data-driven technology solutions is accompanied by an
increasing concern with data privacy. This is of particular importance for
human-centered image recognition tasks, such as pedestrian detection,
re-identification, and tracking. To highlight the importance of privacy issues
and motivate future research, we motivate and introduce the Pedestrian Dataset
De-Identification (PDI) task. PDI evaluates the degree of de-identification and
downstream task training performance for a given de-identification method. As a
first baseline, we propose IncogniMOT, a two-stage full-body de-identification
pipeline based on image synthesis via generative adversarial networks. The
first stage replaces target pedestrians with synthetic identities. To improve
downstream task performance, we then apply stage two, which blends and adapts
the synthetic image parts into the data. To demonstrate the effectiveness of
IncogniMOT, we generate a fully de-identified version of the MOT17 pedestrian
tracking dataset and analyze its application as training data for pedestrian
re-identification, detection, and tracking models. Furthermore, we show how our
data is able to narrow the synthetic-to-real performance gap in a
privacy-conscious manner.


------------------------------------------------------------------------------

Title:
About some compression algorithms

Abstract: We use neural network algorithms for finding compression methods of images in
the framework of iterated function systems which is a collection of the
transformations of the interval $(0, 1)$ satisfying suitable properties.


------------------------------------------------------------------------------

Title:
Seat pan angle optimization for vehicle ride comfort using finite  element model of human spine

Abstract: Ride comfort of the driver/occupant of a vehicle has been usually analyzed by
multibody biodynamic models of human beings. Accurate modeling of critical
segments of the human body, e.g. the spine requires these models to have a very
high number of segments. The resultant increase in degrees of freedom makes
these models difficult to analyze and not able to provide certain details such
as seat pressure distribution, the effect of cushion shapes, material, etc.
This work presents a finite element based model of a human being seated in a
vehicle in which the spine has been modelled in 3-D. It consists of cervical to
coccyx vertebrae, ligaments, and discs and has been validated against modal
frequencies reported in the literature. It was then subjected to sinusoidal
vertical RMS acceleration of 0.1 g for mimicking road induced vibration. The
dynamic characteristics of the human body were studied in terms of the seat to
head transmissibility and intervertebral disc pressure. The effect of the seat
pan angle on these parameters was studied and it was established that the
optimum angle should lie between 15 and 19 degrees. This work is expected to be
followed up by more simulations of this nature to study other human body
comfort and seat design related parameters leading to optimized seat designs
for various ride conditions.


------------------------------------------------------------------------------

Title:
Multi-Fidelity Active Learning with GFlowNets

Abstract: In the last decades, the capacity to generate large amounts of data in
science and engineering applications has been growing steadily. Meanwhile, the
progress in machine learning has turned it into a suitable tool to process and
utilise the available data. Nonetheless, many relevant scientific and
engineering problems present challenges where current machine learning methods
cannot yet efficiently leverage the available data and resources. For example,
in scientific discovery, we are often faced with the problem of exploring very
large, high-dimensional spaces, where querying a high fidelity, black-box
objective function is very expensive. Progress in machine learning methods that
can efficiently tackle such problems would help accelerate currently crucial
areas such as drug and materials discovery. In this paper, we propose the use
of GFlowNets for multi-fidelity active learning, where multiple approximations
of the black-box function are available at lower fidelity and cost. GFlowNets
are recently proposed methods for amortised probabilistic inference that have
proven efficient for exploring large, high-dimensional spaces and can hence be
practical in the multi-fidelity setting too. Here, we describe our algorithm
for multi-fidelity active learning with GFlowNets and evaluate its performance
in both well-studied synthetic tasks and practically relevant applications of
molecular discovery. Our results show that multi-fidelity active learning with
GFlowNets can efficiently leverage the availability of multiple oracles with
different costs and fidelities to accelerate scientific discovery and
engineering design.


------------------------------------------------------------------------------

Title:
GenPlot: Increasing the Scale and Diversity of Chart Derendering Data

Abstract: Vertical bars, horizontal bars, dot, scatter, and line plots provide a
diverse set of visualizations to represent data. To understand these plots, one
must be able to recognize textual components, locate data points in a plot, and
process diverse visual contexts to extract information. In recent works such as
Pix2Struct, Matcha, and Deplot, OCR-free chart-to-text translation has achieved
state-of-the-art results on visual language tasks. These results outline the
importance of chart-derendering as a pre-training objective, yet existing
datasets provide a fixed set of training examples. In this paper, we propose
GenPlot; a plot generator that can generate billions of additional plots for
chart-derendering using synthetic data.


------------------------------------------------------------------------------

Title:
Diffusion with Forward Models: Solving Stochastic Inverse Problems  Without Direct Supervision

Abstract: Denoising diffusion models are a powerful type of generative models used to
capture complex distributions of real-world signals. However, their
applicability is limited to scenarios where training samples are readily
available, which is not always the case in real-world applications. For
example, in inverse graphics, the goal is to generate samples from a
distribution of 3D scenes that align with a given image, but ground-truth 3D
scenes are unavailable and only 2D images are accessible. To address this
limitation, we propose a novel class of denoising diffusion probabilistic
models that learn to sample from distributions of signals that are never
directly observed. Instead, these signals are measured indirectly through a
known differentiable forward model, which produces partial observations of the
unknown signal. Our approach involves integrating the forward model directly
into the denoising process. This integration effectively connects the
generative modeling of observations with the generative modeling of the
underlying signals, allowing for end-to-end training of a conditional
generative model over signals. During inference, our approach enables sampling
from the distribution of underlying signals that are consistent with a given
partial observation. We demonstrate the effectiveness of our method on three
challenging computer vision tasks. For instance, in the context of inverse
graphics, our model enables direct sampling from the distribution of 3D scenes
that align with a single 2D input image.


------------------------------------------------------------------------------

Title:
Design and simulation of memristor-based neural networks

Abstract: In recent times, neural networks have been gaining increasing importance in
fields such as pattern recognition and computer vision. However, their usage
entails significant energy and hardware costs, limiting the domains in which
this technology can be employed.
In this context, the feasibility of utilizing analog circuits based on
memristors as efficient alternatives in neural network inference is being
considered. Memristors stand out for their configurability and low power
consumption.
To study the feasibility of using these circuits, a physical model has been
adapted to accurately simulate the behavior of commercial memristors from
KNOWM. Using this model, multiple neural networks have been designed and
simulated, yielding highly satisfactory results.


------------------------------------------------------------------------------

Title:
Brain Anatomy Prior Modeling to Forecast Clinical Progression of  Cognitive Impairment with Structural MRI

Abstract: Brain structural MRI has been widely used to assess the future progression of
cognitive impairment (CI). Previous learning-based studies usually suffer from
the issue of small-sized labeled training data, while there exist a huge amount
of structural MRIs in large-scale public databases. Intuitively, brain
anatomical structures derived from these public MRIs (even without
task-specific label information) can be used to boost CI progression trajectory
prediction. However, previous studies seldom take advantage of such brain
anatomy prior. To this end, this paper proposes a brain anatomy prior modeling
(BAPM) framework to forecast the clinical progression of cognitive impairment
with small-sized target MRIs by exploring anatomical brain structures.
Specifically, the BAPM consists of a pretext model and a downstream model, with
a shared brain anatomy-guided encoder to model brain anatomy prior explicitly.
Besides the encoder, the pretext model also contains two decoders for two
auxiliary tasks (i.e., MRI reconstruction and brain tissue segmentation), while
the downstream model relies on a predictor for classification. The brain
anatomy-guided encoder is pre-trained with the pretext model on 9,344 auxiliary
MRIs without diagnostic labels for anatomy prior modeling. With this encoder
frozen, the downstream model is then fine-tuned on limited target MRIs for
prediction. We validate the BAPM on two CI-related studies with T1-weighted
MRIs from 448 subjects. Experimental results suggest the effectiveness of BAPM
in (1) four CI progression prediction tasks, (2) MR image reconstruction, and
(3) brain tissue segmentation, compared with several state-of-the-art methods.


------------------------------------------------------------------------------

Title:
Quantum and classical query complexities for determining connectedness  of matroids

Abstract: Connectivity is a fundamental structural property of matroids, and has been
studied algorithmically over 50 years. In 1974, Cunningham proposed a
deterministic algorithm consuming $O(n^{2})$ queries to the independence oracle
to determine whether a matroid is connected. Since then, no algorithm, not even
a random one, has worked better. To the best of our knowledge, the classical
query complexity lower bound and the quantum complexity for this problem have
not been considered. Thus, in this paper we are devoted to addressing these
issues, and our contributions are threefold as follows: (i) First, we prove
that the randomized query complexity of determining whether a matroid is
connected is $\Omega(n^2)$ and thus the algorithm proposed by Cunningham is
optimal in classical computing. (ii) Second, we present a quantum algorithm
with $O(n^{3/2})$ queries, which exhibits provable quantum speedups over
classical ones. (iii) Third, we prove that any quantum algorithm requires
$\Omega(n)$ queries, which indicates that quantum algorithms can achieve at
most a quadratic speedup over classical ones. Therefore, we have a relatively
comprehensive understanding of the potential of quantum computing in
determining the connectedness of matroids.\


------------------------------------------------------------------------------

Title:
Distributed Random Reshuffling Methods with Improved Convergence

Abstract: This paper proposes two distributed random reshuffling methods, namely
Gradient Tracking with Random Reshuffling (GT-RR) and Exact Diffusion with
Random Reshuffling (ED-RR), to solve the distributed optimization problem over
a connected network, where a set of agents aim to minimize the average of their
local cost functions. Both algorithms invoke random reshuffling (RR) update for
each agent, inherit favorable characteristics of RR for minimizing smooth
nonconvex objective functions, and improve the performance of previous
distributed random reshuffling methods both theoretically and empirically.
Specifically, both GT-RR and ED-RR achieve the convergence rate of
$O(1/[(1-\lambda)^{1/3}m^{1/3}T^{2/3}])$ in driving the (minimum) expected
squared norm of the gradient to zero, where $T$ denotes the number of epochs,
$m$ is the sample size for each agent, and $1-\lambda$ represents the spectral
gap of the mixing matrix. When the objective functions further satisfy the
Polyak-{\L}ojasiewicz (PL) condition, we show GT-RR and ED-RR both achieve
$O(1/[(1-\lambda)mT^2])$ convergence rate in terms of the averaged expected
differences between the agents' function values and the global minimum value.
Notably, both results are comparable to the convergence rates of centralized RR
methods (up to constant factors depending on the network topology) and
outperform those of previous distributed random reshuffling algorithms.
Moreover, we support the theoretical findings with a set of numerical
experiments.


------------------------------------------------------------------------------

Title:
Encoding Enhanced Complex CNN for Accurate and Highly Accelerated MRI

Abstract: Magnetic resonance imaging (MRI) using hyperpolarized noble gases provides a
way to visualize the structure and function of human lung, but the long imaging
time limits its broad research and clinical applications. Deep learning has
demonstrated great potential for accelerating MRI by reconstructing images from
undersampled data. However, most existing deep conventional neural networks
(CNN) directly apply square convolution to k-space data without considering the
inherent properties of k-space sampling, limiting k-space learning efficiency
and image reconstruction quality. In this work, we propose an encoding enhanced
(EN2) complex CNN for highly undersampled pulmonary MRI reconstruction. EN2
employs convolution along either the frequency or phase-encoding direction,
resembling the mechanisms of k-space sampling, to maximize the utilization of
the encoding correlation and integrity within a row or column of k-space. We
also employ complex convolution to learn rich representations from the complex
k-space data. In addition, we develop a feature-strengthened modularized unit
to further boost the reconstruction performance. Experiments demonstrate that
our approach can accurately reconstruct hyperpolarized 129Xe and 1H lung MRI
from 6-fold undersampled k-space data and provide lung function measurements
with minimal biases compared with fully-sampled image. These results
demonstrate the effectiveness of the proposed algorithmic components and
indicate that the proposed approach could be used for accelerated pulmonary MRI
in research and clinical lung disease patient care.


------------------------------------------------------------------------------

Title:
The Psychophysics of Human Three-Dimensional Active Visuospatial  Problem-Solving

Abstract: Our understanding of how visual systems detect, analyze and interpret visual
stimuli has advanced greatly. However, the visual systems of all animals do
much more; they enable visual behaviours. How well the visual system performs
while interacting with the visual environment and how vision is used in the
real world have not been well studied, especially in humans. It has been
suggested that comparison is the most primitive of psychophysical tasks. Thus,
as a probe into these active visual behaviours, we use a same-different task:
are two physical 3D objects visually the same? This task seems to be a
fundamental cognitive ability. We pose this question to human subjects who are
free to move about and examine two real objects in an actual 3D space. Past
work has dealt solely with a 2D static version of this problem. We have
collected detailed, first-of-its-kind data of humans performing a visuospatial
task in hundreds of trials. Strikingly, humans are remarkably good at this task
without any training, with a mean accuracy of 93.82%. No learning effect was
observed on accuracy after many trials, but some effect was seen for response
time, number of fixations and extent of head movement. Subjects demonstrated a
variety of complex strategies involving a range of movement and eye fixation
changes, suggesting that solutions were developed dynamically and tailored to
the specific task.


------------------------------------------------------------------------------

Title:
RoboCat: A Self-Improving Foundation Agent for Robotic Manipulation

Abstract: The ability to leverage heterogeneous robotic experience from different
robots and tasks to quickly master novel skills and embodiments has the
potential to transform robot learning. Inspired by recent advances in
foundation models for vision and language, we propose a foundation agent for
robotic manipulation. This agent, named RoboCat, is a visual goal-conditioned
decision transformer capable of consuming multi-embodiment action-labelled
visual experience. This data spans a large repertoire of motor control skills
from simulated and real robotic arms with varying sets of observations and
actions. With RoboCat, we demonstrate the ability to generalise to new tasks
and robots, both zero-shot as well as through adaptation using only 100--1000
examples for the target task. We also show how a trained model itself can be
used to generate data for subsequent training iterations, thus providing a
basic building block for an autonomous improvement loop. We investigate the
agent's capabilities, with large-scale evaluations both in simulation and on
three different real robot embodiments. We find that as we grow and diversify
its training data, RoboCat not only shows signs of cross-task transfer, but
also becomes more efficient at adapting to new tasks.


------------------------------------------------------------------------------

Title:
Rényi--Sobolev Inequalities and Connections to Spectral Graph Theory

Abstract: In this paper, we generalize the log-Sobolev inequalities to R\'enyi--Sobolev
inequalities by replacing the entropy with the two-parameter entropy, which is
a generalized version of entropy closely related to R\'enyi divergences. We
derive the sharp nonlinear dimension-free version of this kind of inequalities.
Interestingly, the resultant inequalities show a phase transition depending on
the parameters. We then connect R\'enyi--Sobolev inequalities to the spectral
graph theory. Our proofs in this paper are based on the information-theoretic
characterization of the R\'enyi--Sobolev inequalities, as well as the method of
types.


------------------------------------------------------------------------------

Title:
Federated Self-Learning with Weak Supervision for Speech Recognition

Abstract: Automatic speech recognition (ASR) models with low-footprint are increasingly
being deployed on edge devices for conversational agents, which enhances
privacy. We study the problem of federated continual incremental learning for
recurrent neural network-transducer (RNN-T) ASR models in the privacy-enhancing
scheme of learning on-device, without access to ground truth human transcripts
or machine transcriptions from a stronger ASR model. In particular, we study
the performance of a self-learning based scheme, with a paired teacher model
updated through an exponential moving average of ASR models. Further, we
propose using possibly noisy weak-supervision signals such as feedback scores
and natural language understanding semantics determined from user behavior
across multiple turns in a session of interactions with the conversational
agent. These signals are leveraged in a multi-task policy-gradient training
approach to improve the performance of self-learning for ASR. Finally, we show
how catastrophic forgetting can be mitigated by combining on-device learning
with a memory-replay approach using selected historical datasets. These
innovations allow for 10% relative improvement in WER on new use cases with
minimal degradation on other test sets in the absence of strong-supervision
signals such as ground-truth transcriptions.


------------------------------------------------------------------------------

Title:
Scalable Evaluation of Hadamard Products with Tensor Product Basis for  Entropy-Stable High-Order Methods

Abstract: A sum-factorization form for the evaluation of Hadamard products with a
tensor product basis is derived in this work. The proposed algorithm allows for
Hadamard products to be computed in $\mathcal{O}\left(n^{d+1}\right)$ flops
rather than $\mathcal{O}\left(n^{2d}\right)$, where $d$ is the dimension of the
problem. With this improvement, entropy conserving and stable schemes, that
require a dense Hadamard product in the general modal case, become
computationally competitive with the modal discontinuous Galerkin (DG) scheme.
We numerically demonstrate the application of the sum-factorized Hadamard
product in our in-house partial differential equation solver PHiLiP based on
the Nonlinearly Stable Flux Reconstruction scheme. We demonstrate that the
entropy conserving flow solver scales at $\mathcal{O}\left(n^{d+1}\right)$ for
three-dimensional compressible flow in curvilinear coordinates, along with a
computational cost comparison with the modal DG and over-integrated DG schemes.


------------------------------------------------------------------------------

Title:
Fine-grained Policy-driven I/O Sharing for Burst Buffers

Abstract: A burst buffer is a common method to bridge the performance gap between the
I/O needs of modern supercomputing applications and the performance of the
shared file system on large-scale supercomputers. However, existing I/O sharing
methods require resource isolation, offline profiling, or repeated execution
that significantly limit the utilization and applicability of these systems.
Here we present ThemisIO, a policy-driven I/O sharing framework for a
remote-shared burst buffer: a dedicated group of I/O nodes, each with a local
storage device. ThemisIO preserves high utilization by implementing opportunity
fairness so that it can reallocate unused I/O resources to other applications.
ThemisIO accurately and efficiently allocates I/O cycles among applications,
purely based on real-time I/O behavior without requiring user-supplied
information or offline-profiled application characteristics. ThemisIO supports
a variety of fair sharing policies, such as user-fair, size-fair, as well as
composite policies, e.g., group-then-user-fair. All these features are enabled
by its statistical token design. ThemisIO can alter the execution order of
incoming I/O requests based on assigned tokens to precisely balance I/O cycles
between applications via time slicing, thereby enforcing processing isolation.
Experiments using I/O benchmarks show that ThemisIO sustains 13.5-13.7% higher
I/O throughput and 19.5-40.4% lower performance variation than existing
algorithms. For real applications, ThemisIO significantly reduces the slowdown
by 59.1-99.8% caused by I/O interference.


------------------------------------------------------------------------------

Title:
The Implicit Bias of Batch Normalization in Linear Models and Two-layer  Linear Convolutional Neural Networks

Abstract: We study the implicit bias of batch normalization trained by gradient
descent. We show that when learning a linear model with batch normalization for
binary classification, gradient descent converges to a uniform margin
classifier on the training data with an $\exp(-\Omega(\log^2 t))$ convergence
rate. This distinguishes linear models with batch normalization from those
without batch normalization in terms of both the type of implicit bias and the
convergence rate. We further extend our result to a class of two-layer,
single-filter linear convolutional neural networks, and show that batch
normalization has an implicit bias towards a patch-wise uniform margin. Based
on two examples, we demonstrate that patch-wise uniform margin classifiers can
outperform the maximum margin classifiers in certain learning problems. Our
results contribute to a better theoretical understanding of batch
normalization.


------------------------------------------------------------------------------

Title:
On Frequency-Wise Normalizations for Better Recording Device  Generalization in Audio Spectrogram Transformers

Abstract: Varying conditions between the data seen at training and at application time
remain a major challenge for machine learning. We study this problem in the
context of Acoustic Scene Classification (ASC) with mismatching recording
devices. Previous works successfully employed frequency-wise normalization of
inputs and hidden layer activations in convolutional neural networks to reduce
the recording device discrepancy. The main objective of this work was to adopt
frequency-wise normalization for Audio Spectrogram Transformers (ASTs), which
have recently become the dominant model architecture in ASC. To this end, we
first investigate how recording device characteristics are encoded in the
hidden layer activations of ASTs. We find that recording device information is
initially encoded in the frequency dimension; however, after the first
self-attention block, it is largely transformed into the token dimension. Based
on this observation, we conjecture that suppressing recording device
characteristics in the input spectrogram is the most effective. We propose a
frequency-centering operation for spectrograms that improves the ASC
performance on unseen recording devices on average by up to 18.2 percentage
points.


------------------------------------------------------------------------------

Title:
Automated Grading and Feedback Tools for Programming Education: A  Systematic Review

Abstract: We conducted a systematic literature review on automated grading and feedback
tools for programming education. We analysed 121 research papers from 2017 to
2021 inclusive and categorised them based on skills assessed, grading approach,
language paradigm, degree of automation and evaluation techniques. Most papers
grade the correctness of object-oriented assignments. Typically, these tools
use a dynamic technique, primarily unit testing, to provide grades and feedback
to the students. However, compared to correctness grading, few tools assess
readability, maintainability, or documentation, focusing solely on the presence
of documentation, not documentation quality.


------------------------------------------------------------------------------

Title:
Learning When to Trust Which Teacher for Weakly Supervised ASR

Abstract: Automatic speech recognition (ASR) training can utilize multiple experts as
teacher models, each trained on a specific domain or accent. Teacher models may
be opaque in nature since their architecture may be not be known or their
training cadence is different from that of the student ASR model. Still, the
student models are updated incrementally using the pseudo-labels generated
independently by the expert teachers. In this paper, we exploit supervision
from multiple domain experts in training student ASR models. This training
strategy is especially useful in scenarios where few or no human transcriptions
are available. To that end, we propose a Smart-Weighter mechanism that selects
an appropriate expert based on the input audio, and then trains the student
model in an unsupervised setting. We show the efficacy of our approach using
LibriSpeech and LibriLight benchmarks and find an improvement of 4 to 25\% over
baselines that uniformly weight all the experts, use a single expert model, or
combine experts using ROVER.


------------------------------------------------------------------------------

Title:
BEVScope: Enhancing Self-Supervised Depth Estimation Leveraging  Bird's-Eye-View in Dynamic Scenarios

Abstract: Depth estimation is a cornerstone of perception in autonomous driving and
robotic systems. The considerable cost and relatively sparse data acquisition
of LiDAR systems have led to the exploration of cost-effective alternatives,
notably, self-supervised depth estimation. Nevertheless, current
self-supervised depth estimation methods grapple with several limitations: (1)
the failure to adequately leverage informative multi-camera views. (2) the
limited capacity to handle dynamic objects effectively. To address these
challenges, we present BEVScope, an innovative approach to self-supervised
depth estimation that harnesses Bird's-Eye-View (BEV) features. Concurrently,
we propose an adaptive loss function, specifically designed to mitigate the
complexities associated with moving objects. Empirical evaluations conducted on
the Nuscenes dataset validate our approach, demonstrating competitive
performance. Code will be released at this https URL


------------------------------------------------------------------------------

Title:
Generalization Across Experimental Parameters in Machine Learning  Analysis of High Resolution Transmission Electron Microscopy Datasets

Abstract: Neural networks are promising tools for high-throughput and accurate
transmission electron microscopy (TEM) analysis of nanomaterials, but are known
to generalize poorly on data that is "out-of-distribution" from their training
data. Given the limited set of image features typically seen in high-resolution
TEM imaging, it is unclear which images are considered out-of-distribution from
others. Here, we investigate how the choice of metadata features in the
training dataset influences neural network performance, focusing on the example
task of nanoparticle segmentation. We train and validate neural networks across
curated, experimentally-collected high-resolution TEM image datasets of
nanoparticles under controlled imaging and material parameters, including
magnification, dosage, nanoparticle diameter, and nanoparticle material.
Overall, we find that our neural networks are not robust across microscope
parameters, but do generalize across certain sample parameters. Additionally,
data preprocessing heavily influences the generalizability of neural networks
trained on nominally similar datasets. Our results highlight the need to
understand how dataset features affect deployment of data-driven algorithms.


------------------------------------------------------------------------------

Title:
A Responsive Framework for Research Portals Data using Semantic Web  Technology

Abstract: As the amount of data on the World Wide Web continues to grow exponentially,
access to semantically structured information remains limited. The Semantic Web
has emerged as a solution to enhance the machine-readability of data, making it
significantly more accessible and interpretable. Various techniques, such as
web scraping and mapping, have been employed by different websites to provide
semantic access. Web scraping involves the extraction of valuable information
from diverse data sources, such as the World Wide Web, utilizing powerful
string manipulation operations.In the research field, researchers face the
challenge of collecting relevant data from multiple sources, which requires
substantial time and effort. This research aims to address this issue by
designing a framework for the semantic organization of research portal data.
The framework focuses on the extraction of information from two specific
research portals, namely Microsoft Academic and IEEE Xplore. Its primary
objective is to gather diverse research-related data from these targeted
sources.By implementing this framework, researchers can streamline the process
of collecting valuable information for their work, saving time and effort. The
semantic organization of research portal data offers enhanced accessibility and
interpretability, facilitating more effective and efficient knowledge
discovery. This research contributes to the advancement of research data
management and promotes the utilization of semantic web technologies in the
academic community.


------------------------------------------------------------------------------

Title:
MSW-Transformer: Multi-Scale Shifted Windows Transformer Networks for  12-Lead ECG Classification

Abstract: Automatic classification of electrocardiogram (ECG) signals plays a crucial
role in the early prevention and diagnosis of cardiovascular diseases. While
ECG signals can be used for the diagnosis of various diseases, their
pathological characteristics exhibit minimal variations, posing a challenge to
automatic classification models. Existing methods primarily utilize
convolutional neural networks to extract ECG signal features for
classification, which may not fully capture the pathological feature
differences of different diseases. Transformer networks have advantages in
feature extraction for sequence data, but the complete network is complex and
relies on large-scale datasets. To address these challenges, we propose a
single-layer Transformer network called Multi-Scale Shifted Windows Transformer
Networks (MSW-Transformer), which uses a multi-window sliding attention
mechanism at different scales to capture features in different dimensions. The
self-attention is restricted to non-overlapping local windows via shifted
windows, and different window scales have different receptive fields. A
learnable feature fusion method is then proposed to integrate features from
different windows to further enhance model performance. Furthermore, we
visualize the attention mechanism of the multi-window shifted mechanism to
achieve better clinical interpretation in the ECG classification task. The
proposed model achieves state-of-the-art performance on five classification
tasks of the PTBXL-2020 12-lead ECG dataset, which includes 5 diagnostic
superclasses, 23 diagnostic subclasses, 12 rhythm classes, 17 morphology
classes, and 44 diagnosis classes, with average macro-F1 scores of 77.85%,
47.57%, 66.13%, 34.60%, and 34.29%, and average sample-F1 scores of 81.26%,
68.27%, 91.32%, 50.07%, and 63.19%, respectively.


------------------------------------------------------------------------------

Title:
Time-Varying Transition Matrices with Multi-task Gaussian Processes

Abstract: In this paper, we present a kernel-based, multi-task Gaussian Process (GP)
model for approximating the underlying function of an individual's mobility
state using a time-inhomogeneous Markov Process with two states: moves and
pauses. Our approach accounts for the correlations between the transition
probabilities by creating a covariance matrix over the tasks. We also introduce
time-variability by assuming that an individual's transition probabilities vary
over time in response to exogenous variables. We enforce the stochasticity and
non-negativity constraints of probabilities in a Markov process through the
incorporation of a set of constraint points in the GP. We also discuss
opportunities to speed up GP estimation and inference in this context by
exploiting Toeplitz and Kronecker product structures. Our numerical experiments
demonstrate the ability of our formulation to enforce the desired constraints
while learning the functional form of transition probabilities.


------------------------------------------------------------------------------

Title:
Provably Powerful Graph Neural Networks for Directed Multigraphs

Abstract: This paper proposes a set of simple adaptations to transform standard
message-passing Graph Neural Networks (GNN) into provably powerful directed
multigraph neural networks. The adaptations include multigraph port numbering,
ego IDs, and reverse message passing. We prove that the combination of these
theoretically enables the detection of any directed subgraph pattern. To
validate the effectiveness of our proposed adaptations in practice, we conduct
experiments on synthetic subgraph detection tasks, which demonstrate
outstanding performance with almost perfect results.
Moreover, we apply our proposed adaptations to two financial crime analysis
tasks. We observe dramatic improvements in detecting money laundering
transactions, improving the minority-class F1 score of a standard
message-passing GNN by up to 45%, and clearly outperforming tree-based and GNN
baselines. Similarly impressive results are observed on a real-world phishing
detection dataset, boosting a standard GNN's F1 score by over 15% and
outperforming all baselines.


------------------------------------------------------------------------------

Title:
SeFNet: Bridging Tabular Datasets with Semantic Feature Nets

Abstract: Machine learning applications cover a wide range of predictive tasks in which
tabular datasets play a significant role. However, although they often address
similar problems, tabular datasets are typically treated as standalone tasks.
The possibilities of using previously solved problems are limited due to the
lack of structured contextual information about their features and the lack of
understanding of the relations between them. To overcome this limitation, we
propose a new approach called Semantic Feature Net (SeFNet), capturing the
semantic meaning of the analyzed tabular features. By leveraging existing
ontologies and domain knowledge, SeFNet opens up new opportunities for sharing
insights between diverse predictive tasks. One such opportunity is the Dataset
Ontology-based Semantic Similarity (DOSS) measure, which quantifies the
similarity between datasets using relations across their features. In this
paper, we present an example of SeFNet prepared for a collection of predictive
tasks in healthcare, with the features' relations derived from the SNOMED-CT
ontology. The proposed SeFNet framework and the accompanying DOSS measure
address the issue of limited contextual information in tabular datasets. By
incorporating domain knowledge and establishing semantic relations between
features, we enhance the potential for meta-learning and enable valuable
insights to be shared across different predictive tasks.


------------------------------------------------------------------------------

Title:
Learning Profitable NFT Image Diffusions via Multiple Visual-Policy  Guided Reinforcement Learning

Abstract: We study the task of generating profitable Non-Fungible Token (NFT) images
from user-input texts. Recent advances in diffusion models have shown great
potential for image generation. However, existing works can fall short in
generating visually-pleasing and highly-profitable NFT images, mainly due to
the lack of 1) plentiful and fine-grained visual attribute prompts for an NFT
image, and 2) effective optimization metrics for generating high-quality NFT
images. To solve these challenges, we propose a Diffusion-based generation
framework with Multiple Visual-Policies as rewards (i.e., Diffusion-MVP) for
NFT images. The proposed framework consists of a large language model (LLM), a
diffusion-based image generator, and a series of visual rewards by design.
First, the LLM enhances a basic human input (such as "panda") by generating
more comprehensive NFT-style prompts that include specific visual attributes,
such as "panda with Ninja style and green background." Second, the
diffusion-based image generator is fine-tuned using a large-scale NFT dataset
to capture fine-grained image styles and accessory compositions of popular NFT
elements. Third, we further propose to utilize multiple visual-policies as
optimization goals, including visual rarity levels, visual aesthetic scores,
and CLIP-based text-image relevances. This design ensures that our proposed
Diffusion-MVP is capable of minting NFT images with high visual quality and
market value. To facilitate this research, we have collected the largest
publicly available NFT image dataset to date, consisting of 1.5 million
high-quality images with corresponding texts and market values. Extensive
experiments including objective evaluations and user studies demonstrate that
our framework can generate NFT images showing more visually engaging elements
and higher market value, compared with SOTA approaches.


------------------------------------------------------------------------------

Title:
Closing the loop: Autonomous experiments enabled by  machine-learning-based online data analysis in synchrotron beamline  environments

Abstract: Recently, there has been significant interest in applying machine learning
(ML) techniques to X-ray scattering experiments, which proves to be a valuable
tool for enhancing research that involves large or rapidly generated datasets.
ML allows for the automated interpretation of experimental results,
particularly those obtained from synchrotron or neutron facilities. The speed
at which ML models can process data presents an important opportunity to
establish a closed-loop feedback system, enabling real-time decision-making
based on online data analysis. In this study, we describe the incorporation of
ML into a closed-loop workflow for X-ray reflectometry (XRR), using the growth
of organic thin films as an example. Our focus lies on the beamline integration
of ML-based online data analysis and closed-loop feedback. We present solutions
that provide an elementary data analysis in real time during the experiment
without introducing the additional software dependencies in the beamline
control software environment. Our data demonstrates the accuracy and robustness
of ML methods for analyzing XRR curves and Bragg reflections and its autonomous
control over a vacuum deposition setup.


------------------------------------------------------------------------------

Title:
A new thermodynamically compatible finite volume scheme for Lagrangian  gas dynamics

Abstract: The equations of Lagrangian gas dynamics fall into the larger class of
overdetermined hyperbolic and thermodynamically compatible (HTC) systems of
partial differential equations. They satisfy an entropy inequality (second
principle of thermodynamics) and conserve total energy (first principle of
thermodynamics). The aim of this work is to construct a novel thermodynamically
compatible cell-centered Lagrangian finite volume scheme on unstructured
meshes. Unlike in existing schemes, we choose to directly discretize the
entropy inequality, hence obtaining total energy conservation as a consequence
of the new thermodynamically compatible discretization of the other equations.
First, the governing equations are written in fluctuation form. Next, the
non-compatible centered numerical fluxes are corrected according to the
approach recently introduced by Abgrall et al., using a scalar correction
factor that is defined at the nodes of the grid. This perfectly fits into the
formalism of nodal solvers which is typically adopted in cell-centered
Lagrangian finite volume methods. Semi-discrete entropy conservative and
entropy stable Lagrangian schemes are devised, and they are adequately blended
together via a convex combination based on either a priori or a posteriori
detectors of discontinuous solutions. The nonlinear stability in the energy
norm is rigorously demonstrated and the new schemes are provably positivity
preserving for density and pressure. Furthermore, they exhibit zero numerical
diffusion for isentropic flows while being still nonlinearly stable. The new
schemes are tested against classical benchmarks for Lagrangian hydrodynamics,
assessing their convergence and robustness and comparing their numerical
dissipation with classical Lagrangian finite volume methods.


------------------------------------------------------------------------------

Title:
BMAD: Benchmarks for Medical Anomaly Detection

Abstract: Anomaly detection (AD) is a fundamental research problem in machine learning
and computer vision, with practical applications in industrial inspection,
video surveillance, and medical diagnosis. In medical imaging, AD is especially
vital for detecting and diagnosing anomalies that may indicate rare diseases or
conditions. However, there is a lack of a universal and fair benchmark for
evaluating AD methods on medical images, which hinders the development of more
generalized and robust AD methods in this specific domain. To bridge this gap,
we introduce a comprehensive evaluation benchmark for assessing anomaly
detection methods on medical images. This benchmark encompasses six reorganized
datasets from five medical domains (i.e. brain MRI, liver CT, retinal OCT,
chest X-ray, and digital histopathology) and three key evaluation metrics, and
includes a total of fourteen state-of-the-art AD algorithms. This standardized
and well-curated medical benchmark with the well-structured codebase enables
comprehensive comparisons among recently proposed anomaly detection methods. It
will facilitate the community to conduct a fair comparison and advance the
field of AD on medical imaging. More information on BMAD is available in our
GitHub repository: this https URL


------------------------------------------------------------------------------

Title:
Learning Costs for Structured Monge Displacements

Abstract: Optimal transport theory has provided machine learning with several tools to
infer a push-forward map between densities from samples. While this theory has
recently seen tremendous methodological developments in machine learning, its
practical implementation remains notoriously difficult, because it is plagued
by both computational and statistical challenges. Because of such difficulties,
existing approaches rarely depart from the default choice of estimating such
maps with the simple squared-Euclidean distance as the ground cost,
$c(x,y)=\|x-y\|^2_2$. We follow a different path in this work, with the
motivation of \emph{learning} a suitable cost structure to encourage maps to
transport points along engineered features. We extend the recently proposed
Monge-Bregman-Occam pipeline~\citep{cuturi2023monge}, that rests on an
alternative cost formulation that is also cost-invariant $c(x,y)=h(x-y)$, but
which adopts a more general form as $h=\tfrac12 \ell_2^2+\tau$, where $\tau$ is
an appropriately chosen regularizer. We first propose a method that builds upon
proximal gradient descent to generate ground truth transports for such
structured costs, using the notion of $h$-transforms and $h$-concave
potentials. We show more generally that such a method can be extended to
compute $h$-transforms for entropic potentials. We study a regularizer that
promotes transport displacements in low-dimensional spaces, and propose to
learn such a basis change using Riemannian gradient descent on the Stiefel
manifold. We show that these changes lead to estimators that are more robust
and easier to interpret.


------------------------------------------------------------------------------

Title:
Visual Analysis of Large Multi-Field AMR Data on GPUs Using Interactive  Volume Lines

Abstract: To visually compare ensembles of volumes, dynamic volume lines (DVLs)
represent each ensemble member as a 1D polyline. To compute these, the volume
cells are sorted on a space-filling curve and scaled by the ensemble's local
variation. The resulting 1D plot can augment or serve as an alternative to a 3D
volume visualization free of visual clutter and occlusion. Interactively
computing DVLs is challenging when the data is large, and the volume grid is
not structured/regular, as is often the case with computational fluid dynamics
simulations. We extend DVLs to support large-scale, multi-field adaptive mesh
refinement (AMR) data that can be explored interactively. Our GPU-based system
updates the DVL representation whenever the data or the alpha transfer function
changes. We demonstrate and evaluate our interactive prototype using large AMR
volumes from astrophysics simulations.


------------------------------------------------------------------------------

Title:
On Cross-Layer Interactions of QUIC, Encrypted DNS and HTTP/3: Design,  Evaluation and Dataset

Abstract: Every Web session involves a DNS resolution. While, in the last decade, we
witnessed a promising trend towards an encrypted Web in general, DNS encryption
has only recently gained traction with the standardisation of DNS over TLS
(DoT) and DNS over HTTPS (DoH). Meanwhile, the rapid rise of QUIC deployment
has now opened up an exciting opportunity to utilise the same protocol to not
only encrypt Web communications, but also DNS. In this paper, we evaluate this
benefit of using QUIC to coalesce name resolution via DNS over QUIC (DoQ), and
Web content delivery via HTTP/3 (H3) with 0-RTT. We compare this scenario using
several possible combinations where H3 is used in conjunction with DoH and DoQ,
as well as the unencrypted DNS over UDP (DoUDP). We observe, that when using H3
1-RTT, page load times with DoH can get inflated by $>$30\% over fixed-line and
by $>$50\% over mobile when compared to unencrypted DNS with DoUDP. However,
this cost of encryption can be drastically reduced when encrypted connections
are coalesced (DoQ + H3 0-RTT), thereby reducing the page load times by 1/3
over fixed-line and 1/2 over mobile, overall making connection coalescing with
QUIC the best option for encrypted communication on the Internet.


------------------------------------------------------------------------------

Title:
MoleCLUEs: Optimizing Molecular Conformers by Minimization of  Differentiable Uncertainty

Abstract: Structure-based models in the molecular sciences can be highly sensitive to
input geometries and give predictions with large variance under subtle
coordinate perturbations. We present an approach to mitigate this failure mode
by generating conformations that explicitly minimize uncertainty in a
predictive model. To achieve this, we compute differentiable estimates of
aleatoric \textit{and} epistemic uncertainties directly from learned
embeddings. We then train an optimizer that iteratively samples embeddings to
reduce these uncertainties according to their gradients. As our predictive
model is constructed as a variational autoencoder, the new embeddings can be
decoded to their corresponding inputs, which we call \textit{MoleCLUEs}, or
(molecular) counterfactual latent uncertainty explanations
\citep{antoran2020getting}. We provide results of our algorithm for the task of
predicting drug properties with maximum confidence as well as analysis of the
differentiable structure simulations.


------------------------------------------------------------------------------

Title:
Accelerating Multiframe Blind Deconvolution via Deep Learning

Abstract: Ground-based solar image restoration is a computationally expensive procedure
that involves nonlinear optimization techniques. The presence of atmospheric
turbulence produces perturbations in individual images that make it necessary
to apply blind deconvolution techniques. These techniques rely on the
observation of many short exposure frames that are used to simultaneously infer
the instantaneous state of the atmosphere and the unperturbed object. We have
recently explored the use of machine learning to accelerate this process, with
promising results. We build upon this previous work to propose several
interesting improvements that lead to better models. As well, we propose a new
method to accelerate the restoration based on algorithm unrolling. In this
method, the image restoration problem is solved with a gradient descent method
that is unrolled and accelerated aided by a few small neural networks. The role
of the neural networks is to correct the estimation of the solution at each
iterative step. The model is trained to perform the optimization in a small
fixed number of steps with a curated dataset. Our findings demonstrate that
both methods significantly reduce the restoration time compared to the standard
optimization procedure. Furthermore, we showcase that these models can be
trained in an unsupervised manner using observed images from three different
instruments. Remarkably, they also exhibit robust generalization capabilities
when applied to new datasets. To foster further research and collaboration, we
openly provide the trained models, along with the corresponding training and
evaluation code, as well as the training dataset, to the scientific community.


------------------------------------------------------------------------------

Title:
How can objects help action recognition?

Abstract: Current state-of-the-art video models process a video clip as a long sequence
of spatio-temporal tokens. However, they do not explicitly model objects, their
interactions across the video, and instead process all the tokens in the video.
In this paper, we investigate how we can use knowledge of objects to design
better video models, namely to process fewer tokens and to improve recognition
accuracy. This is in contrast to prior works which either drop tokens at the
cost of accuracy, or increase accuracy whilst also increasing the computation
required. First, we propose an object-guided token sampling strategy that
enables us to retain a small fraction of the input tokens with minimal impact
on accuracy. And second, we propose an object-aware attention module that
enriches our feature representation with object information and improves
overall accuracy. Our resulting framework achieves better performance when
using fewer tokens than strong baselines. In particular, we match our baseline
with 30%, 40%, and 60% of the input tokens on SomethingElse,
Something-something v2, and Epic-Kitchens, respectively. When we use our model
to process the same number of tokens as our baseline, we improve by 0.6 to 4.2
points on these datasets.


------------------------------------------------------------------------------

Title:
Evolutionary Strategy Guided Reinforcement Learning via MultiBuffer  Communication

Abstract: Evolutionary Algorithms and Deep Reinforcement Learning have both
successfully solved control problems across a variety of domains. Recently,
algorithms have been proposed which combine these two methods, aiming to
leverage the strengths and mitigate the weaknesses of both approaches. In this
paper we introduce a new Evolutionary Reinforcement Learning model which
combines a particular family of Evolutionary algorithm called Evolutionary
Strategies with the off-policy Deep Reinforcement Learning algorithm TD3. The
framework utilises a multi-buffer system instead of using a single shared
replay buffer. The multi-buffer system allows for the Evolutionary Strategy to
search freely in the search space of policies, without running the risk of
overpopulating the replay buffer with poorly performing trajectories which
limit the number of desirable policy behaviour examples thus negatively
impacting the potential of the Deep Reinforcement Learning within the shared
framework. The proposed algorithm is demonstrated to perform competitively with
current Evolutionary Reinforcement Learning algorithms on MuJoCo control tasks,
outperforming the well known state-of-the-art CEM-RL on 3 of the 4 environments
tested.


------------------------------------------------------------------------------

Title:
Crouzeix's conjecture for new classes of matrices

Abstract: For a matrix $A$ which satisfies Crouzeix's conjecture, we construct several
new classes of matrices from $A$ for which the conjecture will also hold. We
discover a new link between cyclicity and Crouzeix's conjecture, which affirms
the conjecture in the positive for a class of matrices. We pose several open
questions, which if proved, will prove Crouzeix's conjecture. We also begin an
investigation into Crouzeix's conjecture for symmetric matrices and in the case
of $3 \times 3$ matrices, we show Crouzeix's conjecture holds for symmetric
matrices if and only if it holds for analytic truncated Toeplitz operators.


------------------------------------------------------------------------------

Title:
Quantum Rényi and $f$-divergences from integral representations

Abstract: Smooth Csisz\'ar $f$-divergences can be expressed as integrals over so-called
hockey stick divergences. This motivates a natural quantum generalization in
terms of quantum Hockey stick divergences, which we explore here. Using this
recipe, the Kullback-Leibler divergence generalises to the Umegaki relative
entropy, in the integral form recently found by Frenkel. We find that the
R\'enyi divergences defined via our new quantum $f$-divergences are not
additive in general, but that their regularisations surprisingly yield the Petz
R\'enyi divergence for $\alpha < 1$ and the sandwiched R\'enyi divergence for
$\alpha > 1$, unifying these two important families of quantum R\'enyi
divergences. Moreover, we find that the contraction coefficients for the new
quantum $f$ divergences collapse for all $f$ that are operator convex,
mimicking the classical behaviour and resolving some long-standing conjectures
by Lesniewski and Ruskai. We derive various inequalities, including new reverse
Pinsker inequalites with applications in differential privacy and also explore
various other applications of the new divergences.


------------------------------------------------------------------------------

Title:
Temporal Conditioning Spiking Latent Variable Models of the Neural  Response to Natural Visual Scenes

Abstract: Developing computational models of neural response is crucial for
understanding sensory processing and neural computations. Current
state-of-the-art neural network methods use temporal filters to handle temporal
dependencies, resulting in an unrealistic and inflexible processing flow.
Meanwhile, these methods target trial-averaged firing rates and fail to capture
important features in spike trains. This work presents the temporal
conditioning spiking latent variable models (TeCoS-LVM) to simulate the neural
response to natural visual stimuli. We use spiking neurons to produce spike
outputs that directly match the recorded trains. This approach helps to avoid
losing information embedded in the original spike trains. We exclude the
temporal dimension from the model parameter space and introduce a temporal
conditioning operation to allow the model to adaptively explore and exploit
temporal dependencies in stimuli sequences in a natural paradigm. We show that
TeCoS-LVM models can produce more realistic spike activities and accurately fit
spike statistics than powerful alternatives. Additionally, learned TeCoS-LVM
models can generalize well to longer time scales. Overall, while remaining
computationally tractable, our model effectively captures key features of
neural coding systems. It thus provides a useful tool for building accurate
predictive computational accounts for various sensory perception circuits.


------------------------------------------------------------------------------

Title:
Deterministic Identification Over Multiple-Access Channels

Abstract: Deterministic identification over K-input multiple-access channels with
average input cost constraints is considered. The capacity region for
deterministic identification is determined for an average-error criterion,
where arbitrarily large codes are achievable. For a maximal-error criterion,
upper and lower bounds on the capacity region are derived. The bounds coincide
if all average partial point-to-point channels are injective under the input
constraint, i.e. all inputs at one terminal are mapped to distinct output
distributions, if averaged over the inputs at all other terminals. The
achievability is proved by treating the MAC as an arbitrarily varying channel
with average state constraints. For injective average channels, the capacity
region is a hyperrectangle. The modulo-2 and modulo-3 binary adder MAC are
presented as examples of channels which are injective under suitable input
constraints. The binary multiplier MAC is presented as an example of a
non-injective channel, where the achievable identification rate region still
includes the Shannon capacity region.


------------------------------------------------------------------------------

Title:
TauPETGen: Text-Conditional Tau PET Image Synthesis Based on Latent  Diffusion Models

Abstract: In this work, we developed a novel text-guided image synthesis technique
which could generate realistic tau PET images from textual descriptions and the
subject's MR image. The generated tau PET images have the potential to be used
in examining relations between different measures and also increasing the
public availability of tau PET datasets. The method was based on latent
diffusion models. Both textual descriptions and the subject's MR prior image
were utilized as conditions during image generation. The subject's MR image can
provide anatomical details, while the text descriptions, such as gender, scan
time, cognitive test scores, and amyloid status, can provide further guidance
regarding where the tau neurofibrillary tangles might be deposited. Preliminary
experimental results based on clinical [18F]MK-6240 datasets demonstrate the
feasibility of the proposed method in generating realistic tau PET images at
different clinical stages.


------------------------------------------------------------------------------

Title:
Retrieving-to-Answer: Zero-Shot Video Question Answering with Frozen  Large Language Models

Abstract: Video Question Answering (VideoQA) has been significantly advanced from the
scaling of recent Large Language Models (LLMs). The key idea is to convert the
visual information into the language feature space so that the capacity of LLMs
can be fully exploited. Existing VideoQA methods typically take two paradigms:
(1) learning cross-modal alignment, and (2) using an off-the-shelf captioning
model to describe the visual data. However, the first design needs costly
training on many extra multi-modal data, whilst the second is further limited
by limited domain generalization. To address these limitations, a simple yet
effective Retrieving-to-Answer (R2A) framework is proposed.Given an input
video, R2A first retrieves a set of semantically similar texts from a generic
text corpus using a pre-trained multi-modal model (e.g., CLIP). With both the
question and the retrieved texts, a LLM (e.g., DeBERTa) can be directly used to
yield a desired answer. Without the need for cross-modal fine-tuning, R2A
allows for all the key components (e.g., LLM, retrieval model, and text corpus)
to plug-and-play. Extensive experiments on several VideoQA benchmarks show that
despite with 1.3B parameters and no fine-tuning, our R2A can outperform the 61
times larger Flamingo-80B model even additionally trained on nearly 2.1B
multi-modal data.


------------------------------------------------------------------------------

Title:
G-NM: A Group of Numerical Time Series Prediction Models

Abstract: In this study, we focus on the development and implementation of a
comprehensive ensemble of numerical time series forecasting models,
collectively referred to as the Group of Numerical Time Series Prediction Model
(G-NM). This inclusive set comprises traditional models such as Autoregressive
Integrated Moving Average (ARIMA), Holt-Winters' method, and Support Vector
Regression (SVR), in addition to modern neural network models including
Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM). G-NM is
explicitly constructed to augment our predictive capabilities related to
patterns and trends inherent in complex natural phenomena. By utilizing time
series data relevant to these events, G-NM facilitates the prediction of such
phenomena over extended periods. The primary objective of this research is to
both advance our understanding of such occurrences and to significantly enhance
the accuracy of our forecasts. G-NM encapsulates both linear and non-linear
dependencies, seasonalities, and trends present in time series data. Each of
these models contributes distinct strengths, from ARIMA's resilience in
handling linear trends and seasonality, SVR's proficiency in capturing
non-linear patterns, to LSTM's adaptability in modeling various components of
time series data. Through the exploitation of the G-NM potential, we strive to
advance the state-of-the-art in large-scale time series forecasting models. We
anticipate that this research will represent a significant stepping stone in
our ongoing endeavor to comprehend and forecast the complex events that
constitute the natural world.


------------------------------------------------------------------------------

Title:
RoTaR: Efficient Row-Based Table Representation Learning via  Teacher-Student Training

Abstract: We propose RoTaR, a row-based table representation learning method, to
address the efficiency and scalability issues faced by existing table
representation learning methods. The key idea of RoTaR is to generate
query-agnostic row representations that could be re-used via query-specific
aggregation. In addition to the row-based architecture, we introduce several
techniques: cell-aware position embedding, teacher-student training paradigm,
and selective backward to improve the performance of RoTaR model.


------------------------------------------------------------------------------

Title:
The Ecological Fallacy in Annotation: Modelling Human Label Variation  goes beyond Sociodemographics

Abstract: Many NLP tasks exhibit human label variation, where different annotators give
different labels to the same texts. This variation is known to depend, at least
in part, on the sociodemographics of annotators. Recent research aims to model
individual annotator behaviour rather than predicting aggregated labels, and we
would expect that sociodemographic information is useful for these models. On
the other hand, the ecological fallacy states that aggregate group behaviour,
such as the behaviour of the average female annotator, does not necessarily
explain individual behaviour. To account for sociodemographics in models of
individual annotator behaviour, we introduce group-specific layers to
multi-annotator models. In a series of experiments for toxic content detection,
we find that explicitly accounting for sociodemographic attributes in this way
does not significantly improve model performance. This result shows that
individual annotation behaviour depends on much more than just
sociodemographics.


------------------------------------------------------------------------------

Title:
Dense Video Object Captioning from Disjoint Supervision

Abstract: We propose a new task and model for dense video object captioning --
detecting, tracking, and captioning trajectories of all objects in a video.
This task unifies spatial and temporal understanding of the video, and requires
fine-grained language description. Our model for dense video object captioning
is trained end-to-end and consists of different modules for spatial
localization, tracking, and captioning. As such, we can train our model with a
mixture of disjoint tasks, and leverage diverse, large-scale datasets which
supervise different parts of our model. This results in noteworthy zero-shot
performance. Moreover, by finetuning a model from this initialization, we can
further improve our performance, surpassing strong image-based baselines by a
significant margin. Although we are not aware of other work performing this
task, we are able to repurpose existing video grounding datasets for our task,
namely VidSTG and VLN. We show our task is more general than grounding, and
models trained on our task can directly be applied to grounding by finding the
bounding box with the maximum likelihood of generating the query sentence. Our
model outperforms dedicated, state-of-the-art models for spatial grounding on
both VidSTG and VLN.


------------------------------------------------------------------------------

Title:
Mining Interest Trends and Adaptively Assigning SampleWeight for  Session-based Recommendation

Abstract: Session-based Recommendation (SR) aims to predict users' next click based on
their behavior within a short period, which is crucial for online platforms.
However, most existing SR methods somewhat ignore the fact that user preference
is not necessarily strongly related to the order of interactions. Moreover,
they ignore the differences in importance between different samples, which
limits the model-fitting performance. To tackle these issues, we put forward
the method, Mining Interest Trends and Adaptively Assigning Sample Weight,
abbreviated as MTAW. Specifically, we model users' instant interest based on
their present behavior and all their previous behaviors. Meanwhile, we
discriminatively integrate instant interests to capture the changing trend of
user interest to make more personalized recommendations. Furthermore, we devise
a novel loss function that dynamically weights the samples according to their
prediction difficulty in the current epoch. Extensive experimental results on
two benchmark datasets demonstrate the effectiveness and superiority of our
method.


------------------------------------------------------------------------------

Title:
Fast quantum algorithm for differential equations

Abstract: Partial differential equations (PDEs) are ubiquitous in science and
engineering. Prior quantum algorithms for solving the system of linear
algebraic equations obtained from discretizing a PDE have a computational
complexity that scales at least linearly with the condition number $\kappa$ of
the matrices involved in the computation. For many practical applications,
$\kappa$ scales polynomially with the size $N$ of the matrices, rendering a
polynomial-in-$N$ complexity for these algorithms. Here we present a quantum
algorithm with a complexity that is polylogarithmic in $N$ but is independent
of $\kappa$ for a large class of PDEs. Our algorithm generates a quantum state
that enables extracting features of the solution. Central to our methodology is
using a wavelet basis as an auxiliary system of coordinates in which the
condition number of associated matrices is independent of $N$ by a simple
diagonal preconditioner. We present numerical simulations showing the effect of
the wavelet preconditioner for several differential equations. Our work could
provide a practical way to boost the performance of quantum-simulation
algorithms where standard methods are used for discretization.


------------------------------------------------------------------------------

Title:
MRFI: An Open Source Multi-Resolution Fault Injection Framework for  Neural Network Processing

Abstract: To ensure resilient neural network processing on even unreliable hardware,
comprehensive reliability analysis against various hardware faults is generally
required before the deep neural network models are deployed, and efficient
error injection tools are highly demanded. However, most existing fault
injection tools remain rather limited to basic fault injection to neurons and
fail to provide fine-grained vulnerability analysis capability. In addition,
many of the fault injection tools still need to change the neural network
models and make the fault injection closely coupled with normal neural network
processing, which further complicates the use of the fault injection tools and
slows down the fault simulation. In this work, we propose MRFI, a highly
configurable multi-resolution fault injection tool for deep neural networks. It
enables users to modify an independent fault configuration file rather than
neural network models for the fault injection and vulnerability analysis.
Particularly, it integrates extensive fault analysis functionalities from
different perspectives and enables multi-resolution investigation of the
vulnerability of neural networks. In addition, it does not modify the major
neural network computing framework of PyTorch. Hence, it allows parallel
processing on GPUs naturally and exhibits fast fault simulation according to
our experiments.


------------------------------------------------------------------------------

Title:
Should I Stop or Should I Go: Early Stopping with Heterogeneous  Populations

Abstract: Randomized experiments often need to be stopped prematurely due to the
treatment having an unintended harmful effect. Existing methods that determine
when to stop an experiment early are typically applied to the data in aggregate
and do not account for treatment effect heterogeneity. In this paper, we study
the early stopping of experiments for harm on heterogeneous populations. We
first establish that current methods often fail to stop experiments when the
treatment harms a minority group of participants. We then use causal machine
learning to develop CLASH, the first broadly-applicable method for
heterogeneous early stopping. We demonstrate CLASH's performance on simulated
and real data and show that it yields effective early stopping for both
clinical trials and A/B tests.


------------------------------------------------------------------------------

Title:
Lingua Manga: A Generic Large Language Model Centric System for Data  Curation

Abstract: Data curation is a wide-ranging area which contains many critical but
time-consuming data processing tasks. However, the diversity of such tasks
makes it challenging to develop a general-purpose data curation system. To
address this issue, we present Lingua Manga, a user-friendly and versatile
system that utilizes pre-trained large language models. Lingua Manga offers
automatic optimization for achieving high performance and label efficiency
while facilitating flexible and rapid development. Through three example
applications with distinct objectives and users of varying levels of technical
proficiency, we demonstrate that Lingua Manga can effectively assist both
skilled programmers and low-code or even no-code users in addressing data
curation challenges.


------------------------------------------------------------------------------

Title:
Textbooks Are All You Need

Abstract: We introduce phi-1, a new large language model for code, with significantly
smaller size than competing models: phi-1 is a Transformer-based model with
1.3B parameters, trained for 4 days on 8 A100s, using a selection of ``textbook
quality" data from the web (6B tokens) and synthetically generated textbooks
and exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attains
pass@1 accuracy 50.6% on HumanEval and 55.5% on MBPP. It also displays
surprising emergent properties compared to phi-1-base, our model before our
finetuning stage on a dataset of coding exercises, and phi-1-small, a smaller
model with 350M parameters trained with the same pipeline as phi-1 that still
achieves 45% on HumanEval.


------------------------------------------------------------------------------

Title:
GPU First -- Execution of Legacy CPU Codes on GPUs

Abstract: Utilizing GPUs is critical for high performance on heterogeneous systems.
However, leveraging the full potential of GPUs for accelerating legacy CPU
applications can be a challenging task for developers. The porting process
requires identifying code regions amenable to acceleration, managing distinct
memories, synchronizing host and device execution, and handling library
functions that may not be directly executable on the device. This complexity
makes it challenging for non-experts to leverage GPUs effectively, or even to
start offloading parts of a large legacy application. In this paper, we propose
a novel compilation scheme called "GPU First" that automatically compiles
legacy CPU applications directly for GPUs without any modification of the
application source. Library calls inside the application are either resolved
through our partial libc GPU implementation or via automatically generated
remote procedure calls to the host. Our approach simplifies the task of
identifying code regions amenable to acceleration and enables rapid testing of
code modifications on actual GPU hardware in order to guide porting efforts.
Our evaluation on two HPC proxy applications with OpenMP CPU and GPU
parallelism, four micro benchmarks with originally GPU only parallelism, as
well as three benchmarks from the SPEC OMP 2012 suite featuring hand-optimized
OpenMP CPU parallelism showcases the simplicity of porting host applications to
the GPU. For existing parallel loops, we often match the performance of
corresponding manually offloaded kernels, with up to 14.36x speedup on the GPU,
validating that our GPU First methodology can effectively guide porting efforts
of large legacy applications.


------------------------------------------------------------------------------

Title:
Towards a robust and reliable deep learning approach for detection of  compact binary mergers in gravitational wave data

Abstract: The ability of deep learning (DL) approaches to learn generalised signal and
noise models, coupled with their fast inference on GPUs, holds great promise
for enhancing gravitational-wave (GW) searches in terms of speed, parameter
space coverage, and search sensitivity. However, the opaque nature of DL models
severely harms their reliability. In this work, we meticulously develop a DL
model stage-wise and work towards improving its robustness and reliability.
First, we address the problems in maintaining the purity of training data by
deriving a new metric that better reflects the visual strength of the "chirp"
signal features in the data. Using a reduced, smooth representation obtained
through a variational auto-encoder (VAE), we build a classifier to search for
compact binary coalescence (CBC) signals. Our tests on real LIGO data show an
impressive performance of the model. However, upon probing the robustness of
the model through adversarial attacks, its simple failure modes were
identified, underlining how such models can still be highly fragile. As a first
step towards bringing robustness, we retrain the model in a novel framework
involving a generative adversarial network (GAN). Over the course of training,
the model learns to eliminate the primary modes of failure identified by the
adversaries. Although absolute robustness is practically impossible to achieve,
we demonstrate some fundamental improvements earned through such training, like
sparseness and reduced degeneracy in the extracted features at different layers
inside the model. Through comparative inference on real LIGO data, we show that
the prescribed robustness is achieved at practically zero cost in terms of
performance. Through a direct search on ~8.8 days of LIGO data, we recover two
significant CBC events from GWTC-2.1, GW190519_153544 and GW190521_074359, and
report the search sensitivity.


------------------------------------------------------------------------------

Title:
A Simple and Effective Pruning Approach for Large Language Models

Abstract: As their size increases, Large Languages Models (LLMs) are natural candidates
for network pruning methods: approaches that drop a subset of network weights
while striving to preserve performance. Existing methods, however, require
either retraining, which is rarely affordable for billion-scale LLMs, or
solving a weight reconstruction problem reliant on second-order information,
which may also be computationally expensive. In this paper, we introduce a
novel, straightforward yet effective pruning method, termed Wanda (Pruning by
Weights and activations), designed to induce sparsity in pretrained LLMs.
Motivated by the recent observation of emergent large magnitude features in
LLMs, our approach prune weights with the smallest magnitudes multiplied by the
corresponding input activations, on a per-output basis. Notably, Wanda requires
no retraining or weight update, and the pruned LLM can be used as is. We
conduct a thorough evaluation of our method on LLaMA across various language
benchmarks. Wanda significantly outperforms the established baseline of
magnitude pruning and competes favorably against recent methods involving
intensive weight update. Code is available at
this https URL


------------------------------------------------------------------------------

Title:
DIAS: A Comprehensive Benchmark for DSA-sequence Intracranial Artery  Segmentation

Abstract: Automatic segmentation of the intracranial artery (IA) in digital subtraction
angiography (DSA) sequence is an essential step in diagnosing IA-related
diseases and guiding neuro-interventional surgery. However, the lack of
publicly available datasets has impeded research in this area. In this paper,
we release DIAS, an IA segmentation dataset, consisting of 120 DSA sequences
from intracranial interventional therapy. In addition to pixel-wise
annotations, this dataset provides two types of scribble annotations for weakly
supervised IA segmentation research. We present a comprehensive benchmark for
evaluating the performance of this challenging dataset by utilizing fully-,
weakly-, and semi-supervised learning approaches. Specifically, we propose a
method that incorporates a dimensionality reduction module into a 2D/3D model
to achieve vessel segmentation in DSA sequences. For weakly-supervised
learning, we propose a scribble learning-based image segmentation framework,
SSCR, which comprises scribble supervision and consistency regularization.
Furthermore, we introduce a random patch-based self-training framework that
utilizes unlabeled DSA sequences to improve segmentation performance. Our
extensive experiments on the DIAS dataset demonstrate the effectiveness of
these methods as potential baselines for future research and clinical
applications.


------------------------------------------------------------------------------

Title:
Bullying10K: A Neuromorphic Dataset towards Privacy-Preserving Bullying  Recognition

Abstract: The prevalence of violence in daily life poses significant threats to
individuals' physical and mental well-being. Using surveillance cameras in
public spaces has proven effective in proactively deterring and preventing such
incidents. However, concerns regarding privacy invasion have emerged due to
their widespread deployment. To address the problem, we leverage Dynamic Vision
Sensors (DVS) cameras to detect violent incidents and preserve privacy since it
captures pixel brightness variations instead of static imagery. We introduce
the Bullying10K dataset, encompassing various actions, complex movements, and
occlusions from real-life scenarios. It provides three benchmarks for
evaluating different tasks: action recognition, temporal action localization,
and pose estimation. With 10,000 event segments, totaling 12 billion events and
255 GB of data, Bullying10K contributes significantly by balancing violence
detection and personal privacy persevering. And it also poses a challenge to
the neuromorphic dataset. It will serve as a valuable resource for training and
developing privacy-protecting video systems. The Bullying10K opens new
possibilities for innovative approaches in these domains.


------------------------------------------------------------------------------

Title:
Deep Level-set Method for Stefan Problems

Abstract: We propose a level-set approach to characterize the region occupied by the
solid in Stefan problems with and without surface tension, based on their
recent probabilistic reformulation. The level-set function is parameterized by
a feed-forward neural network, whose parameters are trained using the
probabilistic formulation of the Stefan growth condition. The algorithm can
handle Stefan problems where the liquid is supercooled and can capture surface
tension effects through the simulation of particles along the moving boundary
together with an efficient approximation of the mean curvature. We demonstrate
the effectiveness of the method on a variety of examples with and without
radial symmetry.


------------------------------------------------------------------------------

Title:
Event Stream GPT: A Data Pre-processing and Modeling Library for  Generative, Pre-trained Transformers over Continuous-time Sequences of  Complex Events

Abstract: Generative, pre-trained transformers (GPTs, a.k.a. "Foundation Models") have
reshaped natural language processing (NLP) through their versatility in diverse
downstream tasks. However, their potential extends far beyond NLP. This paper
provides a software utility to help realize this potential, extending the
applicability of GPTs to continuous-time sequences of complex events with
internal dependencies, such as medical record datasets. Despite their
potential, the adoption of foundation models in these domains has been hampered
by the lack of suitable tools for model construction and evaluation. To bridge
this gap, we introduce Event Stream GPT (ESGPT), an open-source library
designed to streamline the end-to-end process for building GPTs for
continuous-time event sequences. ESGPT allows users to (1) build flexible,
foundation-model scale input datasets by specifying only a minimal
configuration file, (2) leverage a Hugging Face compatible modeling API for
GPTs over this modality that incorporates intra-event causal dependency
structures and autoregressive generation capabilities, and (3) evaluate models
via standardized processes that can assess few and even zero-shot performance
of pre-trained models on user-specified fine-tuning tasks.


------------------------------------------------------------------------------

Title:
Dynamic image reconstruction with motion priors in application to 3D  magnetic particle imaging

Abstract: Various imaging modalities allow for time-dependent image reconstructions
from measurements where its acquisition also has a time-dependent nature.
Magnetic particle imaging (MPI) falls into this class of imaging modalities and
it thus also provides a dynamic inverse problem. Without proper consideration
of the dynamic behavior, motion artifacts in the reconstruction become an
issue. More sophisticated methods need to be developed and applied to the
reconstruction of the time-dependent sequences of images. In this context, we
investigate the incorporation of motion priors in terms of certain
flow-parameter-dependent PDEs in the reconstruction process of time-dependent
3D images in magnetic particle imaging. The present work comprises the method
development for a general 3D+time setting for time-dependent linear forward
operators, analytical investigation of necessary properties in the MPI forward
operator, modeling aspects in dynamic MPI, and extensive numerical experiments
on 3D+time imaging including simulated data as well as measurements from a
rotation phantom and in-vivo data from a mouse.


------------------------------------------------------------------------------

Title:
CATS: A Pragmatic Chinese Answer-to-Sequence Dataset with Large Scale  and High Quality

Abstract: There are three problems existing in the popular data-to-text datasets.
First, the large-scale datasets either contain noise or lack real application
scenarios. Second, the datasets close to real applications are relatively small
in size. Last, current datasets bias in the English language while leaving
other languages underexplored. To alleviate these limitations, in this paper,
we present CATS, a pragmatic Chinese answer-to-sequence dataset with large
scale and high quality. The dataset aims to generate textual descriptions for
the answer in the practical TableQA system. Further, to bridge the structural
gap between the input SQL and table and establish better semantic alignments,
we propose a Unified Graph Transformation approach to establish a joint
encoding space for the two hybrid knowledge resources and convert this task to
a graph-to-text problem. The experiment results demonstrate the effectiveness
of our proposed method. Further analysis on CATS attests to both the high
quality and challenges of the dataset.


------------------------------------------------------------------------------

Title:
Deep Double Self-Expressive Subspace Clustering

Abstract: Deep subspace clustering based on auto-encoder has received wide attention.
However, most subspace clustering based on auto-encoder does not utilize the
structural information in the self-expressive coefficient matrix, which limits
the clustering performance. In this paper, we propose a double self-expressive
subspace clustering algorithm. The key idea of our solution is to view the
self-expressive coefficient as a feature representation of the example to get
another coefficient matrix. Then, we use the two coefficient matrices to
construct the affinity matrix for spectral clustering. We find that it can
reduce the subspace-preserving representation error and improve connectivity.
To further enhance the clustering performance, we proposed a self-supervised
module based on contrastive learning, which can further improve the performance
of the trained network. Experiments on several benchmark datasets demonstrate
that the proposed algorithm can achieve better clustering than state-of-the-art
methods.


------------------------------------------------------------------------------

Title:
A Survey on Automated Software Vulnerability Detection Using Machine  Learning and Deep Learning

Abstract: Software vulnerability detection is critical in software security because it
identifies potential bugs in software systems, enabling immediate remediation
and mitigation measures to be implemented before they may be exploited.
Automatic vulnerability identification is important because it can evaluate
large codebases more efficiently than manual code auditing. Many Machine
Learning (ML) and Deep Learning (DL) based models for detecting vulnerabilities
in source code have been presented in recent years. However, a survey that
summarises, classifies, and analyses the application of ML/DL models for
vulnerability detection is missing. It may be difficult to discover gaps in
existing research and potential for future improvement without a comprehensive
survey. This could result in essential areas of research being overlooked or
under-represented, leading to a skewed understanding of the state of the art in
vulnerability detection. This work address that gap by presenting a systematic
survey to characterize various features of ML/DL-based source code level
software vulnerability detection approaches via five primary research questions
(RQs). Specifically, our RQ1 examines the trend of publications that leverage
ML/DL for vulnerability detection, including the evolution of research and the
distribution of publication venues. RQ2 describes vulnerability datasets used
by existing ML/DL-based models, including their sources, types, and
representations, as well as analyses of the embedding techniques used by these
approaches. RQ3 explores the model architectures and design assumptions of
ML/DL-based vulnerability detection approaches. RQ4 summarises the type and
frequency of vulnerabilities that are covered by existing studies. Lastly, RQ5
presents a list of current challenges to be researched and an outline of a
potential research roadmap that highlights crucial opportunities for future
work.


------------------------------------------------------------------------------

Title:
3D Keypoint Estimation Using Implicit Representation Learning

Abstract: In this paper, we tackle the challenging problem of 3D keypoint estimation of
general objects using a novel implicit representation. Previous works have
demonstrated promising results for keypoint prediction through direct
coordinate regression or heatmap-based inference. However, these methods are
commonly studied for specific subjects, such as human bodies and faces, which
possess fixed keypoint structures. They also suffer in several practical
scenarios where explicit or complete geometry is not given, including images
and partial point clouds. Inspired by the recent success of advanced implicit
representation in reconstruction tasks, we explore the idea of using an
implicit field to represent keypoints. Specifically, our key idea is employing
spheres to represent 3D keypoints, thereby enabling the learnability of the
corresponding signed distance field. Explicit keypoints can be extracted
subsequently by our algorithm based on the Hough transform. Quantitative and
qualitative evaluations also show the superiority of our representation in
terms of prediction accuracy.


------------------------------------------------------------------------------

Title:
Composition of nested embeddings with an application to outlier removal

Abstract: We study the design of embeddings into Euclidean space with outliers. Given a
metric space $(X,d)$ and an integer $k$, the goal is to embed all but $k$
points in $X$ (called the "outliers") into $\ell_2$ with the smallest possible
distortion $c$. Finding the optimal distortion $c$ for a given outlier set size
$k$, or alternately the smallest $k$ for a given target distortion $c$ are both
NP-hard problems. In fact, it is UGC-hard to approximate $k$ to within a factor
smaller than $2$ even when the metric sans outliers is isometrically embeddable
into $\ell_2$. We consider bi-criteria approximations. Our main result is a
polynomial time algorithm that approximates the outlier set size to within an
$O(\log^4 k)$ factor and the distortion to within a constant factor.
The main technical component in our result is an approach for constructing a
composition of two given embeddings from subsets of $X$ into $\ell_2$ which
inherits the distortions of each to within small multiplicative factors.
Specifically, given a low $c_S$ distortion embedding from $S\subset X$ into
$\ell_2$ and a high(er) $c_X$ distortion embedding from the entire set $X$ into
$\ell_2$, we construct a single embedding that achieves the same distortion
$c_S$ over pairs of points in $S$ and an expansion of at most $O(\log k)\cdot
c_X$ over the remaining pairs of points, where $k=|X\setminus S|$. Our
composition theorem extends to embeddings into arbitrary $\ell_p$ metrics for
$p\ge 1$, and may be of independent interest. While unions of embeddings over
disjoint sets have been studied previously, to our knowledge, this is the first
work to consider compositions of nested embeddings.


------------------------------------------------------------------------------

Title:
Explicit Syntactic Guidance for Neural Text Generation

Abstract: Most existing text generation models follow the sequence-to-sequence
paradigm. Generative Grammar suggests that humans generate natural language
texts by learning language grammar. We propose a syntax-guided generation
schema, which generates the sequence guided by a constituency parse tree in a
top-down direction. The decoding process can be decomposed into two parts: (1)
predicting the infilling texts for each constituent in the lexicalized syntax
context given the source sentence; (2) mapping and expanding each constituent
to construct the next-level syntax context. Accordingly, we propose a
structural beam search method to find possible syntax structures
hierarchically. Experiments on paraphrase generation and machine translation
show that the proposed method outperforms autoregressive baselines, while also
demonstrating effectiveness in terms of interpretability, controllability, and
diversity.


------------------------------------------------------------------------------

Title:
Computing a human-like reaction time metric from stable recurrent vision  models

Abstract: The meteoric rise in the adoption of deep neural networks as computational
models of vision has inspired efforts to "align" these models with humans. One
dimension of interest for alignment includes behavioral choices, but moving
beyond characterizing choice patterns to capturing temporal aspects of visual
decision-making has been challenging. Here, we sketch a general-purpose
methodology to construct computational accounts of reaction times from a
stimulus-computable, task-optimized model. Specifically, we introduce a novel
metric leveraging insights from subjective logic theory summarizing evidence
accumulation in recurrent vision models. We demonstrate that our metric aligns
with patterns of human reaction times for stimulus manipulations across four
disparate visual decision-making tasks spanning perceptual grouping, mental
simulation, and scene categorization. This work paves the way for exploring the
temporal alignment of model and human visual strategies in the context of
various other cognitive tasks toward generating testable hypotheses for
neuroscience.


------------------------------------------------------------------------------

Title:
A Comparative Audit of Privacy Policies from Healthcare Organizations in  USA, UK and India

Abstract: Data privacy in healthcare is of paramount importance (and thus regulated
using laws like HIPAA) due to the highly sensitive nature of patient data. To
that end, healthcare organizations mention how they collect/process/store/share
this data (i.e., data practices) via their privacy policies. Thus there is a
need to audit these policies and check compliance with respective laws. This
paper addresses this need and presents a large-scale data-driven study to audit
privacy policies from healthcare organizations in three countries -- USA, UK,
and India.
We developed a three-stage novel \textit{workflow} for our audit. First, we
collected the privacy policies of thousands of healthcare organizations in
these countries and cleaned this privacy policy data using a clustering-based
mixed-method technique. We identified data practices regarding users' private
medical data (medical history) and site privacy (cookie, logs) in these
policies. Second, we adopted a summarization-based technique to uncover exact
broad data practices across countries and notice important differences.
Finally, we evaluated the cross-country data practices using the lens of legal
compliance (with legal expert feedback) and grounded in the theory of
Contextual Integrity (CI). Alarmingly, we identified six themes of
non-alignment (observed in 21.8\% of data practices studied in India) pointed
out by our legal experts. Furthermore, there are four \textit{potential
violations} according to case verdicts from Indian Courts as pointed out by our
legal experts. We conclude this paper by discussing the utility of our auditing
workflow and the implication of our findings for different stakeholders.


------------------------------------------------------------------------------

Title:
TrustGPT: A Benchmark for Trustworthy and Responsible Large Language  Models

Abstract: Large Language Models (LLMs) such as ChatGPT, have gained significant
attention due to their impressive natural language processing capabilities. It
is crucial to prioritize human-centered principles when utilizing these models.
Safeguarding the ethical and moral compliance of LLMs is of utmost importance.
However, individual ethical issues have not been well studied on the latest
LLMs. Therefore, this study aims to address these gaps by introducing a new
benchmark -- TrustGPT. TrustGPT provides a comprehensive evaluation of LLMs in
three crucial areas: toxicity, bias, and value-alignment. Initially, TrustGPT
examines toxicity in language models by employing toxic prompt templates
derived from social norms. It then quantifies the extent of bias in models by
measuring quantifiable toxicity values across different groups. Lastly,
TrustGPT assesses the value of conversation generation models from both active
value-alignment and passive value-alignment tasks. Through the implementation
of TrustGPT, this research aims to enhance our understanding of the performance
of conversation generation models and promote the development of language
models that are more ethical and socially responsible.


------------------------------------------------------------------------------

Title:
Sound reconstruction from human brain activity via a generative model  with brain-like auditory features

Abstract: The successful reconstruction of perceptual experiences from human brain
activity has provided insights into the neural representations of sensory
experiences. However, reconstructing arbitrary sounds has been avoided due to
the complexity of temporal sequences in sounds and the limited resolution of
neuroimaging modalities. To overcome these challenges, leveraging the
hierarchical nature of brain auditory processing could provide a path toward
reconstructing arbitrary sounds. Previous studies have indicated a hierarchical
homology between the human auditory system and deep neural network (DNN)
models. Furthermore, advancements in audio-generative models enable to
transform compressed representations back into high-resolution sounds. In this
study, we introduce a novel sound reconstruction method that combines brain
decoding of auditory features with an audio-generative model. Using fMRI
responses to natural sounds, we found that the hierarchical sound features of a
DNN model could be better decoded than spectrotemporal features. We then
reconstructed the sound using an audio transformer that disentangled compressed
temporal information in the decoded DNN features. Our method shows
unconstrained sounds reconstruction capturing sound perceptual contents and
quality and generalizability by reconstructing sound categories not included in
the training dataset. Reconstructions from different auditory regions remain
similar to actual sounds, highlighting the distributed nature of auditory
representations. To see whether the reconstructions mirrored actual subjective
perceptual experiences, we performed an experiment involving selective auditory
attention to one of overlapping sounds. The results tended to resemble the
attended sound than the unattended. These findings demonstrate that our
proposed model provides a means to externalize experienced auditory contents
from human brain activity.


------------------------------------------------------------------------------

Title:
Comprehensive Training and Evaluation on Deep Reinforcement Learning for  Automated Driving in Various Simulated Driving Maneuvers

Abstract: Developing and testing automated driving models in the real world might be
challenging and even dangerous, while simulation can help with this, especially
for challenging maneuvers. Deep reinforcement learning (DRL) has the potential
to tackle complex decision-making and controlling tasks through learning and
interacting with the environment, thus it is suitable for developing automated
driving while not being explored in detail yet. This study carried out a
comprehensive study by implementing, evaluating, and comparing the two DRL
algorithms, Deep Q-networks (DQN) and Trust Region Policy Optimization (TRPO),
for training automated driving on the highway-env simulation platform.
Effective and customized reward functions were developed and the implemented
algorithms were evaluated in terms of onlane accuracy (how well the car drives
on the road within the lane), efficiency (how fast the car drives), safety (how
likely the car is to crash into obstacles), and comfort (how much the car makes
jerks, e.g., suddenly accelerates or brakes). Results show that the TRPO-based
models with modified reward functions delivered the best performance in most
cases. Furthermore, to train a uniform driving model that can tackle various
driving maneuvers besides the specific ones, this study expanded the
highway-env and developed an extra customized training environment, namely,
ComplexRoads, integrating various driving maneuvers and multiple road scenarios
together. Models trained on the designed ComplexRoads environment can adapt
well to other driving maneuvers with promising overall performance. Lastly,
several functionalities were added to the highway-env to implement this work.
The codes are open on GitHub at this https URL


------------------------------------------------------------------------------

Title:
Query Encoder Distillation via Embedding Alignment is a Strong Baseline  Method to Boost Dense Retriever Online Efficiency

Abstract: The information retrieval community has made significant progress in
improving the efficiency of Dual Encoder (DE) dense passage retrieval systems,
making them suitable for latency-sensitive settings. However, many proposed
procedures are often too complex or resource-intensive, which makes it
difficult for practitioners to adopt them or identify sources of empirical
gains. Therefore, in this work, we propose a trivially simple recipe to serve
as a baseline method for boosting the efficiency of DE retrievers leveraging an
asymmetric architecture. Our results demonstrate that even a 2-layer,
BERT-based query encoder can still retain 92.5% of the full DE performance on
the BEIR benchmark via unsupervised distillation and proper student
initialization. We hope that our findings will encourage the community to
re-evaluate the trade-offs between method complexity and performance
improvements.


------------------------------------------------------------------------------

Title:
HomeRobot: Open-Vocabulary Mobile Manipulation

Abstract: HomeRobot (noun): An affordable compliant robot that navigates homes and
manipulates a wide range of objects in order to complete everyday tasks.
Open-Vocabulary Mobile Manipulation (OVMM) is the problem of picking any object
in any unseen environment, and placing it in a commanded location. This is a
foundational challenge for robots to be useful assistants in human
environments, because it involves tackling sub-problems from across robotics:
perception, language understanding, navigation, and manipulation are all
essential to OVMM. In addition, integration of the solutions to these
sub-problems poses its own substantial challenges. To drive research in this
area, we introduce the HomeRobot OVMM benchmark, where an agent navigates
household environments to grasp novel objects and place them on target
receptacles. HomeRobot has two components: a simulation component, which uses a
large and diverse curated object set in new, high-quality multi-room home
environments; and a real-world component, providing a software stack for the
low-cost Hello Robot Stretch to encourage replication of real-world experiments
across labs. We implement both reinforcement learning and heuristic
(model-based) baselines and show evidence of sim-to-real transfer. Our
baselines achieve a 20% success rate in the real world; our experiments
identify ways future research work improve performance. See videos on our
website: this https URL


------------------------------------------------------------------------------

Title:
Multi-Scale Occ: 4th Place Solution for CVPR 2023 3D Occupancy  Prediction Challenge

Abstract: In this report, we present the 4th place solution for CVPR 2023 3D occupancy
prediction challenge. We propose a simple method called Multi-Scale Occ for
occupancy prediction based on lift-splat-shoot framework, which introduces
multi-scale image features for generating better multi-scale 3D voxel features
with temporal fusion of multiple past frames. Post-processing including model
ensemble, test-time augmentation, and class-wise thresh are adopted to further
boost the final performance. As shown on the leaderboard, our proposed
occupancy prediction method ranks the 4th place with 49.36 mIoU.


------------------------------------------------------------------------------

Title:
Energy-efficient superparamagnetic Ising machine and its application to  traveling salesman problems

Abstract: The growth of artificial intelligence and IoT has created a significant
computational load for solving non-deterministic polynomial-time (NP)-hard
problems, which are difficult to solve using conventional computers. The Ising
computer, based on the Ising model and annealing process, has been highly
sought for finding approximate solutions to NP-hard problems by observing the
convergence of dynamic spin states. However, it faces several challenges,
including high power consumption due to artificial spins and randomness
emulated by complex circuits, as well as low scalability caused by the rapidly
growing connectivity when considering large-scale problems. Here, we present an
experimental Ising annealing computer based on superparamagnetic tunnel
junctions (SMTJs) with all-to-all connections, which successfully solves a
70-city travelling salesman problem (4761-node Ising problem). By taking
advantage of the intrinsic randomness of SMTJs, implementing a proper global
annealing scheme, and using an efficient algorithm, our SMTJ-based Ising
annealer shows superior performance in terms of power consumption and energy
efficiency compared to other Ising schemes. Additionally, our approach provides
a promising way to solve complex problems with limited hardware resources.
Moreover, we propose a crossbar array architecture for scalable integration
using conventional magnetic random access memories. Our results demonstrate
that the SMTJ-based Ising annealing computer with high energy efficiency,
speed, and scalability is a strong candidate for future unconventional
computing schemes.


------------------------------------------------------------------------------

Title:
SALSA VERDE: a machine learning attack on Learning With Errors with  sparse small secrets

Abstract: Learning with Errors (LWE) is a hard math problem used in post-quantum
cryptography. Homomorphic Encryption (HE) schemes rely on the hardness of the
LWE problem for their security, and two LWE-based cryptosystems were recently
standardized by NIST for digital signatures and key exchange (KEM). Thus, it is
critical to continue assessing the security of LWE and specific parameter
choices. For example, HE uses small secrets, and the HE community has
considered standardizing small sparse secrets to improve efficiency and
functionality. However, prior work, SALSA and PICANTE, showed that ML attacks
can recover sparse binary secrets. Building on these, we propose VERDE, an
improved ML attack that can recover sparse binary, ternary, and small Gaussian
secrets. Using improved preprocessing and secret recovery techniques, VERDE can
attack LWE with larger dimensions ($n=512$) and smaller moduli ($\log_2 q=12$
for $n=256$), using less time and power. We propose novel architectures for
scaling. Finally, we develop a theory that explains the success of ML LWE
attacks.


------------------------------------------------------------------------------

Title:
Max-convolution through numerics and tropical geometry

Abstract: The maximum function, on vectors of real numbers, is not differentiable.
Consequently, several differentiable approximations of this function are
popular substitutes. We survey three smooth functions which approximate the
maximum function and analyze their convergence rates. We interpret these
functions through the lens of tropical geometry, where their performance
differences are geometrically salient. As an application, we provide an
algorithm which computes the max-convolution of two integer vectors in
quasi-linear time. We show this algorithm's power in computing adjacent sums
within a vector as well as computing service curves in a network analysis
application.


------------------------------------------------------------------------------

Title:
Graph Neural Stochastic Differential Equations for Learning Brownian  Dynamics

Abstract: Neural networks (NNs) that exploit strong inductive biases based on physical
laws and symmetries have shown remarkable success in learning the dynamics of
physical systems directly from their trajectory. However, these works focus
only on the systems that follow deterministic dynamics, for instance, Newtonian
or Hamiltonian dynamics. Here, we propose a framework, namely Brownian graph
neural networks (BROGNET), combining stochastic differential equations (SDEs)
and GNNs to learn Brownian dynamics directly from the trajectory. We
theoretically show that BROGNET conserves the linear momentum of the system,
which in turn, provides superior performance on learning dynamics as revealed
empirically. We demonstrate this approach on several systems, namely, linear
spring, linear spring with binary particle types, and non-linear spring
systems, all following Brownian dynamics at finite temperatures. We show that
BROGNET significantly outperforms proposed baselines across all the benchmarked
Brownian systems. In addition, we demonstrate zero-shot generalizability of
BROGNET to simulate unseen system sizes that are two orders of magnitude larger
and to different temperatures than those used during training. Altogether, our
study contributes to advancing the understanding of the intricate dynamics of
Brownian motion and demonstrates the effectiveness of graph neural networks in
modeling such complex systems.


------------------------------------------------------------------------------

Title:
Informed POMDP: Leveraging Additional Information in Model-Based RL

Abstract: In this work, we generalize the problem of learning through interaction in a
POMDP by accounting for eventual additional information available at training
time. First, we introduce the informed POMDP, a new learning paradigm offering
a clear distinction between the training information and the execution
observation. Next, we propose an objective for learning a sufficient statistic
from the history for the optimal control that leverages this information. We
then show that this informed objective consists of learning an environment
model from which we can sample latent trajectories. Finally, we show for the
Dreamer algorithm that the convergence speed of the policies is sometimes
greatly improved on several environments by using this informed environment
model. Those results and the simplicity of the proposed adaptation advocate for
a systematic consideration of eventual additional information when learning in
a POMDP using model-based RL.


------------------------------------------------------------------------------

Title:
An Analysis of Twitter Discourse on the War Between Russia and Ukraine

Abstract: On the 21st of February 2022, Russia recognised the Donetsk People's Republic
and the Luhansk People's Republic, three days before launching an invasion of
Ukraine. Since then, an active debate has taken place on social media, mixing
organic discussions with coordinated information campaigns. The scale of this
discourse, alongside the role that information warfare has played in the
invasion, make it vital to better understand this ecosystem. We therefore
present a study of pro-Ukrainian vs. pro-Russian discourse through the lens of
Twitter. We do so from two perspectives: (i) the content that is shared; and
(ii) the users who participate in the sharing. We first explore the scale and
nature of conversations, including analysis of hashtags, toxicity and media
sharing. We then study the users who drive this, highlighting a significant
presence of new users and bots.


------------------------------------------------------------------------------

Title:
Polytope: An Algorithm for Efficient Feature Extraction on Hypercubes

Abstract: Data extraction algorithms on data hypercubes, or datacubes, are
traditionally only capable of cutting boxes of data along the datacube axes.
For many use cases however, this is not a sufficient approach and returns more
data than users might actually need. This not only forces users to apply
post-processing after extraction, but more importantly this consumes more I/O
resources than is necessary. When considering very large datacubes from which
users only want to extract small non-rectangular subsets, the box approach does
not scale well. Indeed, with this traditional approach, I/O systems quickly
reach capacity, trying to read and return unwanted data to users. In this
paper, we propose a novel technique, based on computational geometry concepts,
which instead carefully pre-selects the precise bytes of data which the user
needs in order to then only read those from the datacube. As we discuss later
on, this novel extraction method will considerably help scale access to large
petabyte size data hypercubes in a variety of scientific fields.


------------------------------------------------------------------------------

Title:
KiUT: Knowledge-injected U-Transformer for Radiology Report Generation

Abstract: Radiology report generation aims to automatically generate a clinically
accurate and coherent paragraph from the X-ray image, which could relieve
radiologists from the heavy burden of report writing. Although various image
caption methods have shown remarkable performance in the natural image field,
generating accurate reports for medical images requires knowledge of multiple
modalities, including vision, language, and medical terminology. We propose a
Knowledge-injected U-Transformer (KiUT) to learn multi-level visual
representation and adaptively distill the information with contextual and
clinical knowledge for word prediction. In detail, a U-connection schema
between the encoder and decoder is designed to model interactions between
different modalities. And a symptom graph and an injected knowledge distiller
are developed to assist the report generation. Experimentally, we outperform
state-of-the-art methods on two widely used benchmark datasets: IU-Xray and
MIMIC-CXR. Further experimental results prove the advantages of our
architecture and the complementary benefits of the injected knowledge.


------------------------------------------------------------------------------

Title:
Inter-Cell Network Slicing With Transfer Learning Empowered Multi-Agent  Deep Reinforcement Learning

Abstract: Network slicing enables operators to efficiently support diverse applications
on a common physical infrastructure. The ever-increasing densification of
network deployment leads to complex and non-trivial inter-cell interference,
which requires more than inaccurate analytic models to dynamically optimize
resource management for network slices. In this paper, we develop a DIRP
algorithm with multiple deep reinforcement learning (DRL) agents to
cooperatively optimize resource partition in individual cells to fulfill the
requirements of each slice, based on two alternative reward functions.
Nevertheless, existing DRL approaches usually tie the pretrained model
parameters to specific network environments with poor transferability, which
raises practical deployment concerns in large-scale mobile networks. Hence, we
design a novel transfer learning-aided DIRP (TL-DIRP) algorithm to ease the
transfer of DIRP agents across different network environments in terms of
sample efficiency, model reproducibility, and algorithm scalability. The
TL-DIRP algorithm first centrally trains a generalized model and then transfers
the "generalist" to each local agent as "specialist" with distributed
finetuning and execution. TL-DIRP consists of two steps: 1) centralized
training of a generalized distributed model, 2) transferring the "generalist"
to each "specialist" with distributed finetuning and execution. The numerical
results show that not only DIRP outperforms existing baseline approaches in
terms of faster convergence and higher reward, but more importantly, TL-DIRP
significantly improves the service performance, with reduced exploration cost,
accelerated convergence rate, and enhanced model reproducibility. As compared
to a traffic-aware baseline, TL-DIRP provides about 15% less violation ratio of
the quality of service (QoS) for the worst slice service and 8.8% less
violation on the average service QoS.


------------------------------------------------------------------------------

Title:
FedNoisy: Federated Noisy Label Learning Benchmark

Abstract: Federated learning has gained popularity for distributed learning without
aggregating sensitive data from clients. But meanwhile, the distributed and
isolated nature of data isolation may be complicated by data quality, making it
more vulnerable to noisy labels. Many efforts exist to defend against the
negative impacts of noisy labels in centralized or federated settings. However,
there is a lack of a benchmark that comprehensively considers the impact of
noisy labels in a wide variety of typical FL settings. In this work, we serve
the first standardized benchmark that can help researchers fully explore
potential federated noisy settings. Also, we conduct comprehensive experiments
to explore the characteristics of these data settings and unravel challenging
scenarios on the federated noisy label learning, which may guide method
development in the future. We highlight the 20 basic settings for more than 5
datasets proposed in our benchmark and standardized simulation pipeline for
federated noisy label learning. We hope this benchmark can facilitate idea
verification in federated learning with noisy labels. \texttt{FedNoisy} is
available at \codeword{this https URL}.


------------------------------------------------------------------------------

Title:
Delegated Classification

Abstract: When machine learning is outsourced to a rational agent, conflicts of
interest might arise and severely impact predictive performance. In this work,
we propose a theoretical framework for incentive-aware delegation of machine
learning tasks. We model delegation as a principal-agent game, in which
accurate learning can be incentivized by the principal using performance-based
contracts. Adapting the economic theory of contract design to this setting, we
define budget-optimal contracts and prove they take a simple threshold form
under reasonable assumptions. In the binary-action case, the optimality of such
contracts is shown to be equivalent to the classic Neyman-Pearson lemma,
establishing a formal connection between contract design and statistical
hypothesis testing. Empirically, we demonstrate that budget-optimal contracts
can be constructed using small-scale data, leveraging recent advances in the
study of learning curves and scaling laws. Performance and economic outcomes
are evaluated using synthetic and real-world classification tasks.


------------------------------------------------------------------------------

Title:
Geometric particle-in-cell methods for Vlasov--Poisson equations with  Maxwell--Boltzmann electrons

Abstract: In this paper, variational and Hamiltonian formulations of the
Vlasov--Poisson equations with Maxwell--Boltzmann electrons are introduced.
Structure-preserving particle-in-cell methods are constructed by discretizing
the action integral and the Poisson bracket. We use the Hamiltonian splitting
methods and the discrete gradient methods for time discretizations to preserve
the geometric structure and energy, respectively. The global neutrality
condition is also conserved by the discretizations. The schemes are asymptotic
preserving when taking the quasi-neutral limit, and the limiting schemes are
structure-preserving for the limiting model. Numerical experiments of finite
grid instability, Landau damping, and two-stream instability illustrate the
behavior of the proposed numerical methods.


------------------------------------------------------------------------------

Title:
Annotation Cost Efficient Active Learning for Content Based Image  Retrieval

Abstract: Deep metric learning (DML) based methods have been found very effective for
content-based image retrieval (CBIR) in remote sensing (RS). For accurately
learning the model parameters of deep neural networks, most of the DML methods
require a high number of annotated training images, which can be costly to
gather. To address this problem, in this paper we present an annotation cost
efficient active learning (AL) method (denoted as ANNEAL). The proposed method
aims to iteratively enrich the training set by annotating the most informative
image pairs as similar or dissimilar, %answering a simple yes/no question,
while accurately modelling a deep metric space. This is achieved by two
consecutive steps. In the first step the pairwise image similarity is modelled
based on the available training set. Then, in the second step the most
uncertain and diverse (i.e., informative) image pairs are selected to be
annotated. Unlike the existing AL methods for CBIR, at each AL iteration of
ANNEAL a human expert is asked to annotate the most informative image pairs as
similar/dissimilar. This significantly reduces the annotation cost compared to
annotating images with land-use/land cover class labels. Experimental results
show the effectiveness of our method. The code of ANNEAL is publicly available
at this https URL


------------------------------------------------------------------------------

Title:
IMP-MARL: a Suite of Environments for Large-scale Infrastructure  Management Planning via MARL

Abstract: We introduce IMP-MARL, an open-source suite of multi-agent reinforcement
learning (MARL) environments for large-scale Infrastructure Management Planning
(IMP), offering a platform for benchmarking the scalability of cooperative MARL
methods in real-world engineering applications. In IMP, a multi-component
engineering system is subject to a risk of failure due to its components'
damage condition. Specifically, each agent plans inspections and repairs for a
specific system component, aiming to minimise maintenance costs while
cooperating to minimise system failure risk. With IMP-MARL, we release several
environments including one related to offshore wind structural systems, in an
effort to meet today's needs to improve management strategies to support
sustainable and reliable energy systems. Supported by IMP practical engineering
environments featuring up to 100 agents, we conduct a benchmark campaign, where
the scalability and performance of state-of-the-art cooperative MARL methods
are compared against expert-based heuristic policies. The results reveal that
centralised training with decentralised execution methods scale better with the
number of agents than fully centralised or decentralised RL approaches, while
also outperforming expert-based heuristic policies in most IMP environments.
Based on our findings, we additionally outline remaining cooperation and
scalability challenges that future MARL methods should still address. Through
IMP-MARL, we encourage the implementation of new environments and the further
development of MARL methods.


------------------------------------------------------------------------------

Title:
Blackbird language matrices (BLM), a new task for rule-like  generalization in neural networks: Motivations and Formal Specifications

Abstract: We motivate and formally define a new task for fine-tuning rule-like
generalization in large language models. It is conjectured that the
shortcomings of current LLMs are due to a lack of ability to generalize. It has
been argued that, instead, humans are better at generalization because they
have a tendency at extracting rules from complex data. We try to recreate this
tendency to rule-based generalization. When exposed to tests of analytic
intelligence, for example, the visual RAVEN IQ test, human problem-solvers
identify the relevant objects in the picture and their relevant attributes and
reason based on rules applied to these objects and attributes. Based on the
induced rules, they are able to provide a solution to the test. We propose a
task that translates this IQ task into language. In this paper, we provide the
formal specification for the task and the generative process of its datasets.


------------------------------------------------------------------------------

Title:
Software Engineers' Questions and Answers on Stack Exchange

Abstract: There exists a large number of research works analyzing questions and answers
on the popular Stack Overflow website. However, other sub-sites of the Stack
Exchange platform are studied rarely. In this paper, we analyze the questions
and answers on the Software Engineering Stack Exchange site that encompasses a
broader set of areas, such as testing or software processes. Topics and
quantities of the questions, historical trends, and the authors' sentiment were
analyzed using downloaded datasets. We found that the asked questions are most
frequently related to database systems, quality assurance, and agile software
development. The most attractive topics were career and teamwork problems, and
the least attractive ones were network programming and software modeling.
Historically, the topic of domain-driven design recorded the highest rise, and
jobs and career the most significant fall. The number of new questions dropped,
while the portion of unanswered ones increased.


------------------------------------------------------------------------------

Title:
Lessons learned from a performance analysis and optimization of a  multiscale cellular simulation

Abstract: This work presents a comprehensive performance analysis and optimization of a
multiscale agent-based cellular simulation. The optimizations applied are
guided by detailed performance analysis and include memory management, load
balance, and a locality-aware parallelization. The outcome of this paper is not
only the speedup of 2.4x achieved by the optimized version with respect to the
original PhysiCell code, but also the lessons learned and best practices when
developing parallel HPC codes to obtain efficient and highly performant
applications, especially in the computational biology field.


------------------------------------------------------------------------------

Title:
On Evaluating Multilingual Compositional Generalization with Translated  Datasets

Abstract: Compositional generalization allows efficient learning and human-like
inductive biases. Since most research investigating compositional
generalization in NLP is done on English, important questions remain
underexplored. Do the necessary compositional generalization abilities differ
across languages? Can models compositionally generalize cross-lingually? As a
first step to answering these questions, recent work used neural machine
translation to translate datasets for evaluating compositional generalization
in semantic parsing. However, we show that this entails critical semantic
distortion. To address this limitation, we craft a faithful rule-based
translation of the MCWQ dataset from English to Chinese and Japanese. Even with
the resulting robust benchmark, which we call MCWQ-R, we show that the
distribution of compositions still suffers due to linguistic divergences, and
that multilingual models still struggle with cross-lingual compositional
generalization. Our dataset and methodology will be useful resources for the
study of cross-lingual compositional generalization in other tasks.


------------------------------------------------------------------------------

Title:
The Cultivated Practices of Text-to-Image Generation

Abstract: Humankind is entering a novel creative era in which anybody can synthesize
digital information using generative artificial intelligence (AI).
Text-to-image generation, in particular, has become vastly popular and millions
of practitioners produce AI-generated images and AI art online. This chapter
first gives an overview of the key developments that enabled a healthy
co-creative online ecosystem around text-to-image generation to rapidly emerge,
followed by a high-level description of key elements in this ecosystem. A
particular focus is placed on prompt engineering, a creative practice that has
been embraced by the AI art community. It is then argued that the emerging
co-creative ecosystem constitutes an intelligent system on its own - a system
that both supports human creativity, but also potentially entraps future
generations and limits future development efforts in AI. The chapter discusses
the potential risks and dangers of cultivating this co-creative ecosystem, such
as the bias inherent in today's training data, potential quality degradation in
future image generation systems due to synthetic data becoming common place,
and the potential long-term effects of text-to-image generation on people's
imagination, ambitions, and development.


------------------------------------------------------------------------------

Title:
Learning to Model and Plan for Wheeled Mobility on Vertically  Challenging Terrain

Abstract: Most autonomous navigation systems assume wheeled robots are rigid bodies and
their 2D planar workspaces can be divided into free spaces and obstacles.
However, recent wheeled mobility research, showing that wheeled platforms have
the potential of moving over vertically challenging terrain (e.g., rocky
outcroppings, rugged boulders, and fallen tree trunks), invalidate both
assumptions. Navigating off-road vehicle chassis with long suspension travel
and low tire pressure in places where the boundary between obstacles and free
spaces is blurry requires precise 3D modeling of the interaction between the
chassis and the terrain, which is complicated by suspension and tire
deformation, varying tire-terrain friction, vehicle weight distribution and
momentum, etc. In this paper, we present a learning approach to model wheeled
mobility, i.e., in terms of vehicle-terrain forward dynamics, and plan
feasible, stable, and efficient motion to drive over vertically challenging
terrain without rolling over or getting stuck. We present physical experiments
on two wheeled robots and show that planning using our learned model can
achieve up to 60% improvement in navigation success rate and 46% reduction in
unstable chassis roll and pitch angles.


------------------------------------------------------------------------------

Title:
Observation Routes and External Watchman Routes

Abstract: We introduce the Observation Route Problem ($\textsf{ORP}$) defined as
follows: Given a set of $n$ pairwise disjoint compact regions in the plane,
find a shortest tour (route) such that an observer walking along this tour can
see (observe) some point in each region from some point of the tour. The
observer does \emph{not} need to see the entire boundary of an object. The tour
is \emph{not} allowed to intersect the interior of any region (i.e., the
regions are obstacles and therefore out of bounds). The problem exhibits
similarity to both the Traveling Salesman Problem with Neighborhoods
($\textsf{TSPN}$) and the External Watchman Route Problem ($\textsf{EWRP}$). We
distinguish two variants: the range of visibility is either limited to a
bounding rectangle, or unlimited. We obtain the following results:
(I) Given a family of $n$ disjoint convex bodies in the plane, computing a
shortest observation route does not admit a $(c\log n)$-approximation unless
$\textsf{P} = \textsf{NP}$ for an absolute constant $c>0$. (This holds for both
limited and unlimited vision.)
(II) Given a family of disjoint convex bodies in the plane, computing a
shortest external watchman route is $\textsf{NP}$-hard. (This holds for both
limited and unlimited vision; and even for families of axis-aligned squares.)
(III) Given a family of $n$ disjoint fat convex polygons, an observation tour
whose length is at most $O(\log{n})$ times the optimal can be computed in
polynomial time. (This holds for limited vision.)
(IV) For every $n \geq 5$, there exists a convex polygon with $n$ sides and
all angles obtuse such that its perimeter is \emph{not} a shortest external
watchman route. This refutes a conjecture by Absar and Whitesides (2006).


------------------------------------------------------------------------------

Title:
Pushing the Limits of 3D Shape Generation at Scale

Abstract: We present a significant breakthrough in 3D shape generation by scaling it to
unprecedented dimensions. Through the adaptation of the Auto-Regressive model
and the utilization of large language models, we have developed a remarkable
model with an astounding 3.6 billion trainable parameters, establishing it as
the largest 3D shape generation model to date, named Argus-3D. Our approach
addresses the limitations of existing methods by enhancing the quality and
diversity of generated 3D shapes. To tackle the challenges of high-resolution
3D shape generation, our model incorporates tri-plane features as latent
representations, effectively reducing computational complexity. Additionally,
we introduce a discrete codebook for efficient quantization of these
representations. Leveraging the power of transformers, we enable multi-modal
conditional generation, facilitating the production of diverse and visually
impressive 3D shapes. To train our expansive model, we leverage an ensemble of
publicly-available 3D datasets, consisting of a comprehensive collection of
approximately 900,000 objects from renowned repositories such as ModelNet40,
ShapeNet, Pix3D, 3D-Future, and Objaverse. This diverse dataset empowers our
model to learn from a wide range of object variations, bolstering its ability
to generate high-quality and diverse 3D shapes. Extensive experimentation
demonstrate the remarkable efficacy of our approach in significantly improving
the visual quality of generated 3D shapes. By pushing the boundaries of 3D
generation, introducing novel methods for latent representation learning, and
harnessing the power of transformers for multi-modal conditional generation,
our contributions pave the way for substantial advancements in the field. Our
work unlocks new possibilities for applications in gaming, virtual reality,
product design, and other domains that demand high-quality and diverse 3D
objects.


------------------------------------------------------------------------------

Title:
Pipeline for recording datasets and running neural networks on the Bela  embedded hardware platform

Abstract: Deploying deep learning models on embedded devices is an arduous task:
oftentimes, there exist no platform-specific instructions, and compilation
times can be considerably large due to the limited computational resources
available on-device. Moreover, many music-making applications demand real-time
inference. Embedded hardware platforms for audio, such as Bela, offer an entry
point for beginners into physical audio computing; however, the need for
cross-compilation environments and low-level software development tools for
deploying embedded deep learning models imposes high entry barriers on
non-expert users. We present a pipeline for deploying neural networks in the
Bela embedded hardware platform. In our pipeline, we include a tool to record a
multichannel dataset of sensor signals. Additionally, we provide a dockerised
cross-compilation environment for faster compilation. With this pipeline, we
aim to provide a template for programmers and makers to prototype and
experiment with neural networks for real-time embedded musical applications.


------------------------------------------------------------------------------

Title:
Fingerprinting and Building Large Reproducible Datasets

Abstract: Obtaining a relevant dataset is central to conducting empirical studies in
software engineering. However, in the context of mining software repositories,
the lack of appropriate tooling for large scale mining tasks hinders the
creation of new datasets. Moreover, limitations related to data sources that
change over time (e.g., code bases) and the lack of documentation of extraction
processes make it difficult to reproduce datasets over time. This threatens the
quality and reproducibility of empirical studies.
In this paper, we propose a tool-supported approach facilitating the creation
of large tailored datasets while ensuring their reproducibility. We leveraged
all the sources feeding the Software Heritage append-only archive which are
accessible through a unified programming interface to outline a reproducible
and generic extraction process. We propose a way to define a unique fingerprint
to characterize a dataset which, when provided to the extraction process,
ensures that the same dataset will be extracted.
We demonstrate the feasibility of our approach by implementing a prototype.
We show how it can help reduce the limitations researchers face when creating
or reproducing datasets.


------------------------------------------------------------------------------

Title:
A Collection of Simulated Event Logs for Fairness Assessment in Process  Mining

Abstract: The analysis of fairness in process mining is a significant aspect of
data-driven decision-making, yet the advancement in this field is constrained
due to the scarcity of event data that incorporates fairness considerations. To
bridge this gap, we present a collection of simulated event logs, spanning four
critical domains, which encapsulate a variety of discrimination scenarios. By
simulating these event logs with CPN Tools, we ensure data with known ground
truth, thereby offering a robust foundation for fairness analysis. These logs
are made freely available under the CC-BY-4.0 license and adhere to the XES
standard, thereby assuring broad compatibility with various process mining
tools. This initiative aims to empower researchers with the requisite resources
to test and develop fairness techniques within process mining, ultimately
contributing to the pursuit of equitable, data-driven decision-making
processes.


------------------------------------------------------------------------------

Title:
Allowing Blockchain Loans with Low Collateral

Abstract: Collateral is an item of value serving as security for the repayment of a
loan. In blockchain-based loans, cryptocurrencies serve as the collateral. The
high volatility of cryptocurrencies implies a serious barrier of entry with a
common practice that collateral values equal multiple times the value of the
loan. As assets serving as collateral are locked, this requirement prevents
many candidates from obtaining loans. In this paper, we aim to make loans more
accessible by offering loans with lower collateral, while keeping the risk for
lenders bound. We use a credit score based on data recovered from the
blockchain to predict how likely someone is to repay a loan. Our protocol does
not risk the initial amount granted by liquidity providers, but only risks part
of the interest yield gained by the protocol in the past.


------------------------------------------------------------------------------

Title:
Exploring the Performance and Efficiency of Transformer Models for NLP  on Mobile Devices

Abstract: Deep learning (DL) is characterised by its dynamic nature, with new deep
neural network (DNN) architectures and approaches emerging every few years,
driving the field's advancement. At the same time, the ever-increasing use of
mobile devices (MDs) has resulted in a surge of DNN-based mobile applications.
Although traditional architectures, like CNNs and RNNs, have been successfully
integrated into MDs, this is not the case for Transformers, a relatively new
model family that has achieved new levels of accuracy across AI tasks, but
poses significant computational challenges. In this work, we aim to make steps
towards bridging this gap by examining the current state of Transformers'
on-device execution. To this end, we construct a benchmark of representative
models and thoroughly evaluate their performance across MDs with different
computational capabilities. Our experimental results show that Transformers are
not accelerator-friendly and indicate the need for software and hardware
optimisations to achieve efficient deployment.


------------------------------------------------------------------------------

Title:
Masked Diffusion Models are Fast Learners

Abstract: Diffusion models have emerged as the de-facto technique for image generation,
yet they entail significant computational overhead, hindering the technique's
broader application in the research community. We propose a prior-based
denoising training framework, the first to incorporate the pre-train and
fine-tune paradigm into the diffusion model training process, which
substantially improves training efficiency and shows potential in facilitating
various downstream tasks. Our approach centers on masking a high proportion
(e.g., up to 90%) of the input image and employing masked score matching to
denoise the visible areas, thereby guiding the diffusion model to learn more
salient features from training data as prior knowledge. By utilizing this
masked learning process in a pre-training stage, we efficiently train the
ViT-based diffusion model on CelebA-HQ 256x256 in the pixel space, achieving a
4x acceleration and enhancing the quality of generated images compared to DDPM.
Moreover, our masked pre-training technique is universally applicable to
various diffusion models that directly generate images in the pixel space and
facilitates learning pre-trained models with excellent generalizability: a
diffusion model pre-trained on VGGFace2 attains a 46% quality improvement
through fine-tuning with merely 10% local data. Our code is available at
this https URL


------------------------------------------------------------------------------

Title:
Sampling from Gaussian Process Posteriors using Stochastic Gradient  Descent

Abstract: Gaussian processes are a powerful framework for quantifying uncertainty and
for sequential decision-making but are limited by the requirement of solving
linear systems. In general, this has a cubic cost in dataset size and is
sensitive to conditioning. We explore stochastic gradient algorithms as a
computationally efficient method of approximately solving these linear systems:
we develop low-variance optimization objectives for sampling from the posterior
and extend these to inducing points. Counterintuitively, stochastic gradient
descent often produces accurate predictions, even in cases where it does not
converge quickly to the optimum. We explain this through a spectral
characterization of the implicit bias from non-convergence. We show that
stochastic gradient descent produces predictive distributions close to the true
posterior both in regions with sufficient data coverage, and in regions
sufficiently far away from the data. Experimentally, stochastic gradient
descent achieves state-of-the-art performance on sufficiently large-scale or
ill-conditioned regression tasks. Its uncertainty estimates match the
performance of significantly more expensive baselines on a large-scale
Bayesian~optimization~task.


------------------------------------------------------------------------------

Title:
Align, Adapt and Inject: Sound-guided Unified Image Generation

Abstract: Text-guided image generation has witnessed unprecedented progress due to the
development of diffusion models. Beyond text and image, sound is a vital
element within the sphere of human perception, offering vivid representations
and naturally coinciding with corresponding scenes. Taking advantage of sound
therefore presents a promising avenue for exploration within image generation
research. However, the relationship between audio and image supervision remains
significantly underdeveloped, and the scarcity of related, high-quality
datasets brings further obstacles. In this paper, we propose a unified
framework 'Align, Adapt, and Inject' (AAI) for sound-guided image generation,
editing, and stylization. In particular, our method adapts input sound into a
sound token, like an ordinary word, which can plug and play with existing
powerful diffusion-based Text-to-Image (T2I) models. Specifically, we first
train a multi-modal encoder to align audio representation with the pre-trained
textual manifold and visual manifold, respectively. Then, we propose the audio
adapter to adapt audio representation into an audio token enriched with
specific semantics, which can be injected into a frozen T2I model flexibly. In
this way, we are able to extract the dynamic information of varied sounds,
while utilizing the formidable capability of existing T2I models to facilitate
sound-guided image generation, editing, and stylization in a convenient and
cost-effective manner. The experiment results confirm that our proposed AAI
outperforms other text and sound-guided state-of-the-art methods. And our
aligned multi-modal encoder is also competitive with other approaches in the
audio-visual retrieval and audio-text retrieval tasks.


------------------------------------------------------------------------------

Title:
Learning Locally Interpretable Rule Ensemble

Abstract: This paper proposes a new framework for learning a rule ensemble model that
is both accurate and interpretable. A rule ensemble is an interpretable model
based on the linear combination of weighted rules. In practice, we often face
the trade-off between the accuracy and interpretability of rule ensembles. That
is, a rule ensemble needs to include a sufficiently large number of weighted
rules to maintain its accuracy, which harms its interpretability for human
users. To avoid this trade-off and learn an interpretable rule ensemble without
degrading accuracy, we introduce a new concept of interpretability, named local
interpretability, which is evaluated by the total number of rules necessary to
express individual predictions made by the model, rather than to express the
model itself. Then, we propose a regularizer that promotes local
interpretability and develop an efficient algorithm for learning a rule
ensemble with the proposed regularizer by coordinate descent with local search.
Experimental results demonstrated that our method learns rule ensembles that
can explain individual predictions with fewer rules than the existing methods,
including RuleFit, while maintaining comparable accuracy.


------------------------------------------------------------------------------

Title:
Safe and Scalable Real-Time Trajectory Planning Framework for Urban Air  Mobility

Abstract: This paper presents a real-time trajectory planning framework for Urban Air
Mobility (UAM) that is both safe and scalable. The proposed framework employs a
decentralized, free-flight concept of operation in which each aircraft
independently performs separation assurance and conflict resolution, generating
safe trajectories by accounting for the future states of nearby aircraft. The
framework consists of two main components: a data-driven reachability analysis
tool and an efficient Markov Decision Process (MDP) based decision maker. The
reachability analysis over-approximates the reachable set of each aircraft
through a discrepancy function learned online from simulated trajectories. The
decision maker, on the other hand, uses a 6-degrees-of-freedom guidance model
of fixed-wing aircraft to ensure collision-free trajectory planning.
Additionally, the proposed framework incorporates reward shaping and action
shielding techniques to enhance safety performance. The proposed framework is
evaluated through simulation experiments involving up to 32 aircraft in a UAM
setting, with performance measured by the number of Near Mid Air Collisions
(NMAC) and computational time. The results demonstrate the safety and
scalability of the proposed framework.


------------------------------------------------------------------------------

Title:
Reversible Adversarial Examples with Beam Search Attack and Grayscale  Invariance

Abstract: Reversible adversarial examples (RAE) combine adversarial attacks and
reversible data-hiding technology on a single image to prevent illegal access.
Most RAE studies focus on achieving white-box attacks. In this paper, we
propose a novel framework to generate reversible adversarial examples, which
combines a novel beam search based black-box attack and reversible data hiding
with grayscale invariance (RDH-GI). This RAE uses beam search to evaluate the
adversarial gain of historical perturbations and guide adversarial
perturbations. After the adversarial examples are generated, the framework
RDH-GI embeds the secret data that can be recovered losslessly. Experimental
results show that our method can achieve an average Peak Signal-to-Noise Ratio
(PSNR) of at least 40dB compared to source images with limited query budgets.
Our method can also achieve a targeted black-box reversible adversarial attack
for the first time.


------------------------------------------------------------------------------

Title:
A Survey of Multivariate Polynomial Commitment Schemes

Abstract: A commitment scheme is a cryptographic tool that allows one to commit to a
hidden value, with the option to open it later at requested places without
revealing the secret itself. Commitment schemes have important applications in
zero-knowledge proofs and secure multi-party computation, just to name a few.
This survey introduces a few multivariate polynomial commitment schemes that
are built from a variety of mathematical structures. We study how Orion is
constructed using hash functions; Dory, Bulletproofs, and Vampire using the
inner-product argument; Signatures of Correct Computation using polynomial
factoring; DARK and Dew using groups of unknown order; and Orion+ using a
CP-SNARK. For each protocol, we prove its completeness and state its security
assumptions.


------------------------------------------------------------------------------

Title:
UUKG: Unified Urban Knowledge Graph Dataset for Urban Spatiotemporal  Prediction

Abstract: Accurate Urban SpatioTemporal Prediction (USTP) is of great importance to the
development and operation of the smart city. As an emerging building block,
multi-sourced urban data are usually integrated as urban knowledge graphs
(UrbanKGs) to provide critical knowledge for urban spatiotemporal prediction
models. However, existing UrbanKGs are often tailored for specific downstream
prediction tasks and are not publicly available, which limits the potential
advancement. This paper presents UUKG, the unified urban knowledge graph
dataset for knowledge-enhanced urban spatiotemporal predictions. Specifically,
we first construct UrbanKGs consisting of millions of triplets for two
metropolises by connecting heterogeneous urban entities such as administrative
boroughs, POIs, and road segments. Moreover, we conduct qualitative and
quantitative analysis on constructed UrbanKGs and uncover diverse high-order
structural patterns, such as hierarchies and cycles, that can be leveraged to
benefit downstream USTP tasks. To validate and facilitate the use of UrbanKGs,
we implement and evaluate 15 KG embedding methods on the KG completion task and
integrate the learned KG embeddings into 9 spatiotemporal models for five
different USTP tasks. The extensive experimental results not only provide
benchmarks of knowledge-enhanced USTP models under different task settings but
also highlight the potential of state-of-the-art high-order structure-aware
UrbanKG embedding methods. We hope the proposed UUKG fosters research on urban
knowledge graphs and broad smart city applications. The dataset and source code
are available at this https URL


------------------------------------------------------------------------------

Title:
Plausibility-Based Heuristics for Latent Space Classical Planning

Abstract: Recent work on LatPlan has shown that it is possible to learn models for
domain-independent classical planners from unlabeled image data. Although PDDL
models acquired by LatPlan can be solved using standard PDDL planners, the
resulting latent-space plan may be invalid with respect to the underlying,
ground-truth domain (e.g., the latent-space plan may include
hallucinatory/invalid states). We propose Plausibility-Based Heuristics, which
are domain-independent plausibility metrics which can be computed for each
state evaluated during search and uses as a heuristic function for best-first
search. We show that PBH significantly increases the number of valid found
plans on image-based tile puzzle and Towers of Hanoi domains.


------------------------------------------------------------------------------

Title:
Exploring Antitrust and Platform Power in Generative AI

Abstract: The concentration of power in a few digital technology companies has become a
subject of increasing interest in both academic and non-academic discussions.
One of the most noteworthy contributions to the debate is Lina Khan's Amazon's
Antitrust Paradox. In this work, Khan contends that Amazon has systematically
exerted its dominance in online retail to eliminate competitors and
subsequently charge above-market prices. This work contributed to Khan's
appointment as the chair of the US Federal Trade Commission (FTC), one of the
most influential antitrust organizations. Today, several ongoing antitrust
lawsuits in the US and Europe involve major technology companies like Apple,
Google/Alphabet, and Facebook/Meta. In the realm of generative AI, we are once
again witnessing the same companies taking the lead in technological
advancements, leaving little room for others to compete. This article examines
the market dominance of these corporations in the technology stack behind
generative AI from an antitrust law perspective.


------------------------------------------------------------------------------

Title:
Data Structures for Density Estimation

Abstract: We study statistical/computational tradeoffs for the following density
estimation problem: given $k$ distributions $v_1, \ldots, v_k$ over a discrete
domain of size $n$, and sampling access to a distribution $p$, identify $v_i$
that is "close" to $p$. Our main result is the first data structure that, given
a sublinear (in $n$) number of samples from $p$, identifies $v_i$ in time
sublinear in $k$. We also give an improved version of the algorithm of Acharya
et al. (2018) that reports $v_i$ in time linear in $k$. The experimental
evaluation of the latter algorithm shows that it achieves a significant
reduction in the number of operations needed to achieve a given accuracy
compared to prior work.


------------------------------------------------------------------------------

Title:
Democratizing LLMs for Low-Resource Languages by Leveraging their  English Dominant Abilities with Linguistically-Diverse Prompts

Abstract: Large language models (LLMs) are known to effectively perform tasks by simply
observing few exemplars. However, in low-resource languages, obtaining such
hand-picked exemplars can still be challenging, where unsupervised techniques
may be necessary. Moreover, competent generative capabilities of LLMs are
observed only in high-resource languages, while their performances among
under-represented languages fall behind due to pre-training data imbalance. To
elicit LLMs' ability onto low-resource languages without any supervised data,
we propose to assemble synthetic exemplars from a diverse set of high-resource
languages to prompt the LLMs to translate from any language into English. These
prompts are then used to create intra-lingual exemplars to perform tasks in the
target languages. Our unsupervised prompting method performs on par with
supervised few-shot learning in LLMs of different sizes for translations
between English and 13 Indic and 21 African low-resource languages. We also
show that fine-tuning a 7B model on data generated from our method helps it
perform competitively with a 175B model. In non-English translation tasks, our
method even outperforms supervised prompting by up to 3 chrF++ in many
low-resource languages. When evaluated on zero-shot multilingual summarization,
our method surpasses other English-pivoting baselines by up to 4 ROUGE-L and is
also favored by GPT-4.


------------------------------------------------------------------------------

Title:
Hierarchical GNNs for Large Graph Generation

Abstract: Large graphs are present in a variety of domains, including social networks,
civil infrastructure, and the physical sciences to name a few. Graph generation
is similarly widespread, with applications in drug discovery, network analysis
and synthetic datasets among others. While GNN (Graph Neural Network) models
have been applied in these domains their high in-memory costs restrict them to
small graphs. Conversely less costly rule-based methods struggle to reproduce
complex structures. We propose HIGGS (Hierarchical Generation of Graphs) as a
model-agnostic framework of producing large graphs with realistic local
structures. HIGGS uses GNN models with conditional generation capabilities to
sample graphs in hierarchies of resolution. As a result HIGGS has the capacity
to extend the scale of generated graphs from a given GNN model by quadratic
order. As a demonstration we implement HIGGS using DiGress, a recent
graph-diffusion model, including a novel edge-predictive-diffusion variant
edge-DiGress. We use this implementation to generate categorically attributed
graphs with tens of thousands of nodes. These HIGGS generated graphs are far
larger than any previously produced using GNNs. Despite this jump in scale we
demonstrate that the graphs produced by HIGGS are, on the local scale, more
realistic than those from the rule-based model BTER.


------------------------------------------------------------------------------

Title:
Computation of the Wright function from its integral representation

Abstract: The Wright function arises in the theory of the fractional differential
equations. It is a very general mathematical object having diverse connections
with other special and elementary functions. The Wright function provides a
unified treatment of several classes of special functions, such as the
Gaussian, Airy, Bessel, error functions, etc. The manuscript presents a novel
numerical technique for approximation of the Wright function using quadratures.
The algorithm is implemented as a standalone library using the
double-exponential quadrature integration technique using the method of
stationary phase. Function plots for a variety of parameter values are
demonstrated.


------------------------------------------------------------------------------

Title:
End-to-end 2D-3D Registration between Image and LiDAR Point Cloud for  Vehicle Localization

Abstract: Robot localization using a previously built map is essential for a variety of
tasks including highly accurate navigation and mobile manipulation. A popular
approach to robot localization is based on image-to-point cloud registration,
which combines illumination-invariant LiDAR-based mapping with economical
image-based localization. However, the recent works for image-to-point cloud
registration either divide the registration into separate modules or project
the point cloud to the depth image to register the RGB and depth images. In
this paper, we present I2PNet, a novel end-to-end 2D-3D registration network.
I2PNet directly registers the raw 3D point cloud with the 2D RGB image using
differential modules with a unique target. The 2D-3D cost volume module for
differential 2D-3D association is proposed to bridge feature extraction and
pose regression. 2D-3D cost volume module implicitly constructs the soft
point-to-pixel correspondence on the intrinsic-independent normalized plane of
the pinhole camera model. Moreover, we introduce an outlier mask prediction
module to filter the outliers in the 2D-3D association before pose regression.
Furthermore, we propose the coarse-to-fine 2D-3D registration architecture to
increase localization accuracy. We conduct extensive localization experiments
on the KITTI Odometry and nuScenes datasets. The results demonstrate that
I2PNet outperforms the state-of-the-art by a large margin. In addition, I2PNet
has a higher efficiency than the previous works and can perform the
localization in real-time. Moreover, we extend the application of I2PNet to the
camera-LiDAR online calibration and demonstrate that I2PNet outperforms recent
approaches on the online calibration task.


------------------------------------------------------------------------------

Title:
Hallucination is the last thing you need

Abstract: The legal profession necessitates a multidimensional approach that involves
synthesizing an in-depth comprehension of a legal issue with insightful
commentary based on personal experience, combined with a comprehensive
understanding of pertinent legislation, regulation, and case law, in order to
deliver an informed legal solution. The present offering with generative AI
presents major obstacles in replicating this, as current models struggle to
integrate and navigate such a complex interplay of understanding, experience,
and fact-checking procedures. It is noteworthy that where generative AI outputs
understanding and experience, which reflect the aggregate of various subjective
views on similar topics, this often deflects the model's attention from the
crucial legal facts, thereby resulting in hallucination. Hence, this paper
delves into the feasibility of three independent LLMs, each focused on
understanding, experience, and facts, synthesising as one single ensemble model
to effectively counteract the current challenges posed by the existing
monolithic generative AI models. We introduce an idea of mutli-length
tokenisation to protect key information assets like common law judgements, and
finally we interrogate the most advanced publicly available models for legal
hallucination, with some interesting results.


------------------------------------------------------------------------------

Title:
Depth and DOF Cues Make A Better Defocus Blur Detector

Abstract: Defocus blur detection (DBD) separates in-focus and out-of-focus regions in
an image. Previous approaches mistakenly mistook homogeneous areas in focus for
defocus blur regions, likely due to not considering the internal factors that
cause defocus blur. Inspired by the law of depth, depth of field (DOF), and
defocus, we propose an approach called D-DFFNet, which incorporates depth and
DOF cues in an implicit manner. This allows the model to understand the defocus
phenomenon in a more natural way. Our method proposes a depth feature
distillation strategy to obtain depth knowledge from a pre-trained monocular
depth estimation model and uses a DOF-edge loss to understand the relationship
between DOF and depth. Our approach outperforms state-of-the-art methods on
public benchmarks and a newly collected large benchmark dataset, EBD. Source
codes and EBD dataset are available at: https:github.com/yuxinjin-whu/D-DFFNet.


------------------------------------------------------------------------------

Title:
An implicit-explicit solver for a two-fluid single-temperature model

Abstract: We present an implicit-explicit finite volume scheme for two-fluid
single-temperature flow in all Mach number regimes which is based on a
symmetric hyperbolic thermodynamically compatible description of the fluid
flow. The scheme is stable for large time steps controlled by the interface
transport and is computational efficient due to a linear implicit character.
The latter is achieved by linearizing along constant reference states given by
the asymptotic analysis of the single-temperature model. Thus, the use of a
stiffly accurate IMEX Runge Kutta time integration and the centered treatment
of pressure based quantities provably guarantee the asymptotic preserving
property of the scheme for weakly compressible Euler equations with variable
volume fraction. The properties of the first and second order scheme are
validated by several numerical test cases.


------------------------------------------------------------------------------

Title:
New Binary Self-Dual Cyclic Codes with Square-Root-Like Minimum  Distances

Abstract: The construction of self-dual codes over small fields such that their minimum
distances are as large as possible is a long-standing challenging problem in
the coding theory. In 2009, a family of binary self-dual cyclic codes with
lengths $n_i$ and minimum distances $d_i \geq \frac{1}{2} \sqrt{n_i}$, $n_i$
goes to the infinity for $i=1,2, \ldots$, was constructed. In this paper, we
construct a family of (repeated-root) binary self-dual cyclic codes with
lengths $n$ and minimum distances at least $\sqrt{n}-2$. New families of
lengths $n=q^m-1$, $m=3, 5, \ldots$, self-dual codes over ${\bf F}_q$, $q
\equiv 1$ $mod$ $4$, with their minimum distances larger than or equal to
$\sqrt{\frac{q}{2}}\sqrt{n}-q$ are also constructed.


------------------------------------------------------------------------------

Title:
CAPRI: Context-Aware Interpretable Point-of-Interest Recommendation  Framework

Abstract: Point-of-Interest (POI ) recommendation systems have gained popularity for
their unique ability to suggest geographical destinations with the
incorporation of contextual information such as time, location, and user-item
interaction. Existing recommendation frameworks lack the contextual fusion
required for POI systems. This paper presents CAPRI, a novel POI recommendation
framework that effectively integrates context-aware models, such as GeoSoCa,
LORE, and USG, and introduces a novel strategy for the efficient merging of
contextual information. CAPRI integrates an evaluation module that expands the
evaluation scope beyond accuracy to include novelty, personalization,
diversity, and fairness. With an aim to establish a new industry standard for
reproducible results in the realm of POI recommendation systems, we have made
CAPRI openly accessible on GitHub, facilitating easy access and contribution to
the continued development and refinement of this innovative framework.


------------------------------------------------------------------------------

Title:
Audio-Driven 3D Facial Animation from In-the-Wild Videos

Abstract: Given an arbitrary audio clip, audio-driven 3D facial animation aims to
generate lifelike lip motions and facial expressions for a 3D head. Existing
methods typically rely on training their models using limited public 3D
datasets that contain a restricted number of audio-3D scan pairs. Consequently,
their generalization capability remains limited. In this paper, we propose a
novel method that leverages in-the-wild 2D talking-head videos to train our 3D
facial animation model. The abundance of easily accessible 2D talking-head
videos equips our model with a robust generalization capability. By combining
these videos with existing 3D face reconstruction methods, our model excels in
generating consistent and high-fidelity lip synchronization. Additionally, our
model proficiently captures the speaking styles of different individuals,
allowing it to generate 3D talking-heads with distinct personal styles.
Extensive qualitative and quantitative experimental results demonstrate the
superiority of our method.


------------------------------------------------------------------------------

Title:
Transparency in App Analytics: Analyzing the Collection of User  Interaction Data

Abstract: The rise of mobile apps has brought greater convenience and many options for
users. However, many apps use analytics services to collect a wide range of
user interaction data, with privacy policies often failing to reveal the types
of interaction data collected or the extent of the data collection practices.
This lack of transparency potentially breaches data protection laws and also
undermines user trust. We conducted an analysis of the top 20 analytic
libraries for Android apps to identify common practices of interaction data
collection and used this information to develop a standardized collection claim
template for summarizing an app's data collection practices wrt. user
interaction data. We selected the top 100 apps from popular categories on
Google Play and used automatic static analysis to extract collection evidence
from their data collection implementations. Our analysis found that a
significant majority of these apps actively collected interaction data from UI
types such as View (89%), Button (76%), and Textfield (63%), highlighting the
pervasiveness of user interaction data collection. By comparing the collection
evidence to the claims derived from privacy policy analysis, we manually
fact-checked the completeness and accuracy of these claims for the top 10 apps.
We found that, except for one app, they all failed to declare all types of
interaction data they collect and did not specify some of the collection
techniques used.


------------------------------------------------------------------------------

Title:
A Versatility-Performance Balanced Hardware Architecture for Scene Text  Detection

Abstract: Detecting and extracting textual information from natural scene images needs
Scene Text Detection (STD) algorithms. Fully Convolutional Neural Networks
(FCNs) are usually utilized as the backbone model to extract features in these
instance segmentation based STD algorithms. FCNs naturally come with high
computational complexity. Furthermore, to keep up with the growing variety of
models, flexible architectures are needed. In order to accelerate various STD
algorithms efficiently, a versatility-performance balanced hardware
architecture is proposed, together with a simple but efficient way of
configuration. This architecture is able to compute different FCN models
without hardware redesign. The optimization is focused on hardware with finely
designed computing modules, while the versatility of different network
reconfigurations is achieved by microcodes instead of a strenuously designed
compiler. Multiple parallel techniques at different levels and several
complexity-reduction methods are explored to speed up the FCN computation.
Results from implementation show that, given the same tasks, the proposed
system achieves a better throughput compared with the studied GPU.
Particularly, our system reduces the comprehensive Operation Expense (OpEx) at
GPU by 46\%, while the power efficiency is enhanced by 32\%. This work has been
deployed in commercial applications and provided stable consumer text detection
services.


------------------------------------------------------------------------------

Title:
Provably Robust Temporal Difference Learning for Heavy-Tailed Rewards

Abstract: In a broad class of reinforcement learning applications, stochastic rewards
have heavy-tailed distributions, which lead to infinite second-order moments
for stochastic (semi)gradients in policy evaluation and direct policy
optimization. In such instances, the existing RL methods may fail miserably due
to frequent statistical outliers. In this work, we establish that temporal
difference (TD) learning with a dynamic gradient clipping mechanism, and
correspondingly operated natural actor-critic (NAC), can be provably
robustified against heavy-tailed reward distributions. It is shown in the
framework of linear function approximation that a favorable tradeoff between
bias and variability of the stochastic gradients can be achieved with this
dynamic gradient clipping mechanism. In particular, we prove that robust
versions of TD learning achieve sample complexities of order
$\mathcal{O}(\varepsilon^{-\frac{1}{p}})$ and
$\mathcal{O}(\varepsilon^{-1-\frac{1}{p}})$ with and without the full-rank
assumption on the feature matrix, respectively, under heavy-tailed rewards with
finite moments of order $(1+p)$ for some $p\in(0,1]$, both in expectation and
with high probability. We show that a robust variant of NAC based on Robust TD
learning achieves $\tilde{\mathcal{O}}(\varepsilon^{-4-\frac{2}{p}})$ sample
complexity. We corroborate our theoretical results with numerical experiments.


------------------------------------------------------------------------------

Title:
FAIR: A Causal Framework for Accurately Inferring Judgments Reversals

Abstract: Artificial intelligence researchers have made significant advances in legal
intelligence in recent years. However, the existing studies have not focused on
the important value embedded in judgments reversals, which limits the
improvement of the efficiency of legal intelligence. In this paper, we propose
a causal Framework for Accurately Inferring case Reversals (FAIR), which models
the problem of judgments reversals based on real Chinese judgments. We mine the
causes of judgments reversals by causal inference methods and inject the
obtained causal relationships into the neural network as a priori knowledge.
And then, our framework is validated on a challenging dataset as a legal
judgment prediction task. The experimental results show that our framework can
tap the most critical factors in judgments reversal, and the obtained causal
relationships can effectively improve the neural network's performance. In
addition, we discuss the generalization ability of large language models for
legal intelligence tasks using ChatGPT as an example. Our experiment has found
that the generalization ability of large language models still has defects, and
mining causal relationships can effectively improve the accuracy and explain
ability of model predictions.


------------------------------------------------------------------------------

Title:
One-to-Many Spectral Upsampling of Reflectances and Transmittances

Abstract: Spectral rendering is essential for the production of physically-plausible
synthetic images, but requires to introduce several changes in the content
generation pipeline. In particular, the authoring of spectral material
properties (e.g., albedo maps, indices of refraction, transmittance
coefficients) raises new problems.While a large panel of computer graphics
methods exists to upsample a RGB color to a spectrum, they all provide a
one-to-one mapping. This limits the ability to control interesting color
changes such as the Usambara effect or metameric spectra. In this work, we
introduce a one-to-many mapping in which we show how we can explore the set of
all spectra reproducing a given input color. We apply this method to different
colour changing effects such as vathochromism -- the change of color with
depth, and metamerism.


------------------------------------------------------------------------------

Title:
Intersectionality in Conversational AI Safety: How Bayesian Multilevel  Models Help Understand Diverse Perceptions of Safety

Abstract: Conversational AI systems exhibit a level of human-like behavior that
promises to have profound impacts on many aspects of daily life -- how people
access information, create content, and seek social support. Yet these models
have also shown a propensity for biases, offensive language, and conveying
false information. Consequently, understanding and moderating safety risks in
these models is a critical technical and social challenge. Perception of safety
is intrinsically subjective, where many factors -- often intersecting -- could
determine why one person may consider a conversation with a chatbot safe and
another person could consider the same conversation unsafe. In this work, we
focus on demographic factors that could influence such diverse perceptions. To
this end, we contribute an analysis using Bayesian multilevel modeling to
explore the connection between rater demographics and how raters report safety
of conversational AI systems. We study a sample of 252 human raters stratified
by gender, age group, race/ethnicity group, and locale. This rater pool
provided safety labels for 1,340 human-chatbot conversations. Our results show
that intersectional effects involving demographic characteristics such as
race/ethnicity, gender, and age, as well as content characteristics, such as
degree of harm, all play significant roles in determining the safety of
conversational AI systems. For example, race/ethnicity and gender show strong
intersectional effects, particularly among South Asian and East Asian women. We
also find that conversational degree of harm impacts raters of all
race/ethnicity groups, but that Indigenous and South Asian raters are
particularly sensitive to this harm. Finally, we observe the effect of
education is uniquely intersectional for Indigenous raters, highlighting the
utility of multilevel frameworks for uncovering underrepresented social
perspectives.


------------------------------------------------------------------------------

Title:
Stochastic Galerkin method and port-Hamiltonian form for linear  dynamical systems of second order

Abstract: We investigate linear dynamical systems of second order. Uncertainty
quantification is applied, where physical parameters are substituted by random
variables. A stochastic Galerkin method yields a linear dynamical system of
second order with high dimensionality. A structure-preserv\-ing model order
reduction (MOR) produces a small linear dynamical system of second order again.
We arrange an associated port-Hamiltonian (pH) formulation of first order for
the second-order systems. Each pH system implies a Hamiltonian function
describing an internal energy. We examine the properties of the Hamiltonian
function for the stochastic Galerkin systems. We show numerical results using a
test example, where both the stochastic Galerkin method and
structure-preserving MOR are applied.


------------------------------------------------------------------------------

Title:
Safe, Efficient, Comfort, and Energy-saving Automated Driving through  Roundabout Based on Deep Reinforcement Learning

Abstract: Traffic scenarios in roundabouts pose substantial complexity for automated
driving. Manually mapping all possible scenarios into a state space is
labor-intensive and challenging. Deep reinforcement learning (DRL) with its
ability to learn from interacting with the environment emerges as a promising
solution for training such automated driving models. This study explores,
employs, and implements various DRL algorithms, namely Deep Deterministic
Policy Gradient (DDPG), Proximal Policy Optimization (PPO), and Trust Region
Policy Optimization (TRPO) to instruct automated vehicles' driving through
roundabouts. The driving state space, action space, and reward function are
designed. The reward function considers safety, efficiency, comfort, and energy
consumption to align with real-world requirements. All three tested DRL
algorithms succeed in enabling automated vehicles to drive through the
roundabout. To holistically evaluate the performance of these algorithms, this
study establishes an evaluation methodology considering multiple indicators
such as safety, efficiency, and comfort level. A method employing the Analytic
Hierarchy Process is also developed to weigh these evaluation indicators.
Experimental results on various testing scenarios reveal that the TRPO
algorithm outperforms DDPG and PPO in terms of safety and efficiency, and PPO
performs best in terms of comfort level. Lastly, to verify the model's
adaptability and robustness regarding other driving scenarios, this study also
deploys the model trained by TRPO to a range of different testing scenarios,
e.g., highway driving and merging. Experimental results demonstrate that the
TRPO model trained on only roundabout driving scenarios exhibits a certain
degree of proficiency in highway driving and merging scenarios. This study
provides a foundation for the application of automated driving with DRL in real
traffic environments.


------------------------------------------------------------------------------

Title:
Revisiting the Direct Fourier Filtering Technique for the Maximal Decay  Rate of Boundary-damped Wave Equation by Finite Differences and Finite  Elements

Abstract: The one-dimensional PDE model of the wave equation with a state feedback
controller at its boundary, which describes wave dynamics of a wide-range of
controlled mechanical systems, has exponentially stable solutions. However, it
is known that the reduced models of the wave equation by the standard Finite
Differences and Finite Elements suffer from the lack of exponential stability
(and exact observability without a state feedback controller) uniformly as the
discretization parameter tends to zero. This is due to the loss of uniform gap
among the high-frequency eigenvalues as the discretization parameter tends to
zero. One common remedy to overcome this discrepancy is the direct Fourier
filtering of the reduced models, where the high-frequency spurious eigenvalues
are filtered out. After filtering, besides from the strong convergency, the
exponential decay rate, mimicking the one for the partial differential equation
counterpart, can be retained uniformly. However, the existing results in the
literature are solely based on an observability inequality of the control-free
model, to which the filtering is implemented. Moreover, the decay rate as a
function of the filtering parameter is implicit. In this paper, exponential
stability results for both filtered Finite Difference and Finite Element
reduced models are established directly by a Lyapunov-based approach and a
thorough eigenvalue estimation.The maximal decay rate is explicitly provided as
a function of the feedback gain and filtering parameter. Our results,
expectedly, mimic the ones of the PDE counterpart uniformly as the
discretization parameter tends to zero. Several numerical tests are provided to
support our results.


------------------------------------------------------------------------------

Title:
FDINet: Protecting against DNN Model Extraction via Feature Distortion  Index

Abstract: Machine Learning as a Service (MLaaS) platforms have gained popularity due to
their accessibility, cost-efficiency, scalability, and rapid development
capabilities. However, recent research has highlighted the vulnerability of
cloud-based models in MLaaS to model extraction attacks. In this paper, we
introduce FDINET, a novel defense mechanism that leverages the feature
distribution of deep neural network (DNN) models. Concretely, by analyzing the
feature distribution from the adversary's queries, we reveal that the feature
distribution of these queries deviates from that of the model's training set.
Based on this key observation, we propose Feature Distortion Index (FDI), a
metric designed to quantitatively measure the feature distribution deviation of
received queries. The proposed FDINET utilizes FDI to train a binary detector
and exploits FDI similarity to identify colluding adversaries from distributed
extraction attacks. We conduct extensive experiments to evaluate FDINET against
six state-of-the-art extraction attacks on four benchmark datasets and four
popular model architectures. Empirical results demonstrate the following
findings FDINET proves to be highly effective in detecting model extraction,
achieving a 100% detection accuracy on DFME and DaST. FDINET is highly
efficient, using just 50 queries to raise an extraction alarm with an average
confidence of 96.08% for GTSRB. FDINET exhibits the capability to identify
colluding adversaries with an accuracy exceeding 91%. Additionally, it
demonstrates the ability to detect two types of adaptive attacks.


------------------------------------------------------------------------------

Title:
On the power of counting the total number of computation paths of NPTMs

Abstract: Complexity classes defined by modifying the acceptance condition of NP
computations have been extensively studied. For example, the class UP, which
contains decision problems solvable by non-deterministic polynomial-time Turing
machines (NPTMs) with at most one accepting path -- equivalently NP problems
with at most one solution -- has played a significant role in cryptography,
since P=/=UP is equivalent to the existence of one-way functions. In this
paper, we define and examine variants of several such classes where the
acceptance condition concerns the total number of computation paths of an NPTM,
instead of the number of accepting ones. This direction reflects the
relationship between the counting classes #P and TotP, which are the classes of
functions that count the number of accepting paths and the total number of
paths of NPTMs, respectively. The former is the well-studied class of counting
versions of NP problems, introduced by Valiant (1979). The latter contains all
self-reducible counting problems in #P whose decision version is in P, among
them prominent #P-complete problems such as Non-negative Permanent, #PerfMatch,
and #Dnf-Sat, thus playing a significant role in the study of approximable
counting problems.
We show that almost all classes introduced in this work coincide with their
'# accepting paths'-definable counterparts. As a result, we present a novel
family of complete problems for the classes parity-P, Modkp, SPP, WPP, C=P, and
PP that are defined via TotP-complete problems under parsimonious reductions.


------------------------------------------------------------------------------

Title:
Did the Models Understand Documents? Benchmarking Models for Language  Understanding in Document-Level Relation Extraction

Abstract: Document-level relation extraction (DocRE) attracts more research interest
recently. While models achieve consistent performance gains in DocRE, their
underlying decision rules are still understudied: Do they make the right
predictions according to rationales? In this paper, we take the first step
toward answering this question and then introduce a new perspective on
comprehensively evaluating a model. Specifically, we first conduct annotations
to provide the rationales considered by humans in DocRE. Then, we conduct
investigations and reveal the fact that: In contrast to humans, the
representative state-of-the-art (SOTA) models in DocRE exhibit different
decision rules. Through our proposed RE-specific attacks, we next demonstrate
that the significant discrepancy in decision rules between models and humans
severely damages the robustness of models and renders them inapplicable to
real-world RE scenarios. After that, we introduce mean average precision (MAP)
to evaluate the understanding and reasoning capabilities of models. According
to the extensive experimental results, we finally appeal to future work to
consider evaluating both performance and the understanding ability of models
for the development of their applications. We make our annotations and code
publicly available.


------------------------------------------------------------------------------

Title:
Solving acoustic scattering problems by the isogeometric boundary  element method

Abstract: We solve acoustic scattering problems by means of the isogeometric boundary
integral equation method. In order to avoid spurious modes, we apply the
combined field integral equations for either sound-hard scatterers or
sound-soft scatterers. These integral equations are discretized by Galerkin's
method, which especially enables the mathematically correct regularization of
the hypersingular integral operator. In order to circumvent densely populated
system matrices, we employ the isogeometric fast multipole method. The result
is an algorithm that scales essentially linear in the number of boundary
elements. Numerical experiments are performed which show the feasibility and
the performance of the approach.


------------------------------------------------------------------------------

Title:
GIO: Gradient Information Optimization for Training Dataset Selection

Abstract: It is often advantageous to train models on a subset of the available train
examples, because the examples are of variable quality or because one would
like to train with fewer examples, without sacrificing performance. We present
Gradient Information Optimization (GIO), a scalable, task-agnostic approach to
this data selection problem that requires only a small set of (unlabeled)
examples representing a target distribution. GIO begins from a natural,
information-theoretic objective that is intractable in practice. Our
contribution is in showing that it can be made highly scalable through a simple
relaxation of the objective and a highly efficient implementation. In
experiments with machine translation, spelling correction, and image
recognition, we show that GIO delivers outstanding results with very small
train sets. These findings are robust to different representation models and
hyperparameters for GIO itself. GIO is task- and domain-agnostic and can be
applied out-of-the-box to new datasets and domains.


------------------------------------------------------------------------------

Title:
Data Availability Sampling in Ethereum: Analysis of P2P Networking  Requirements

Abstract: Despite their increasing popularity, blockchains still suffer from severe
scalability limitations. Recently, Ethereum proposed a novel approach to block
validation based on Data Availability Sampling (DAS), that has the potential to
improve its transaction per second rate by more than two orders of magnitude.
DAS should also significantly reduce per-transaction validation costs. At the
same time, DAS introduces new communication patterns in the Ethereum
Peer-to-Peer (P2P) network. These drastically increase the amount of exchanged
data and impose stringent latency objectives. In this paper, we review the new
requirements for P2P networking associated with DAS, discuss open challenges,
and identify new research directions.


------------------------------------------------------------------------------

Title:
Multi-user Reset Controller for Redirected Walking Using Reinforcement  Learning

Abstract: The reset technique of Redirected Walking (RDW) forcibly reorients the user's
direction overtly to avoid collisions with boundaries, obstacles, or other
users in the physical space. However, excessive resetting can decrease the
user's sense of immersion and presence. Several RDW studies have been conducted
to address this issue. Among them, much research has been done on reset
techniques that reduce the number of resets by devising reset direction rules
(e.g.,~ 2:1-turn, reset-to-center) or optimizing them for a given environment.
However, existing optimization studies on reset techniques have mainly focused
on a single-user environment. In a multi-user environment, the dynamic movement
of other users and static obstacles in the physical space increase the
possibility of resetting. In this study, we propose a multi-user reset
controller (MRC) that resets the user taking into account both physical
obstacles and multi-user movement to minimize the number of resets. MRC is
trained using multi-agent reinforcement learning to determine the optimal reset
direction in different environments. This approach enables MRC to effectively
account for different environmental contexts, including arbitrary physical
obstacles and the dynamic movements of other users in the same physical space.
We compared MRC with other reset techniques through simulation tests and user
studies, and our results show that MRC reduces the mean number of resets by up
to 55\%. Overall, our study confirmed that MRC is an effective reset technique
in multi-user environments. Supplemental materials are available at an
anonymous link:
(this https URL).


------------------------------------------------------------------------------

Title:
Transforming Graphs for Enhanced Attribute-Based Clustering: An  Innovative Graph Transformer Method

Abstract: Graph Representation Learning (GRL) is an influential methodology, enabling a
more profound understanding of graph-structured data and aiding graph
clustering, a critical task across various domains. The recent incursion of
attention mechanisms, originally an artifact of Natural Language Processing
(NLP), into the realm of graph learning has spearheaded a notable shift in
research trends. Consequently, Graph Attention Networks (GATs) and Graph
Attention Auto-Encoders have emerged as preferred tools for graph clustering
tasks. Yet, these methods primarily employ a local attention mechanism, thereby
curbing their capacity to apprehend the intricate global dependencies between
nodes within graphs. Addressing these impediments, this study introduces an
innovative method known as the Graph Transformer Auto-Encoder for Graph
Clustering (GTAGC). By melding the Graph Auto-Encoder with the Graph
Transformer, GTAGC is adept at capturing global dependencies between nodes.
This integration amplifies the graph representation and surmounts the
constraints posed by the local attention mechanism. The architecture of GTAGC
encompasses graph embedding, integration of the Graph Transformer within the
autoencoder structure, and a clustering component. It strategically alternates
between graph embedding and clustering, thereby tailoring the Graph Transformer
for clustering tasks, whilst preserving the graph's global structural
information. Through extensive experimentation on diverse benchmark datasets,
GTAGC has exhibited superior performance against existing state-of-the-art
graph clustering methodologies. This pioneering approach represents a novel
contribution to the field of graph clustering, paving the way for promising
avenues in future research.


------------------------------------------------------------------------------

Title:
A Universal Unbiased Method for Classification from Aggregate  Observations

Abstract: In conventional supervised classification, true labels are required for
individual instances. However, it could be prohibitive to collect the true
labels for individual instances, due to privacy concerns or unaffordable
annotation costs. This motivates the study on classification from aggregate
observations (CFAO), where the supervision is provided to groups of instances,
instead of individual instances. CFAO is a generalized learning framework that
contains various learning problems, such as multiple-instance learning and
learning from label proportions. The goal of this paper is to present a novel
universal method of CFAO, which holds an unbiased estimator of the
classification risk for arbitrary losses -- previous research failed to achieve
this goal. Practically, our method works by weighing the importance of each
label for each instance in the group, which provides purified supervision for
the classifier to learn. Theoretically, our proposed method not only guarantees
the risk consistency due to the unbiased risk estimator but also can be
compatible with arbitrary losses. Extensive experiments on various problems of
CFAO demonstrate the superiority of our proposed method.


------------------------------------------------------------------------------

Title:
ChatGPT is not Enough: Enhancing Large Language Models with Knowledge  Graphs for Fact-aware Language Modeling

Abstract: Recently, ChatGPT, a representative large language model (LLM), has gained
considerable attention due to its powerful emergent abilities. Some researchers
suggest that LLMs could potentially replace structured knowledge bases like
knowledge graphs (KGs) and function as parameterized knowledge bases. However,
while LLMs are proficient at learning probabilistic language patterns based on
large corpus and engaging in conversations with humans, they, like previous
smaller pre-trained language models (PLMs), still have difficulty in recalling
facts while generating knowledge-grounded contents. To overcome these
limitations, researchers have proposed enhancing data-driven PLMs with
knowledge-based KGs to incorporate explicit factual knowledge into PLMs, thus
improving their performance to generate texts requiring factual knowledge and
providing more informed responses to user queries. This paper reviews the
studies on enhancing PLMs with KGs, detailing existing knowledge graph enhanced
pre-trained language models (KGPLMs) as well as their applications. Inspired by
existing studies on KGPLM, this paper proposes to enhance LLMs with KGs by
developing knowledge graph-enhanced large language models (KGLLMs). KGLLM
provides a solution to enhance LLMs' factual reasoning ability, opening up new
avenues for LLM research.


------------------------------------------------------------------------------

Title:
Multi-task Collaborative Pre-training and Individual-adaptive-tokens  Fine-tuning: A Unified Framework for Brain Representation Learning

Abstract: Structural magnetic resonance imaging (sMRI) provides accurate estimates of
the brain's structural organization and learning invariant brain
representations from sMRI is an enduring issue in neuroscience. Previous deep
representation learning models ignore the fact that the brain, as the core of
human cognitive activity, is distinct from other organs whose primary attribute
is anatomy. Therefore, capturing the semantic structure that dominates
interindividual cognitive variability is key to accurately representing the
brain. Given that this high-level semantic information is subtle, distributed,
and interdependently latent in the brain structure, sMRI-based models need to
capture fine-grained details and understand how they relate to the overall
global structure. However, existing models are optimized by simple objectives,
making features collapse into homogeneity and worsening simultaneous
representation of fine-grained information and holistic semantics, causing a
lack of biological plausibility and interpretation of cognition. Here, we
propose MCIAT, a unified framework that combines Multi-task Collaborative
pre-training and Individual-Adaptive-Tokens fine-tuning. Specifically, we first
synthesize restorative learning, age prediction auxiliary learning and
adversarial learning as a joint proxy task for deep semantic representation
learning. Then, a mutual-attention-based token selection method is proposed to
highlight discriminative features. The proposed MCIAT achieves state-of-the-art
diagnosis performance on the ADHD-200 dataset compared with several sMRI-based
approaches and shows superior generalization on the MCIC and OASIS datasets.
Moreover, we studied 12 behavioral tasks and found significant associations
between cognitive functions and MCIAT-established representations, which
verifies the interpretability of our proposed framework.


------------------------------------------------------------------------------

Title:
Helping Code Reviewer Prioritize: Pinpointing Personal Data and its  Processing

Abstract: Ensuring compliance with the General Data Protection Regulation (GDPR) is a
crucial aspect of software development. This task, due to its time-consuming
nature and requirement for specialized knowledge, is often deferred or
delegated to specialized code reviewers. These reviewers, particularly when
external to the development organization, may lack detailed knowledge of the
software under review, necessitating the prioritization of their resources.
To address this, we have designed two specialized views of a codebase to help
code reviewers in prioritizing their work related to personal data: one view
displays the types of personal data representation, while the other provides an
abstract depiction of personal data processing, complemented by an optional
detailed exploration of specific code snippets. Leveraging static analysis, our
method identifies personal data-related code segments, thereby expediting the
review process. Our approach, evaluated on four open-source GitHub
applications, demonstrated a precision rate of 0.87 in identifying personal
data flows. Additionally, we fact-checked the privacy statements of 15 Android
applications. This solution, designed to augment the efficiency of GDPR-related
privacy analysis tasks such as the Record of Processing Activities (ROPA), aims
to conserve resources, thereby saving time and enhancing productivity for code
reviewers.


------------------------------------------------------------------------------

Title:
A sharp discrete maximal regularity for the discontinuous Galerkin  time-stepping method

Abstract: Maximal regularity is a kind of a priori estimates for parabolic-type
equations and plays an important role in the theory of nonlinear differential
equations. The aim of this paper is to investigate the temporally discrete
counterpart of maximal regularity for the discontinuous Galerkin (DG)
time-stepping method. We will establish an estimate sharper than the existing
discrete maximal regularity, and our estimate is the best possible result. To
show the main result, we introduce the temporally regularized Green's function
and then reduce the discrete maximal regularity to a weighted error estimate
for its DG approximation. We will also obtain an optimal order error estimate
for the DG time-stepping method as an application.


------------------------------------------------------------------------------

Title:
The Pricing And Hedging Of Constant Function Market Makers

Abstract: We investigate the most common type of blockchain-based decentralized
exchange, which are known as constant function market makers (CFMMs). We
examine the the market microstructure around CFMMs and present a model for
valuing the liquidity provider (LP) mechanism and estimating the value of the
associated derivatives. We develop a model with two types of traders that have
different information and contribute methods for simulating the behavior of
each trader and accounting for trade PnL. We also develop ideas around the
equilibrium distribution of fair price conditional on the arrival of traders.
Finally, we show how these findings might be used to think about parameters for
alternative CFMMs.


------------------------------------------------------------------------------

Title:
Prepare the Chair for the Bear! Robot Imagination of Sitting Affordance  to Reorient Previously Unseen Chairs

Abstract: In this letter, a paradigm for the classification and manipulation of
previously unseen objects is established and demonstrated through a real
example of chairs. We present a novel robot manipulation method, guided by the
understanding of object stability, perceptibility, and affordance, which allows
the robot to prepare previously unseen and randomly oriented chairs for a teddy
bear to sit on. Specifically, the robot encounters an unknown object and first
reconstructs a complete 3D model from perceptual data via active and autonomous
manipulation. By inserting this model into a physical simulator (i.e., the
robot's "imagination"), the robot assesses whether the object is a chair and
determines how to reorient it properly to be used, i.e., how to reorient it to
an upright and accessible pose. If the object is classified as a chair, the
robot reorients the object to this pose and seats the teddy bear onto the
chair. The teddy bear is a proxy for an elderly person, hospital patient, or
child. Experiment results show that our method achieves a high success rate on
the real robot task of chair preparation. Also, it outperforms several baseline
methods on the task of upright pose prediction for chairs.


------------------------------------------------------------------------------

Title:
MILD: Modeling the Instance Learning Dynamics for Learning with Noisy  Labels

Abstract: Despite deep learning has achieved great success, it often relies on a large
amount of training data with accurate labels, which are expensive and
time-consuming to collect. A prominent direction to reduce the cost is to learn
with noisy labels, which are ubiquitous in the real-world applications. A
critical challenge for such a learning task is to reduce the effect of network
memorization on the falsely-labeled data. In this work, we propose an iterative
selection approach based on the Weibull mixture model, which identifies clean
data by considering the overall learning dynamics of each data instance. In
contrast to the previous small-loss heuristics, we leverage the observation
that deep network is easy to memorize and hard to forget clean data. In
particular, we measure the difficulty of memorization and forgetting for each
instance via the transition times between being misclassified and being
memorized in training, and integrate them into a novel metric for selection.
Based on the proposed metric, we retain a subset of identified clean data and
repeat the selection procedure to iteratively refine the clean subset, which is
finally used for model training. To validate our method, we perform extensive
experiments on synthetic noisy datasets and real-world web data, and our
strategy outperforms existing noisy-label learning methods.


------------------------------------------------------------------------------

Title:
Minimum Eigenvalue Based Covariance Matrix Estimation with Limited  Samples

Abstract: In this paper, we consider the interference rejection combining (IRC)
receiver, which improves the cell-edge user throughput via suppressing
inter-cell interference and requires estimating the covariance matrix including
the inter-cell interference with high accuracy. In order to solve the problem
of sample covariance matrix estimation with limited samples, a regularization
parameter optimization based on the minimum eigenvalue criterion is developed.
It is different from traditional methods that aim at minimizing the mean
squared error, but goes straight at the objective of optimizing the final
performance of the IRC receiver. A lower bound of the minimum eigenvalue that
is easier to calculate is also derived. Simulation results demonstrate that the
proposed approach is effective and can approach the performance of the oracle
estimator in terms of the mutual information metric.


------------------------------------------------------------------------------

Title:
Evaluation of motion comfort using advanced active human body models and  efficient simplified models

Abstract: Active muscles are crucial for maintaining postural stability when seated in
a moving vehicle. Advanced active 3D non-linear full body models have been
developed for impact and comfort simulation, including large numbers of
individual muscle elements, and detailed non-linear models of the joint
structures. While such models have an apparent potential to provide insight
into postural stabilization, they are computationally demanding, making them
less practical in particular for driving comfort where long time periods are to
be studied. In vibration comfort and in general biomechanical research,
linearized models are effectively used. This paper evaluates the effectiveness
of simplified 3D full body human models to capture vibration comfort. An
efficient seated human body model is developed and validated using experimental
data. We evaluate the required complexity in terms of joints and degrees of
freedom for the spine, and explore how well linear spring-damper models can
approximate reflexive postural stabilization. Results indicate that linear
stiffness and damping models can capture well the human response. However, the
results are improved by adding proportional integral derivative (PID)
controllers to maintain the defined initial body posture. The integrator is
shown to be essential to prevent drift from the defined posture. The joint
angular relative displacement is used as the input reference to each PID
controller. With this model, a faster than real time solution is obtained when
used with a simple seat model. The paper also discusses advantages and
disadvantages of various models and provides insight into which models are more
appropriate for motion comfort analysis. The results of this paper can provide
valuable insights for designers, and researchers in the automotive and seating
industries to improve the comfort and safety of seats and vehicles occupants.


------------------------------------------------------------------------------

Title:
HabiCrowd: A High Performance Simulator for Crowd-Aware Visual  Navigation

Abstract: Visual navigation, a foundational aspect of Embodied AI (E-AI), has been
significantly studied in the past few years. While many 3D simulators have been
introduced to support visual navigation tasks, scarcely works have been
directed towards combining human dynamics, creating the gap between simulation
and real-world applications. Furthermore, current 3D simulators incorporating
human dynamics have several limitations, particularly in terms of computational
efficiency, which is a promise of E-AI simulators. To overcome these
shortcomings, we introduce HabiCrowd, the first standard benchmark for
crowd-aware visual navigation that integrates a crowd dynamics model with
diverse human settings into photorealistic environments. Empirical evaluations
demonstrate that our proposed human dynamics model achieves state-of-the-art
performance in collision avoidance, while exhibiting superior computational
efficiency compared to its counterparts. We leverage HabiCrowd to conduct
several comprehensive studies on crowd-aware visual navigation tasks and
human-robot interactions. The source code and data can be found at
this https URL


------------------------------------------------------------------------------

Title:
One model to rule them all: ranking Slovene summarizers

Abstract: Text summarization is an essential task in natural language processing, and
researchers have developed various approaches over the years, ranging from
rule-based systems to neural networks. However, there is no single model or
approach that performs well on every type of text. We propose a system that
recommends the most suitable summarization model for a given text. The proposed
system employs a fully connected neural network that analyzes the input content
and predicts which summarizer should score the best in terms of ROUGE score for
a given input. The meta-model selects among four different summarization
models, developed for the Slovene language, using different properties of the
input, in particular its Doc2Vec document representation. The four Slovene
summarization models deal with different challenges associated with text
summarization in a less-resourced language. We evaluate the proposed SloMetaSum
model performance automatically and parts of it manually. The results show that
the system successfully automates the step of manually selecting the best
model.


------------------------------------------------------------------------------

Title:
Collision Avoidance Detour for Multi-Agent Trajectory Forecasting

Abstract: We present our approach, Collision Avoidance Detour (CAD), which won the 3rd
place award in the 2023 Waymo Open Dataset Challenge - Sim Agents, held at the
2023 CVPR Workshop on Autonomous Driving. To satisfy the motion prediction
factorization requirement, we partition all the valid objects into three
mutually exclusive sets: Autonomous Driving Vehicle (ADV),
World-tracks-to-predict, and World-others. We use different motion models to
forecast their future trajectories independently. Furthermore, we also apply
collision avoidance detour resampling, additive Gaussian noise, and
velocity-based heading estimation to improve the realism of our simulation
result.


------------------------------------------------------------------------------

Title:
Stable and Consistent Prediction of 3D Characteristic Orientation via  Invariant Residual Learning

Abstract: Learning to predict reliable characteristic orientations of 3D point clouds
is an important yet challenging problem, as different point clouds of the same
class may have largely varying appearances. In this work, we introduce a novel
method to decouple the shape geometry and semantics of the input point cloud to
achieve both stability and consistency. The proposed method integrates
shape-geometry-based SO(3)-equivariant learning and shape-semantics-based
SO(3)-invariant residual learning, where a final characteristic orientation is
obtained by calibrating an SO(3)-equivariant orientation hypothesis using an
SO(3)-invariant residual rotation. In experiments, the proposed method not only
demonstrates superior stability and consistency but also exhibits
state-of-the-art performances when applied to point cloud part segmentation,
given randomly rotated inputs.


------------------------------------------------------------------------------

Title:
Non-Integer-Oversampling Digital Signal Processing for Coherent Passive  Optical Networks

Abstract: Beyond 100G passive optical networks (PONs) will be required to meet the
ever-increasing traffic demand in the future. Coherent optical technologies are
the competitive solutions for the future beyond 100G PON but also face
challenges such as the high computational complexity of digital signal
processing (DSP). A high oversampling rate in coherent optical technologies
results in the high computational complexity of DSP. Therefore, DSP running in
a non-integer-oversampling below 2 samples-per-symbol (sps) is preferred, which
can not only reduce computational complexity but also obviously lower the
requirement for the analog-to-digital converter. In this paper, we propose a
non-integer-oversampling DSP for meeting the requirements of coherent PON. The
proposed DSP working at 9/8-sps and 5/4-sps oversampling rates can be reduced
by 44.04% and 40.78% computational complexity compared to that working at the
2-sps oversampling rate, respectively. Moreover, a 400-Gb/s-net-rate coherent
PON based on digital subcarrier multiplexing was demonstrated to verify the
feasibility of the non-integer-oversampling DSP. There is almost no penalty on
the receiver sensitivity when the non-integer-oversampling DSP is adopted. In
conclusion, the non-integer-oversampling DSP shows great potential in the
future coherent PON.


------------------------------------------------------------------------------

Title:
The Age of Synthetic Realities: Challenges and Opportunities

Abstract: Synthetic realities are digital creations or augmentations that are
contextually generated through the use of Artificial Intelligence (AI) methods,
leveraging extensive amounts of data to construct new narratives or realities,
regardless of the intent to deceive. In this paper, we delve into the concept
of synthetic realities and their implications for Digital Forensics and society
at large within the rapidly advancing field of AI. We highlight the crucial
need for the development of forensic techniques capable of identifying harmful
synthetic creations and distinguishing them from reality. This is especially
important in scenarios involving the creation and dissemination of fake news,
disinformation, and misinformation. Our focus extends to various forms of
media, such as images, videos, audio, and text, as we examine how synthetic
realities are crafted and explore approaches to detecting these malicious
creations. Additionally, we shed light on the key research challenges that lie
ahead in this area. This study is of paramount importance due to the rapid
progress of AI generative techniques and their impact on the fundamental
principles of Forensic Science.


------------------------------------------------------------------------------

Title:
Contrastive Disentangled Learning on Graph for Node Classification

Abstract: Contrastive learning methods have attracted considerable attention due to
their remarkable success in analyzing graph-structured data. Inspired by the
success of contrastive learning, we propose a novel framework for contrastive
disentangled learning on graphs, employing a disentangled graph encoder and two
carefully crafted self-supervision signals. Specifically, we introduce a
disentangled graph encoder to enforce the framework to distinguish various
latent factors corresponding to underlying semantic information and learn the
disentangled node embeddings. Moreover, to overcome the heavy reliance on
labels, we design two self-supervision signals, namely node specificity and
channel independence, which capture informative knowledge without the need for
labeled data, thereby guiding the automatic disentanglement of nodes. Finally,
we perform node classification tasks on three citation networks by using the
disentangled node embeddings, and the relevant analysis is provided.
Experimental results validate the effectiveness of the proposed framework
compared with various baselines.


------------------------------------------------------------------------------

Title:
Augmenting Sub-model to Improve Main Model

Abstract: Image classification has improved with the development of training
techniques. However, these techniques often require careful parameter tuning to
balance the strength of regularization, limiting their potential benefits. In
this paper, we propose a novel way to use regularization called Augmenting
Sub-model (AugSub). AugSub consists of two models: the main model and the
sub-model. While the main model employs conventional training recipes, the
sub-model leverages the benefit of additional regularization. AugSub achieves
this by mitigating adverse effects through a relaxed loss function similar to
self-distillation loss. We demonstrate the effectiveness of AugSub with three
drop techniques: dropout, drop-path, and random masking. Our analysis shows
that all AugSub improves performance, with the training loss converging even
faster than regular training. Among the three, AugMask is identified as the
most practical method due to its performance and cost efficiency. We further
validate AugMask across diverse training recipes, including DeiT-III, ResNet,
MAE fine-tuning, and Swin Transformer. The results show that AugMask
consistently provides significant performance gain. AugSub provides a practical
and effective solution for introducing additional regularization under various
training recipes. Code is available at
\url{this https URL}.


------------------------------------------------------------------------------

Title:
Multi-pass Training and Cross-information Fusion for Low-resource  End-to-end Accented Speech Recognition

Abstract: Low-resource accented speech recognition is one of the important challenges
faced by current ASR technology in practical applications. In this study, we
propose a Conformer-based architecture, called Aformer, to leverage both the
acoustic information from large non-accented and limited accented training
data. Specifically, a general encoder and an accent encoder are designed in the
Aformer to extract complementary acoustic information. Moreover, we propose to
train the Aformer in a multi-pass manner, and investigate three
cross-information fusion methods to effectively combine the information from
both general and accent encoders. All experiments are conducted on both the
accented English and Mandarin ASR tasks. Results show that our proposed methods
outperform the strong Conformer baseline by relative 10.2% to 24.5%
word/character error rate reduction on six in-domain and out-of-domain accented
test sets.


------------------------------------------------------------------------------

Title:
Towards Theory-based Moral AI: Moral AI with Aggregating Models Based on  Normative Ethical Theory

Abstract: Moral AI has been studied in the fields of philosophy and artificial
intelligence. Although most existing studies are only theoretical, recent
developments in AI have made it increasingly necessary to implement AI with
morality. On the other hand, humans are under the moral uncertainty of not
knowing what is morally right. In this paper, we implement the Maximizing
Expected Choiceworthiness (MEC) algorithm, which aggregates outputs of models
based on three normative theories of normative ethics to generate the most
appropriate output. MEC is a method for making appropriate moral judgments
under moral uncertainty. Our experimental results suggest that the output of
MEC correlates to some extent with commonsense morality and that MEC can
produce equally or more appropriate output than existing methods.


------------------------------------------------------------------------------

Title:
UM-CAM: Uncertainty-weighted Multi-resolution Class Activation Maps for  Weakly-supervised Fetal Brain Segmentation

Abstract: Accurate segmentation of the fetal brain from Magnetic Resonance Image (MRI)
is important for prenatal assessment of fetal development. Although deep
learning has shown the potential to achieve this task, it requires a large fine
annotated dataset that is difficult to collect. To address this issue,
weakly-supervised segmentation methods with image-level labels have gained
attention, which are commonly based on class activation maps from a
classification network trained with image tags. However, most of these methods
suffer from incomplete activation regions, due to the low-resolution
localization without detailed boundary cues. To this end, we propose a novel
weakly-supervised method with image-level labels based on semantic features and
context information exploration. We first propose an Uncertainty-weighted
Multi-resolution Class Activation Map (UM-CAM) to generate high-quality
pixel-level supervision. Then, we design a Geodesic distance-based Seed
Expansion (GSE) method to provide context information for rectifying the
ambiguous boundaries of UM-CAM. Extensive experiments on a fetal brain dataset
show that our UM-CAM can provide more accurate activation regions with fewer
false positive regions than existing CAM variants, and our proposed method
outperforms state-of-the-art weakly-supervised methods with image-level labels.


------------------------------------------------------------------------------

Title:
Timestamped Embedding-Matching Acoustic-to-Word CTC ASR

Abstract: In this work, we describe a novel method of training an embedding-matching
word-level connectionist temporal classification (CTC) automatic speech
recognizer (ASR) such that it directly produces word start times and durations,
required by many real-world applications, in addition to the transcription. The
word timestamps enable the ASR to output word segmentations and word confusion
networks without relying on a secondary model or forced alignment process when
testing. Our proposed system has similar word segmentation accuracy as a hybrid
DNN-HMM (Deep Neural Network-Hidden Markov Model) system, with less than 3ms
difference in mean absolute error in word start times on TIMIT data. At the
same time, we observed less than 5% relative increase in the word error rate
compared to the non-timestamped system when using the same audio training data
and nearly identical model size. We also contribute more rigorous analysis of
multiple-hypothesis embedding-matching ASR in general.


------------------------------------------------------------------------------

Title:
Variational Disentangled Graph Auto-Encoders for Link Prediction

Abstract: With the explosion of graph-structured data, link prediction has emerged as
an increasingly important task. Embedding methods for link prediction utilize
neural networks to generate node embeddings, which are subsequently employed to
predict links between nodes. However, the existing embedding methods typically
take a holistic strategy to learn node embeddings and ignore the entanglement
of latent factors. As a result, entangled embeddings fail to effectively
capture the underlying information and are vulnerable to irrelevant
information, leading to unconvincing and uninterpretable link prediction
results. To address these challenges, this paper proposes a novel framework
with two variants, the disentangled graph auto-encoder (DGAE) and the
variational disentangled graph auto-encoder (VDGAE). Our work provides a
pioneering effort to apply the disentanglement strategy to link prediction. The
proposed framework infers the latent factors that cause edges in the graph and
disentangles the representation into multiple channels corresponding to unique
latent factors, which contributes to improving the performance of link
prediction. To further encourage the embeddings to capture mutually exclusive
latent factors, we introduce mutual information regularization to enhance the
independence among different channels. Extensive experiments on various
real-world benchmarks demonstrate that our proposed methods achieve
state-of-the-art results compared to a variety of strong baselines on link
prediction tasks. Qualitative analysis on the synthetic dataset also
illustrates that the proposed methods can capture distinct latent factors that
cause links, providing empirical evidence that our models are able to explain
the results of link prediction to some extent. All code will be made publicly
available upon publication of the paper.


------------------------------------------------------------------------------

Title:
Unfolding Framework with Prior of Convolution-Transformer Mixture and  Uncertainty Estimation for Video Snapshot Compressive Imaging

Abstract: We consider the problem of video snapshot compressive imaging (SCI), where
sequential high-speed frames are modulated by different masks and captured by a
single measurement. The underlying principle of reconstructing multi-frame
images from only one single measurement is to solve an ill-posed problem. By
combining optimization algorithms and neural networks, deep unfolding networks
(DUNs) score tremendous achievements in solving inverse problems. In this
paper, our proposed model is under the DUN framework and we propose a 3D
Convolution-Transformer Mixture (CTM) module with a 3D efficient and scalable
attention model plugged in, which helps fully learn the correlation between
temporal and spatial dimensions by virtue of Transformer. To our best
knowledge, this is the first time that Transformer is employed to video SCI
reconstruction. Besides, to further investigate the high-frequency information
during the reconstruction process which are neglected in previous studies, we
introduce variance estimation characterizing the uncertainty on a
pixel-by-pixel basis. Extensive experimental results demonstrate that our
proposed method achieves state-of-the-art (SOTA) (with a 1.2dB gain in PSNR
over previous SOTA algorithm) results. We will release the code.


------------------------------------------------------------------------------

Title:
RM-PRT: Realistic Robotic Manipulation Simulator and Benchmark with  Progressive Reasoning Tasks

Abstract: Recently, the advent of pre-trained large-scale language models (LLMs) like
ChatGPT and GPT-4 have significantly advanced the machine's natural language
understanding capabilities. This breakthrough has allowed us to seamlessly
integrate these open-source LLMs into a unified robot simulator environment to
help robots accurately understand and execute human natural language
instructions. To this end, in this work, we introduce a realistic robotic
manipulation simulator and build a Robotic Manipulation with Progressive
Reasoning Tasks (RM-PRT) benchmark on this basis. Specifically, the RM-PRT
benchmark builds a new high-fidelity digital twin scene based on Unreal Engine
5, which includes 782 categories, 2023 objects, and 15K natural language
instructions generated by ChatGPT for a detailed evaluation of robot
manipulation. We propose a general pipeline for the RM-PRT benchmark that takes
as input multimodal prompts containing natural language instructions and
automatically outputs actions containing the movement and position transitions.
We set four natural language understanding tasks with progressive reasoning
levels and evaluate the robot's ability to understand natural language
instructions in two modes of adsorption and grasping. In addition, we also
conduct a comprehensive analysis and comparison of the differences and
advantages of 10 different LLMs in instruction understanding and generation
quality. We hope the new simulator and benchmark will facilitate future
research on language-guided robotic manipulation. Project website:
this https URL .


------------------------------------------------------------------------------

Title:
Understanding Contrastive Learning Through the Lens of Margins

Abstract: Self-supervised learning, or SSL, holds the key to expanding the usage of
machine learning in real-world tasks by alleviating heavy human supervision.
Contrastive learning and its varieties have been SSL strategies in various
fields. We use margins as a stepping stone for understanding how contrastive
learning works at a deeper level and providing potential directions to improve
representation learning. Through gradient analysis, we found that margins scale
gradients in three different ways: emphasizing positive samples, de-emphasizing
positive samples when angles of positive samples are wide, and attenuating the
diminishing gradients as the estimated probability approaches the target
probability. We separately analyze each and provide possible directions for
improving SSL frameworks. Our experimental results demonstrate that these
properties can contribute to acquiring better representations, which can
enhance performance in both seen and unseen datasets.


------------------------------------------------------------------------------

Title:
Int-HRL: Towards Intention-based Hierarchical Reinforcement Learning

Abstract: While deep reinforcement learning (RL) agents outperform humans on an
increasing number of tasks, training them requires data equivalent to decades
of human gameplay. Recent hierarchical RL methods have increased sample
efficiency by incorporating information inherent to the structure of the
decision problem but at the cost of having to discover or use human-annotated
sub-goals that guide the learning process. We show that intentions of human
players, i.e. the precursor of goal-oriented decisions, can be robustly
predicted from eye gaze even for the long-horizon sparse rewards task of
Montezuma's Revenge - one of the most challenging RL tasks in the Atari2600
game suite. We propose Int-HRL: Hierarchical RL with intention-based sub-goals
that are inferred from human eye gaze. Our novel sub-goal extraction pipeline
is fully automatic and replaces the need for manual sub-goal annotation by
human experts. Our evaluations show that replacing hand-crafted sub-goals with
automatically extracted intentions leads to a HRL agent that is significantly
more sample efficient than previous methods.


------------------------------------------------------------------------------

Title:
Cooperative Multi-Agent Learning for Navigation via Structured State  Abstraction

Abstract: Cooperative multi-agent reinforcement learning (MARL) for navigation enables
agents to cooperate to achieve their navigation goals. Using emergent
communication, agents learn a communication protocol to coordinate and share
information that is needed to achieve their navigation tasks. In emergent
communication, symbols with no pre-specified usage rules are exchanged, in
which the meaning and syntax emerge through training. Learning a navigation
policy along with a communication protocol in a MARL environment is highly
complex due to the huge state space to be explored. To cope with this
complexity, this work proposes a novel neural network architecture, for jointly
learning an adaptive state space abstraction and a communication protocol among
agents participating in navigation tasks. The goal is to come up with an
adaptive abstractor that significantly reduces the size of the state space to
be explored, without degradation in the policy performance. Simulation results
show that the proposed method reaches a better policy, in terms of achievable
rewards, resulting in fewer training iterations compared to the case where raw
states or fixed state abstraction are used. Moreover, it is shown that a
communication protocol emerges during training which enables the agents to
learn better policies within fewer training iterations.


------------------------------------------------------------------------------

Title:
TransRef: Multi-Scale Reference Embedding Transformer for  Reference-Guided Image Inpainting

Abstract: Image inpainting for completing complicated semantic environments and diverse
hole patterns of corrupted images is challenging even for state-of-the-art
learning-based inpainting methods trained on large-scale data. A reference
image capturing the same scene of a corrupted image offers informative guidance
for completing the corrupted image as it shares similar texture and structure
priors to that of the holes of the corrupted image. In this work, we propose a
transformer-based encoder-decoder network, named TransRef, for reference-guided
image inpainting. Specifically, the guidance is conducted progressively through
a reference embedding procedure, in which the referencing features are
subsequently aligned and fused with the features of the corrupted image. For
precise utilization of the reference features for guidance, a reference-patch
alignment (Ref-PA) module is proposed to align the patch features of the
reference and corrupted images and harmonize their style differences, while a
reference-patch transformer (Ref-PT) module is proposed to refine the embedded
reference feature. Moreover, to facilitate the research of reference-guided
image restoration tasks, we construct a publicly accessible benchmark dataset
containing 50K pairs of input and reference images. Both quantitative and
qualitative evaluations demonstrate the efficacy of the reference information
and the proposed method over the state-of-the-art methods in completing complex
holes. Code and dataset can be accessed at this https URL


------------------------------------------------------------------------------

Title:
CrossKD: Cross-Head Knowledge Distillation for Dense Object Detection

Abstract: Knowledge Distillation (KD) has been validated as an effective model
compression technique for learning compact object detectors. Existing
state-of-the-art KD methods for object detection are mostly based on feature
imitation, which is generally observed to be better than prediction mimicking.
In this paper, we show that the inconsistency of the optimization objectives
between the ground-truth signals and distillation targets is the key reason for
the inefficiency of prediction mimicking. To alleviate this issue, we present a
simple yet effective distillation scheme, termed CrossKD, which delivers the
intermediate features of the student's detection head to the teacher's
detection head. The resulting cross-head predictions are then forced to mimic
the teacher's predictions. Such a distillation manner relieves the student's
head from receiving contradictory supervision signals from the ground-truth
annotations and the teacher's predictions, greatly improving the student's
detection performance. On MS COCO, with only prediction mimicking losses
applied, our CrossKD boosts the average precision of GFL ResNet-50 with 1x
training schedule from 40.2 to 43.7, outperforming all existing KD methods for
object detection. Code is available at this https URL


------------------------------------------------------------------------------

Title:
HDVIO: Improving Localization and Disturbance Estimation with Hybrid  Dynamics VIO

Abstract: Visual-inertial odometry (VIO) is the most common approach for estimating the
state of autonomous micro aerial vehicles using only onboard sensors. Existing
methods improve VIO performance by including a dynamics model in the estimation
pipeline. However, such methods degrade in the presence of low-fidelity vehicle
models and continuous external disturbances, such as wind. Our proposed method,
HDVIO, overcomes these limitations by using a hybrid dynamics model that
combines a point-mass vehicle model with a learning-based component that
captures complex aerodynamic effects. HDVIO estimates the external force and
the full robot state by leveraging the discrepancy between the actual motion
and the predicted motion of the hybrid dynamics model. Our hybrid dynamics
model uses a history of thrust and IMU measurements to predict the vehicle
dynamics. To demonstrate the performance of our method, we present results on
both public and novel drone dynamics datasets and show real-world experiments
of a quadrotor flying in strong winds up to 25 km/h. The results show that our
approach improves the motion and external force estimation compared to the
state-of-the-art by up to 33% and 40%, respectively. Furthermore, differently
from existing methods, we show that it is possible to predict the vehicle
dynamics accurately while having no explicit knowledge of its full state.


------------------------------------------------------------------------------

Title:
From structure mining to unsupervised exploration of atomic octahedral  networks

Abstract: Networks of atom-centered coordination octahedra commonly occur in inorganic
and hybrid solid-state materials. Characterizing their spatial arrangements and
characteristics is crucial for relating structures to properties for many
materials families. The traditional method using case-by-case inspection
becomes prohibitive for discovering trends and similarities in large datasets.
Here, we operationalize chemical intuition to automate the geometric parsing,
quantification, and classification of coordination octahedral networks. We find
axis-resolved tilting trends in ABO$_{3}$ perovskite polymorphs, which assist
in detecting oxidation state changes. Moreover, we develop a scale-invariant
encoding scheme to represent these networks, which, combined with
human-assisted unsupervised machine learning, allows us to taxonomize the
inorganic framework polytypes in hybrid iodoplumbates (A$_x$Pb$_y$I$_z$).
Consequently, we uncover a violation of Pauling's third rule and the design
principles underpinning their topological diversity. Our results offer a
glimpse into the vast design space of atomic octahedral networks and inform
high-throughput, targeted screening of specific structure types.


------------------------------------------------------------------------------

Title:
PyRCA: A Library for Metric-based Root Cause Analysis

Abstract: We introduce PyRCA, an open-source Python machine learning library of Root
Cause Analysis (RCA) for Artificial Intelligence for IT Operations (AIOps). It
provides a holistic framework to uncover the complicated metric causal
dependencies and automatically locate root causes of incidents. It offers a
unified interface for multiple commonly used RCA models, encompassing both
graph construction and scoring tasks. This library aims to provide IT
operations staff, data scientists, and researchers a one-step solution to rapid
model development, model evaluation and deployment to online applications. In
particular, our library includes various causal discovery methods to support
causal graph construction, and multiple types of root cause scoring methods
inspired by Bayesian analysis, graph analysis and causal analysis, etc. Our GUI
dashboard offers practitioners an intuitive point-and-click interface,
empowering them to easily inject expert knowledge through human interaction.
With the ability to visualize causal graphs and the root cause of incidents,
practitioners can quickly gain insights and improve their workflow efficiency.
This technical report introduces PyRCA's architecture and major
functionalities, while also presenting benchmark performance numbers in
comparison to various baseline models. Additionally, we demonstrate PyRCA's
capabilities through several example use cases.


------------------------------------------------------------------------------

Title:
MSVD-Indonesian: A Benchmark for Multimodal Video-Text Tasks in  Indonesian

Abstract: Multimodal learning on video and text data has been receiving growing
attention from many researchers in various research tasks, including
text-to-video retrieval, video-to-text retrieval, and video captioning.
Although many algorithms have been proposed for those challenging tasks, most
of them are developed on English language datasets. Despite Indonesian being
one of the most spoken languages in the world, the research progress on the
multimodal video-text with Indonesian sentences is still under-explored, likely
due to the absence of the public benchmark dataset. To address this issue, we
construct the first public Indonesian video-text dataset by translating English
sentences from the MSVD dataset to Indonesian sentences. Using our dataset, we
then train neural network models which were developed for the English
video-text dataset on three tasks, i.e., text-to-video retrieval, video-to-text
retrieval, and video captioning. The recent neural network-based approaches to
video-text tasks often utilized a feature extractor that is primarily
pretrained on an English vision-language dataset. Since the availability of the
pretraining resources with Indonesian sentences is relatively limited, the
applicability of those approaches to our dataset is still questionable. To
overcome the lack of pretraining resources, we apply cross-lingual transfer
learning by utilizing the feature extractors pretrained on the English dataset,
and we then fine-tune the models on our Indonesian dataset. Our experimental
results show that this approach can help to improve the performance for the
three tasks on all metrics. Finally, we discuss potential future works using
our dataset, inspiring further research in the Indonesian multimodal video-text
tasks. We believe that our dataset and our experimental results could provide
valuable contributions to the community. Our dataset is available on GitHub.


------------------------------------------------------------------------------

Title:
MuDPT: Multi-modal Deep-symphysis Prompt Tuning for Large Pre-trained  Vision-Language Models

Abstract: Prompt tuning, like CoOp, has recently shown promising vision recognizing and
transfer learning ability on various downstream tasks with the emergence of
large pre-trained vision-language models like CLIP. However, we identify that
existing uni-modal prompt tuning approaches may result in sub-optimal
performance since this uni-modal design breaks the original alignment of
textual and visual representations in the pre-trained model. Inspired by the
nature of pre-trained vision-language models, we aim to achieve completeness in
prompt tuning and propose a novel approach called Multi-modal Deep-symphysis
Prompt Tuning, dubbed as MuDPT, which extends independent multi-modal prompt
tuning by additionally learning a model-agnostic transformative network to
allow deep hierarchical bi-directional prompt fusion. We evaluate the
effectiveness of MuDPT on few-shot vision recognition and out-of-domain
generalization tasks. Compared with the state-of-the-art methods, MuDPT
achieves better recognition and generalization ability with an apparent margin
thanks to synergistic alignment of textual and visual representations. Our code
is available at: this https URL


------------------------------------------------------------------------------

Title:
Generative Retrieval as Dense Retrieval

Abstract: Generative retrieval is a promising new neural retrieval paradigm that aims
to optimize the retrieval pipeline by performing both indexing and retrieval
with a single transformer model. However, this new paradigm faces challenges
with updating the index and scaling to large collections. In this paper, we
analyze two prominent variants of generative retrieval and show that they can
be conceptually viewed as bi-encoders for dense retrieval. Specifically, we
analytically demonstrate that the generative retrieval process can be
decomposed into dot products between query and document vectors, similar to
dense retrieval. This analysis leads us to propose a new variant of generative
retrieval, called Tied-Atomic, which addresses the updating and scaling issues
by incorporating techniques from dense retrieval. In experiments on two
datasets, NQ320k and the full MSMARCO, we confirm that this approach does not
reduce retrieval effectiveness while enabling the model to scale to large
collections.


------------------------------------------------------------------------------

Title:
RoMe: Towards Large Scale Road Surface Reconstruction via Mesh  Representation

Abstract: Large-scale road surface reconstruction is becoming important to autonomous
driving systems, as it provides valuable training and testing data effectively.
In this paper, we introduce a simple yet efficient method, RoMe, for
large-scale Road surface reconstruction via Mesh representations. To simplify
the problem, RoMe decomposes a 3D road surface into a triangle-mesh and a
multilayer perception network to model the road elevation implicitly. To retain
fine surface details, each mesh vertex has two extra attributes, namely color
and semantics. To improve the efficiency of RoMe in large-scale environments, a
novel waypoint sampling method is introduced. As such, RoMe can properly
preserve road surface details, with only linear computational complexity to
road areas. In addition, to improve the accuracy of RoMe, extrinsics
optimization is proposed to mitigate inaccurate extrinsic calibrations.
Experimental results on popular public datasets also demonstrate the high
efficiency and accuracy of RoMe.


------------------------------------------------------------------------------

Title:
Improving Image Captioning Descriptiveness by Ranking and LLM-based  Fusion

Abstract: State-of-The-Art (SoTA) image captioning models often rely on the Microsoft
COCO (MS-COCO) dataset for training. This dataset contains annotations provided
by human annotators, who typically produce captions averaging around ten
tokens. However, this constraint presents a challenge in effectively capturing
complex scenes and conveying detailed information. Furthermore, captioning
models tend to exhibit bias towards the ``average'' caption, which captures
only the more general aspects. What would happen if we were able to
automatically generate longer captions, thereby making them more detailed?
Would these captions, evaluated by humans, be more or less representative of
the image content compared to the original MS-COCO captions? In this paper, we
present a novel approach to address previous challenges by showcasing how
captions generated from different SoTA models can be effectively fused,
resulting in richer captions. Our proposed method leverages existing models
from the literature, eliminating the need for additional training. Instead, it
utilizes an image-text based metric to rank the captions generated by SoTA
models for a given image. Subsequently, the top two captions are fused using a
Large Language Model (LLM). Experimental results demonstrate the effectiveness
of our approach, as the captions generated by our model exhibit higher
consistency with human judgment when evaluated on the MS-COCO test set. By
combining the strengths of various SoTA models, our method enhances the quality
and appeal of image captions, bridging the gap between automated systems and
the rich, informative nature of human-generated descriptions. This advance
opens up new possibilities for generating captions that are more suitable for
the training of both vision-language and captioning models.


------------------------------------------------------------------------------

Title:
EMoG: Synthesizing Emotive Co-speech 3D Gesture with Diffusion Model

Abstract: Although previous co-speech gesture generation methods are able to synthesize
motions in line with speech content, it is still not enough to handle diverse
and complicated motion distribution. The key challenges are: 1) the one-to-many
nature between the speech content and gestures; 2) the correlation modeling
between the body joints. In this paper, we present a novel framework (EMoG) to
tackle the above challenges with denoising diffusion models: 1) To alleviate
the one-to-many problem, we incorporate emotion clues to guide the generation
process, making the generation much easier; 2) To model joint correlation, we
propose to decompose the difficult gesture generation into two sub-problems:
joint correlation modeling and temporal dynamics modeling. Then, the two
sub-problems are explicitly tackled with our proposed Joint Correlation-aware
transFormer (JCFormer). Through extensive evaluations, we demonstrate that our
proposed method surpasses previous state-of-the-art approaches, offering
substantial superiority in gesture synthesis.


------------------------------------------------------------------------------

Title:
Low Latency Edge Classification GNN for Particle Trajectory Tracking on  FPGAs

Abstract: In-time particle trajectory reconstruction in the Large Hadron Collider is
challenging due to the high collision rate and numerous particle hits. Using
GNN (Graph Neural Network) on FPGA has enabled superior accuracy with flexible
trajectory classification. However, existing GNN architectures have inefficient
resource usage and insufficient parallelism for edge classification. This paper
introduces a resource-efficient GNN architecture on FPGAs for low latency
particle tracking. The modular architecture facilitates design scalability to
support large graphs. Leveraging the geometric properties of hit detectors
further reduces graph complexity and resource usage. Our results on Xilinx
UltraScale+ VU9P demonstrate 1625x and 1574x performance improvement over CPU
and GPU respectively.


------------------------------------------------------------------------------

Title:
Learning Variable Impedance Skills from Demonstrations with Passivity  Guarantee

Abstract: Robots are increasingly being deployed not only in workplaces but also in
households. Effectively execute of manipulation tasks by robots relies on
variable impedance control with contact forces. Furthermore, robots should
possess adaptive capabilities to handle the considerable variations exhibited
by different robotic tasks in dynamic environments, which can be obtained
through human demonstrations. This paper presents a learning-from-demonstration
framework that integrates force sensing and motion information to facilitate
variable impedance control. The proposed approach involves the estimation of
full stiffness matrices from human demonstrations, which are then combined with
sensed forces and motion information to create a model using the non-parametric
method. This model allows the robot to replicate the demonstrated task while
also responding appropriately to new task conditions through the use of the
state-dependent stiffness profile. Additionally, a novel tank based variable
impedance control approach is proposed to ensure passivity by using the learned
stiffness. The proposed approach was evaluated using two virtual variable
stiffness systems. The first evaluation demonstrates that the stiffness
estimated approach exhibits superior robustness compared to traditional methods
when tested on manual datasets, and the second evaluation illustrates that the
novel tank based approach is more easily implementable compared to traditional
variable impedance control approaches.


------------------------------------------------------------------------------

Title:
Meerkat Behaviour Recognition Dataset

Abstract: Recording animal behaviour is an important step in evaluating the well-being
of animals and further understanding the natural world. Current methods for
documenting animal behaviour within a zoo setting, such as scan sampling,
require excessive human effort, are unfit for around-the-clock monitoring, and
may produce human-biased results. Several animal datasets already exist that
focus predominantly on wildlife interactions, with some extending to action or
behaviour recognition. However, there is limited data in a zoo setting or data
focusing on the group behaviours of social animals. We introduce a large
meerkat (Suricata Suricatta) behaviour recognition video dataset with diverse
annotated behaviours, including group social interactions, tracking of
individuals within the camera view, skewed class distribution, and varying
illumination conditions. This dataset includes videos from two positions within
the meerkat enclosure at the Wellington Zoo (Wellington, New Zealand), with
848,400 annotated frames across 20 videos and 15 unannotated videos.


------------------------------------------------------------------------------

Title:
Adversarial Search and Track with Multiagent Reinforcement Learning in  Sparsely Observable Environment

Abstract: We study a search and tracking (S&T) problem for a team of dynamic search
agents to capture an adversarial evasive agent with only sparse temporal and
spatial knowledge of its location in this paper. The domain is challenging for
traditional Reinforcement Learning (RL) approaches as the large space leads to
sparse observations of the adversary and in turn sparse rewards for the search
agents. Additionally, the opponent's behavior is reactionary to the search
agents, which causes a data distribution shift for RL during training as search
agents improve their policies. We propose a differentiable Multi-Agent RL
(MARL) architecture that utilizes a novel filtering module to supplement
estimated adversary location information and enables the effective learning of
a team policy. Our algorithm learns how to balance information from prior
knowledge and a motion model to remain resilient to the data distribution shift
and outperforms all baseline methods with a 46% increase of detection rate.


------------------------------------------------------------------------------

Title:
Traversing Between Modes in Function Space for Fast Ensembling

Abstract: Deep ensemble is a simple yet powerful way to improve the performance of deep
neural networks. Under this motivation, recent works on mode connectivity have
shown that parameters of ensembles are connected by low-loss subspaces, and one
can efficiently collect ensemble parameters in those subspaces. While this
provides a way to efficiently train ensembles, for inference, multiple forward
passes should still be executed using all the ensemble parameters, which often
becomes a serious bottleneck for real-world deployment. In this work, we
propose a novel framework to reduce such costs. Given a low-loss subspace
connecting two modes of a neural network, we build an additional neural network
that predicts the output of the original neural network evaluated at a certain
point in the low-loss subspace. The additional neural network, which we call a
"bridge", is a lightweight network that takes minimal features from the
original network and predicts outputs for the low-loss subspace without forward
passes through the original network. We empirically demonstrate that we can
indeed train such bridge networks and significantly reduce inference costs with
the help of bridge networks.


------------------------------------------------------------------------------

Title:
Progressive Neural Representation for Sequential Video Compilation

Abstract: Neural Implicit Representations (NIR) have gained significant attention
recently due to their ability to represent complex and high-dimensional data.
Unlike explicit representations, which require storing and manipulating
individual data points, implicit representations capture information through a
learned mapping function without explicitly representing the data points
themselves. They often prune or quantize neural networks after training to
accelerate encoding/decoding speed, yet we find that conventional methods fail
to transfer learned representations to new videos. This work studies the
continuous expansion of implicit video representations as videos arrive
sequentially over time, where the model can only access the videos from the
current session. We propose a novel neural video representation, Progressive
Neural Representation (PNR), that finds an adaptive substructure from the
supernet for a given video based on Lottery Ticket Hypothesis. At each training
session, our PNR transfers the learned knowledge of the previously obtained
subnetworks to learn the representation of the current video while keeping the
past subnetwork weights intact. Therefore it can almost perfectly preserve the
decoding ability (i.e., catastrophic forgetting) of the NIR on previous videos.
We demonstrate the effectiveness of our proposed PNR on the neural sequential
video representation compilation on the novel UVG8/17 video sequence
benchmarks.


------------------------------------------------------------------------------

Title:
A Collision-Based Hybrid Method for the BGK Equation

Abstract: We apply the collision-based hybrid introduced in \cite{hauck} to the
Boltzmann equation with the BGK operator and a hyperbolic scaling. An implicit
treatment of the source term is used to handle stiffness associated with the
BGK operator. Although it helps the numerical scheme become stable with a large
time step size, it is still not obvious to achieve the desired order of
accuracy due to the relationship between the size of the spatial cell and the
mean free path. Without asymptotic preserving property, a very restricted grid
size is required to resolve the mean free path, which is not practical. Our
approaches are based on the noncollision-collision decomposition of the BGK
equation. We introduce the arbitrary order of nodal discontinuous Galerkin (DG)
discretization in space with a semi-implicit time-stepping method; we employ
the backward Euler time integration for the uncollided equation and the 2nd
order predictor-corrector scheme for the collided equation, i.e., both source
terms in uncollided and collided equations are treated implicitly and only
streaming term in the collided equation is solved explicitly. This improves the
computational efficiency without the complexity of the numerical
implementation. Numerical results are presented for various Knudsen numbers to
present the effectiveness and accuracy of our hybrid method. Also, we compare
the solutions of the hybrid and non-hybrid schemes.


------------------------------------------------------------------------------

Title:
Decentralized Quantum Federated Learning for Metaverse: Analysis, Design  and Implementation

Abstract: With the emerging developments of the Metaverse, a virtual world where people
can interact, socialize, play, and conduct their business, it has become
critical to ensure that the underlying systems are transparent, secure, and
trustworthy. To this end, we develop a decentralized and trustworthy quantum
federated learning (QFL) framework. The proposed QFL leverages the power of
blockchain to create a secure and transparent system that is robust against
cyberattacks and fraud. In addition, the decentralized QFL system addresses the
risks associated with a centralized server-based approach. With extensive
experiments and analysis, we evaluate classical federated learning (CFL) and
QFL in a distributed setting and demonstrate the practicality and benefits of
the proposed design. Our theoretical analysis and discussions develop a
genuinely decentralized financial system essential for the Metaverse.
Furthermore, we present the application of blockchain-based QFL in a hybrid
metaverse powered by a metaverse observer and world model. Our implementation
details and code are publicly available 1.


------------------------------------------------------------------------------

Title:
ChatGPT Chemistry Assistant for Text Mining and Prediction of MOF  Synthesis

Abstract: We use prompt engineering to guide ChatGPT in the automation of text mining
of metal-organic frameworks (MOFs) synthesis conditions from diverse formats
and styles of the scientific literature. This effectively mitigates ChatGPT's
tendency to hallucinate information -- an issue that previously made the use of
Large Language Models (LLMs) in scientific fields challenging. Our approach
involves the development of a workflow implementing three different processes
for text mining, programmed by ChatGPT itself. All of them enable parsing,
searching, filtering, classification, summarization, and data unification with
different tradeoffs between labor, speed, and accuracy. We deploy this system
to extract 26,257 distinct synthesis parameters pertaining to approximately 800
MOFs sourced from peer-reviewed research articles. This process incorporates
our ChemPrompt Engineering strategy to instruct ChatGPT in text mining,
resulting in impressive precision, recall, and F1 scores of 90-99%.
Furthermore, with the dataset built by text mining, we constructed a
machine-learning model with over 86% accuracy in predicting MOF experimental
crystallization outcomes and preliminarily identifying important factors in MOF
crystallization. We also developed a reliable data-grounded MOF chatbot to
answer questions on chemical reactions and synthesis procedures. Given that the
process of using ChatGPT reliably mines and tabulates diverse MOF synthesis
information in a unified format, while using only narrative language requiring
no coding expertise, we anticipate that our ChatGPT Chemistry Assistant will be
very useful across various other chemistry sub-disciplines.


------------------------------------------------------------------------------

Title:
BASS: Boolean Automorphisms Signature Scheme

Abstract: We offer a digital signature scheme using Boolean automorphisms of a
multivariate polynomial algebra over integers. Verification part of this scheme
is based on the approximation of the number of zeros of a multivariate Boolean
function.


------------------------------------------------------------------------------

Title:
Frequency & Channel Attention for computationally efficient sound event  detection

Abstract: We explore on various attention methods on frequency and channel dimensions
for sound event detection (SED) in order to enhance performance with minimal
increase in computational cost while leveraging domain knowledge to address the
frequency dimension of audio data. We have introduced frequency dynamic
convolution in a previous work to release the translational equivariance issue
associated with 2D convolution on the frequency dimension of 2D audio data.
Although this approach demonstrated state-of-the-art SED performance, it
resulted in 2.5 times heavier model in terms of the number of parameters. To
achieve comparable SED performance with computationally efficient methods to
enhance practicality, we explore on lighter alternative attention methods. In
addition, we focus of attention methods on frequency and channel dimensions as
those are shown to be critical in SED. Joint application of SE modules on both
frequency and channel dimension shows comparable performance to frequency
dynamic convolution with only 2.7% increase in the model size compared to the
baseline model. In addition, we performed class-wise comparison of various
attention methods to further discuss their characteristics.


------------------------------------------------------------------------------

Title:
Insufficiently Justified Disparate Impact: A New Criterion for Subgroup  Fairness

Abstract: In this paper, we develop a new criterion, "insufficiently justified
disparate impact" (IJDI), for assessing whether recommendations (binarized
predictions) made by an algorithmic decision support tool are fair. Our novel,
utility-based IJDI criterion evaluates false positive and false negative error
rate imbalances, identifying statistically significant disparities between
groups which are present even when adjusting for group-level differences in
base rates. We describe a novel IJDI-Scan approach which can efficiently
identify the intersectional subpopulations, defined across multiple observed
attributes of the data, with the most significant IJDI. To evaluate IJDI-Scan's
performance, we conduct experiments on both simulated and real-world data,
including recidivism risk assessment and credit scoring. Further, we implement
and evaluate approaches to mitigating IJDI for the detected subpopulations in
these domains.


------------------------------------------------------------------------------

Title:
Habitat Synthetic Scenes Dataset (HSSD-200): An Analysis of 3D Scene  Scale and Realism Tradeoffs for ObjectGoal Navigation

Abstract: We contribute the Habitat Synthetic Scene Dataset, a dataset of 211
high-quality 3D scenes, and use it to test navigation agent generalization to
realistic 3D environments. Our dataset represents real interiors and contains a
diverse set of 18,656 models of real-world objects. We investigate the impact
of synthetic 3D scene dataset scale and realism on the task of training
embodied agents to find and navigate to objects (ObjectGoal navigation). By
comparing to synthetic 3D scene datasets from prior work, we find that scale
helps in generalization, but the benefits quickly saturate, making visual
fidelity and correlation to real-world scenes more important. Our experiments
show that agents trained on our smaller-scale dataset can match or outperform
agents trained on much larger datasets. Surprisingly, we observe that agents
trained on just 122 scenes from our dataset outperform agents trained on 10,000
scenes from the ProcTHOR-10K dataset in terms of zero-shot generalization in
real-world scanned environments.


------------------------------------------------------------------------------

Title:
A novel Counterfactual method for aspect-based sentiment analysis

Abstract: Aspect-based-sentiment-analysis (ABSA) is a fine-grained sentiment evaluation
task, which analyze the emotional polarity of the evaluation aspects.
Generally, the emotional polarity of an aspect exists in the corresponding
opinion expression, whose diversity has great impacts on model's performance.
To mitigate this problem, we propose a novel and simple counterfactual data
augmentation method that reverses the opinion expression of the aspects.
Specially, the integrated gradients are calculated to identify and mask the
opinion expression. Then, a prompt with the reverse expression polarity is
combined to the original text, and a pre-trained language model (PLM), T5, is
finally was employed to predict the masks. The experimental results show the
proposed counterfactual data augmentation method perform better than current
methods on three open-source datasets, i.e. Laptop, Restaurant and MAMS.


------------------------------------------------------------------------------

Title:
DICES Dataset: Diversity in Conversational AI Evaluation for Safety

Abstract: Machine learning approaches often require training and evaluation datasets
with a clear separation between positive and negative examples. This risks
simplifying and even obscuring the inherent subjectivity present in many tasks.
Preserving such variance in content and diversity in datasets is often
expensive and laborious. This is especially troubling when building safety
datasets for conversational AI systems, as safety is both socially and
culturally situated. To demonstrate this crucial aspect of conversational AI
safety, and to facilitate in-depth model performance analyses, we introduce the
DICES (Diversity In Conversational AI Evaluation for Safety) dataset that
contains fine-grained demographic information about raters, high replication of
ratings per item to ensure statistical power for analyses, and encodes rater
votes as distributions across different demographics to allow for in-depth
explorations of different aggregation strategies. In short, the DICES dataset
enables the observation and measurement of variance, ambiguity, and diversity
in the context of conversational AI safety. We also illustrate how the dataset
offers a basis for establishing metrics to show how raters' ratings can
intersects with demographic categories such as racial/ethnic groups, age
groups, and genders. The goal of DICES is to be used as a shared resource and
benchmark that respects diverse perspectives during safety evaluation of
conversational AI systems.


------------------------------------------------------------------------------

Title:
Mitigating Speculation-based Attacks through Configurable  Hardware/Software Co-design

Abstract: New speculation-based attacks that affect large numbers of modern systems are
disclosed regularly. Currently, CPU vendors regularly fall back to heavy-handed
mitigations like using barriers or enforcing strict programming guidelines
resulting in significant performance overhead. What is missing is a solution
that allows for efficient mitigation and is flexible enough to address both
current and future speculation vulnerabilities, without additional hardware
changes.
In this work, we present SpecControl, a novel hardware/software co-design,
that enables new levels of security while reducing the performance overhead
that has been demonstrated by state-of-the-art methodologies. SpecControl
introduces a communication interface that allows compilers and application
developers to inform the hardware about true branch dependencies, confidential
control-flow instructions, and fine-grained instruction constraints in order to
apply restrictions only when necessary. We evaluate SpecControl against known
speculative execution attacks and in addition, present a new speculative fetch
attack variant on the Pattern History Table (PHT) in branch predictors that
shows how similar previously reported vulnerabilities are more dangerous by
enabling unprivileged attacks, especially with the state-of-the-art branch
predictors. SpecControl provides stronger security guarantees compared to the
existing defenses while reducing the performance overhead of two
state-of-the-art defenses from 51% and 43% to just 23%.


------------------------------------------------------------------------------

Title:
Warm-Start Actor-Critic: From Approximation Error to Sub-optimality Gap

Abstract: Warm-Start reinforcement learning (RL), aided by a prior policy obtained from
offline training, is emerging as a promising RL approach for practical
applications. Recent empirical studies have demonstrated that the performance
of Warm-Start RL can be improved \textit{quickly} in some cases but become
\textit{stagnant} in other cases, especially when the function approximation is
used. To this end, the primary objective of this work is to build a fundamental
understanding on ``\textit{whether and when online learning can be
significantly accelerated by a warm-start policy from offline RL?}''.
Specifically, we consider the widely used Actor-Critic (A-C) method with a
prior policy. We first quantify the approximation errors in the Actor update
and the Critic update, respectively. Next, we cast the Warm-Start A-C algorithm
as Newton's method with perturbation, and study the impact of the approximation
errors on the finite-time learning performance with inaccurate Actor/Critic
updates. Under some general technical conditions, we derive the upper bounds,
which shed light on achieving the desired finite-learning performance in the
Warm-Start A-C algorithm. In particular, our findings reveal that it is
essential to reduce the algorithm bias in online learning.
We also obtain lower bounds on the sub-optimality gap of the Warm-Start A-C
algorithm to quantify the impact of the bias and error propagation.


------------------------------------------------------------------------------

Title:
RS5M: A Large Scale Vision-Language Dataset for Remote Sensing  Vision-Language Foundation Model

Abstract: Pre-trained Vision-Language Foundation Models utilizing extensive image-text
paired data have demonstrated unprecedented image-text association
capabilities, achieving remarkable results across various downstream tasks. A
critical challenge is how to make use of existing large-scale pre-trained VLMs,
which are trained on common objects, to perform the domain-specific transfer
for accomplishing domain-related downstream tasks. In this paper, we propose a
new framework that includes the Domain Foundation Model (DFM), bridging the gap
between the General Foundation Model (GFM) and domain-specific downstream
tasks. Moreover, we present an image-text paired dataset in the field of remote
sensing (RS), RS5M, which has 5 million RS images with English descriptions.
The dataset is obtained from filtering publicly available image-text paired
datasets and captioning label-only RS datasets with pre-trained VLM. These
constitute the first large-scale RS image-text paired dataset. Additionally, we
tried several Parameter-Efficient Fine-Tuning methods on RS5M to implement the
DFM. Experimental results show that our proposed dataset are highly effective
for various tasks, improving upon the baseline by $8 \% \sim 16 \%$ in
zero-shot classification tasks, and obtaining good results in both
Vision-Language Retrieval and Semantic Localization tasks. Finally, we show
successful results of training the RS Stable Diffusion model using the RS5M,
uncovering more use cases of the dataset.


------------------------------------------------------------------------------

Title:
An Introduction to the Compute Express Link (CXL) Interconnect

Abstract: The Compute Express Link (CXL) is an open industry-standard interconnect
between processors and devices such as accelerators, memory buffers, smart
network interfaces, persistent memory, and solid-state drives. CXL offers
coherency and memory semantics with bandwidth that scales with PCIe bandwidth
while achieving significantly lower latency than PCIe. All major CPU vendors,
device vendors, and datacenter operators have adopted CXL as a common standard.
This enables an inter-operable ecosystem that supports key computing use cases
including highly efficient accelerators, server memory bandwidth and capacity
expansion, multi-server resource pooling and sharing, and efficient
peer-to-peer communication. This survey provides an introduction to CXL
covering the standards CXL 1.0, CXL 2.0, and CXL 3.0. We further survey CXL
implementations, discuss CXL's impact on the datacenter landscape, and future
directions.


------------------------------------------------------------------------------

Title:
Representation Sparsification with Hybrid Thresholding for Fast  SPLADE-based Document Retrieval

Abstract: Learned sparse document representations using a transformer-based neural
model has been found to be attractive in both relevance effectiveness and time
efficiency. This paper describes a representation sparsification scheme based
on hard and soft thresholding with an inverted index approximation for faster
SPLADE-based document retrieval. It provides analytical and experimental
results on the impact of this learnable hybrid thresholding scheme.


------------------------------------------------------------------------------

Title:
Hybrid Multi-Criteria Preference Ranking by Subsorting

Abstract: Multi-criteria recommender systems can improve the quality of recommendations
by considering user preferences on multiple criteria. One promising approach
proposed recently is multi-criteria ranking, which uses Pareto ranking to
assign a ranking score based on the dominance relationship between predicted
ratings across criteria. However, applying Pareto ranking to all criteria may
result in non-differentiable ranking scores. To alleviate this issue, we
proposed a hybrid multi-criteria ranking method by using subsorting. More
specifically, we utilize one ranking method as the major sorting approach,
while we apply another preference ordering method as subsorting. Our
experimental results on the OpenTable and Yahoo!Movies data present the
advantages of this hybrid ranking approach. In addition, the experiments also
reveal more insights about the sustainability of the multi-criteria ranking for
top-N item recommendations.


------------------------------------------------------------------------------

Title:
Hyperbolic Active Learning for Semantic Segmentation under Domain Shift

Abstract: For the task of semantic segmentation (SS) under domain shift, active
learning (AL) acquisition strategies based on image regions and pseudo labels
are state-of-the-art (SoA). The presence of diverse pseudo-labels within a
region identifies pixels between different classes, which is a labeling
efficient active learning data acquisition strategy. However, by design,
pseudo-label variations are limited to only select the contours of classes,
limiting the final AL performance. We approach AL for SS in the Poincar\'e
hyperbolic ball model for the first time and leverage the variations of the
radii of pixel embeddings within regions as a novel data acquisition strategy.
This stems from a novel geometric property of a hyperbolic space trained
without enforced hierarchies, which we experimentally prove. Namely, classes
are mapped into compact hyperbolic areas with a comparable intra-class radii
variance, as the model places classes of increasing explainable difficulty at
denser hyperbolic areas, i.e. closer to the Poincar\'e ball edge. The variation
of pixel embedding radii identifies well the class contours, but they also
select a few intra-class peculiar details, which boosts the final performance.
Our proposed HALO (Hyperbolic Active Learning Optimization) surpasses the
supervised learning performance for the first time in AL for SS under domain
shift, by only using a small portion of labels (i.e., 1%). The extensive
experimental analysis is based on two established benchmarks, i.e. GTAV
$\rightarrow$ Cityscapes and SYNTHIA $\rightarrow$ Cityscapes, where we set a
new SoA. The code will be released.


------------------------------------------------------------------------------

Title:
Large Language Models are Fixated by Red Herrings: Exploring Creative  Problem Solving and Einstellung Effect using the Only Connect Wall Dataset

Abstract: The quest for human imitative AI has been an enduring topic in AI research
since its inception. The technical evolution and emerging capabilities of the
latest cohort of large language models (LLMs) have reinvigorated the subject
beyond academia to the cultural zeitgeist. While recent NLP evaluation
benchmark tasks test some aspects of human-imitative behaviour (e.g.,
BIG-bench's 'human-like behavior' tasks), few, if not none, examine creative
problem solving abilities. Creative problem solving in humans is a well-studied
topic in cognitive neuroscience with standardized tests that predominantly use
the ability to associate (heterogeneous) connections among clue words as a
metric for creativity. Exposure to misleading stimuli - distractors dubbed red
herrings - impede human performance in such tasks via the fixation effect and
Einstellung paradigm. In cognitive neuroscience studies, such fixations are
experimentally induced by pre-exposing participants to orthographically similar
incorrect words to subsequent word-fragments or clues. The popular British quiz
show Only Connect's Connecting Wall segment essentially mimics Mednick's Remote
Associates Test (RAT) formulation with built-in, deliberate red herrings, which
makes it an ideal proxy dataset to explore and study fixation effect and
Einstellung paradigm from cognitive neuroscience in LLMs. In addition to
presenting the novel Only Connect Wall (OCW) dataset, we also report results
from our evaluation of selected pre-trained language models and LLMs (including
OpenAI's GPT series) on creative problem solving tasks like grouping clue words
by heterogeneous connections, and identifying correct open knowledge domain
connections in respective groups. The code and link to the dataset are
available at this https URL


------------------------------------------------------------------------------

Title:
Reasoning over the Air: A Reasoning-based Implicit Semantic-Aware  Communication Framework

Abstract: Semantic-aware communication is a novel paradigm that draws inspiration from
human communication focusing on the delivery of the meaning of messages. It has
attracted significant interest recently due to its potential to improve the
efficiency and reliability of communication and enhance users' QoE. Most
existing works focus on transmitting and delivering the explicit semantic
meaning that can be directly identified from the source signal. This paper
investigates the implicit semantic-aware communication in which the hidden
information that cannot be directly observed from the source signal must be
recognized and interpreted by the intended users. To this end, a novel implicit
semantic-aware communication (iSAC) architecture is proposed for representing,
communicating, and interpreting the implicit semantic meaning between source
and destination users. A projection-based semantic encoder is proposed to
convert the high-dimensional graphical representation of explicit semantics
into a low-dimensional semantic constellation space for efficient physical
channel transmission. To enable the destination user to learn and imitate the
implicit semantic reasoning process of source user, a generative adversarial
imitation learning-based solution, called G-RML, is proposed. Different from
existing communication solutions, the source user in G-RML does not focus only
on sending as much of the useful messages as possible; but, instead, it tries
to guide the destination user to learn a reasoning mechanism to map any
observed explicit semantics to the corresponding implicit semantics that are
most relevant to the semantic meaning. Compared to the existing solutions, our
proposed G-RML requires much less communication and computational resources and
scales well to the scenarios involving the communication of rich semantic
meanings consisting of a large number of concepts and relations.


------------------------------------------------------------------------------

Title:
Towards Characterizing Domain Counterfactuals For Invertible Latent  Causal Models

Abstract: Learning latent causal models from data has many important applications such
as robustness, model extrapolation, and counterfactuals. Most prior theoretic
work has focused on full causal discovery (i.e., recovering the true latent
variables) but requires strong assumptions such as linearity or fails to have
any analysis of the equivalence class of solutions (e.g., IRM). Instead of full
causal discovery, we focus on a specific type of causal query called the domain
counterfactual, which hypothesizes what a sample would have looked like if it
had been generated in a different domain (or environment). Concretely, we
assume domain-specific invertible latent structural causal models and a shared
invertible observation function, both of which are less restrictive assumptions
than prior theoretic works. Under these assumptions, we define domain
counterfactually equivalent models and prove that any model can be transformed
into an equivalent model via two invertible functions. This constructive
property provides a tight characterization of the domain counterfactual
equivalence classes. Building upon this result, we prove that every equivalence
class contains a model where all intervened variables are at the end when
topologically sorted by the causal DAG, i.e., all non-intervened variables have
non-intervened ancestors. This surprising result suggests that an algorithm
that only allows intervention in the last $k$ latent variables may improve
model estimation for counterfactuals. In experiments, we enforce the sparse
intervention hypothesis via this theoretic result by constraining that the
latent SCMs can only differ in the last few causal mechanisms and demonstrate
the feasibility of this algorithm in simulated and image-based experiments.


------------------------------------------------------------------------------

Title:
GraphGLOW: Universal and Generalizable Structure Learning for Graph  Neural Networks

Abstract: Graph structure learning is a well-established problem that aims at
optimizing graph structures adaptive to specific graph datasets to help message
passing neural networks (i.e., GNNs) to yield effective and robust node
embeddings. However, the common limitation of existing models lies in the
underlying \textit{closed-world assumption}: the testing graph is the same as
the training graph. This premise requires independently training the structure
learning model from scratch for each graph dataset, which leads to prohibitive
computation costs and potential risks for serious over-fitting. To mitigate
these issues, this paper explores a new direction that moves forward to learn a
universal structure learning model that can generalize across graph datasets in
an open world. We first introduce the mathematical definition of this novel
problem setting, and describe the model formulation from a probabilistic
data-generative aspect. Then we devise a general framework that coordinates a
single graph-shared structure learner and multiple graph-specific GNNs to
capture the generalizable patterns of optimal message-passing topology across
datasets. The well-trained structure learner can directly produce adaptive
structures for unseen target graphs without any fine-tuning. Across diverse
datasets and various challenging cross-graph generalization protocols, our
experiments show that even without training on target graphs, the proposed
model i) significantly outperforms expressive GNNs trained on input
(non-optimized) topology, and ii) surprisingly performs on par with
state-of-the-art models that independently optimize adaptive structures for
specific target graphs, with notably orders-of-magnitude acceleration for
training on the target graph.


------------------------------------------------------------------------------

Title:
Online Vector Bin Packing and Hypergraph Coloring Illuminated: Simpler  Proofs and New Connections

Abstract: This paper studies the online vector bin packing (OVBP) problem and the
related problem of online hypergraph coloring (OHC). Firstly, we use a double
counting argument to prove an upper bound of the competitive ratio of
$FirstFit$ for OVBP. Our proof is conceptually simple, and strengthens the
result in Azar et. al. by removing the dependency on the bin size parameter.
Secondly, we introduce a notion of an online incidence matrix that is defined
for every instance of OHC. Using this notion, we provide a reduction from OHC
to OVBP, which allows us to carry known lower bounds of the competitive ratio
of algorithms for OHC to OVBP. Our approach significantly simplifies the
previous argument from Azar et. al. that relied on using intricate graph
structures. In addition, we slightly improve their lower bounds. Lastly, we
establish a tight bound of the competitive ratio of algorithms for OHC, where
input is restricted to be a hypertree, thus resolving a conjecture in
Nagy-Gyorgy et. al. The crux of this proof lies in solving a certain
combinatorial partition problem about multi-family of subsets, which might be
of independent interest.


------------------------------------------------------------------------------

Title:
The Unintended Consequences of Discount Regularization: Improving  Regularization in Certainty Equivalence Reinforcement Learning

Abstract: Discount regularization, using a shorter planning horizon when calculating
the optimal policy, is a popular choice to restrict planning to a less complex
set of policies when estimating an MDP from sparse or noisy data (Jiang et al.,
2015). It is commonly understood that discount regularization functions by
de-emphasizing or ignoring delayed effects. In this paper, we reveal an
alternate view of discount regularization that exposes unintended consequences.
We demonstrate that planning under a lower discount factor produces an
identical optimal policy to planning using any prior on the transition matrix
that has the same distribution for all states and actions. In fact, it
functions like a prior with stronger regularization on state-action pairs with
more transition data. This leads to poor performance when the transition matrix
is estimated from data sets with uneven amounts of data across state-action
pairs. Our equivalence theorem leads to an explicit formula to set
regularization parameters locally for individual state-action pairs rather than
globally. We demonstrate the failures of discount regularization and how we
remedy them using our state-action-specific method across simple empirical
examples as well as a medical cancer simulator.


------------------------------------------------------------------------------

Title:
Phase Repair for Time-Domain Convolutional Neural Networks in Music  Super-Resolution

Abstract: Audio Super-Resolution (SR) is an important topic in the field of audio
processing. Many models are designed in time domain due to the advantage of
waveform processing, such as being able to avoid the phase problem. However, in
prior works it is shown that Time-Domain Convolutional Neural Network (TD-CNN)
approaches tend to produce annoying artifacts in their output. In order to
confirm the source of the artifact, we conduct an AB listening test and found
phase to be the cause. We further propose Time-Domain Phase Repair (TD-PR) to
improve TD-CNNs' performance by repairing the phase of the TD-CNNs' output. In
this paper, we focus on the music SR task, which is challenging due to the wide
frequency response and dynamic range of music. Our proposed method can handle
various narrow-bandwidth from 2.5kHz to 4kHz with a target bandwidth of 8kHz.
We conduct both objective and subjective evaluation to assess the proposed
method. The objective evaluation result indicates the proposed method achieves
the SR task effectively. Moreover, the proposed TD-PR obtains the much higher
mean opinion scores than all TD-CNN baselines, which indicates that the
proposed TD-PR significantly improves perceptual quality. Samples are available
on the demo page.


------------------------------------------------------------------------------

Title:
Neural Inventory Control in Networks via Hindsight Differentiable Policy  Optimization

Abstract: Inventory management offers unique opportunities for reliably evaluating and
applying deep reinforcement learning (DRL). Rather than evaluate DRL algorithms
by comparing against one another or against human experts, we can compare to
the optimum itself in several problem classes with hidden structure. Our DRL
methods consistently recover near-optimal policies in such settings, despite
being applied with up to 600-dimensional raw state vectors. In others, they can
vastly outperform problem-specific heuristics. To reliably apply DRL, we
leverage two insights. First, one can directly optimize the hindsight
performance of any policy using stochastic gradient descent. This uses (i) an
ability to backtest any policy's performance on a subsample of historical
demand observations, and (ii) the differentiability of the total cost incurred
on any subsample with respect to policy parameters. Second, we propose a
natural neural network architecture to address problems with weak (or
aggregate) coupling constraints between locations in an inventory network. This
architecture employs weight duplication for ``sibling'' locations in the
network, and state summarization. We justify this architecture through an
asymptotic guarantee, and empirically affirm its value in handling large-scale
problems.


------------------------------------------------------------------------------

Title:
Comparative Evaluation of Recent Universal Adversarial Perturbations in  Image Classification

Abstract: The vulnerability of Convolutional Neural Networks (CNNs) to adversarial
samples has recently garnered significant attention in the machine learning
community. Furthermore, recent studies have unveiled the existence of universal
adversarial perturbations (UAPs) that are image-agnostic and highly
transferable across different CNN models. In this survey, our primary focus
revolves around the recent advancements in UAPs specifically within the image
classification task. We categorize UAPs into two distinct categories, i.e.,
noise-based attacks and generator-based attacks, thereby providing a
comprehensive overview of representative methods within each category. By
presenting the computational details of these methods, we summarize various
loss functions employed for learning UAPs. Furthermore, we conduct a
comprehensive evaluation of different loss functions within consistent training
frameworks, including noise-based and generator-based. The evaluation covers a
wide range of attack settings, including black-box and white-box attacks,
targeted and untargeted attacks, as well as the examination of defense
mechanisms.
Our quantitative evaluation results yield several important findings
pertaining to the effectiveness of different loss functions, the selection of
surrogate CNN models, the impact of training data and data size, and the
training frameworks involved in crafting universal attackers. Finally, to
further promote future research on universal adversarial attacks, we provide
some visualizations of the perturbations and discuss the potential research
directions.


------------------------------------------------------------------------------

Title:
From array algebra to energy efficiency on GPUs: Data and hardware  shapes with dimension-lifting to optimize memory-processor layouts

Abstract: We present a new formulation for parallel matrix multiplication (MM) to
out-perform the standard row-column code design. This algorithm is formulated
in the MoA formalism (A Mathematics of Arrays) and combines an array view of
hardware (dimension-lifting) to extend indexing to physical memory/processing
units, with a contiguous data layout derived from static transformations. This
view of a hardware-software model is thus a bridging model in the sense of
Valiant's BSP. OpenACCcode was derived from the MoA expressions's normal form,
producing optimal block sizes using the static information of types and shapes.
Experiments were run on Nvidia V100 GPUs and reveal energy consumption which is
quadratic in N, i.e. linear in the size of matrix. More generally this approach
may be an ideal way of formulating, optimizing, and mapping array algorithms to
embedded hardware. This work builds upon recently published results of NREL
scientists.
.


------------------------------------------------------------------------------

Title:
Sparse Modular Activation for Efficient Sequence Modeling

Abstract: Linear State Space Models (SSMs) have demonstrated strong performance in a
variety of sequence modeling tasks due to their efficient encoding of the
recurrent structure. However, in more comprehensive tasks like language
modeling and machine translation, self-attention-based models still outperform
SSMs. Hybrid models employing both SSM and self-attention generally show
promising performance, but current approaches apply attention modules
statically and uniformly to all elements in the input sequences, leading to
sub-optimal quality-efficiency trade-offs. In this work, we introduce Sparse
Modular Activation (SMA), a general mechanism enabling neural networks to
sparsely and dynamically activate sub-modules for sequence elements in a
differentiable manner. Through allowing each element to skip non-activated
sub-modules, SMA reduces computation and memory consumption at both training
and inference stages of sequence modeling. As a specific instantiation of SMA,
we design a novel neural architecture, SeqBoat, which employs SMA to sparsely
activate a Gated Attention Unit (GAU) based on the state representations
learned from an SSM. By constraining the GAU to only conduct local attention on
the activated inputs, SeqBoat can achieve linear inference complexity with
theoretically infinite attention span, and provide substantially better
quality-efficiency trade-off than the chunking-based models. With experiments
on a wide range of tasks, including language modeling, speech classification
and long-range arena, SeqBoat brings new state-of-the-art results among hybrid
models with linear complexity and reveals the amount of attention needed for
each task through the learned sparse activation patterns.


------------------------------------------------------------------------------

Title:
Quilt-1M: One Million Image-Text Pairs for Histopathology

Abstract: Recent accelerations in multi-modal applications have been made possible with
the plethora of image and text data available online. However, the scarcity of
analogous data in the medical field, specifically in histopathology, has halted
comparable progress. To enable similar representation learning for
histopathology, we turn to YouTube, an untapped resource of videos, offering
$1,087$ hours of valuable educational histopathology videos from expert
clinicians. From YouTube, we curate Quilt: a large-scale vision-language
dataset consisting of $768,826$ image and text pairs. Quilt was automatically
curated using a mixture of models, including large language models, handcrafted
algorithms, human knowledge databases, and automatic speech recognition. In
comparison, the most comprehensive datasets curated for histopathology amass
only around $200$K samples. We combine Quilt with datasets from other sources,
including Twitter, research papers, and the internet in general, to create an
even larger dataset: Quilt-1M, with $1$M paired image-text samples, marking it
as the largest vision-language histopathology dataset to date. We demonstrate
the value of Quilt-1M by fine-tuning a pre-trained CLIP model. Our model
outperforms state-of-the-art models on both zero-shot and linear probing tasks
for classifying new histopathology images across $13$ diverse patch-level
datasets of $8$ different sub-pathologies and cross-modal retrieval tasks.


------------------------------------------------------------------------------

Title:
Efficient and reliable divergence-conforming methods for an  elasticity-poroelasticity interface problem

Abstract: We present a finite element discretisation to model the interaction between a
poroelastic structure and an elastic medium. The consolidation problem
considers fully coupled deformations across an interface, ensuring continuity
of displacement and total traction, as well as no-flux for the fluid phase. Our
formulation of the poroelasticity equations incorporates displacement, fluid
pressure, and total pressure, while the elasticity equations adopt a
displacement-pressure formulation. Notably, the transmission conditions at the
interface are enforced without the need for Lagrange multipliers. We
demonstrate the stability and convergence of the divergence-conforming finite
element method across various polynomial degrees. The a priori error bounds
remain robust, even when considering large variations in intricate model
parameters such as Lam\'e constants, permeability, and storativity coefficient.
To enhance computational efficiency and reliability, we develop residual-based
a posteriori error estimators that are independent of the aforementioned
coefficients. Additionally, we devise parameter-robust and optimal block
diagonal preconditioners. Through numerical examples, including adaptive
scenarios, we illustrate the scheme's properties such as convergence and
parameter robustness.


------------------------------------------------------------------------------

Title:
Using Motif Transitions for Temporal Graph Generation

Abstract: Graph generative models are highly important for sharing surrogate data and
benchmarking purposes. Real-world complex systems often exhibit dynamic nature,
where the interactions among nodes change over time in the form of a temporal
network. Most temporal network generation models extend the static graph
generation models by incorporating temporality in the generation process. More
recently, temporal motifs are used to generate temporal networks with better
success. However, existing models are often restricted to a small set of
predefined motif patterns due to the high computational cost of counting
temporal motifs. In this work, we develop a practical temporal graph generator,
Motif Transition Model (MTM), to generate synthetic temporal networks with
realistic global and local features. Our key idea is modeling the arrival of
new events as temporal motif transition processes. We first calculate the
transition properties from the input graph and then simulate the motif
transition processes based on the transition probabilities and transition
rates. We demonstrate that our model consistently outperforms the baselines
with respect to preserving various global and local temporal graph statistics
and runtime performance.


------------------------------------------------------------------------------

Title:
Analysis of the Benefits and Efficacy of the Addition of Variants and  Reality Paths to the Blackboard Architecture

Abstract: While the Blackboard Architecture has been in use since the 1980s, it has
recently been proposed for modeling computer networks to assess their security.
To do this, it must account for complex network attack patterns involving
multiple attack routes and possible mid-attack system state changes. This paper
proposes a data structure which can be used to model paths from an ingress
point to a given egress point in Blackboard Architecture-modeled computer
networks. It is designed to contain the pertinent information required for a
systematic traversal through a changing network. This structure, called a
reality path, represents a single potential pathway through the network with a
given set of facts in a particular sequence of states. Another structure,
called variants, is used during traversal of nodes (called containers) modeled
in the network. The two structures - reality paths and variants - facilitate
the use of a traversal algorithm, which will find all possible attack paths in
Blackboard Architecture-modeled networks. This paper introduces and assesses
the efficacy of variants and reality paths


------------------------------------------------------------------------------

Title:
Correcting Underrepresentation and Intersectional Bias for Fair  Classification

Abstract: We consider the problem of learning from data corrupted by
underrepresentation bias, where positive examples are filtered from the data at
different, unknown rates for a fixed number of sensitive groups. We show that
with a small amount of unbiased data, we can efficiently estimate the
group-wise drop-out parameters, even in settings where intersectional group
membership makes learning each intersectional rate computationally infeasible.
Using this estimate for the group-wise drop-out rate, we construct a
re-weighting scheme that allows us to approximate the loss of any hypothesis on
the true distribution, even if we only observe the empirical error on a biased
sample. Finally, we present an algorithm encapsulating this learning and
re-weighting process, and we provide strong PAC-style guarantees that, with
high probability, our estimate of the risk of the hypothesis over the true
distribution will be arbitrarily close to the true risk.


------------------------------------------------------------------------------

Title:
GUMSum: Multi-Genre Data and Evaluation for English Abstractive  Summarization

Abstract: Automatic summarization with pre-trained language models has led to
impressively fluent results, but is prone to 'hallucinations', low performance
on non-news genres, and outputs which are not exactly summaries. Targeting ACL
2023's 'Reality Check' theme, we present GUMSum, a small but carefully crafted
dataset of English summaries in 12 written and spoken genres for evaluation of
abstractive summarization. Summaries are highly constrained, focusing on
substitutive potential, factuality, and faithfulness. We present guidelines and
evaluate human agreement as well as subjective judgments on recent system
outputs, comparing general-domain untuned approaches, a fine-tuned one, and a
prompt-based approach, to human performance. Results show that while GPT3
achieves impressive scores, it still underperforms humans, with varying quality
across genres. Human judgments reveal different types of errors in supervised,
prompted, and human-generated summaries, shedding light on the challenges of
producing a good summary.


------------------------------------------------------------------------------

Title:
Neuro-Symbolic Bi-Directional Translation -- Deep Learning  Explainability for Climate Tipping Point Research

Abstract: In recent years, there has been an increase in using deep learning for
climate and weather modeling. Though results have been impressive,
explainability and interpretability of deep learning models are still a
challenge. A third wave of Artificial Intelligence (AI), which includes logic
and reasoning, has been described as a way to address these issues.
Neuro-symbolic AI is a key component of this integration of logic and reasoning
with deep learning. In this work we propose a neuro-symbolic approach called
Neuro-Symbolic Question-Answer Program Translator, or NS-QAPT, to address
explainability and interpretability for deep learning climate simulation,
applied to climate tipping point discovery. The NS-QAPT method includes a
bidirectional encoder-decoder architecture that translates between
domain-specific questions and executable programs used to direct the climate
simulation, acting as a bridge between climate scientists and deep learning
models. We show early compelling results of this translation method and
introduce a domain-specific language and associated executable programs for a
commonly known tipping point, the collapse of the Atlantic Meridional
Overturning Circulation (AMOC).


------------------------------------------------------------------------------

Title:
Dynamic Perceiver for Efficient Visual Recognition

Abstract: Early exiting has become a promising approach to improving the inference
efficiency of deep networks. By structuring models with multiple classifiers
(exits), predictions for ``easy'' samples can be generated at earlier exits,
negating the need for executing deeper layers. Current multi-exit networks
typically implement linear classifiers at intermediate layers, compelling
low-level features to encapsulate high-level semantics. This sub-optimal design
invariably undermines the performance of later exits. In this paper, we propose
Dynamic Perceiver (Dyn-Perceiver) to decouple the feature extraction procedure
and the early classification task with a novel dual-branch architecture. A
feature branch serves to extract image features, while a classification branch
processes a latent code assigned for classification tasks. Bi-directional
cross-attention layers are established to progressively fuse the information of
both branches. Early exits are placed exclusively within the classification
branch, thus eliminating the need for linear separability in low-level
features. Dyn-Perceiver constitutes a versatile and adaptable framework that
can be built upon various architectures. Experiments on image classification,
action recognition, and object detection demonstrate that our method
significantly improves the inference efficiency of different backbones,
outperforming numerous competitive approaches across a broad range of
computational budgets. Evaluation on both CPU and GPU platforms substantiate
the superior practical efficiency of Dyn-Perceiver. Code is available at
this https URL


------------------------------------------------------------------------------

Title:
HK-LegiCoST: Leveraging Non-Verbatim Transcripts for Speech Translation

Abstract: We introduce HK-LegiCoST, a new three-way parallel corpus of
Cantonese-English translations, containing 600+ hours of Cantonese audio, its
standard traditional Chinese transcript, and English translation, segmented and
aligned at the sentence level. We describe the notable challenges in corpus
preparation: segmentation, alignment of long audio recordings, and
sentence-level alignment with non-verbatim transcripts. Such transcripts make
the corpus suitable for speech translation research when there are significant
differences between the spoken and written forms of the source language. Due to
its large size, we are able to demonstrate competitive speech translation
baselines on HK-LegiCoST and extend them to promising cross-corpus results on
the FLEURS Cantonese subset. These results deliver insights into speech
recognition and translation research in languages for which non-verbatim or
``noisy'' transcription is common due to various factors, including vernacular
and dialectal speech.


------------------------------------------------------------------------------

Title:
UVSCAN: Detecting Third-Party Component Usage Violations in IoT Firmware

Abstract: Nowadays, IoT devices integrate a wealth of third-party components (TPCs) in
firmware to shorten the development cycle. TPCs usually have strict usage
specifications, e.g., checking the return value of the function. Violating the
usage specifications of TPCs can cause serious consequences, e.g., NULL pointer
dereference. Therefore, this massive amount of TPC integrations, if not
properly implemented, will lead to pervasive vulnerabilities in IoT devices.
Detecting vulnerabilities automatically in TPC integration is challenging from
several perspectives: (1) There is a gap between the high-level specifications
from TPC documents, and the low-level implementations in the IoT firmware. (2)
IoT firmware is mostly the closed-source binary, which loses a lot of
information when compiling from the source code and has diverse architectures.
To address these challenges, we design and implement UVScan, an automated and
scalable system to detect TPC usage violations in IoT firmware. In UVScan, we
first propose a novel natural language processing (NLP)-based rule extraction
framework, which extracts API specifications from inconsistently formatted TPC
documents. We then design a rule-driven NLP-guided binary analysis engine,
which maps the logical information from the high-level TPC document to the
low-level binary, and detects TPC usage violations in IoT firmware across
different architectures. We evaluate UVScan from four perspectives on four
popular TPCs and six ground-truth datasets. The results show that UVScan
achieves more than 70% precision and recall, and has a significant performance
improvement compared with even the source-level API misuse detectors.


------------------------------------------------------------------------------

Title:
Nonlinear Feature Aggregation: Two Algorithms driven by Theory

Abstract: Many real-world machine learning applications are characterized by a huge
number of features, leading to computational and memory issues, as well as the
risk of overfitting. Ideally, only relevant and non-redundant features should
be considered to preserve the complete information of the original data and
limit the dimensionality. Dimensionality reduction and feature selection are
common preprocessing techniques addressing the challenge of efficiently dealing
with high-dimensional data. Dimensionality reduction methods control the number
of features in the dataset while preserving its structure and minimizing
information loss. Feature selection aims to identify the most relevant features
for a task, discarding the less informative ones. Previous works have proposed
approaches that aggregate features depending on their correlation without
discarding any of them and preserving their interpretability through
aggregation with the mean. A limitation of methods based on correlation is the
assumption of linearity in the relationship between features and target. In
this paper, we relax such an assumption in two ways. First, we propose a
bias-variance analysis for general models with additive Gaussian noise, leading
to a dimensionality reduction algorithm (NonLinCFA) which aggregates non-linear
transformations of features with a generic aggregation function. Then, we
extend the approach assuming that a generalized linear model regulates the
relationship between features and target. A deviance analysis leads to a second
dimensionality reduction algorithm (GenLinCFA), applicable to a larger class of
regression problems and classification settings. Finally, we test the
algorithms on synthetic and real-world datasets, performing regression and
classification tasks, showing competitive performances.


------------------------------------------------------------------------------

Title:
LARG, Language-based Automatic Reward and Goal Generation

Abstract: Goal-conditioned and Multi-Task Reinforcement Learning (GCRL and MTRL)
address numerous problems related to robot learning, including locomotion,
navigation, and manipulation scenarios. Recent works focusing on
language-defined robotic manipulation tasks have led to the tedious production
of massive human annotations to create dataset of textual descriptions
associated with trajectories. To leverage reinforcement learning with
text-based task descriptions, we need to produce reward functions associated
with individual tasks in a scalable manner. In this paper, we leverage recent
capabilities of Large Language Models (LLMs) and introduce \larg,
Language-based Automatic Reward and Goal Generation, an approach that converts
a text-based task description into its corresponding reward and goal-generation
functions We evaluate our approach for robotic manipulation and demonstrate its
ability to train and execute policies in a scalable manner, without the need
for handcrafted reward functions.


------------------------------------------------------------------------------

Title:
Markovian Embeddings for Coalitional Bargaining Games

Abstract: We examine the Markovian properties of coalition bargaining games, in
particular, the case where past rejected proposals cannot be repeated. We
propose a Markovian embedding with filtrations to render the sates Markovian
and thus, fit into the framework of stochastic games.


------------------------------------------------------------------------------

Title:
CoNi-MPC: Cooperative Non-inertial Frame Based Model Predictive Control

Abstract: This paper presents a novel solution for UAV control in cooperative
multi-robot systems, which can be used in various scenarios such as
leader-following, landing on a moving base, or specific relative motion with a
target. Unlike classical methods that tackle UAV control in the world frame, we
directly control the UAV in the target coordinate frame, without making motion
assumptions about the target. In detail, we formulate a non-linear model
predictive controller of a UAV within a non-inertial frame (i.e., the target
frame). The system requires the relative states (pose and velocity), the
angular velocity and the accelerations of the target, which can be obtained by
relative localization methods and ubiquitous MEMS IMU sensors, respectively.
This framework eliminates dependencies that are vital in classical solutions,
such as accurate state estimation for both the agent and target, prior
knowledge of the target motion model, and continuous trajectory re-planning for
some complex tasks. We have performed extensive simulations to investigate the
control performance considering the varying motion characteristics of the
target. Furthermore, we conducted considerable real robot experiments,
employing laboratory motion-capture systems or relative localization methods
implemented outdoors, to validate the applicability and feasibility of the
proposed approach.


------------------------------------------------------------------------------

Title:
CF-GODE: Continuous-Time Causal Inference for Multi-Agent Dynamical  Systems

Abstract: Multi-agent dynamical systems refer to scenarios where multiple units
interact with each other and evolve collectively over time. To make informed
decisions in multi-agent dynamical systems, such as determining the optimal
vaccine distribution plan, it is essential for decision-makers to estimate the
continuous-time counterfactual outcomes. However, existing studies of causal
inference over time rely on the assumption that units are mutually independent,
which is not valid for multi-agent dynamical systems. In this paper, we aim to
bridge this gap and study how to estimate counterfactual outcomes in
multi-agent dynamical systems. Causal inference in a multi-agent dynamical
system has unique challenges: 1) Confounders are time-varying and are present
in both individual unit covariates and those of other units; 2) Units are
affected by not only their own but also others' treatments; 3) The treatments
are naturally dynamic, such as receiving vaccines and boosters in a seasonal
manner. We model a multi-agent dynamical system as a graph and propose
CounterFactual GraphODE (CF-GODE), a causal model that estimates
continuous-time counterfactual outcomes in the presence of inter-dependencies
between units. To facilitate continuous-time estimation, we propose
Treatment-Induced GraphODE, a novel ordinary differential equation based on
GNN, which incorporates dynamical treatments as additional inputs to predict
potential outcomes over time. To remove confounding bias, we propose two domain
adversarial learning based objectives that learn balanced continuous
representation trajectories, which are not predictive of treatments and
interference. We further provide theoretical justification to prove their
effectiveness. Experiments on two semi-synthetic datasets confirm that CF-GODE
outperforms baselines on counterfactual estimation. We also provide extensive
analyses to understand how our model works.


------------------------------------------------------------------------------

Title:
Autonomous Driving with Deep Reinforcement Learning in CARLA Simulation

Abstract: Nowadays, autonomous vehicles are gaining traction due to their numerous
potential applications in resolving a variety of other real-world challenges.
However, developing autonomous vehicles need huge amount of training and
testing before deploying it to real world. While the field of reinforcement
learning (RL) has evolved into a powerful learning framework to the development
of deep representation learning, and it is now capable of learning complicated
policies in high-dimensional environments like in autonomous vehicles. In this
regard, we make an effort, using Deep Q-Learning, to discover a method by which
an autonomous car may maintain its lane at top speed while avoiding other
vehicles. After that, we used CARLA simulation environment to test and verify
our newly acquired policy based on the problem formulation.


------------------------------------------------------------------------------

Title:
Learn to Accumulate Evidence from All Training Samples: Theory and  Practice

Abstract: Evidential deep learning, built upon belief theory and subjective logic,
offers a principled and computationally efficient way to turn a deterministic
neural network uncertainty-aware. The resultant evidential models can quantify
fine-grained uncertainty using the learned evidence. To ensure theoretically
sound evidential models, the evidence needs to be non-negative, which requires
special activation functions for model training and inference. This constraint
often leads to inferior predictive performance compared to standard softmax
models, making it challenging to extend them to many large-scale datasets. To
unveil the real cause of this undesired behavior, we theoretically investigate
evidential models and identify a fundamental limitation that explains the
inferior performance: existing evidential activation functions create zero
evidence regions, which prevent the model to learn from training samples
falling into such regions. A deeper analysis of evidential activation functions
based on our theoretical underpinning inspires the design of a novel
regularizer that effectively alleviates this fundamental limitation. Extensive
experiments over many challenging real-world datasets and settings confirm our
theoretical findings and demonstrate the effectiveness of our proposed
approach.


------------------------------------------------------------------------------

Title:
New Cross-Core Cache-Agnostic and Prefetcher-based Side-Channels and  Covert-Channels

Abstract: In this paper, we reveal the existence of a new class of prefetcher, the XPT
prefetcher, in the modern Intel processors which has never been officially
documented. It speculatively issues a load, bypassing last-level cache (LLC)
lookups, when it predicts that a load request will result in an LLC miss. We
demonstrate that XPT prefetcher is shared among different cores, which enables
an attacker to build cross-core side-channel and covert-channel attacks. We
propose PrefetchX, a cross-core attack mechanism, to leak users' sensitive data
and activities.
We empirically demonstrate that PrefetchX can be used to extract private keys
of real-world RSA applications. Furthermore, we show that PrefetchX can enable
side-channel attacks that can monitor keystrokes and network traffic patterns
of users. Our two cross-core covert-channel attacks also see a low error rate
and a 1.7MB/s maximum channel capacity. Due to the cache-independent feature of
PrefetchX, current cache-based mitigations are not effective against our
attacks. Overall, our work uncovers a significant vulnerability in the XPT
prefetcher, which can be exploited to compromise the confidentiality of
sensitive information in both crypto and non-crypto-related applications among
processor cores.


------------------------------------------------------------------------------

Title:
Adversarial Training Should Be Cast as a Non-Zero-Sum Game

Abstract: One prominent approach toward resolving the adversarial vulnerability of deep
neural networks is the two-player zero-sum paradigm of adversarial training, in
which predictors are trained against adversarially-chosen perturbations of
data. Despite the promise of this approach, algorithms based on this paradigm
have not engendered sufficient levels of robustness, and suffer from
pathological behavior like robust overfitting. To understand this shortcoming,
we first show that the commonly used surrogate-based relaxation used in
adversarial training algorithms voids all guarantees on the robustness of
trained classifiers. The identification of this pitfall informs a novel
non-zero-sum bilevel formulation of adversarial training, wherein each player
optimizes a different objective function. Our formulation naturally yields a
simple algorithmic framework that matches and in some cases outperforms
state-of-the-art attacks, attains comparable levels of robustness to standard
adversarial training algorithms, and does not suffer from robust overfitting.


------------------------------------------------------------------------------

Title:
BayLing: Bridging Cross-lingual Alignment and Instruction Following  through Interactive Translation for Large Language Models

Abstract: Large language models (LLMs) have demonstrated remarkable prowess in language
understanding and generation. Advancing from foundation LLMs to
instructionfollowing LLMs, instruction tuning plays a vital role in aligning
LLMs to human preferences. However, the existing LLMs are usually focused on
English, leading to inferior performance in non-English languages. In order to
improve the performance for non-English languages, it is necessary to collect
language-specific training data for foundation LLMs and construct
language-specific instructions for instruction tuning, both of which are heavy
loads. To minimize human workload, we propose to transfer the capabilities of
language generation and instruction following from English to other languages
through an interactive translation task. We have developed BayLing, an
instruction-following LLM by utilizing LLaMA as the foundation LLM and
automatically constructing interactive translation instructions for instructing
tuning. Extensive assessments demonstrate that BayLing achieves comparable
performance to GPT-3.5-turbo, despite utilizing a considerably smaller
parameter size of only 13 billion. Experimental results on translation tasks
show that BayLing achieves 95% of single-turn translation capability compared
to GPT-4 with automatic evaluation and 96% of interactive translation
capability compared to GPT-3.5-turbo with human evaluation. To estimate the
performance on general tasks, we created a multi-turn instruction test set
called BayLing-80. The experimental results on BayLing-80 indicate that BayLing
achieves 89% of performance compared to GPT-3.5-turbo. BayLing also
demonstrates outstanding performance on knowledge assessment of Chinese GaoKao
and English SAT, second only to GPT-3.5-turbo among a multitude of
instruction-following LLMs. Demo, homepage, code and models of BayLing are
available.


------------------------------------------------------------------------------

Title:
Learning Models of Adversarial Agent Behavior under Partial  Observability

Abstract: The need for opponent modeling and tracking arises in several real-world
scenarios, such as professional sports, video game design, and drug-trafficking
interdiction. In this work, we present graPh neurAl Network aDvErsarial
MOdeliNg wIth mUtual informMation for modeling the behavior of an adversarial
opponent agent. PANDEMONIUM is a novel graph neural network (GNN) based
approach that uses mutual information maximization as an auxiliary objective to
predict the current and future states of an adversarial opponent with partial
observability. To evaluate PANDEMONIUM, we design two large-scale,
pursuit-evasion domains inspired by real-world scenarios, where a team of
heterogeneous agents is tasked with tracking and interdicting a single
adversarial agent, and the adversarial agent must evade detection while
achieving its own objectives. With the mutual information formulation,
PANDEMONIUM outperforms all baselines in both domains and achieves 31.68%
higher log-likelihood on average for future adversarial state predictions
across both domains.


------------------------------------------------------------------------------

Title:
Spatial Secrecy Spectral Efficiency Optimization Enabled by  Reconfigurable Intelligent Surfaces

Abstract: Reconfigurable Intelligent Surfaces (RISs) constitute a strong candidate
physical-layer technology for the $6$-th Generation (6G) of wireless networks,
offering new design degrees of freedom for efficiently addressing demanding
performance objectives. In this paper, we consider a Multiple-Input
Single-Output (MISO) physical-layer security system incorporating a reflective
RIS to safeguard wireless communications between a legitimate transmitter and
receiver under the presence of an eavesdropper. In contrast to current studies
optimizing RISs for given positions of the legitimate and eavesdropping nodes,
in this paper, we focus on devising RIS-enabled secrecy for given geographical
areas of potential nodes' placement. We propose a novel secrecy metric,
capturing the spatially averaged secrecy spectral efficiency, and present a
joint design of the transmit digital beamforming and the RIS analog phase
profile, which is realized via a combination of alternating optimization and
minorization-maximization. The proposed framework bypasses the need for
instantaneous knowledge of the eavesdropper's channel or position, and targets
providing an RIS-boosted secure area of legitimate communications with a single
configuration of the free parameters. Our simulation results showcase
significant performance gains with the proposed secrecy scheme, even for cases
where the eavesdropper shares similar pathloss attenuation with the legitimate
receiver.


------------------------------------------------------------------------------

Title:
Tame a Wild Camera: In-the-Wild Monocular Camera Calibration

Abstract: 3D sensing for monocular in-the-wild images, e.g., depth estimation and 3D
object detection, has become increasingly important. However, the unknown
intrinsic parameter hinders their development and deployment. Previous methods
for the monocular camera calibration rely on specific 3D objects or strong
geometry prior, such as using a checkerboard or imposing a Manhattan World
assumption. This work solves the problem from the other perspective by
exploiting the monocular 3D prior. Our method is assumption-free and calibrates
the complete $4$ Degree-of-Freedom (DoF) intrinsic parameters. First, we
demonstrate intrinsic is solved from two well-studied monocular priors, i.e.,
monocular depthmap, and surface normal map. However, this solution imposes a
low-bias and low-variance requirement for depth estimation. Alternatively, we
introduce a novel monocular 3D prior, the incidence field, defined as the
incidence rays between points in 3D space and pixels in the 2D imaging plane.
The incidence field is a pixel-wise parametrization of the intrinsic invariant
to image cropping and resizing. With the estimated incidence field, a robust
RANSAC algorithm recovers intrinsic. We demonstrate the effectiveness of our
method by showing superior performance on synthetic and zero-shot testing
datasets. Beyond calibration, we demonstrate downstream applications in image
manipulation detection & restoration, uncalibrated two-view pose estimation,
and 3D sensing. Codes, models, and data will be held in
this https URL


------------------------------------------------------------------------------

Title:
A spatio-temporal network for video semantic segmentation in surgical  videos

Abstract: Semantic segmentation in surgical videos has applications in intra-operative
guidance, post-operative analytics and surgical education. Segmentation models
need to provide accurate and consistent predictions since temporally
inconsistent identification of anatomical structures can impair usability and
hinder patient safety. Video information can alleviate these challenges leading
to reliable models suitable for clinical use. We propose a novel architecture
for modelling temporal relationships in videos. The proposed model includes a
spatio-temporal decoder to enable video semantic segmentation by improving
temporal consistency across frames. The encoder processes individual frames
whilst the decoder processes a temporal batch of adjacent frames. The proposed
decoder can be used on top of any segmentation encoder to improve temporal
consistency. Model performance was evaluated on the CholecSeg8k dataset and a
private dataset of robotic Partial Nephrectomy procedures. Segmentation
performance was improved when the temporal decoder was applied across both
datasets. The proposed model also displayed improvements in temporal
consistency.


------------------------------------------------------------------------------

Title:
Frequency effects in Linear Discriminative Learning

Abstract: Word frequency is a strong predictor in most lexical processing tasks. Thus,
any model of word recognition needs to account for how word frequency effects
arise. The Discriminative Lexicon Model (DLM; Baayen et al., 2018a, 2019)
models lexical processing with linear mappings between words' forms and their
meanings. So far, the mappings can either be obtained incrementally via
error-driven learning, a computationally expensive process able to capture
frequency effects, or in an efficient, but frequency-agnostic closed-form
solution modelling the theoretical endstate of learning (EL) where all words
are learned optimally. In this study we show how an efficient, yet
frequency-informed mapping between form and meaning can be obtained
(Frequency-informed learning; FIL). We find that FIL well approximates an
incremental solution while being computationally much cheaper. FIL shows a
relatively low type- and high token-accuracy, demonstrating that the model is
able to process most word tokens encountered by speakers in daily life
correctly. We use FIL to model reaction times in the Dutch Lexicon Project
(Keuleers et al., 2010) and find that FIL predicts well the S-shaped
relationship between frequency and the mean of reaction times but
underestimates the variance of reaction times for low frequency words. FIL is
also better able to account for priming effects in an auditory lexical decision
task in Mandarin Chinese (Lee, 2007), compared to EL. Finally, we used ordered
data from CHILDES (Brown, 1973; Demuth et al., 2006) to compare mappings
obtained with FIL and incremental learning. The mappings are highly correlated,
but with FIL some nuances based on word ordering effects are lost. Our results
show how frequency effects in a learning model can be simulated efficiently by
means of a closed-form solution, and raise questions about how to best account
for low-frequency words in cognitive models.


------------------------------------------------------------------------------

Title:
UncLe-SLAM: Uncertainty Learning for Dense Neural SLAM

Abstract: We present an uncertainty learning framework for dense neural simultaneous
localization and mapping (SLAM). Estimating pixel-wise uncertainties for the
depth input of dense SLAM methods allows to re-weigh the tracking and mapping
losses towards image regions that contain more suitable information that is
more reliable for SLAM. To this end, we propose an online framework for sensor
uncertainty estimation that can be trained in a self-supervised manner from
only 2D input data. We further discuss the advantages of the uncertainty
learning for the case of multi-sensor input. Extensive analysis,
experimentation, and ablations show that our proposed modeling paradigm
improves both mapping and tracking accuracy and often performs better than
alternatives that require ground truth depth or 3D. Our experiments show that
we achieve a 38% and 27% lower absolute trajectory tracking error (ATE) on the
7-Scenes and TUM-RGBD datasets respectively. On the popular Replica dataset on
two types of depth sensors we report an 11% F1-score improvement on RGBD SLAM
compared to the recent state-of-the-art neural implicit approaches. Our source
code will be made available.


------------------------------------------------------------------------------

Title:
Adaptive Federated Learning with Auto-Tuned Clients

Abstract: Federated learning (FL) is a distributed machine learning framework where the
global model of a central server is trained via multiple collaborative steps by
participating clients without sharing their data. While being a flexible
framework, where the distribution of local data, participation rate, and
computing power of each client can greatly vary, such flexibility gives rise to
many new challenges, especially in the hyperparameter tuning on both the server
and the client side. We propose $\Delta$-SGD, a simple step size rule for SGD
that enables each client to use its own step size by adapting to the local
smoothness of the function each client is optimizing. We provide theoretical
and empirical results where the benefit of the client adaptivity is shown in
various FL scenarios. In particular, our proposed method achieves TOP-1
accuracy in 73% and TOP-2 accuracy in 100% of the experiments considered
without additional tuning.


------------------------------------------------------------------------------

Title:
Adversarial Robustness of Prompt-based Few-Shot Learning for Natural  Language Understanding

Abstract: State-of-the-art few-shot learning (FSL) methods leverage prompt-based
fine-tuning to obtain remarkable results for natural language understanding
(NLU) tasks. While much of the prior FSL methods focus on improving downstream
task performance, there is a limited understanding of the adversarial
robustness of such methods. In this work, we conduct an extensive study of
several state-of-the-art FSL methods to assess their robustness to adversarial
perturbations. To better understand the impact of various factors towards
robustness (or the lack of it), we evaluate prompt-based FSL methods against
fully fine-tuned models for aspects such as the use of unlabeled data, multiple
prompts, number of few-shot examples, model size and type. Our results on six
GLUE tasks indicate that compared to fully fine-tuned models, vanilla FSL
methods lead to a notable relative drop in task performance (i.e., are less
robust) in the face of adversarial perturbations. However, using (i) unlabeled
data for prompt-based FSL and (ii) multiple prompts flip the trend. We further
demonstrate that increasing the number of few-shot examples and model size lead
to increased adversarial robustness of vanilla FSL methods. Broadly, our work
sheds light on the adversarial robustness evaluation of prompt-based FSL
methods for NLU tasks.


------------------------------------------------------------------------------

Title:
Evaluation of an information security management system at a Mexican  higher education institution

Abstract: The purpose of this research was to know the degree of administrative
knowledge, the degree of training of human resources, the degree of commitment
of administrators and the degree of effectiveness of the administration for
information security risk based on ISO/IEC 27001.The population consisted of 81
subjects (66 administrators and 15 ITD personnel). Those evaluated were
employers of the administrative office of the university and also staff of the
Information Technology Department (ITD). To make the comparisons, three groups
of managers were formed according to classifications of administrative staff,
the classification was as follows: (a) first-line manager, (b) middle
management and (c) top management. About the results, it can be corroborated
that administrative staff with a lower rank have more problems in making the
best decisions in relation to the implementation of an ISMS, it should be noted
that the first-line manager is the one who has more contact with the students
and is the one who is less involved in the implementation of an ISMS. It can
also be inferred that the institution\'s planners are not fully trained in the
institution\'s information security efforts. This in turn prevents the
generation of proposals for initiatives to implement an ISMS. With this
shortcoming, it is possible that security breaches could be generated.


------------------------------------------------------------------------------

Title:
Parallel Data Compression Techniques

Abstract: With endless amounts of data and very limited bandwidth, fast data
compression is one solution for the growing datasharing problem. Compression
helps lower transfer times and save memory, but if the compression takes too
long, this no longer seems viable. Multi-core processors enable parallel data
compression; however, parallelizing the algorithms is anything but
straightforward since compression is inherently serial. This paper explores
techniques to parallelize three compression schemes: Huffman coding, LZSS, and
MP3 coding


------------------------------------------------------------------------------

Title:
LoSparse: Structured Compression of Large Language Models based on  Low-Rank and Sparse Approximation

Abstract: Transformer models have achieved remarkable results in various natural
language tasks, but they are often prohibitively large, requiring massive
memories and computational resources. To reduce the size and complexity of
these models, we propose LoSparse (Low-Rank and Sparse approximation), a novel
model compression technique that approximates a weight matrix by the sum of a
low-rank matrix and a sparse matrix. Our method combines the advantages of both
low-rank approximations and pruning, while avoiding their limitations. Low-rank
approximation compresses the coherent and expressive parts in neurons, while
pruning removes the incoherent and non-expressive parts in neurons. Pruning
enhances the diversity of low-rank approximations, and low-rank approximation
prevents pruning from losing too many expressive neurons. We evaluate our
method on natural language understanding, question answering, and natural
language generation tasks. We show that it significantly outperforms existing
compression methods.


------------------------------------------------------------------------------

Title:
GD-VDM: Generated Depth for better Diffusion-based Video Generation

Abstract: The field of generative models has recently witnessed significant progress,
with diffusion models showing remarkable performance in image generation. In
light of this success, there is a growing interest in exploring the application
of diffusion models to other modalities. One such challenge is the generation
of coherent videos of complex scenes, which poses several technical
difficulties, such as capturing temporal dependencies and generating long,
high-resolution videos. This paper proposes GD-VDM, a novel diffusion model for
video generation, demonstrating promising results. GD-VDM is based on a
two-phase generation process involving generating depth videos followed by a
novel diffusion Vid2Vid model that generates a coherent real-world video. We
evaluated GD-VDM on the Cityscapes dataset and found that it generates more
diverse and complex scenes compared to natural baselines, demonstrating the
efficacy of our approach.


------------------------------------------------------------------------------

Title:
Primitive Generation and Semantic-related Alignment for Universal  Zero-Shot Segmentation

Abstract: We study universal zero-shot segmentation in this work to achieve panoptic,
instance, and semantic segmentation for novel categories without any training
samples. Such zero-shot segmentation ability relies on inter-class
relationships in semantic space to transfer the visual knowledge learned from
seen categories to unseen ones. Thus, it is desired to well bridge
semantic-visual spaces and apply the semantic relationships to visual feature
learning. We introduce a generative model to synthesize features for unseen
categories, which links semantic and visual spaces as well as addresses the
issue of lack of unseen training data. Furthermore, to mitigate the domain gap
between semantic and visual spaces, firstly, we enhance the vanilla generator
with learned primitives, each of which contains fine-grained attributes related
to categories, and synthesize unseen features by selectively assembling these
primitives. Secondly, we propose to disentangle the visual feature into the
semantic-related part and the semantic-unrelated part that contains useful
visual classification clues but is less relevant to semantic representation.
The inter-class relationships of semantic-related visual features are then
required to be aligned with those in semantic space, thereby transferring
semantic knowledge to visual feature learning. The proposed approach achieves
impressively state-of-the-art performance on zero-shot panoptic segmentation,
instance segmentation, and semantic segmentation. Code is available at
this https URL


------------------------------------------------------------------------------

Title:
Taming Small-sample Bias in Low-budget Active Learning

Abstract: Active learning (AL) aims to minimize the annotation cost by only querying a
few informative examples for each model training stage. However, training a
model on a few queried examples suffers from the small-sample bias. In this
paper, we address this small-sample bias issue in low-budget AL by exploring a
regularizer called Firth bias reduction, which can provably reduce the bias
during the model training process but might hinder learning if its coefficient
is not adaptive to the learning progress. Instead of tuning the coefficient for
each query round, which is sensitive and time-consuming, we propose the
curriculum Firth bias reduction (CHAIN) that can automatically adjust the
coefficient to be adaptive to the training process. Under both deep learning
and linear model settings, experiments on three benchmark datasets with several
widely used query strategies and hyperparameter searching methods show that
CHAIN can be used to build more efficient AL and can substantially improve the
progress made by each active learning query.


------------------------------------------------------------------------------

Title:
CAMMARL: Conformal Action Modeling in Multi Agent Reinforcement Learning

Abstract: Before taking actions in an environment with more than one intelligent agent,
an autonomous agent may benefit from reasoning about the other agents and
utilizing a notion of a guarantee or confidence about the behavior of the
system. In this article, we propose a novel multi-agent reinforcement learning
(MARL) algorithm CAMMARL, which involves modeling the actions of other agents
in different situations in the form of confident sets, i.e., sets containing
their true actions with a high probability. We then use these estimates to
inform an agent's decision-making. For estimating such sets, we use the concept
of conformal predictions, by means of which, we not only obtain an estimate of
the most probable outcome but get to quantify the operable uncertainty as well.
For instance, we can predict a set that provably covers the true predictions
with high probabilities (e.g., 95%). Through several experiments in two fully
cooperative multi-agent tasks, we show that CAMMARL elevates the capabilities
of an autonomous agent in MARL by modeling conformal prediction sets over the
behavior of other agents in the environment and utilizing such estimates to
enhance its policy learning. All developed codes can be found here:
this https URL


------------------------------------------------------------------------------

Title:
Evaluating Loss Functions and Learning Data Pre-Processing for Climate  Downscaling Deep Learning Models

Abstract: Deep learning models have gained popularity in climate science, following
their success in computer vision and other domains. For instance, researchers
are increasingly employing deep learning techniques for downscaling climate
data, drawing inspiration from image super-resolution models. However, there
are notable differences between image data and climate data. While image data
typically falls within a specific range (e.g., [0, 255]) and exhibits a
relatively uniform or normal distribution, climate data can possess arbitrary
value ranges and highly uneven distributions, such as precipitation data. This
non-uniform distribution presents challenges when attempting to directly apply
existing computer vision models to climate science tasks. Few studies have
addressed this issue thus far. In this study, we explore the effects of loss
functions and non-linear data pre-processing methods for deep learning models
in the context of climate downscaling. We employ a climate downscaling
experiment as an example to evaluate these factors. Our findings reveal that L1
loss and L2 loss perform similarly on some more balanced data like temperature
data while for some imbalanced data like precipitation data, L2 loss performs
significantly better than L1 loss. Additionally, we propose an approach to
automatically learn the non-linear pre-processing function, which further
enhances model accuracy and achieves the best results.


------------------------------------------------------------------------------

Title:
Quantitative dynamics of design thinking and creativity perspectives in  company context

Abstract: This study is intended to provide in-depth insights into how design thinking
and creativity issues are understood and possibly evolve in the course of
design discussions in a company context. For that purpose, we use the seminar
transcripts of the Design Thinking Research Symposium 12 (DTRS12) dataset
"Tech-centred Design Thinking: Perspectives from a Rising Asia," which are
primarily concerned with how Korean companies implement design thinking and
what role designers currently play. We employed a novel method of information
processing based on constructed dynamic semantic networks to investigate the
seminar discussions according to company representatives and company size. We
compared the quantitative dynamics in two seminars: the first involved
managerial representatives of four companies, and the second involved
specialized designers and management of a design center of single company. On
the basis of dynamic semantic networks, we quantified the changes in four
semantic measures -- abstraction, polysemy, information content, and pairwise
word similarity -- in chronologically reconstructed individual design-thinking
processes. Statistical analyses show that design thinking in the seminar with
four companies, exhibits significant differences in the dynamics of
abstraction, polysemy, and information content, compared to the seminar with
the design center of single company. Both the decrease in polysemy and
abstraction and the increase in information content in the individual
design-thinking processes in the seminar with four companies indicate that
design managers are focused on more concrete design issues, with more
information and less ambiguous content to the final design product. By
contrast, specialized designers manifest more abstract thinking and appear to
exhibit a slightly higher level of divergence in their design processes.


------------------------------------------------------------------------------

Title:
Cross-Modal Attribute Insertions for Assessing the Robustness of  Vision-and-Language Learning

Abstract: The robustness of multimodal deep learning models to realistic changes in the
input text is critical for their applicability to important tasks such as
text-to-image retrieval and cross-modal entailment. To measure robustness,
several existing approaches edit the text data, but do so without leveraging
the cross-modal information present in multimodal data. Information from the
visual modality, such as color, size, and shape, provide additional attributes
that users can include in their inputs. Thus, we propose cross-modal attribute
insertions as a realistic perturbation strategy for vision-and-language data
that inserts visual attributes of the objects in the image into the
corresponding text (e.g., "girl on a chair" to "little girl on a wooden
chair"). Our proposed approach for cross-modal attribute insertions is modular,
controllable, and task-agnostic. We find that augmenting input text using
cross-modal insertions causes state-of-the-art approaches for text-to-image
retrieval and cross-modal entailment to perform poorly, resulting in relative
drops of 15% in MRR and 20% in $F_1$ score, respectively. Crowd-sourced
annotations demonstrate that cross-modal insertions lead to higher quality
augmentations for multimodal data than augmentations using text-only data, and
are equivalent in quality to original examples. We release the code to
encourage robustness evaluations of deep vision-and-language models:
this https URL


------------------------------------------------------------------------------

Title:
Eliminating Lipschitz Singularities in Diffusion Models

Abstract: Diffusion models, which employ stochastic differential equations to sample
images through integrals, have emerged as a dominant class of generative
models. However, the rationality of the diffusion process itself receives
limited attention, leaving the question of whether the problem is well-posed
and well-conditioned. In this paper, we uncover a vexing propensity of
diffusion models: they frequently exhibit the infinite Lipschitz near the zero
point of timesteps. This poses a threat to the stability and accuracy of the
diffusion process, which relies on integral operations. We provide a
comprehensive evaluation of the issue from both theoretical and empirical
perspectives. To address this challenge, we propose a novel approach, dubbed
E-TSDM, which eliminates the Lipschitz singularity of the diffusion model near
zero. Remarkably, our technique yields a substantial improvement in
performance, e.g., on the high-resolution FFHQ dataset ($256\times256$).
Moreover, as a byproduct of our method, we manage to achieve a dramatic
reduction in the Frechet Inception Distance of other acceleration methods
relying on network Lipschitz, including DDIM and DPM-Solver, by over 33$\%$. We
conduct extensive experiments on diverse datasets to validate our theory and
method. Our work not only advances the understanding of the general diffusion
process, but also provides insights for the design of diffusion models.


------------------------------------------------------------------------------

Title:
Social network modeling and applications, a tutorial

Abstract: Social networks have been widely studied over the last century from multiple
disciplines to understand societal issues such as inequality in employment
rates, managerial performance, and epidemic spread. Today, these and many more
issues can be studied at global scale thanks to the digital footprints that we
generate when browsing the Web or using social media platforms. Unfortunately,
scientists often struggle to access to such data primarily because it is
proprietary, and even when it is shared with privacy guarantees, such data is
either no representative or too big. In this tutorial, we will discuss recent
advances and future directions in network modeling. In particular, we focus on
how to exploit synthetic networks to study real-world problems such as data
privacy, spreading dynamics, algorithmic bias, and ranking inequalities. We
start by reviewing different types of generative models for social networks
including node-attributed and scale-free networks. Then, we showcase how to
perform a network selection analysis to characterize the mechanisms of edge
formation of any given real-world network.


------------------------------------------------------------------------------

Title:
Protecting IoT Servers Against Flood Attacks with the Quasi  Deterministic Transmission Policy

Abstract: IoT Servers that receive and process packets from IoT devices should meet the
QoS needs of incoming packets, and support Attack Detection software that
analyzes the incoming traffic to identify and discard packets that may be part
of a Cyberattack. Since UDP Flood Attacks can overwhelm IoT Servers by creating
congestion that paralyzes their operation and limits their ability to conduct
timely Attack Detection, this paper proposes and evaluates a simple
architecture to protect a Server that is connected to a Local Area Network,
using a Quasi Deterministic Transmission Policy Forwarder (SQF) at its input
port. This Forwarder shapes the incoming traffic, sends it to the Server in a
manner which does not modify the overall delay of the packets, and avoids
congestion inside the Server. The relevant theoretical background is briefly
reviewed, and measurements during a UDP Flood Attack are provided to compare
the Server performance, with and without the Forwarder. It is seen that during
a UDP Flood Attack, the Forwarder protects the Server from congestion allowing
it to effectively identify Attack Packets. On the other hand, the resulting
Forwarder congestion can also be eliminated at the Forwarder with "drop"
commands generated by the Forwarder itself, or sent by the Server to the
Forwarder.


------------------------------------------------------------------------------

Title:
Quasipolynomiality of the Smallest Missing Induced Subgraph

Abstract: We study the problem of finding the smallest graph that does not occur as an
induced subgraph of a given graph. This missing induced subgraph has at most
logarithmic size and can be found by a brute-force search, in an $n$-vertex
graph, in time $n^{O(\log n)}$. We show that under the Exponential Time
Hypothesis this quasipolynomial time bound is optimal. We also consider
variations of the problem in which either the missing subgraph or the given
graph comes from a restricted graph family; for instance, we prove that the
smallest missing planar induced subgraph of a given planar graph can be found
in polynomial time.


------------------------------------------------------------------------------

Title:
Evaluating Privacy Questions From Stack Overflow: Can ChatGPT Compete?

Abstract: Stack Overflow and other similar forums are used commonly by developers to
seek answers for their software development as well as privacy-related
concerns. Recently, ChatGPT has been used as an alternative to generate code or
produce responses to developers' questions. In this paper, we aim to understand
developers' privacy challenges by evaluating the types of privacy-related
questions asked on Stack Overflow. We then conduct a comparative analysis
between the accepted responses given by Stack Overflow users and the responses
produced by ChatGPT for those extracted questions to identify if ChatGPT could
serve as a viable alternative. Our results show that most privacy-related
questions are related to choice/consent, aggregation, and identification.
Furthermore, our findings illustrate that ChatGPT generates similarly correct
responses for about 56% of questions, while for the rest of the responses, the
answers from Stack Overflow are slightly more accurate than ChatGPT.


------------------------------------------------------------------------------

Title:
Differentially Private Over-the-Air Federated Learning Over MIMO Fading  Channels

Abstract: Federated learning (FL) enables edge devices to collaboratively train machine
learning models, with model communication replacing direct data uploading.
While over-the-air model aggregation improves communication efficiency,
uploading models to an edge server over wireless networks can pose privacy
risks. Differential privacy (DP) is a widely used quantitative technique to
measure statistical data privacy in FL. Previous research has focused on
over-the-air FL with a single-antenna server, leveraging communication noise to
enhance user-level DP. This approach achieves the so-called "free DP" by
controlling transmit power rather than introducing additional DP-preserving
mechanisms at devices, such as adding artificial noise. In this paper, we study
differentially private over-the-air FL over a multiple-input multiple-output
(MIMO) fading channel. We show that FL model communication with a
multiple-antenna server amplifies privacy leakage as the multiple-antenna
server employs separate receive combining for model aggregation and information
inference. Consequently, relying solely on communication noise, as done in the
multiple-input single-output system, cannot meet high privacy requirements, and
a device-side privacy-preserving mechanism is necessary for optimal DP design.
We analyze the learning convergence and privacy loss of the studied FL system
and propose a transceiver design algorithm based on alternating optimization.
Numerical results demonstrate that the proposed method achieves a better
privacy-learning trade-off compared to prior work.


------------------------------------------------------------------------------

Title:
Forest Parameter Prediction by Multiobjective Deep Learning of  Regression Models Trained with Pseudo-Target Imputation

Abstract: In prediction of forest parameters with data from remote sensing (RS),
regression models have traditionally been trained on a small sample of ground
reference data. This paper proposes to impute this sample of true prediction
targets with data from an existing RS-based prediction map that we consider as
pseudo-targets. This substantially increases the amount of target training data
and leverages the use of deep learning (DL) for semi-supervised regression
modelling. We use prediction maps constructed from airborne laser scanning
(ALS) data to provide accurate pseudo-targets and free data from Sentinel-1's
C-band synthetic aperture radar (SAR) as regressors. A modified U-Net
architecture is adapted with a selection of different training objectives. We
demonstrate that when a judicious combination of loss functions is used, the
semi-supervised imputation strategy produces results that surpass traditional
ALS-based regression models, even though \sen data are considered as inferior
for forest monitoring. These results are consistent for experiments on
above-ground biomass prediction in Tanzania and stem volume prediction in
Norway, representing a diversity in parameters and forest types that emphasises
the robustness of the approach.


------------------------------------------------------------------------------

Title:
Few-shot Learning for Inference in Medical Imaging with Subspace Feature  Representations

Abstract: Unlike the field of visual scene recognition where tremendous advances have
taken place due to the availability of very large datasets to train deep neural
networks, inference from medical images is often hampered by the fact that only
small amounts of data may be available. When working with very small dataset
problems, of the order of a few hundred items of data, the power of deep
learning may still be exploited by using a model pre-trained on natural images
as a feature extractor and carrying out classic pattern recognition techniques
in this feature space, the so-called few-shot learning problem. In regimes
where the dimension of this feature space is comparable to or even larger than
the number of items of data, dimensionality reduction is a necessity and is
often achieved by principal component analysis, i.e., singular value
decomposition (SVD). In this paper, noting the inappropriateness of using SVD
for this setting, we usher in and explore two alternatives based on
discriminant analysis and non-negative matrix factorization (NMF). Using 14
different datasets spanning $11$ distinct disease types, we demonstrate that
discriminant subspaces at low dimensions achieve significant improvements over
SVD-based subspaces and the original feature space. We also show that NMF at
modest dimensions is a competitive alternative to SVD in this setting.


------------------------------------------------------------------------------

Title:
Modular Simulation Environment Towards OTN AI-based Solutions

Abstract: The current trend for highly dynamic and virtualized networking
infrastructure made automated networking a critical requirement. Multiple
solutions have been proposed to address this, including the most sought-after
machine learning ML-based solutions. However, the main hurdle when developing
Next Generation Network is the availability of large datasets, especially in 5G
and beyond and Optical Transport Networking (OTN) traffic. This need led
researchers to look for viable simulation environments to generate the
necessary volume with highly configurable real-life scenarios, which can be
costly in setup and require subscription-based products and even the purchase
of dedicated hardware, depending on the supplier. We aim to address this issue
by generating high-volume and fidelity datasets by proposing a modular solution
to adapt to the user's available resources. These datasets can be used to
develop better-aforementioned ML solutions resulting in higher accuracy and
adaptation to real-life networking traffic.


------------------------------------------------------------------------------

Title:
Concavity-Induced Distance for Unoriented Point Cloud Decomposition

Abstract: We propose Concavity-induced Distance (CID) as a novel way to measure the
dissimilarity between a pair of points in an unoriented point cloud. CID
indicates the likelihood of two points or two sets of points belonging to
different convex parts of an underlying shape represented as a point cloud.
After analyzing its properties, we demonstrate how CID can benefit point cloud
analysis without the need for meshing or normal estimation, which is beneficial
for robotics applications when dealing with raw point cloud observations. By
randomly selecting very few points for manual labeling, a CID-based point cloud
instance segmentation via label propagation achieves comparable average
precision as recent supervised deep learning approaches, on S3DIS and ScanNet
datasets. Moreover, CID can be used to group points into approximately convex
parts whose convex hulls can be used as compact scene representations in
robotics, and it outperforms the baseline method in terms of grouping quality.
Our project website is available at: this https URL


------------------------------------------------------------------------------

Title:
virtCCA: Virtualized Arm Confidential Compute Architecture with  TrustZone

Abstract: ARM introduces the Confidential Compute Architecture (CCA) in the forthcoming
ARMv9-A architecture recently. CCA enables the support of confidential virtual
machines (cVMs) within a separated world (known as the Realm world), protected
from the untrusted normal world. While CCA points to a convincing future of
confidential computing, it is foreseen that the CCA hardware will not be
available soon according to ARM's roadmap. Upon this request, we present
\textit{virtCCA}, an architecture that facilitates virtualized CCA using
TrustZone, a mature hardware feature on existing ARM platforms. Specifically,
we use the Secure EL2 (S-EL2) extension introduced since ARMv8.4 to support the
memory isolation among the cVMs. We introduce direct shadow memory mapping --
an efficient memory protection scheme -- to overcome the limitations of
existing hardware. virtCCA is compatible with the CCA specifications at the API
level, and we build the entire CCA software and firmware stack atop virtCCA,
including the TrustZone Management Monitor (TMM) for enforcing isolation among
cVMs and supporting cVM life cycle management, as well as the enhancement of
the normal world KVM for support of cVMs. We implemented virtCCA on both QEMU
and ARM Fixed Virtual Platform (FVP). The evaluation on micro-benchmarks and
macro-benchmarks shows that the overhead of running cVMs is acceptable,
compared with the counterpart of running normal world VMs. On a set of
real-world workloads the overhead is less than 8%, with the worst case of 17%
for I/O intensive workloads.


------------------------------------------------------------------------------

Title:
Less Can Be More: Exploring Population Rating Dispositions with  Partitioned Models in Recommender Systems

Abstract: In this study, we partition users by rating disposition - looking first at
their percentage of negative ratings, and then at the general use of the rating
scale. We hypothesize that users with different rating dispositions may use the
recommender system differently and therefore the agreement with their past
ratings may be less predictive of the future agreement.
We use data from a large movie rating website to explore whether users should
be grouped by disposition, focusing on identifying their various rating
distributions that may hurt recommender effectiveness. We find that such
partitioning not only improves computational efficiency but also improves top-k
performance and predictive accuracy. Though such effects are largest for the
user-based KNN CF, smaller for item-based KNN CF, and smallest for latent
factor algorithms such as SVD.


------------------------------------------------------------------------------

Title:
Deep Learning of Dynamical System Parameters from Return Maps as Images

Abstract: We present a novel approach to system identification (SI) using deep learning
techniques. Focusing on parametric system identification (PSI), we use a
supervised learning approach for estimating the parameters of discrete and
continuous-time dynamical systems, irrespective of chaos. To accomplish this,
we transform collections of state-space trajectory observations into image-like
data to retain the state-space topology of trajectories from dynamical systems
and train convolutional neural networks to estimate the parameters of dynamical
systems from these images. We demonstrate that our approach can learn parameter
estimation functions for various dynamical systems, and by using training-time
data augmentation, we are able to learn estimation functions whose parameter
estimates are robust to changes in the sample fidelity of their inputs. Once
trained, these estimation models return parameter estimations for new systems
with negligible time and computation costs.


------------------------------------------------------------------------------

Title:
Spatiotemporal Pyramidal CNN with Depth-Wise Separable Convolution for  Eye Blinking Detection in the Wild

Abstract: Eye blinking detection in the wild plays an essential role in deception
detection, driving fatigue detection, etc. Despite the fact that numerous
attempts have already been made, the majority of them have encountered
difficulties, such as the derived eye images having different resolutions as
the distance between the face and the camera changes; or the requirement of a
lightweight detection model to obtain a short inference time in order to
perform in real-time. In this research, two problems are addressed: how the eye
blinking detection model can learn efficiently from different resolutions of
eye pictures in diverse conditions; and how to reduce the size of the detection
model for faster inference time. We propose to utilize upsampling and
downsampling the input eye images to the same resolution as one potential
solution for the first problem, then find out which interpolation method can
result in the highest performance of the detection model. For the second
problem, although a recent spatiotemporal convolutional neural network used for
eye blinking detection has a strong capacity to extract both spatial and
temporal characteristics, it remains having a high number of network
parameters, leading to high inference time. Therefore, using Depth-wise
Separable Convolution rather than conventional convolution layers inside each
branch is considered in this paper as a feasible solution.


------------------------------------------------------------------------------

Title:
Cuckoo Hashing in Cryptography: Optimal Parameters, Robustness and  Applications

Abstract: Cuckoo hashing is a powerful primitive that enables storing items using small
space with efficient querying. At a high level, cuckoo hashing maps $n$ items
into $b$ entries storing at most $\ell$ items such that each item is placed
into one of $k$ randomly chosen entries. Additionally, there is an overflow
stash that can store at most $s$ items. Many cryptographic primitives rely upon
cuckoo hashing to privately embed and query data where it is integral to ensure
small failure probability when constructing cuckoo hashing tables as it
directly relates to the privacy guarantees.
As our main result, we present a more query-efficient cuckoo hashing
construction using more hash functions. For construction failure probability
$\epsilon$, the query overhead of our scheme is $O(1 +
\sqrt{\log(1/\epsilon)/\log n})$. Our scheme has quadratically smaller query
overhead than prior works for any target failure probability $\epsilon$. We
also prove lower bounds matching our construction. Our improvements come from a
new understanding of the locality of cuckoo hashing failures for small sets of
items.
We also initiate the study of robust cuckoo hashing where the input set may
be chosen with knowledge of the hash functions. We present a cuckoo hashing
scheme using more hash functions with query overhead $\tilde{O}(\log \lambda)$
that is robust against poly$(\lambda)$ adversaries. Furthermore, we present
lower bounds showing that this construction is tight and that extending
previous approaches of large stashes or entries cannot obtain robustness except
with $\Omega(n)$ query overhead.
As applications of our results, we obtain improved constructions for batch
codes and PIR. In particular, we present the most efficient explicit batch code
and blackbox reduction from single-query PIR to batch PIR.


------------------------------------------------------------------------------

Title:
Front-door Adjustment Beyond Markov Equivalence with Limited Graph  Knowledge

Abstract: Causal effect estimation from data typically requires assumptions about the
cause-effect relations either explicitly in the form of a causal graph
structure within the Pearlian framework, or implicitly in terms of
(conditional) independence statements between counterfactual variables within
the potential outcomes framework. When the treatment variable and the outcome
variable are confounded, front-door adjustment is an important special case
where, given the graph, causal effect of the treatment on the target can be
estimated using post-treatment variables. However, the exact formula for
front-door adjustment depends on the structure of the graph, which is difficult
to learn in practice. In this work, we provide testable conditional
independence statements to compute the causal effect using front-door-like
adjustment without knowing the graph under limited structural side information.
We show that our method is applicable in scenarios where knowing the Markov
equivalence class is not sufficient for causal effect estimation. We
demonstrate the effectiveness of our method on a class of random graphs as well
as real causal fairness benchmarks.


------------------------------------------------------------------------------

Title:
An Isotonic Mechanism for Overlapping Ownership

Abstract: This paper extends the Isotonic Mechanism from the single-owner to
multi-owner settings, in an effort to make it applicable to peer review where a
paper often has multiple authors. Our approach starts by partitioning all
submissions of a machine learning conference into disjoint blocks, each of
which shares a common set of co-authors. We then employ the Isotonic Mechanism
to elicit a ranking of the submissions from each author and to produce adjusted
review scores that align with both the reported ranking and the original review
scores. The generalized mechanism uses a weighted average of the adjusted
scores on each block. We show that, under certain conditions, truth-telling by
all authors is a Nash equilibrium for any valid partition of the overlapping
ownership sets. However, we demonstrate that while the mechanism's performance
in terms of estimation accuracy depends on the partition structure, optimizing
this structure is computationally intractable in general. We develop a nearly
linear-time greedy algorithm that provably finds a performant partition with
appealing robust approximation guarantees. Extensive experiments on both
synthetic data and real-world conference review data demonstrate the
effectiveness of this generalized Isotonic Mechanism.


------------------------------------------------------------------------------

Title:
InRank: Incremental Low-Rank Learning

Abstract: The theory of greedy low-rank learning (GLRL) aims to explain the impressive
generalization capabilities of deep learning. It proves that stochastic
gradient-based training implicitly regularizes neural networks towards low-rank
solutions through a gradual increase of the rank during training. However,
there is a gap between theory and practice since GLRL requires an infinitesimal
initialization of the weights, which is not practical due to the fact that it
is a saddle point. In this work, we remove the assumption of infinitesimal
initialization by focusing on cumulative weight updates. We prove the
cumulative weight updates follow an incremental low-rank trajectory for
arbitrary orthogonal initialization of weights in a three-layer linear network.
Empirically, we demonstrate that our theory holds on a broad range of neural
networks (e.g., transformers) and standard training algorithms (e.g., SGD,
Adam). However, existing training algorithms do not exploit the low-rank
property to improve computational efficiency as the networks are not
parameterized in low-rank. To remedy this, we design a new training algorithm
Incremental Low-Rank Learning (InRank), which explicitly expresses cumulative
weight updates as low-rank matrices while incrementally augmenting their ranks
during training. We evaluate InRank on GPT-2, and our results indicate that
InRank achieves comparable prediction performance as the full-rank counterpart
while requiring at most 33% of the total ranks throughout training. We also
propose an efficient version of InRank that achieves a reduction of 20% in
total training time and 37% in memory usage when training GPT-medium on
WikiText-103 from scratch.


------------------------------------------------------------------------------

Title:
ETL for the integration of remote sensing data

Abstract: Modern in-orbit satellites and other available remote sensing tools have
generated a huge availability of public data waiting to be exploited in
different formats hosted on different servers. In this context, ETL formalism
becomes relevant for the integration and analysis of the combined information
from all these sources. Throughout this work, we present the theoretical and
practical foundations to build a modular analysis infrastructure that allows
the creation of ETLs to download, transform and integrate data coming from
different instruments in different formats. Part of this work is already
implemented in a Python library which is intended to be integrated into already
available workflow management tools based on acyclic-directed graphs which also
have different adapters to impact the combined data in different warehouses.


------------------------------------------------------------------------------

Title:
Enhancing Documents with Multidimensional Relevance Statements in  Cross-encoder Re-ranking

Abstract: In this paper, we propose a novel approach to consider multiple dimensions of
relevance beyond topicality in cross-encoder re-ranking. On the one hand,
current multidimensional retrieval models often use na\"ive solutions at the
re-ranking stage to aggregate multiple relevance scores into an overall one. On
the other hand, cross-encoder re-rankers are effective in considering
topicality but are not designed to straightforwardly account for other
relevance dimensions. To overcome these issues, we envisage enhancing the
candidate documents -- which are retrieved by a first-stage lexical retrieval
model -- with "relevance statements" related to additional dimensions of
relevance and then performing a re-ranking on them with cross-encoders. In
particular, here we consider an additional relevance dimension beyond
topicality, which is credibility. We test the effectiveness of our solution in
the context of the Consumer Health Search task, considering publicly available
datasets. Our results show that the proposed approach statistically outperforms
both aggregation-based and cross-encoder re-rankers.


------------------------------------------------------------------------------

Title:
Supervised Auto-Encoding Twin-Bottleneck Hashing

Abstract: Deep hashing has shown to be a complexity-efficient solution for the
Approximate Nearest Neighbor search problem in high dimensional space. Many
methods usually build the loss function from pairwise or triplet data points to
capture the local similarity structure. Other existing methods construct the
similarity graph and consider all points simultaneously. Auto-encoding
Twin-bottleneck Hashing is one such method that dynamically builds the graph.
Specifically, each input data is encoded into a binary code and a continuous
variable, or the so-called twin bottlenecks. The similarity graph is then
computed from these binary codes, which get updated consistently during the
training. In this work, we generalize the original model into a supervised deep
hashing network by incorporating the label information. In addition, we examine
the differences of codes structure between these two networks and consider the
class imbalance problem especially in multi-labeled datasets. Experiments on
three datasets yield statistically significant improvement against the original
model. Results are also comparable and competitive to other supervised methods.


------------------------------------------------------------------------------

Title:
Sim-to-real transfer of active suspension control using deep  reinforcement learning

Abstract: We explore sim-to-real transfer of deep reinforcement learning controllers
for a heavy vehicle with active suspensions designed for traversing rough
terrain. While related research primarily focuses on lightweight robots with
electric motors and fast actuation, this study uses a forestry vehicle with a
complex hydraulic driveline and slow actuation. We simulate the vehicle using
multibody dynamics and apply system identification to find an appropriate set
of simulation parameters. We then train policies in simulation using various
techniques to mitigate the sim-to-real gap, including domain randomization,
action delays, and a reward penalty to encourage smooth control. In reality,
the policies trained with action delays and a penalty for erratic actions
perform at nearly the same level as in simulation. In experiments on level
ground, the motion trajectories closely overlap when turning to either side, as
well as in a route tracking scenario. When faced with a ramp that requires
active use of the suspensions, the simulated and real motions are in close
alignment. This shows that the actuator model together with system
identification yields a sufficiently accurate model of the actuators. We
observe that policies trained without the additional action penalty exhibit
fast switching or bang-bang control. These present smooth motions and high
performance in simulation but transfer poorly to reality. We find that policies
make marginal use of the local height map for perception, showing no
indications of look-ahead planning. However, the strong transfer capabilities
entail that further development concerning perception and performance can be
largely confined to simulation.


------------------------------------------------------------------------------

Title:
Confidence-Based Model Selection: When to Take Shortcuts for  Subpopulation Shifts

Abstract: Effective machine learning models learn both robust features that directly
determine the outcome of interest (e.g., an object with wheels is more likely
to be a car), and shortcut features (e.g., an object on a road is more likely
to be a car). The latter can be a source of error under distributional shift,
when the correlations change at test-time. The prevailing sentiment in the
robustness literature is to avoid such correlative shortcut features and learn
robust predictors. However, while robust predictors perform better on
worst-case distributional shifts, they often sacrifice accuracy on majority
subpopulations. In this paper, we argue that shortcut features should not be
entirely discarded. Instead, if we can identify the subpopulation to which an
input belongs, we can adaptively choose among models with different strengths
to achieve high performance on both majority and minority subpopulations. We
propose COnfidence-baSed MOdel Selection (CosMoS), where we observe that model
confidence can effectively guide model selection. Notably, CosMoS does not
require any target labels or group annotations, either of which may be
difficult to obtain or unavailable. We evaluate CosMoS on four datasets with
spurious correlations, each with multiple test sets with varying levels of data
distribution shift. We find that CosMoS achieves 2-5% lower average regret
across all subpopulations, compared to using only robust predictors or other
model aggregation methods.


------------------------------------------------------------------------------

Title:
Optimal Fault-Tolerant Spanners in Euclidean and Doubling Metrics:  Breaking the $Ω(\log n)$ Lightness Barrier

Abstract: An essential requirement of spanners in many applications is to be
fault-tolerant: a $(1+\epsilon)$-spanner of a metric space is called (vertex)
$f$-fault-tolerant ($f$-FT) if it remains a $(1+\epsilon)$-spanner (for the
non-faulty points) when up to $f$ faulty points are removed from the spanner.
Fault-tolerant (FT) spanners for Euclidean and doubling metrics have been
extensively studied since the 90s.
For low-dimensional Euclidean metrics, Czumaj and Zhao in SoCG'03 [CZ03]
showed that the optimal guarantees $O(f n)$, $O(f)$ and $O(f^2)$ on the size,
degree and lightness of $f$-FT spanners can be achieved via a greedy algorithm,
which na\"{\i}vely runs in $O(n^3) \cdot 2^{O(f)}$ time. The question of
whether the optimal bounds of [CZ03] can be achieved via a fast construction
has remained elusive, with the lightness parameter being the bottleneck.
Moreover, in the wider family of doubling metrics, it is not even clear whether
there exists an $f$-FT spanner with lightness that depends solely on $f$ (even
exponentially): all existing constructions have lightness $\Omega(\log n)$
since they are built on the net-tree spanner, which is induced by a
hierarchical net-tree of lightness $\Omega(\log n)$.
In this paper we settle in the affirmative these longstanding open questions.
Specifically, we design a construction of $f$-FT spanners that is optimal with
respect to all the involved parameters (size, degree, lightness and running
time): For any $n$-point doubling metric, any $\epsilon > 0$, and any integer
$1 \le f \le n-2$, our construction provides, within time $O(n \log n + f n)$,
an $f$-FT $(1+\epsilon)$-spanner with size $O(f n)$, degree $O(f)$ and
lightness $O(f^2)$.


------------------------------------------------------------------------------

Title:
Fine-Tuning Language Models for Scientific Writing Support

Abstract: We support scientific writers in determining whether a written sentence is
scientific, to which section it belongs, and suggest paraphrasings to improve
the sentence. Firstly, we propose a regression model trained on a corpus of
scientific sentences extracted from peer-reviewed scientific papers and
non-scientific text to assign a score that indicates the scientificness of a
sentence. We investigate the effect of equations and citations on this score to
test the model for potential biases. Secondly, we create a mapping of section
titles to a standard paper layout in AI and machine learning to classify a
sentence to its most likely section. We study the impact of context, i.e.,
surrounding sentences, on the section classification performance. Finally, we
propose a paraphraser, which suggests an alternative for a given sentence that
includes word substitutions, additions to the sentence, and structural changes
to improve the writing style. We train various large language models on
sentences extracted from arXiv papers that were peer reviewed and published at
A*, A, B, and C ranked conferences. On the scientificness task, all models
achieve an MSE smaller than $2\%$. For the section classification, BERT
outperforms WideMLP and SciBERT in most cases. We demonstrate that using
context enhances the classification of a sentence, achieving up to a $90\%$
F1-score. Although the paraphrasing models make comparatively few alterations,
they produce output sentences close to the gold standard. Large fine-tuned
models such as T5 Large perform best in experiments considering various
measures of difference between input sentence and gold standard. Code is
provided under this https URL


------------------------------------------------------------------------------

Title:
Co-design Hardware and Algorithm for Vector Search

Abstract: Vector search has emerged as the foundation for large-scale information
retrieval and machine learning systems, with search engines like Google and
Bing processing tens of thousands of queries per second on petabyte-scale
document datasets by evaluating vector similarities between encoded query texts
and web documents. As performance demands for vector search systems surge,
accelerated hardware offers a promising solution in the post-Moore's Law era.
We introduce \textit{FANNS}, an end-to-end and scalable vector search framework
on FPGAs. Given a user-provided recall requirement on a dataset and a hardware
resource budget, \textit{FANNS} automatically co-designs hardware and
algorithm, subsequently generating the corresponding accelerator. The framework
also supports scale-out by incorporating a hardware TCP/IP stack in the
accelerator. \textit{FANNS} attains up to 23.0$\times$ and 37.2$\times$ speedup
compared to FPGA and CPU baselines, respectively, and demonstrates superior
scalability to GPUs, achieving 5.5$\times$ and 7.6$\times$ speedup in median
and 95\textsuperscript{th} percentile (P95) latency within an eight-accelerator
configuration. The remarkable performance of \textit{FANNS} lays a robust
groundwork for future FPGA integration in data centers and AI supercomputers.


------------------------------------------------------------------------------

Title:
LightRidge: An End-to-end Agile Design Framework for Diffractive Optical  Neural Networks

Abstract: To lower the barrier to diffractive optical neural networks (DONNs) design,
exploration, and deployment, we propose LightRidge, the first end-to-end
optical ML compilation framework, which consists of (1) precise and
differentiable optical physics kernels that enable complete explorations of
DONNs architectures, (2) optical physics computation kernel acceleration that
significantly reduces the runtime cost in training, emulation, and deployment
of DONNs, and (3) versatile and flexible optical system modeling and
user-friendly domain-specific-language (DSL). As a result, LightRidge framework
enables efficient end-to-end design and deployment of DONNs, and significantly
reduces the efforts for programming, hardware-software codesign, and chip
integration. Our results are experimentally conducted with physical optical
systems, where we demonstrate: (1) the optical physics kernels precisely
correlated to low-level physics and systems, (2) significant speedups in
runtime with physics-aware emulation workloads compared to the state-of-the-art
commercial system, (3) effective architectural design space exploration
verified by the hardware prototype and on-chip integration case study, and (4)
novel DONN design principles including successful demonstrations of advanced
image classification and image segmentation task using DONNs architecture and
topology.


------------------------------------------------------------------------------

Title:
OpenSTL: A Comprehensive Benchmark of Spatio-Temporal Predictive  Learning

Abstract: Spatio-temporal predictive learning is a learning paradigm that enables
models to learn spatial and temporal patterns by predicting future frames from
given past frames in an unsupervised manner. Despite remarkable progress in
recent years, a lack of systematic understanding persists due to the diverse
settings, complex implementation, and difficult reproducibility. Without
standardization, comparisons can be unfair and insights inconclusive. To
address this dilemma, we propose OpenSTL, a comprehensive benchmark for
spatio-temporal predictive learning that categorizes prevalent approaches into
recurrent-based and recurrent-free models. OpenSTL provides a modular and
extensible framework implementing various state-of-the-art methods. We conduct
standard evaluations on datasets across various domains, including synthetic
moving object trajectory, human motion, driving scenes, traffic flow and
weather forecasting. Based on our observations, we provide a detailed analysis
of how model architecture and dataset properties affect spatio-temporal
predictive learning performance. Surprisingly, we find that recurrent-free
models achieve a good balance between efficiency and performance than recurrent
models. Thus, we further extend the common MetaFormers to boost recurrent-free
spatial-temporal predictive learning. We open-source the code and models at
this https URL


------------------------------------------------------------------------------

Title:
Concept Extrapolation: A Conceptual Primer

Abstract: This article is a primer on concept extrapolation - the ability to take a
concept, a feature, or a goal that is defined in one context and extrapolate it
safely to a more general context. Concept extrapolation aims to solve model
splintering - a ubiquitous occurrence wherein the features or concepts shift as
the world changes over time. Through discussing value splintering and value
extrapolation the article argues that concept extrapolation is necessary for
Artificial Intelligence alignment.


------------------------------------------------------------------------------

Title:
CAT-Walk: Inductive Hypergraph Learning via Set Walks

Abstract: Temporal hypergraphs provide a powerful paradigm for modeling time-dependent,
higher-order interactions in complex systems. Representation learning for
hypergraphs is essential for extracting patterns of the higher-order
interactions that are critically important in real-world problems in social
network analysis, neuroscience, finance, etc. However, existing methods are
typically designed only for specific tasks or static hypergraphs. We present
CAT-Walk, an inductive method that learns the underlying dynamic laws that
govern the temporal and structural processes underlying a temporal hypergraph.
CAT-Walk introduces a temporal, higher-order walk on hypergraphs, SetWalk, that
extracts higher-order causal patterns. CAT-Walk uses a novel adaptive and
permutation invariant pooling strategy, SetMixer, along with a set-based
anonymization process that hides the identity of hyperedges. Finally, we
present a simple yet effective neural network model to encode hyperedges. Our
evaluation on 10 hypergraph benchmark datasets shows that CAT-Walk attains
outstanding performance on temporal hyperedge prediction benchmarks in both
inductive and transductive settings. It also shows competitive performance with
state-of-the-art methods for node classification.


------------------------------------------------------------------------------

Title:
Generative Sequential Recommendation with GPTRec

Abstract: Sequential recommendation is an important recommendation task that aims to
predict the next item in a sequence. Recently, adaptations of language models,
particularly Transformer-based models such as SASRec and BERT4Rec, have
achieved state-of-the-art results in sequential recommendation. In these
models, item ids replace tokens in the original language models. However, this
approach has limitations. First, the vocabulary of item ids may be many times
larger than in language models. Second, the classical Top-K recommendation
approach used by these models may not be optimal for complex recommendation
objectives, including auxiliary objectives such as diversity, coverage or
coherence. Recent progress in generative language models inspires us to revisit
generative approaches to address these challenges. This paper presents the
GPTRec sequential recommendation model, which is based on the GPT-2
architecture. GPTRec can address large vocabulary issues by splitting item ids
into sub-id tokens using a novel SVD Tokenisation algorithm based on quantised
item embeddings from an SVD decomposition of the user-item interaction matrix.
The paper also presents a novel Next-K recommendation strategy, which generates
recommendations item-by-item, considering already recommended items. The Next-K
strategy can be used for producing complex interdependent recommendation lists.
We experiment with GPTRec on the MovieLens-1M dataset and show that using
sub-item tokenisation GPTRec can match the quality of SASRec while reducing the
embedding table by 40%. We also show that the recommendations generated by
GPTRec on MovieLens-1M using the Next-K recommendation strategy match the
quality of SASRec in terms of NDCG@10, meaning that the model can serve as a
strong starting point for future research.


------------------------------------------------------------------------------

Title:
A Lightweight Causal Model for Interpretable Subject-level Prediction

Abstract: Recent years have seen a growing interest in methods for predicting a
variable of interest, such as a subject's diagnosis, from medical images.
Methods based on discriminative modeling excel at making accurate predictions,
but are challenged in their ability to explain their decisions in anatomically
meaningful terms. In this paper, we propose a simple technique for
single-subject prediction that is inherently interpretable. It augments the
generative models used in classical human brain mapping techniques, in which
cause-effect relations can be encoded, with a multivariate noise model that
captures dominant spatial correlations. Experiments demonstrate that the
resulting model can be efficiently inverted to make accurate subject-level
predictions, while at the same time offering intuitive causal explanations of
its inner workings. The method is easy to use: training is fast for typical
training set sizes, and only a single hyperparameter needs to be set by the
user. Our code is available at
this https URL


------------------------------------------------------------------------------

Title:
Overcoming the order barrier two in splitting methods when applied to  semilinear parabolic problems with non-periodic boundary conditions

Abstract: In general, high order splitting methods suffer from an order reduction
phenomena when applied to the time integration of partial differential
equations with non-periodic boundary conditions. In the last decade, there were
introduced several modifications to prevent the second order Strang Splitting
method from such a phenomena. In this article, inspired by these recent
corrector techniques, we introduce a splitting method of order three for a
class of semilinear parabolic problems that avoids order reduction in the
context of non-periodic boundary conditions. We give a proof for the third
order convergence of the method in a simplified linear setting and confirm the
result by numerical experiments. Moreover, we show numerically that the result
also persists with a nonlinear source term.


------------------------------------------------------------------------------

Title:
FSAR: Federated Skeleton-based Action Recognition with Adaptive Topology  Structure and Knowledge Distillation

Abstract: Existing skeleton-based action recognition methods typically follow a
centralized learning paradigm, which can pose privacy concerns when exposing
human-related videos. Federated Learning (FL) has attracted much attention due
to its outstanding advantages in privacy-preserving. However, directly applying
FL approaches to skeleton videos suffers from unstable training. In this paper,
we investigate and discover that the heterogeneous human topology graph
structure is the crucial factor hindering training stability. To address this
limitation, we pioneer a novel Federated Skeleton-based Action Recognition
(FSAR) paradigm, which enables the construction of a globally generalized model
without accessing local sensitive data. Specifically, we introduce an Adaptive
Topology Structure (ATS), separating generalization and personalization by
learning a domain-invariant topology shared across clients and a
domain-specific topology decoupled from global model aggregation.Furthermore,
we explore Multi-grain Knowledge Distillation (MKD) to mitigate the discrepancy
between clients and server caused by distinct updating patterns through
aligning shallow block-wise motion features. Extensive experiments on multiple
datasets demonstrate that FSAR outperforms state-of-the-art FL-based methods
while inherently protecting privacy.


------------------------------------------------------------------------------

Title:
Multigrid preconditioning for regularized least-squares problems

Abstract: In this paper, we are concerned with efficiently solving the sequences of
regularized linear least squares problems associated with employing
Tikhonov-type regularization with regularization operators designed to enforce
edge recovery. An optimal regularization parameter, which balances the fidelity
to the data with the edge-enforcing constraint term, is typically not known a
priori. This adds to the total number of regularized linear least squares
problems that must be solved before the final image can be recovered.
Therefore, in this paper, we determine effective multigrid preconditioners for
these sequences of systems. We focus our approach on the sequences that arise
as a result of the edge-preserving method introduced in [6], where we can
exploit an interpretation of the regularization term as a diffusion operator;
however, our methods are also applicable in other edge-preserving settings,
such as iteratively reweighted least squares problems. Particular attention is
paid to the selection of components of the multigrid preconditioner in order to
achieve robustness for different ranges of the regularization parameter value.
In addition, we present a parameter culling approach that, when used with the
L-curve heuristic, reduces the total number of solves required. We demonstrate
our preconditioning and parameter culling routines on examples in computed
tomography and image deblurring.


------------------------------------------------------------------------------

Title:
JiuZhang 2.0: A Unified Chinese Pre-trained Language Model for  Multi-task Mathematical Problem Solving

Abstract: Although pre-trained language models~(PLMs) have recently advanced the
research progress in mathematical reasoning, they are not specially designed as
a capable multi-task solver, suffering from high cost for multi-task deployment
(\eg a model copy for a task) and inferior performance on complex mathematical
problems in practical applications. To address these issues, in this paper, we
propose \textbf{JiuZhang~2.0}, a unified Chinese PLM specially for multi-task
mathematical problem solving. Our idea is to maintain a moderate-sized model
and employ the \emph{cross-task knowledge sharing} to improve the model
capacity in a multi-task setting. Specially, we construct a
Mixture-of-Experts~(MoE) architecture for modeling mathematical text, so as to
capture the common mathematical knowledge across tasks. For optimizing the MoE
architecture, we design \emph{multi-task continual pre-training} and
\emph{multi-task fine-tuning} strategies for multi-task adaptation. These
training strategies can effectively decompose the knowledge from the task data
and establish the cross-task sharing via expert networks. In order to further
improve the general capacity of solving different complex tasks, we leverage
large language models~(LLMs) as complementary models to iteratively refine the
generated solution by our PLM, via in-context learning. Extensive experiments
have demonstrated the effectiveness of our model.


------------------------------------------------------------------------------

Title:
Cyber Key Terrain Identification Using Adjusted PageRank Centrality

Abstract: The cyber terrain contains devices, network services, cyber personas, and
other network entities involved in network operations. Designing a method that
automatically identifies key network entities to network operations is
challenging. However, such a method is essential for determining which cyber
assets should the cyber defense focus on. In this paper, we propose an approach
for the classification of IP addresses belonging to cyber key terrain according
to their network position using the PageRank centrality computation adjusted by
machine learning. We used hill climbing and random walk algorithms to
distinguish PageRank's damping factors based on source and destination ports
captured in IP flows. The one-time learning phase on a static data sample
allows near-real-time stream-based classification of key hosts from IP flow
data in operational conditions without maintaining a complete network graph. We
evaluated the approach on a dataset from a cyber defense exercise and on data
from the campus network. The results show that cyber key terrain identification
using the adjusted computation of centrality is more precise than its original
version.


------------------------------------------------------------------------------

Title:
Eigenpatches -- Adversarial Patches from Principal Components

Abstract: Adversarial patches are still a simple yet powerful white box attack that can
be used to fool object detectors by suppressing possible detections. The
patches of these so-called evasion attacks are computational expensive to
produce and require full access to the attacked detector. This paper addresses
the problem of computational expensiveness by analyzing 375 generated patches,
calculating the principal components of these and show, that linear
combinations of the resulting "eigenpatches" can be used to fool object
detections successfully.


------------------------------------------------------------------------------

Title:
Underactuated Two Stage CubeSat Control Law

Abstract: This paper details the implementation of a two stage underactuated control
law for a CubeSat to reduce all angular rates to zero. This CubeSat has a
diagonal inertia matrix where the principal axes of inertia are not equal. This
is needed for momentum transfer between axes as shown by previous work in this
area. The momentum transfer will provide control to the uncontrolled axis to
reduce all angular rates of a CubeSat to zero. The underactuated control
utilizes actuators on only two axes but still reduces all three angular rates
to zero. The first stage utilizes one axis to control the uncontrolled axis to
zero and then the second stage reduces the other two axes to zero. This control
law is similar to other piece wise control laws but the difference lies in only
using one axis to control the third axis. The control mechanism is an
integrated two axis propulsion system. The control law is derived to show its
viability and a fully non-linear six degree of freedom simulation tool is
utilized to verify the derivation. The simulation tool is developed by the
Facility of Aerospace Systems and Technology. Simulation results are shown for
a 1U, 2U, and 6U CubeSat with three and two axis control. The two stage
underactuated control law is compared to a proportional controller as well as a
feedback linearization control law for comparison.


------------------------------------------------------------------------------

Title:
Fairness-aware Message Passing for Graph Neural Networks

Abstract: Graph Neural Networks (GNNs) have shown great power in various domains.
However, their predictions may inherit societal biases on sensitive attributes,
limiting their adoption in real-world applications. Although many efforts have
been taken for fair GNNs, most existing works just adopt widely used fairness
techniques in machine learning to graph domains and ignore or don't have a
thorough understanding of the message passing mechanism with fairness
constraints, which is a distinctive feature of GNNs. To fill the gap, we
propose a novel fairness-aware message passing framework GMMD, which is derived
from an optimization problem that considers both graph smoothness and
representation fairness. GMMD can be intuitively interpreted as encouraging a
node to aggregate representations of other nodes from different sensitive
groups while subtracting representations of other nodes from the same sensitive
group, resulting in fair representations. We also provide a theoretical
analysis to justify that GMMD can guarantee fairness, which leads to a simpler
and theory-guided variant GMMD-S. Extensive experiments on graph benchmarks
show that our proposed framework can significantly improve the fairness of
various backbone GNN models while maintaining high accuracy.


------------------------------------------------------------------------------

Title:
Simple and Fast Group Robustness by Automatic Feature Reweighting

Abstract: A major challenge to out-of-distribution generalization is reliance on
spurious features -- patterns that are predictive of the class label in the
training data distribution, but not causally related to the target. Standard
methods for reducing the reliance on spurious features typically assume that we
know what the spurious feature is, which is rarely true in the real world.
Methods that attempt to alleviate this limitation are complex, hard to tune,
and lead to a significant computational overhead compared to standard training.
In this paper, we propose Automatic Feature Reweighting (AFR), an extremely
simple and fast method for updating the model to reduce the reliance on
spurious features. AFR retrains the last layer of a standard ERM-trained base
model with a weighted loss that emphasizes the examples where the ERM model
predicts poorly, automatically upweighting the minority group without group
labels. With this simple procedure, we improve upon the best reported results
among competing methods trained without spurious attributes on several vision
and natural language classification benchmarks, using only a fraction of their
compute.


------------------------------------------------------------------------------

Title:
Evaluating the Zero-shot Robustness of Instruction-tuned Language Models

Abstract: Instruction fine-tuning has recently emerged as a promising approach for
improving the zero-shot capabilities of Large Language Models (LLMs) on new
tasks. This technique has shown particular strength in improving the
performance of modestly sized LLMs, sometimes inducing performance competitive
with much larger model variants. In this paper we ask two questions: (1) How
sensitive are instruction-tuned models to the particular phrasings of
instructions, and, (2) How can we make them more robust to such natural
language variation? To answer the former, we collect a set of 319 instructions
manually written by NLP practitioners for over 80 unique tasks included in
widely used benchmarks, and we evaluate the variance and average performance of
these instructions as compared to instruction phrasings observed during
instruction fine-tuning. We find that using novel (unobserved) but appropriate
instruction phrasings consistently degrades model performance, sometimes
substantially so. Further, such natural instructions yield a wide variance in
downstream performance, despite their semantic equivalence. Put another way,
instruction-tuned models are not especially robust to instruction re-phrasings.
We propose a simple method to mitigate this issue by introducing ``soft
prompt'' embedding parameters and optimizing these to maximize the similarity
between representations of semantically equivalent instructions. We show that
this method consistently improves the robustness of instruction-tuned models.


------------------------------------------------------------------------------

Title:
A lunar reconnaissance drone for cooperative exploration and  high-resolution mapping of extreme locations

Abstract: An efficient characterization of scientifically significant locations is
essential prior to the return of humans to the Moon. The highest resolution
imagery acquired from orbit of south-polar shadowed regions and other relevant
locations remains, at best, an order of magnitude larger than the
characteristic length of most of the robotic systems to be deployed. This
hinders the planning and successful implementation of prospecting missions and
poses a high risk for the traverse of robots and humans, diminishing the
potential overall scientific and commercial return of any mission. We herein
present the design of a lightweight, compact, autonomous, and reusable lunar
reconnaissance drone capable of assisting other ground-based robotic assets,
and eventually humans, in the characterization and high-resolution mapping
(~0.1 m/px) of particularly challenging and hard-to-access locations on the
lunar surface. The proposed concept consists of two main subsystems: the drone
and its service station. With a total combined wet mass of 100 kg, the system
is capable of 11 flights without refueling the service station, enabling almost
9 km of accumulated flight distance. The deployment of such a system could
significantly impact the efficiency of upcoming exploration missions,
increasing the distance covered per day of exploration and significantly
reducing the need for recurrent contacts with ground stations on Earth.


------------------------------------------------------------------------------

Title:
BioREx: Improving Biomedical Relation Extraction by Leveraging  Heterogeneous Datasets

Abstract: Biomedical relation extraction (RE) is the task of automatically identifying
and characterizing relations between biomedical concepts from free text. RE is
a central task in biomedical natural language processing (NLP) research and
plays a critical role in many downstream applications, such as literature-based
discovery and knowledge graph construction. State-of-the-art methods were used
primarily to train machine learning models on individual RE datasets, such as
protein-protein interaction and chemical-induced disease relation. Manual
dataset annotation, however, is highly expensive and time-consuming, as it
requires domain knowledge. Existing RE datasets are usually domain-specific or
small, which limits the development of generalized and high-performing RE
models. In this work, we present a novel framework for systematically
addressing the data heterogeneity of individual datasets and combining them
into a large dataset. Based on the framework and dataset, we report on BioREx,
a data-centric approach for extracting relations. Our evaluation shows that
BioREx achieves significantly higher performance than the benchmark system
trained on the individual dataset, setting a new SOTA from 74.4% to 79.6% in
F-1 measure on the recently released BioRED corpus. We further demonstrate that
the combined dataset can improve performance for five different RE tasks. In
addition, we show that on average BioREx compares favorably to current
best-performing methods such as transfer learning and multi-task learning.
Finally, we demonstrate BioREx's robustness and generalizability in two
independent RE tasks not previously seen in training data: drug-drug N-ary
combination and document-level gene-disease RE. The integrated dataset and
optimized method have been packaged as a stand-alone tool available at
this https URL


------------------------------------------------------------------------------

Title:
OpenP5: Benchmarking Foundation Models for Recommendation

Abstract: This paper presents OpenP5, an open-source library for benchmarking
foundation models for recommendation under the Pre-train, Personalized Prompt
and Predict Paradigm (P5). We consider the implementation of P5 on three
dimensions: 1) downstream task, 2) recommendation dataset, and 3) item indexing
method. For 1), we provide implementation over two downstream tasks: sequential
recommendation and straightforward recommendation. For 2), we surveyed
frequently used datasets in recommender system research in recent years and
provide implementation on ten datasets. In particular, we provide both
single-dataset implementation and the corresponding checkpoints (P5) and
another Super P5 (SP5) implementation that is pre-trained on all of the
datasets, which supports recommendation across various domains with one model.
For 3), we provide implementation of three item indexing methods to create item
IDs: random indexing, sequential indexing, and collaborative indexing. We also
provide comprehensive evaluation results of the library over the two downstream
tasks, the ten datasets, and the three item indexing methods to facilitate
reproducibility and future research. We open-source the code and the
pre-trained checkpoints of the OpenP5 library at
this https URL


------------------------------------------------------------------------------

Title:
Pipit: Enabling programmatic analysis of parallel execution traces

Abstract: Performance analysis is an important part of the oft-repeated, iterative
process of performance tuning during the development of parallel programs.
Per-process per-thread traces (detailed logs of events with timestamps) enable
in-depth analysis of parallel program execution to identify various kinds of
performance issues. Often times, trace collection tools provide a graphical
tool to analyze the trace output. However, these GUI-based tools only support
specific file formats, are difficult to scale when the data is large, limit
data exploration to the implemented graphical views, and do not support
automated comparisons of two or more datasets. In this paper, we present a
programmatic approach to analyzing parallel execution traces by leveraging
pandas, a powerful Python-based data analysis library. We have developed a
Python library, Pipit, on top of pandas that can read traces in different file
formats (OTF2, HPCToolkit, Projections, Nsight, etc.) and provide a uniform
data structure in the form of a pandas DataFrame. Pipit provides operations to
aggregate, filter, and transform the events in a trace to present the data in
different ways. We also provide several functions to quickly identify
performance issues in parallel executions.


------------------------------------------------------------------------------

Title:
Understanding the Challenges of Deploying Live-Traceability Solutions

Abstract: Software traceability is the process of establishing and maintaining
relationships between artifacts in a software system. This process is crucial
to many engineering processes, particularly for safety critical projects;
however, it is labor-intensive and error-prone. Automated traceability has been
a long awaited tool for project managers of these systems, and due to the
semantic similarities between linked artifacts, NLP techniques, such as
transformer models, may be leveraged to accomplish this task. SAFA.ai is a
startup focusing on fine-tuning project-specific models that deliver automated
traceability in a near real-time environment. The following paper describes the
challenges that characterize commercializing software traceability and
highlights possible future directions.


------------------------------------------------------------------------------

Title:
Temporal Data Meets LLM -- Explainable Financial Time Series Forecasting

Abstract: This paper presents a novel study on harnessing Large Language Models' (LLMs)
outstanding knowledge and reasoning abilities for explainable financial time
series forecasting. The application of machine learning models to financial
time series comes with several challenges, including the difficulty in
cross-sequence reasoning and inference, the hurdle of incorporating multi-modal
signals from historical news, financial knowledge graphs, etc., and the issue
of interpreting and explaining the model results. In this paper, we focus on
NASDAQ-100 stocks, making use of publicly accessible historical stock price
data, company metadata, and historical economic/financial news. We conduct
experiments to illustrate the potential of LLMs in offering a unified solution
to the aforementioned challenges. Our experiments include trying
zero-shot/few-shot inference with GPT-4 and instruction-based fine-tuning with
a public LLM model Open LLaMA. We demonstrate our approach outperforms a few
baselines, including the widely applied classic ARMA-GARCH model and a
gradient-boosting tree model. Through the performance comparison results and a
few examples, we find LLMs can make a well-thought decision by reasoning over
information from both textual news and price time series and extracting
insights, leveraging cross-sequence information, and utilizing the inherent
knowledge embedded within the LLM. Additionally, we show that a publicly
available LLM such as Open-LLaMA, after fine-tuning, can comprehend the
instruction to generate explainable forecasts and achieve reasonable
performance, albeit relatively inferior in comparison to GPT-4.


------------------------------------------------------------------------------

Title:
AVOIDDS: Aircraft Vision-based Intruder Detection Dataset and Simulator

Abstract: Designing robust machine learning systems remains an open problem, and there
is a need for benchmark problems that cover both environmental changes and
evaluation on a downstream task. In this work, we introduce AVOIDDS, a
realistic object detection benchmark for the vision-based aircraft
detect-and-avoid problem. We provide a labeled dataset consisting of 72,000
photorealistic images of intruder aircraft with various lighting conditions,
weather conditions, relative geometries, and geographic locations. We also
provide an interface that evaluates trained models on slices of this dataset to
identify changes in performance with respect to changing environmental
conditions. Finally, we implement a fully-integrated, closed-loop simulator of
the vision-based detect-and-avoid problem to evaluate trained models with
respect to the downstream collision avoidance task. This benchmark will enable
further research in the design of robust machine learning systems for use in
safety-critical applications. The AVOIDDS dataset and code are publicly
available at
$\href{this https URL}{purl.stanford.edu/hj293cv5980}$
and
$\href{this https URL}{github.com/sisl/VisionBasedAircraftDAA}$,
respectively.


------------------------------------------------------------------------------

Title:
DFlow: Efficient Dataflow-based Invocation Workflow Execution for  Function-as-a-Service

Abstract: The Serverless Computing is becoming increasingly popular due to its ease of
use and fine-grained billing. These features make it appealing for stateful
application or serverless workflow. However, current serverless workflow
systems utilize a controlflow-based invocation pattern to invoke functions. In
this execution pattern, the function invocation depends on the state of the
function. A function can only begin executing once all its precursor functions
have completed. As a result, this pattern may potentially lead to longer
end-to-end execution time. We design and implement the DFlow, a novel
dataflow-based serverless workflow system that achieves high performance for
serverless workflow. DFlow introduces a distributed scheduler (DScheduler) by
using the dataflow-based invocation pattern to invoke functions. In this
pattern, the function invocation depends on the data dependency between
functions. The function can start to execute even its precursor functions are
still running. DFlow further features a distributed store (DStore) that
utilizes effective fine-grained optimization techniques to eliminate function
interaction, thereby enabling efficient data exchange. With the support of
DScheduler and DStore, DFlow can achieving an average improvement of 60% over
CFlow, 40% over FaaSFlow, 25% over FaasFlowRedis, and 40% over KNIX on 99%-ile
latency respectively. Further, it can improve network bandwidth utilization by
2x-4x over CFlow and 1.5x-3x over FaaSFlow, FaaSFlowRedis and KNIX,
respectively. DFlow effectively reduces the cold startup latency, achieving an
average improvement of 5.6x over CFlow and 1.1x over FaaSFlow


------------------------------------------------------------------------------

Title:
A Framework for Fast and Stable Representations of Multiparameter  Persistent Homology Decompositions

Abstract: Topological data analysis (TDA) is an area of data science that focuses on
using invariants from algebraic topology to provide multiscale shape
descriptors for geometric data sets such as point clouds. One of the most
important such descriptors is {\em persistent homology}, which encodes the
change in shape as a filtration parameter changes; a typical parameter is the
feature scale. For many data sets, it is useful to simultaneously vary multiple
filtration parameters, for example feature scale and density. While the
theoretical properties of single parameter persistent homology are well
understood, less is known about the multiparameter case. In particular, a
central question is the problem of representing multiparameter persistent
homology by elements of a vector space for integration with standard machine
learning algorithms. Existing approaches to this problem either ignore most of
the multiparameter information to reduce to the one-parameter case or are
heuristic and potentially unstable in the face of noise. In this article, we
introduce a new general representation framework that leverages recent results
on {\em decompositions} of multiparameter persistent homology. This framework
is rich in information, fast to compute, and encompasses previous approaches.
Moreover, we establish theoretical stability guarantees under this framework as
well as efficient algorithms for practical computation, making this framework
an applicable and versatile tool for analyzing geometric and point cloud data.
We validate our stability results and algorithms with numerical experiments
that demonstrate statistical convergence, prediction accuracy, and fast running
times on several real data sets.


------------------------------------------------------------------------------

Title:
RemoteCLIP: A Vision Language Foundation Model for Remote Sensing

Abstract: General-purpose foundation models have become increasingly important in the
field of artificial intelligence. While self-supervised learning (SSL) and
Masked Image Modeling (MIM) have led to promising results in building such
foundation models for remote sensing, these models primarily learn low-level
features, require annotated data for fine-tuning, and not applicable for
retrieval and zero-shot applications due to the lack of language understanding.
In response to these limitations, we propose RemoteCLIP, the first
vision-language foundation model for remote sensing that aims to learn robust
visual features with rich semantics, as well as aligned text embeddings for
seamless downstream application. To address the scarcity of pre-training data,
we leverage data scaling, converting heterogeneous annotations based on
Box-to-Caption (B2C) and Mask-to-Box (M2B) conversion, and further
incorporating UAV imagery, resulting a 12xlarger pretraining dataset.
RemoteCLIP can be applied to a variety of downstream tasks, including zero-shot
image classification, linear probing, k-NN classification, few-shot
classification, image-text retrieval, and object counting. Evaluations on 16
datasets, including a newly introduced RemoteCount benchmark to test the object
counting ability, show that RemoteCLIP consistently outperforms baseline
foundation models across different model scales. Impressively, RemoteCLIP
outperform previous SoTA by 9.14% mean recall on RSICD dataset and by 8.92% on
RSICD dataset. For zero-shot classification, our RemoteCLIP outperform CLIP
baseline by up to 6.39% average accuracy on 12 downstream datasets.


------------------------------------------------------------------------------

Title:
Causal Effect Regularization: Automated Detection and Removal of  Spurious Attributes

Abstract: In many classification datasets, the task labels are spuriously correlated
with some input attributes. Classifiers trained on such datasets often rely on
these attributes for prediction, especially when the spurious correlation is
high, and thus fail to generalize whenever there is a shift in the attributes'
correlation at deployment. If we assume that the spurious attributes are known
a priori, several methods have been proposed to learn a classifier that is
invariant to the specified attributes. However, in real-world data, information
about spurious attributes is typically unavailable. Therefore, we propose a
method to automatically identify spurious attributes by estimating their causal
effect on the label and then use a regularization objective to mitigate the
classifier's reliance on them. Compared to a recent method for identifying
spurious attributes, we find that our method is more accurate in removing the
attribute from the learned model, especially when spurious correlation is high.
Specifically, across synthetic, semi-synthetic, and real-world datasets, our
method shows significant improvement in a metric used to quantify the
dependence of a classifier on spurious attributes ($\Delta$Prob), while
obtaining better or similar accuracy. In addition, our method mitigates the
reliance on spurious attributes even under noisy estimation of causal effects.
To explain the empirical robustness of our method, we create a simple linear
classification task with two sets of attributes: causal and spurious. We prove
that our method only requires that the ranking of estimated causal effects is
correct across attributes to select the correct classifier.


------------------------------------------------------------------------------

Title:
Static and Dynamic Jamming Games Over Wireless Channels With Mobile  Strategic Players

Abstract: We study a wireless jamming problem consisting of the competition between a
legitimate receiver and a jammer, as a zero-sum game with the value to
maximize/minimize being the channel capacity at the receiver's side. Most of
the approaches found in the literature consider the two players to be
stationary nodes. Instead, we investigate what happens when they can change
location, specifically moving along a linear geometry. We frame this at first
as a static game, which can be solved in closed form, and subsequently we
extend it to a dynamic game, under three different versions for what concerns
completeness/perfection of mutual information about the adversary's position,
corresponding to different assumptions of concealment/sequentiality of the
moves, respectively. We first provide some theoretical conditions that hold for
the static game and also help identify good strategies valid under any setup,
including dynamic games. Since dynamic games, although more realistic, are
characterized by an exploding strategy space, we exploit reinforcement learning
to obtain efficient strategies leading to equilibrium outcomes. We show how
theoretical findings can be used to train smart agents to play the game, and
validate our approach in practical setups.


------------------------------------------------------------------------------

Title:
Efficient Generalized Temporal Pattern Mining in Big Time Series Using  Mutual Information

Abstract: Big time series are increasingly available from an ever wider range of
IoT-enabled sensors deployed in various environments. Significant insights can
be gained by mining temporal patterns from these time series. Temporal pattern
mining (TPM) extends traditional pattern mining by adding event time intervals
into extracted patterns, making them more expressive at the expense of
increased time and space complexities. Besides frequent temporal patterns
(FTPs), which occur frequently in the entire dataset, another useful type of
temporal patterns are so-called rare temporal patterns (RTPs), which appear
rarely but with high confidence. Mining rare temporal patterns yields
additional challenges. For FTP mining, the temporal information and complex
relations between events already create an exponential search space. For RTP
mining, the support measure is set very low, leading to a further combinatorial
explosion and potentially producing too many uninteresting patterns. Thus,
there is a need for a generalized approach which can mine both frequent and
rare temporal patterns. This paper presents our Generalized Temporal Pattern
Mining from Time Series (GTPMfTS) approach with the following specific
contributions: (1) The end-to-end GTPMfTS process taking time series as input
and producing frequent/rare temporal patterns as output. (2) The efficient
Generalized Temporal Pattern Mining (GTPM) algorithm mines frequent and rare
temporal patterns using efficient data structures for fast retrieval of events
and patterns during the mining process, and employs effective pruning
techniques for significantly faster mining. (3) An approximate version of GTPM
that uses mutual information, a measure of data correlation, to prune
unpromising time series from the search space.


------------------------------------------------------------------------------

Title:
Performance and Reliability Analysis for Practical Byzantine Fault  Tolerance with Repairable Voting Nodes

Abstract: The practical Byzantine fault tolerant (PBFT) consensus protocol is one of
the basic consensus protocols in the development of blockchain technology. At
the same time, the PBFT consensus protocol forms a basis for some other
important BFT consensus protocols, such as Tendermint, Streamlet, HotStuff, and
LibraBFT. In general, the voting nodes may always fail so that they can leave
the PBFT-based blockchain system in a random time interval, making the number
of timely available voting nodes uncertain. Thus, this uncertainty leads to the
analysis of the PBFT-based blockchain systems with repairable voting nodes
being more challenging. In this paper, we develop a novel PBFT consensus
protocol with repairable voting nodes and study such a new blockchain system
using a multi-dimensional Markov process and the first passage time method.
Based on this, we provide performance and reliability analysis, including
throughput, availability, and reliability, for the new PBFT-based blockchain
system with repairable voting nodes. Furthermore, we provide an approximate
algorithm for computing the throughput of the new PBFT-based blockchain system.
We employ numerical examples to demonstrate the validity of our theoretical
results and illustrate how the key system parameters influence performance
measures of the PBFT-based blockchain system with repairable voting nodes. We
hope the methodology and results developed in this paper will stimulate future
research endeavors and open up new research trajectories in this field.


------------------------------------------------------------------------------

Title:
Graph Self-Supervised Learning for Endoscopic Image Matching

Abstract: Accurate feature matching and correspondence in endoscopic images play a
crucial role in various clinical applications, including patient follow-up and
rapid anomaly localization through panoramic image generation. However,
developing robust and accurate feature matching techniques faces challenges due
to the lack of discriminative texture and significant variability between
patients. To address these limitations, we propose a novel self-supervised
approach that combines Convolutional Neural Networks for capturing local visual
appearance and attention-based Graph Neural Networks for modeling spatial
relationships between key-points. Our approach is trained in a fully
self-supervised scheme without the need for labeled data. Our approach
outperforms state-of-the-art handcrafted and deep learning-based methods,
demonstrating exceptional performance in terms of precision rate (1) and
matching score (99.3%). We also provide code and materials related to this
work, which can be accessed at
this https URL


------------------------------------------------------------------------------

Title:
Multilingual Few-Shot Learning via Language Model Retrieval

Abstract: Transformer-based language models have achieved remarkable success in
few-shot in-context learning and drawn a lot of research interest. However,
these models' performance greatly depends on the choice of the example prompts
and also has high variability depending on how samples are chosen. In this
paper, we conduct a comprehensive study of retrieving semantically similar
few-shot samples and using them as the context, as it helps the model decide
the correct label without any gradient update in the multilingual and
cross-lingual settings. We evaluate the proposed method on five natural
language understanding datasets related to intent detection, question
classification, sentiment analysis, and topic classification. The proposed
method consistently outperforms random sampling in monolingual and
cross-lingual tasks in non-English languages.


------------------------------------------------------------------------------

Title:
ArctyrEX : Accelerated Encrypted Execution of General-Purpose  Applications

Abstract: Fully Homomorphic Encryption (FHE) is a cryptographic method that guarantees
the privacy and security of user data during computation. FHE algorithms can
perform unlimited arithmetic computations directly on encrypted data without
decrypting it. Thus, even when processed by untrusted systems, confidential
data is never exposed. In this work, we develop new techniques for accelerated
encrypted execution and demonstrate the significant performance advantages of
our approach. Our current focus is the Fully Homomorphic Encryption over the
Torus (CGGI) scheme, which is a current state-of-the-art method for evaluating
arbitrary functions in the encrypted domain. CGGI represents a computation as a
graph of homomorphic logic gates and each individual bit of the plaintext is
transformed into a polynomial in the encrypted domain. Arithmetic on such data
becomes very expensive: operations on bits become operations on entire
polynomials. Therefore, evaluating even relatively simple nonlinear functions,
such as a sigmoid, can take thousands of seconds on a single CPU thread. Using
our novel framework for end-to-end accelerated encrypted execution called
ArctyrEX, developers with no knowledge of complex FHE libraries can simply
describe their computation as a C program that is evaluated over $40\times$
faster on an NVIDIA DGX A100 and $6\times$ faster with a single A100 relative
to a 256-threaded CPU baseline.


------------------------------------------------------------------------------

Title:
Benchmarking Robustness of Deep Reinforcement Learning approaches to  Online Portfolio Management

Abstract: Deep Reinforcement Learning approaches to Online Portfolio Selection have
grown in popularity in recent years. The sensitive nature of training
Reinforcement Learning agents implies a need for extensive efforts in market
representation, behavior objectives, and training processes, which have often
been lacking in previous works. We propose a training and evaluation process to
assess the performance of classical DRL algorithms for portfolio management. We
found that most Deep Reinforcement Learning algorithms were not robust, with
strategies generalizing poorly and degrading quickly during backtesting.


------------------------------------------------------------------------------

Title:
Diagonal Waveform and Algorithm to Estimate Range and Velocity in  Multi-Object Scenarios

Abstract: Waveform design for joint communication and sensing (JCAS) is an important
research direction, focusing on providing an optimal tradeoff between
communication and sensing performance. In this paper, we first describe the
conventional grid-type waveform structure and the corresponding two-dimension
(2D)-discrete Fourier transform (DFT) algorithm. We then introduce an emerging
diagonal scheme, including a diagonal waveform structure and corresponding
1D-DFT diagonal algorithm. The diagonal scheme substantially reduces the
signaling overhead and computational complexity compared to the conventional
2D-DFT algorithm while still achieving the same radar performance. But the
previous study of diagonal waveform used a single target to evaluate the
performance of the diagonal scheme. This paper verifies the diagonal waveform
with simulations demonstrating its feasibility in a traffic monitoring scenario
with multiple vehicles.


------------------------------------------------------------------------------

Title:
Semilinear fractional elliptic PDEs with gradient nonlinearities on open  balls: existence of solutions and probabilistic representation

Abstract: We provide sufficient conditions for the existence of viscosity solutions of
fractional semilinear elliptic PDEs of index $\alpha \in (1,2)$ with polynomial
gradient nonlinearities on $d$-dimensional balls, $d\geq 2$. Our approach uses
a tree-based probabilistic representation based on $\alpha$-stable branching
processes, and allows us to take into account gradient nonlinearities not
covered by deterministic finite difference methods so far. Numerical
illustrations demonstrate the accuracy of the method in dimension $d=10$,
solving a challenge encountered with the use of deterministic finite difference
methods in high-dimensional settings.


------------------------------------------------------------------------------

Title:
Attack-Resilient Design for Connected and Automated Vehicles

Abstract: By sharing local sensor information via Vehicle-to-Vehicle (V2V) wireless
communication networks, Cooperative Adaptive Cruise Control (CACC) is a
technology that enables Connected and Automated Vehicles (CAVs) to drive
autonomously on the highway in closely-coupled platoons. The use of CACC
technologies increases safety and the traffic throughput, and decreases fuel
consumption and CO2 emissions. However, CAVs heavily rely on embedded software,
hardware, and communication networks that make them vulnerable to a range of
cyberattacks. Cyberattacks to a particular CAV compromise the entire platoon as
CACC schemes propagate corrupted data to neighboring vehicles potentially
leading to traffic delays and collisions. Physics-based monitors can be used to
detect the presence of False Data Injection (FDI) attacks to CAV sensors;
however, unavoidable system disturbances and modelling uncertainty often
translates to conservative detection results. Given enough system knowledge,
adversaries are still able to launch a range of attacks that can surpass the
detection scheme by hiding within the system disturbances and uncertainty -- we
refer to this class of attacks as \textit{stealthy FDI attacks}. Stealthy
attacks are hard to deal with as they affect the platoon dynamics without being
noticed. In this manuscript, we propose a co-design methodology (built around a
series convex programs) to synthesize distributed attack monitors and
$H_{\infty}$ CACC controllers that minimize the joint effect of stealthy FDI
attacks and system disturbances on the platoon dynamics while guaranteeing a
prescribed platooning performance (in terms of tracking and string stability).
Computer simulations are provided to illustrate the performance of out tools.


------------------------------------------------------------------------------

Title:
Semi-Supervised Learning for hyperspectral images by non parametrically  predicting view assignment

Abstract: Hyperspectral image (HSI) classification is gaining a lot of momentum in
present time because of high inherent spectral information within the images.
However, these images suffer from the problem of curse of dimensionality and
usually require a large number samples for tasks such as classification,
especially in supervised setting. Recently, to effectively train the deep
learning models with minimal labelled samples, the unlabeled samples are also
being leveraged in self-supervised and semi-supervised setting. In this work,
we leverage the idea of semi-supervised learning to assist the discriminative
self-supervised pretraining of the models. The proposed method takes different
augmented views of the unlabeled samples as input and assigns them the same
pseudo-label corresponding to the labelled sample from the downstream task. We
train our model on two HSI datasets, namely Houston dataset (from data fusion
contest, 2013) and Pavia university dataset, and show that the proposed
approach performs better than self-supervised approach and supervised training.


------------------------------------------------------------------------------

Title:
Vocal Timbre Effects with Differentiable Digital Signal Processing

Abstract: We explore two approaches to creatively altering vocal timbre using
Differentiable Digital Signal Processing (DDSP). The first approach is inspired
by classic cross-synthesis techniques. A pretrained DDSP decoder predicts a
filter for a noise source and a harmonic distribution, based on pitch and
loudness information extracted from the vocal input. Before synthesis, the
harmonic distribution is modified by interpolating between the predicted
distribution and the harmonics of the input. We provide a real-time
implementation of this approach in the form of a Neutone model. In the second
approach, autoencoder models are trained on datasets consisting of both vocal
and instrument training data. To apply the effect, the trained autoencoder
attempts to reconstruct the vocal input. We find that there is a desirable
"sweet spot" during training, where the model has learned to reconstruct the
phonetic content of the input vocals, but is still affected by the timbre of
the instrument mixed into the training data. After further training, that
effect disappears. A perceptual evaluation compares the two approaches. We find
that the autoencoder in the second approach is able to reconstruct intelligible
lyrical content without any explicit phonetic information provided during
training.


------------------------------------------------------------------------------

Title:
Knowledge Transfer-Driven Few-Shot Class-Incremental Learning

Abstract: Few-shot class-incremental learning (FSCIL) aims to continually learn new
classes using a few samples while not forgetting the old classes. The key of
this task is effective knowledge transfer from the base session to the
incremental sessions. Despite the advance of existing FSCIL methods, the
proposed knowledge transfer learning schemes are sub-optimal due to the
insufficient optimization for the model's plasticity. To address this issue, we
propose a Random Episode Sampling and Augmentation (RESA) strategy that relies
on diverse pseudo incremental tasks as agents to achieve the knowledge
transfer. Concretely, RESA mimics the real incremental setting and constructs
pseudo incremental tasks globally and locally, where the global pseudo
incremental tasks are designed to coincide with the learning objective of FSCIL
and the local pseudo incremental tasks are designed to improve the model's
plasticity, respectively. Furthermore, to make convincing incremental
predictions, we introduce a complementary model with a squared
Euclidean-distance classifier as the auxiliary module, which couples with the
widely used cosine classifier to form our whole architecture. By such a way,
equipped with model decoupling strategy, we can maintain the model's stability
while enhancing the model's plasticity. Extensive quantitative and qualitative
experiments on three popular FSCIL benchmark datasets demonstrate that our
proposed method, named Knowledge Transfer-driven Relation Complementation
Network (KT-RCNet), outperforms almost all prior methods. More precisely, the
average accuracy of our proposed KT-RCNet outperforms the second-best method by
a margin of 5.26%, 3.49%, and 2.25% on miniImageNet, CIFAR100, and CUB200,
respectively. Our code is available at
this https URL


------------------------------------------------------------------------------

Title:
FDTI: Fine-grained Deep Traffic Inference with Roadnet-enriched Graph

Abstract: This paper proposes the fine-grained traffic prediction task (e.g. interval
between data points is 1 minute), which is essential to traffic-related
downstream applications. Under this setting, traffic flow is highly influenced
by traffic signals and the correlation between traffic nodes is dynamic. As a
result, the traffic data is non-smooth between nodes, and hard to utilize
previous methods which focus on smooth traffic data. To address this problem,
we propose Fine-grained Deep Traffic Inference, termed as FDTI. Specifically,
we construct a fine-grained traffic graph based on traffic signals to model the
inter-road relations. Then, a physically-interpretable dynamic mobility
convolution module is proposed to capture vehicle moving dynamics controlled by
the traffic signals. Furthermore, traffic flow conservation is introduced to
accurately infer future volume. Extensive experiments demonstrate that our
method achieves state-of-the-art performance and learned traffic dynamics with
good properties. To the best of our knowledge, we are the first to conduct the
city-level fine-grained traffic prediction.


------------------------------------------------------------------------------

Title:
Understanding Depth Map Progressively: Adaptive Distance Interval  Separation for Monocular 3d Object Detection

Abstract: Monocular 3D object detection aims to locate objects in different scenes with
just a single image. Due to the absence of depth information, several monocular
3D detection techniques have emerged that rely on auxiliary depth maps from the
depth estimation task. There are multiple approaches to understanding the
representation of depth maps, including treating them as pseudo-LiDAR point
clouds, leveraging implicit end-to-end learning of depth information, or
considering them as an image input. However, these methods have certain
drawbacks, such as their reliance on the accuracy of estimated depth maps and
suboptimal utilization of depth maps due to their image-based nature. While
LiDAR-based methods and convolutional neural networks (CNNs) can be utilized
for pseudo point clouds and depth maps, respectively, it is always an
alternative. In this paper, we propose a framework named the Adaptive Distance
Interval Separation Network (ADISN) that adopts a novel perspective on
understanding depth maps, as a form that lies between LiDAR and images. We
utilize an adaptive separation approach that partitions the depth map into
various subgraphs based on distance and treats each of these subgraphs as an
individual image for feature extraction. After adaptive separations, each
subgraph solely contains pixels within a learned interval range. If there is a
truncated object within this range, an evident curved edge will appear, which
we can leverage for texture extraction using CNNs to obtain rich depth
information in pixels. Meanwhile, to mitigate the inaccuracy of depth
estimation, we designed an uncertainty module. To take advantage of both images
and depth maps, we use different branches to learn localization detection tasks
and appearance tasks separately.


------------------------------------------------------------------------------

Title:
Early Jamming Detection in Mobile Indoor Scenarios via Deep Learning

Abstract: The current state of the art on jamming detection relies on link-layer
metrics. A few examples are the bit-error rate, the packet delivery ratio, the
throughput, and the increase of the signal-to-noise ratio. As a result, these
techniques can only detect jamming ex-post, i.e., once the attack has already
taken down the communication link. These solutions are unfit in mobile
scenarios, e.g., drones, which might lose the link to the remote controller,
being unable to predict the attack.
Our solution is rooted in the idea that a drone flying against a jammed area
is experiencing an increasing effect of the jamming. Therefore, drones might
use this phenomenon to detect jamming early, i.e., before it completely
disrupts the communication link. Such an approach would allow drones and
possibly their pilots to make an informed decision and maintain full control of
the navigation, providing security and safety. In this paper, we propose
Bloodhound+, a solution for early jamming detection on mobile devices. Our
approach analyzes raw physical-layer information (I-Q samples) acquired from
the channel. We assemble this information into grayscale images, and we use
sparse autoencoders to detect image anomalies caused by jamming attacks. To
test our solution against a wide set of configurations, we acquired a large
dataset of indoor measurements using multiple hardware, jamming strategies, and
communication parameters. Our results indicate that Bloodhound+ can detect
indoor jamming up to 20 meters away from the jamming source at the minimum
available relative jamming power, with a minimum accuracy of 99.7%. Our
solution is also robust to various sampling rates adopted by the jammer, as
well as to the type of signal used for jamming.


------------------------------------------------------------------------------

Title:
Toward the Cure of Privacy Policy Reading Phobia: Automated Generation  of Privacy Nutrition Labels From Privacy Policies

Abstract: Software applications have become an omnipresent part of modern society. The
consequent privacy policies of these applications play a significant role in
informing customers how their personal information is collected, stored, and
used. However, customers rarely read and often fail to understand privacy
policies because of the ``Privacy Policy Reading Phobia'' (PPRP). To tackle
this emerging challenge, we propose the first framework that can automatically
generate privacy nutrition labels from privacy policies. Based on our ground
truth applications about the Data Safety Report from the Google Play app store,
our framework achieves a 0.75 F1-score on generating first-party data
collection practices and an average of 0.93 F1-score on general security
practices. We also analyse the inconsistencies between ground truth and curated
privacy nutrition labels on the market, and our framework can detect 90.1%
under-claim issues. Our framework demonstrates decent generalizability across
different privacy nutrition label formats, such as Google's Data Safety Report
and Apple's App Privacy Details.


------------------------------------------------------------------------------

Title:
RaViTT: Random Vision Transformer Tokens

Abstract: Vision Transformers (ViTs) have successfully been applied to image
classification problems where large annotated datasets are available. On the
other hand, when fewer annotations are available, such as in biomedical
applications, image augmentation techniques like introducing image variations
or combinations have been proposed. However, regarding ViT patch sampling, less
has been explored outside grid-based strategies. In this work, we propose
Random Vision Transformer Tokens (RaViTT), a random patch sampling strategy
that can be incorporated into existing ViTs. We experimentally evaluated RaViTT
for image classification, comparing it with a baseline ViT and state-of-the-art
(SOTA) augmentation techniques in 4 datasets, including ImageNet-1k and
CIFAR-100. Results show that RaViTT increases the accuracy of the baseline in
all datasets and outperforms the SOTA augmentation techniques in 3 out of 4
datasets by a significant margin +1.23% to +4.32%. Interestingly, RaViTT
accuracy improvements can be achieved even with fewer tokens, thus reducing the
computational load of any ViT model for a given accuracy value.


------------------------------------------------------------------------------

Title:
Dual-Gated Fusion with Prefix-Tuning for Multi-Modal Relation Extraction

Abstract: Multi-Modal Relation Extraction (MMRE) aims at identifying the relation
between two entities in texts that contain visual clues. Rich visual content is
valuable for the MMRE task, but existing works cannot well model finer
associations among different modalities, failing to capture the truly helpful
visual information and thus limiting relation extraction performance. In this
paper, we propose a novel MMRE framework to better capture the deeper
correlations of text, entity pair, and image/objects, so as to mine more
helpful information for the task, termed as DGF-PT. We first propose a
prompt-based autoregressive encoder, which builds the associations of
intra-modal and inter-modal features related to the task, respectively by
entity-oriented and object-oriented prefixes. To better integrate helpful
visual information, we design a dual-gated fusion module to distinguish the
importance of image/objects and further enrich text representations. In
addition, a generative decoder is introduced with entity type restriction on
relations, better filtering out candidates. Extensive experiments conducted on
the benchmark dataset show that our approach achieves excellent performance
compared to strong competitors, even in the few-shot situation.


------------------------------------------------------------------------------

Title:
Scaling of Class-wise Training Losses for Post-hoc Calibration

Abstract: The class-wise training losses often diverge as a result of the various
levels of intra-class and inter-class appearance variation, and we find that
the diverging class-wise training losses cause the uncalibrated prediction with
its reliability. To resolve the issue, we propose a new calibration method to
synchronize the class-wise training losses. We design a new training loss to
alleviate the variance of class-wise training losses by using multiple
class-wise scaling factors. Since our framework can compensate the training
losses of overfitted classes with those of under-fitted classes, the integrated
training loss is preserved, preventing the performance drop even after the
model calibration. Furthermore, our method can be easily employed in the
post-hoc calibration methods, allowing us to use the pre-trained model as an
initial model and reduce the additional computation for model calibration. We
validate the proposed framework by employing it in the various post-hoc
calibration methods, which generally improves calibration performance while
preserving accuracy, and discover through the investigation that our approach
performs well with unbalanced datasets and untuned hyperparameters.


------------------------------------------------------------------------------

Title:
Explaining the Model and Feature Dependencies by Decomposition of the  Shapley Value

Abstract: Shapley values have become one of the go-to methods to explain complex models
to end-users. They provide a model agnostic post-hoc explanation with
foundations in game theory: what is the worth of a player (in machine learning,
a feature value) in the objective function (the output of the complex machine
learning model). One downside is that they always require outputs of the model
when some features are missing. These are usually computed by taking the
expectation over the missing features. This however introduces a non-trivial
choice: do we condition on the unknown features or not? In this paper we
examine this question and claim that they represent two different explanations
which are valid for different end-users: one that explains the model and one
that explains the model combined with the feature dependencies in the data. We
propose a new algorithmic approach to combine both explanations, removing the
burden of choice and enhancing the explanatory power of Shapley values, and
show that it achieves intuitive results on simple problems. We apply our method
to two real-world datasets and discuss the explanations. Finally, we
demonstrate how our method is either equivalent or superior to state-to-of-art
Shapley value implementations while simultaneously allowing for increased
insight into the model-data structure.


------------------------------------------------------------------------------

Title:
Transformer Training Strategies for Forecasting Multiple Load Time  Series

Abstract: Recent work uses Transformers for load forecasting, which are the state of
the art for sequence modeling tasks in data-rich domains. In the smart grid of
the future, accurate load forecasts must be provided on the level of individual
clients of an energy supplier. While the total amount of electrical load data
available to an energy supplier will increase with the ongoing smart meter
rollout, the amount of data per client will always be limited. We test whether
the Transformer benefits from a transfer learning strategy, where a global
model is trained on the load time series data from multiple clients. We find
that the global model is superior to two other training strategies commonly
used in related work: multivariate models and local models. A comparison to
linear models and multi-layer perceptrons shows that Transformers are effective
for electrical load forecasting when they are trained with the right strategy.


------------------------------------------------------------------------------

Title:
Tourist Attractions Recommendation based on Attention Knowledge Graph  Convolution Network

Abstract: The recommendation algorithm based on knowledge graphs is at a relatively
mature stage. However, there are still some problems in the recommendation of
specific areas. For example, in the tourism field, selecting suitable tourist
attraction attributes process is complicated as the recommendation basis for
tourist attractions. In this paper, we propose the improved Attention Knowledge
Graph Convolution Network model, named (Att-KGCN), which automatically
discovers the neighboring entities of the target scenic spot semantically. The
attention layer aggregates relatively similar locations and represents them
with an adjacent vector. Then, according to the tourist's preferred choices,
the model predicts the probability of similar spots as a recommendation system.
A knowledge graph dataset of tourist attractions used based on tourism data on
Socotra Island-Yemen. Through experiments, it is verified that the Attention
Knowledge Graph Convolution Network has a good effect on the recommendation of
tourist attractions and can make more recommendations for tourists' choices.


------------------------------------------------------------------------------

Title:
Enjeux de communication dans la multirepr{é}sentation cartographique  reproductible

Abstract: This chapter deepens cartographic communication through a cartographic
multirepresentation exercise. Using a single dataset on World population data,
the chapter presents a series of 13 different maps to illustrate how mapping is
primarily a matter of choices and methods.


------------------------------------------------------------------------------

Title:
MotionGPT: Finetuned LLMs are General-Purpose Motion Generators

Abstract: Generating realistic human motion from given action descriptions has
experienced significant advancements because of the emerging requirement of
digital humans. While recent works have achieved impressive results in
generating motion directly from textual action descriptions, they often support
only a single modality of the control signal, which limits their application in
the real digital human industry. This paper presents a Motion General-Purpose
generaTor (MotionGPT) that can use multimodal control signals, e.g., text and
single-frame poses, for generating consecutive human motions by treating
multimodal signals as special input tokens in large language models (LLMs).
Specifically, we first quantize multimodal control signals into discrete codes
and then formulate them in a unified prompt instruction to ask the LLMs to
generate the motion answer. Our MotionGPT demonstrates a unified human motion
generation model with multimodal control signals by tuning a mere 0.4% of LLM
parameters. To the best of our knowledge, MotionGPT is the first method to
generate human motion by multimodal control signals, which we hope can shed
light on this new direction. Codes shall be released upon acceptance.


------------------------------------------------------------------------------

Title:
Towards Open-World Recommendation with Knowledge Augmentation from Large  Language Models

Abstract: Recommender systems play a vital role in various online services. However,
the insulated nature of training and deploying separately within a specific
domain limits their access to open-world knowledge. Recently, the emergence of
large language models (LLMs) has shown promise in bridging this gap by encoding
extensive world knowledge and demonstrating reasoning capability. Nevertheless,
previous attempts to directly use LLMs as recommenders have not achieved
satisfactory results. In this work, we propose an Open-World Knowledge
Augmented Recommendation Framework with Large Language Models, dubbed KAR, to
acquire two types of external knowledge from LLMs -- the reasoning knowledge on
user preferences and the factual knowledge on items. We introduce factorization
prompting to elicit accurate reasoning on user preferences. The generated
reasoning and factual knowledge are effectively transformed and condensed into
augmented vectors by a hybrid-expert adaptor in order to be compatible with the
recommendation task. The obtained vectors can then be directly used to enhance
the performance of any recommendation model. We also ensure efficient inference
by preprocessing and prestoring the knowledge from the LLM. Extensive
experiments show that KAR significantly outperforms the state-of-the-art
baselines and is compatible with a wide range of recommendation algorithms.


------------------------------------------------------------------------------

Title:
Understanding Generalization in the Interpolation Regime using the Rate  Function

Abstract: In this paper, we present a novel characterization of the smoothness of a
model based on basic principles of Large Deviation Theory. In contrast to prior
work, where the smoothness of a model is normally characterized by a real value
(e.g., the weights' norm), we show that smoothness can be described by a simple
real-valued function. Based on this concept of smoothness, we propose an
unifying theoretical explanation of why some interpolators generalize
remarkably well and why a wide range of modern learning techniques (i.e.,
stochastic gradient descent, $\ell_2$-norm regularization, data augmentation,
invariant architectures, and overparameterization) are able to find them. The
emergent conclusion is that all these methods provide complimentary procedures
that bias the optimizer to smoother interpolators, which, according to this
theoretical analysis, are the ones with better generalization error.


------------------------------------------------------------------------------

Title:
Optimal Pure Strategies for a Discrete Search Game

Abstract: Consider a two-person zero-sum search game between a Hider and a Searcher.
The Hider chooses to hide in one of $n$ discrete locations (or "boxes") and the
Searcher chooses a search sequence specifying which order to look in these
boxes until finding the Hider. A search at box $i$ takes $t_i$ time units and
finds the Hider - if hidden there - independently with probability $q_i$, for
$i=1,\ldots,n$. The Searcher wants to minimize the expected total time needed
to find the Hider, while the Hider wants to maximize it. It is shown in the
literature that the Searcher has an optimal search strategy that mixes up to
$n$ distinct search sequences with appropriate probabilities. This paper
investigates the existence of optimal pure strategies for the Searcher - a
single deterministic search sequence that achieves the optimal expected total
search time regardless of where the Hider hides. We identify several cases in
which the Searcher has an optimal pure strategy, and several cases in which
such optimal pure strategy does not exist. An optimal pure search strategy has
significant practical value because the Searcher does not need to randomize
their actions and will avoid second guessing themselves if the chosen search
sequence from an optimal mixed strategy does not turn out well.


------------------------------------------------------------------------------

Title:
Invariant Descriptors of Motion and Force Trajectories for Interpreting  Object Manipulation Tasks in Contact

Abstract: Invariant descriptors of point and rigid-body motion trajectories have been
proposed in the past as representative task models for motion recognition and
generalization. Currently, no invariant descriptor exists for representing
force trajectories which appear in contact tasks. This paper introduces
invariant descriptors for force trajectories by exploiting the duality between
motion and force. Two types of invariant descriptors are presented depending on
whether the trajectories consist of screw coordinates or vector coordinates.
Methods and software are provided for robustly calculating the invariant
descriptors from noisy measurements using optimal control. Using experimental
human demonstrations of a 3D contour following task, invariant descriptors are
shown to result in task representations that do not depend on the calibration
of reference frames or sensor locations. Tuning of the optimal control problems
is shown to be fast and intuitive. Similarly as for motions in free space, the
proposed invariant descriptors for motion and force trajectories may prove
useful for the recognition and generalization of constrained motions such as
during object manipulation in contact.


------------------------------------------------------------------------------

Title:
Fairness Index Measures to Evaluate Bias in Biometric Recognition

Abstract: The demographic disparity of biometric systems has led to serious concerns
regarding their societal impact as well as applicability of such systems in
private and public domains. A quantitative evaluation of demographic fairness
is an important step towards understanding, assessment, and mitigation of
demographic bias in biometric applications. While few, existing fairness
measures are based on post-decision data (such as verification accuracy) of
biometric systems, we discuss how pre-decision data (score distributions)
provide useful insights towards demographic fairness. In this paper, we
introduce multiple measures, based on the statistical characteristics of score
distributions, for the evaluation of demographic fairness of a generic
biometric verification system. We also propose different variants for each
fairness measure depending on how the contribution from constituent demographic
groups needs to be combined towards the final measure. In each case, the
behavior of the measure has been illustrated numerically and graphically on
synthetic data. The demographic imbalance in benchmarking datasets is often
overlooked during fairness assessment. We provide a novel weighing strategy to
reduce the effect of such imbalance through a non-linear function of sample
sizes of demographic groups. The proposed measures are independent of the
biometric modality, and thus, applicable across commonly used biometric
modalities (e.g., face, fingerprint, etc.).


------------------------------------------------------------------------------

Title:
Road Barlow Twins: Redundancy Reduction for Road Environment Descriptors  and Motion Prediction

Abstract: Anticipating the future motion of traffic agents is vital for self-driving
vehicles to ensure their safe operation. We introduce a novel self-supervised
pre-training method as well as a transformer model for motion prediction. Our
method is based on Barlow Twins and applies the redundancy reduction principle
to embeddings generated from HD maps. Additionally, we introduce a novel
approach for redundancy reduction, where a potentially large and variable set
of road environment tokens is transformed into a fixed-size set of road
environment descriptors (RED). Our experiments reveal that the proposed
pre-training method can improve minADE and minFDE by 12% and 15% and outperform
contrastive learning with PreTraM and SimCLR in a semi-supervised setting. Our
REDMotion model achieves results that are competitive with those of recent
related methods such as MultiPath++ or Scene Transformer. Code is available at:
this https URL


------------------------------------------------------------------------------

Title:
TeleViT: Teleconnection-driven Transformers Improve Subseasonal to  Seasonal Wildfire Forecasting

Abstract: Wildfires are increasingly exacerbated as a result of climate change,
necessitating advanced proactive measures for effective mitigation. It is
important to forecast wildfires weeks and months in advance to plan forest fuel
management, resource procurement and allocation. To achieve such accurate
long-term forecasts at a global scale, it is crucial to employ models that
account for the Earth system's inherent spatio-temporal interactions, such as
memory effects and teleconnections. We propose a teleconnection-driven vision
transformer (TeleViT), capable of treating the Earth as one interconnected
system, integrating fine-grained local-scale inputs with global-scale inputs,
such as climate indices and coarse-grained global variables. Through
comprehensive experimentation, we demonstrate the superiority of TeleViT in
accurately predicting global burned area patterns for various forecasting
windows, up to four months in advance. The gain is especially pronounced in
larger forecasting windows, demonstrating the improved ability of deep learning
models that exploit teleconnections to capture Earth system dynamics. Code
available at this https URL


------------------------------------------------------------------------------

Title:
INC: A Scalable Incremental Weighted Sampler

Abstract: The fundamental problem of weighted sampling involves sampling of satisfying
assignments of Boolean formulas, which specify sampling sets, and according to
distributions defined by pre-specified weight functions to weight functions.
The tight integration of sampling routines in various applications has
highlighted the need for samplers to be incremental, i.e., samplers are
expected to handle updates to weight functions.
The primary contribution of this work is an efficient knowledge
compilation-based weighted sampler, INC, designed for incremental sampling. INC
builds on top of the recently proposed knowledge compilation language,
OBDD[AND], and is accompanied by rigorous theoretical guarantees. Our extensive
experiments demonstrate that INC is faster than state-of-the-art approach for
majority of the evaluation. In particular, we observed a median of 1.69X
runtime improvement over the prior state-of-the-art approach.


------------------------------------------------------------------------------

Title:
A Practical Max-Min Fair Resource Allocation Algorithm for  Rate-Splitting Multiple Access

Abstract: This letter introduces a novel resource allocation algorithm for achieving
max-min fairness (MMF) in a rate-splitting multiple access (RSMA) empowered
multi-antenna broadcast channel. Specifically, we derive the closed-form
solution for the optimal allocation of the common rate among users and the
power between the common and private streams for a given practical
low-complexity beamforming direction design. Numerical results show that the
proposed algorithm achieves at least 90% of the MMF rate obtained by the
conventional iterative optimization algorithm while only takes an average of
0.1 millisecond computational time, which is three orders of magnitude lower
than the conventional algorithm. It is therefore a practical resource
allocation algorithm for RSMA.


------------------------------------------------------------------------------

Title:
Pattern Mining for Anomaly Detection in Graphs: Application to Fraud in  Public Procurement

Abstract: In the context of public procurement, several indicators called red flags are
used to estimate fraud risk. They are computed according to certain contract
attributes and are therefore dependent on the proper filling of the contract
and award notices. However, these attributes are very often missing in
practice, which prohibits red flags computation. Traditional fraud detection
approaches focus on tabular data only, considering each contract separately,
and are therefore very sensitive to this issue. In this work, we adopt a
graph-based method allowing leveraging relations between contracts, to
compensate for the missing attributes. We propose PANG (Pattern-Based Anomaly
Detection in Graphs), a general supervised framework relying on pattern
extraction to detect anomalous graphs in a collection of attributed graphs.
Notably, it is able to identify induced subgraphs, a type of pattern widely
overlooked in the literature. When benchmarked on standard datasets, its
predictive performance is on par with state-of-the-art methods, with the
additional advantage of being explainable. These experiments also reveal that
induced patterns are more discriminative on certain datasets. When applying
PANG to public procurement data, the prediction is superior to other methods,
and it identifies subgraph patterns that are characteristic of fraud-prone
situations, thereby making it possible to better understand fraudulent
behavior.


------------------------------------------------------------------------------

Title:
RepoFusion: Training Code Models to Understand Your Repository

Abstract: Despite the huge success of Large Language Models (LLMs) in coding assistants
like GitHub Copilot, these models struggle to understand the context present in
the repository (e.g., imports, parent classes, files with similar names, etc.),
thereby producing inaccurate code completions. This effect is more pronounced
when using these assistants for repositories that the model has not seen during
training, such as proprietary software or work-in-progress code projects.
Recent work has shown the promise of using context from the repository during
inference. In this work, we extend this idea and propose RepoFusion, a
framework to train models to incorporate relevant repository context.
Experiments on single-line code completion show that our models trained with
repository context significantly outperform much larger code models as
CodeGen-16B-multi ($\sim73\times$ larger) and closely match the performance of
the $\sim 70\times$ larger StarCoderBase model that was trained with the
Fill-in-the-Middle objective. We find these results to be a novel and
compelling demonstration of the gains that training with repository context can
bring. We carry out extensive ablation studies to investigate the impact of
design choices such as context type, number of contexts, context length, and
initialization within our framework. Lastly, we release Stack-Repo, a dataset
of 200 Java repositories with permissive licenses and near-deduplicated files
that are augmented with three types of repository contexts. Additionally, we
are making available the code and trained checkpoints for our work. Our
released resources can be found at \url{this https URL}.


------------------------------------------------------------------------------

Title:
New Perspectives and Systematic Approaches for Analyzing Negative  Damping-Induced Sustained Oscillation

Abstract: Sustained oscillations (SOs) have been widely observed with the rising
penetration of power electronics converters in the systems. Even though the
origin of SOs can be revealed by negative damping modes using conventional
linear analysis, there is a lack of rigorous computation for such a nonlinear
periodic state. Hence, related analytical methods are proposed in this paper:
a) By supposing that the hard limit is not triggered, a set of nonlinear
equations are formed that consider the product coupling of modulations and the
Jacobi-Anger expansion of trigonometric functions. The steady-state harmonics
are initially solved by Newton_Raphson iteration, then the hard limit is
optionally modeled by extracting the Fourier series, and the targeted variables
are updated. b) By implementing the extended multiharmonic linearization, an
authentic linear analysis of SO is achieved, where an increasing number of
positive damping modes can be identified from the loop impedance. It is
emphasized that the two processes should be executed in sequence and uphold the
collective principle of harmonic balance, while the modularity and scalability
should be extremely high. Simulations of a two-level voltage source converter
in PSCAD and RT-LAB verify the theories.


------------------------------------------------------------------------------

Title:
3D VR Sketch Guided 3D Shape Prototyping and Exploration

Abstract: 3D shape modeling is labor-intensive and time-consuming and requires years of
expertise. Recently, 2D sketches and text inputs were considered as conditional
modalities to 3D shape generation networks to facilitate 3D shape modeling.
However, text does not contain enough fine-grained information and is more
suitable to describe a category or appearance rather than geometry, while 2D
sketches are ambiguous, and depicting complex 3D shapes in 2D again requires
extensive practice. Instead, we explore virtual reality sketches that are drawn
directly in 3D. We assume that the sketches are created by novices, without any
art training, and aim to reconstruct physically-plausible 3D shapes. Since such
sketches are potentially ambiguous, we tackle the problem of the generation of
multiple 3D shapes that follow the input sketch structure. Limited in the size
of the training data, we carefully design our method, training the model
step-by-step and leveraging multi-modal 3D shape representation. To guarantee
the plausibility of generated 3D shapes we leverage the normalizing flow that
models the distribution of the latent space of 3D shapes. To encourage the
fidelity of the generated 3D models to an input sketch, we propose a dedicated
loss that we deploy at different stages of the training process. We plan to
make our code publicly available.


------------------------------------------------------------------------------

Title:
The Myth of Meritocracy and the Matilda Effect in STEM: Paper Acceptance  and Paper Citation

Abstract: Biases against women in the workplace have been documented in various
studies. There is also a growing body of literature on biases within academia.
But particularly in STEM, due to the heavily male-dominated field, studies
suggest that if one's gender is identifiable, women are more likely to get
their papers rejected and not cited as often as men. We propose two simple
modifications to tackle gender bias in STEM that can be applied to (but not
only) IEEE conferences and journals. Regarding paper acceptance, we propose a
double-blind review, and regarding paper citation, we propose one single letter
to identify the authors' first names, followed by their family names. We also
propose other modifications regarding gender bias in STEM and academia and
encourage further reforms supported by current research on this topic with
gender-segregated data.


------------------------------------------------------------------------------

Title:
Performance of data-driven inner speech decoding with same-task EEG-fMRI  data fusion and bimodal models

Abstract: Decoding inner speech from the brain signal via hybridisation of fMRI and EEG
data is explored to investigate the performance benefits over unimodal models.
Two different bimodal fusion approaches are examined: concatenation of
probability vectors output from unimodal fMRI and EEG machine learning models,
and data fusion with feature engineering. Same task inner speech data are
recorded from four participants, and different processing strategies are
compared and contrasted to previously-employed hybridisation methods. Data
across participants are discovered to encode different underlying structures,
which results in varying decoding performances between subject-dependent fusion
models. Decoding performance is demonstrated as improved when pursuing bimodal
fMRI-EEG fusion strategies, if the data show underlying structure.


------------------------------------------------------------------------------

Title:
Shape Guided Gradient Voting for Domain Generalization

Abstract: Domain generalization aims to address the domain shift between training and
testing data. To learn the domain invariant representations, the model is
usually trained on multiple domains. It has been found that the gradients of
network weight relative to a specific task loss can characterize the task
itself. In this work, with the assumption that the gradients of a specific
domain samples under the classification task could also reflect the property of
the domain, we propose a Shape Guided Gradient Voting (SGGV) method for domain
generalization. Firstly, we introduce shape prior via extra inputs of the
network to guide gradient descending towards a shape-biased direction for
better generalization. Secondly, we propose a new gradient voting strategy to
remove the outliers for robust optimization in the presence of shape guidance.
To provide shape guidance, we add edge/sketch extracted from the training data
as an explicit way, and also use texture augmented images as an implicit way.
We conduct experiments on several popular domain generalization datasets in
image classification task, and show that our shape guided gradient updating
strategy brings significant improvement of the generalization.


------------------------------------------------------------------------------

Title:
AdaStop: sequential testing for efficient and reliable comparisons of  Deep RL Agents

Abstract: The reproducibility of many experimental results in Deep Reinforcement
Learning (RL) is under question. To solve this reproducibility crisis, we
propose a theoretically sound methodology to compare multiple Deep RL
algorithms. The performance of one execution of a Deep RL algorithm is random
so that independent executions are needed to assess it precisely. When
comparing several RL algorithms, a major question is how many executions must
be made and how can we assure that the results of such a comparison is
theoretically sound. Researchers in Deep RL often use less than 5 independent
executions to compare algorithms: we claim that this is not enough in general.
Moreover, when comparing several algorithms at once, the error of each
comparison accumulates and must be taken into account with a multiple tests
procedure to preserve low error guarantees. To address this problem in a
statistically sound way, we introduce AdaStop, a new statistical test based on
multiple group sequential tests. When comparing algorithms, AdaStop adapts the
number of executions to stop as early as possible while ensuring that we have
enough information to distinguish algorithms that perform better than the
others in a statistical significant way. We prove both theoretically and
empirically that AdaStop has a low probability of making an error (Family-Wise
Error). Finally, we illustrate the effectiveness of AdaStop in multiple
use-cases, including toy examples and difficult cases such as Mujoco
environments.


------------------------------------------------------------------------------

Title:
Practical First-Order Bayesian Optimization Algorithms

Abstract: First Order Bayesian Optimization (FOBO) is a sample efficient sequential
approach to find the global maxima of an expensive-to-evaluate black-box
objective function by suitably querying for the function and its gradient
evaluations. Such methods assume Gaussian process (GP) models for both, the
function and its gradient, and use them to construct an acquisition function
that identifies the next query point. In this paper, we propose a class of
practical FOBO algorithms that efficiently utilizes the information from the
gradient GP to identify potential query points with zero gradients. We
construct a multi-level acquisition function where in the first step, we
optimize a lower level acquisition function with multiple restarts to identify
potential query points with zero gradient value. We then use the upper level
acquisition function to rank these query points based on their function values
to potentially identify the global maxima. As a final step, the potential point
of maxima is chosen as the actual query point. We validate the performance of
our proposed algorithms on several test functions and show that our algorithms
outperform state-of-the-art FOBO algorithms. We also illustrate the application
of our algorithms in finding optimal set of hyper-parameters in machine
learning and in learning the optimal policy in reinforcement learning tasks.


------------------------------------------------------------------------------

Title:
FHA-Kitchens: A Novel Dataset for Fine-Grained Hand Action Recognition  in Kitchen Scenes

Abstract: A typical task in the field of video understanding is hand action
recognition, which has a wide range of applications. Existing works either
mainly focus on full-body actions, or the defined action categories are
relatively coarse-grained. In this paper, we propose FHA-Kitchens, a novel
dataset of fine-grained hand actions in kitchen scenes. In particular, we focus
on human hand interaction regions and perform deep excavation to further refine
hand action information and interaction regions. Our FHA-Kitchens dataset
consists of 2,377 video clips and 30,047 images collected from 8 different
types of dishes, and all hand interaction regions in each image are labeled
with high-quality fine-grained action classes and bounding boxes. We represent
the action information in each hand interaction region as a triplet, resulting
in a total of 878 action triplets. Based on the constructed dataset, we
benchmark representative action recognition and detection models on the
following three tracks: (1) supervised learning for hand interaction region and
object detection, (2) supervised learning for fine-grained hand action
recognition, and (3) intra- and inter-class domain generalization for hand
interaction region detection. The experimental results offer compelling
empirical evidence that highlights the challenges inherent in fine-grained hand
action recognition, while also shedding light on potential avenues for future
research, particularly in relation to pre-training strategy, model design, and
domain generalization. The dataset will be released at
this https URL


------------------------------------------------------------------------------

Title:
Understanding the Effects of Permanent Faults in GPU's Parallelism  Management and Control Units

Abstract: Graphics Processing Units (GPUs) are over-stressed to accelerate
High-Performance Computing applications and are used to accelerate Deep Neural
Networks in several domains where they have a life expectancy of many years.
These conditions expose the GPUs hardware to (premature) aging, causing
permanent faults to arise after the usual end-of-manufacturing test. Techniques
to assess the impact of permanent faults in GPUs are then strongly required,
thus allowing to estimate the reliability risk and to possibly mitigate it. In
this paper, we present a method to evaluate the effects of permanent faults
affecting the GPU scheduler and control units, which are the most peculiar and
stressed resources, along with the first figures that allow quantifying these
effects. We characterize over 5.83x10^5 permanent fault effects in the
scheduler and controllers of a gate-level GPU model. Then, we map the observed
error categories in software by instrumenting the code of 13 applications and
two convolutional neural networks, injecting more than 1.65x10^5 permanent
errors. Our two-level fault injection strategy reduces the evaluation time from
hundreds of years of gate-level evaluation to hundreds of hours.We found that
faults in the GPU parallelism management units can modify the opcode, the
addresses, and the status of thread(s) and warp(s). The large majority (up to
99%) of these hardware permanent errors impacts the running software execution.
Errors affecting the instruction operation or resource management hang the
code, while 45% of errors in the parallelism management or control-flow induce
silent data corruptions.


------------------------------------------------------------------------------

Title:
Learning an Interpretable End-to-End Network for Real-Time Acoustic  Beamforming

Abstract: Recently, many forms of audio industrial applications, such as sound
monitoring and source localization, have begun exploiting smart multi-modal
devices equipped with a microphone array. Regrettably, model-based methods are
often difficult to employ for such devices due to their high computational
complexity, as well as the difficulty of appropriately selecting the
user-determined parameters. As an alternative, one may use deep network-based
methods, but these are often difficult to generalize, nor can they generate the
desired beamforming map directly. In this paper, a computationally efficient
acoustic beamforming algorithm is proposed, which may be unrolled to form a
model-based deep learning network for real-time imaging, here termed the
DAMAS-FISTA-Net. By exploiting the natural structure of an acoustic beamformer,
the proposed network inherits the physical knowledge of the acoustic system,
and thus learns the underlying physical properties of the propagation. As a
result, all the network parameters may be learned end-to-end, guided by a
model-based prior using back-propagation. Notably, the proposed network enables
an excellent interpretability and the ability of being able to process the raw
data directly. Extensive numerical experiments using both simulated and
real-world data illustrate the preferable performance of the DAMAS-FISTA-Net as
compared to alternative approaches.


------------------------------------------------------------------------------

Title:
A deep dive into explainable self-supervised transformers for point  clouds

Abstract: In this paper we delve into the properties of transformers, attained through
self-supervision, in the point cloud domain. Specifically, we evaluate the
effectiveness of Masked Autoencoding as a pretraining scheme, and explore
Momentum Contrast as an alternative. In our study we investigate the impact of
data quantity on the learned features, and uncover similarities in the
transformer's behavior across domains. Through comprehensive visualiations, we
observe that the transformer learns to attend to semantically meaningful
regions, indicating that pretraining leads to a better understanding of the
underlying geometry. Moreover, we examine the finetuning process and its effect
on the learned representations. Based on that, we devise an unfreezing strategy
which consistently outperforms our baseline without introducing any other
modifications to the model or the training pipeline, and achieve
state-of-the-art results in the classification task among transformer models.


------------------------------------------------------------------------------

Title:
Unsupervised Framework for Evaluating and Explaining Structural Node  Embeddings of Graphs

Abstract: An embedding is a mapping from a set of nodes of a network into a real vector
space. Embeddings can have various aims like capturing the underlying graph
topology and structure, node-to-node relationship, or other relevant
information about the graph, its subgraphs or nodes themselves. A practical
challenge with using embeddings is that there are many available variants to
choose from. Selecting a small set of most promising embeddings from the long
list of possible options for a given task is challenging and often requires
domain expertise. Embeddings can be categorized into two main types: classical
embeddings and structural embeddings. Classical embeddings focus on learning
both local and global proximity of nodes, while structural embeddings learn
information specifically about the local structure of nodes' neighbourhood. For
classical node embeddings there exists a framework which helps data scientists
to identify (in an unsupervised way) a few embeddings that are worth further
investigation. Unfortunately, no such framework exists for structural
embeddings. In this paper we propose a framework for unsupervised ranking of
structural graph embeddings. The proposed framework, apart from assigning an
aggregate quality score for a structural embedding, additionally gives a data
scientist insights into properties of this embedding. It produces information
which predefined node features the embedding learns, how well it learns them,
and which dimensions in the embedded space represent the predefined node
features. Using this information the user gets a level of explainability to an
otherwise complex black-box embedding algorithm.


------------------------------------------------------------------------------

Title:
Instruct-NeuralTalker: Editing Audio-Driven Talking Radiance Fields with  Instructions

Abstract: Recent neural talking radiance field methods have shown great success in
photorealistic audio-driven talking face synthesis. In this paper, we propose a
novel interactive framework that utilizes human instructions to edit such
implicit neural representations to achieve real-time personalized talking face
generation. Given a short speech video, we first build an efficient talking
radiance field, and then apply the latest conditional diffusion model for image
editing based on the given instructions and guiding implicit representation
optimization towards the editing target. To ensure audio-lip synchronization
during the editing process, we propose an iterative dataset updating strategy
and utilize a lip-edge loss to constrain changes in the lip region. We also
introduce a lightweight refinement network for complementing image details and
achieving controllable detail generation in the final rendered image. Our
method also enables real-time rendering at up to 30FPS on consumer hardware.
Multiple metrics and user verification show that our approach provides a
significant improvement in rendering quality compared to state-of-the-art
methods.


------------------------------------------------------------------------------

Title:
Blockchain-Enabled Federated Learning: A Reference Architecture  Incorporating a DID Access System

Abstract: Recently, Blockchain-Enabled Federated Learning (BCFL), an innovative
approach that combines the advantages of Federated Learning and Blockchain
technology, is receiving great attention. Federated Learning (FL) allows
multiple participants to jointly train machine learning models in a
decentralized manner while maintaining data privacy and security. This paper
proposes a reference architecture for blockchain-enabled federated learning,
which enables multiple entities to collaboratively train machine learning
models while preserving data privacy and security. A critical component of this
architecture is the implementation of a decentralized identifier (DID)-based
access system. DID introduces a decentralized, self-sovereign identity (ID)
management system that allows participants to manage their IDs independently of
central authorities. Within this proposed architecture, participants can
authenticate and gain access to the federated learning platform via their DIDs,
which are securely stored on the blockchain. The access system administers
access control and permissions through the execution of smart contracts,
further enhancing the security and decentralization of the system. This
approach, integrating blockchain-enabled federated learning with a DID access
system, offers a robust solution for collaborative machine learning in a
distributed and secure manner. As a result, participants can contribute to
global model training while maintaining data privacy and identity control
without the need to share local data. These DIDs are stored on the blockchain
and the access system uses smart contracts to manage access control and
permissions. The source code will be available to the public soon.


------------------------------------------------------------------------------

Title:
Confidential Computing in Edge-Cloud Hierarchy

Abstract: The paper introduces confidential computing approaches focused on protecting
hierarchical data within edge-cloud network. Edge-cloud network suggests
splitting and sharing data between the main cloud and the range of networks
near the endpoint devices. The proposed solutions allow data in this two-level
hierarchy to be protected via embedding traditional encryption at rest and in
transit while leaving the remaining security issues, such as sensitive data and
operations in use, in the scope of trusted execution environment. Hierarchical
data for each network device are linked and identified through distinct paths
between edge and main cloud using individual blockchain. Methods for data and
cryptographic key splitting between the edge and the main cloud are based on
strong authentication techniques ensuring the shared data confidentiality,
integrity and availability.


------------------------------------------------------------------------------

Title:
Handwritten Text Recognition from Crowdsourced Annotations

Abstract: In this paper, we explore different ways of training a model for handwritten
text recognition when multiple imperfect or noisy transcriptions are available.
We consider various training configurations, such as selecting a single
transcription, retaining all transcriptions, or computing an aggregated
transcription from all available annotations. In addition, we evaluate the
impact of quality-based data selection, where samples with low agreement are
removed from the training set. Our experiments are carried out on municipal
registers of the city of Belfort (France) written between 1790 and 1946. %
results The results show that computing a consensus transcription or training
on multiple transcriptions are good alternatives. However, selecting training
samples based on the degree of agreement between annotators introduces a bias
in the training data and does not improve the results. Our dataset is publicly
available on Zenodo: this https URL


------------------------------------------------------------------------------

Title:
SelfTalk: A Self-Supervised Commutative Training Diagram to Comprehend  3D Talking Faces

Abstract: Speech-driven 3D face animation technique, extending its applications to
various multimedia fields. Previous research has generated promising realistic
lip movements and facial expressions from audio signals. However, traditional
regression models solely driven by data face several essential problems, such
as difficulties in accessing precise labels and domain gaps between different
modalities, leading to unsatisfactory results lacking precision and coherence.
To enhance the visual accuracy of generated lip movement while reducing the
dependence on labeled data, we propose a novel framework SelfTalk, by involving
self-supervision in a cross-modals network system to learn 3D talking faces.
The framework constructs a network system consisting of three modules: facial
animator, speech recognizer, and lip-reading interpreter. The core of SelfTalk
is a commutative training diagram that facilitates compatible features exchange
among audio, text, and lip shape, enabling our models to learn the intricate
connection between these factors. The proposed framework leverages the
knowledge learned from the lip-reading interpreter to generate more plausible
lip shapes. Extensive experiments and user studies demonstrate that our
proposed approach achieves state-of-the-art performance both qualitatively and
quantitatively. We recommend watching the supplementary video.


------------------------------------------------------------------------------

Title:
AMRs Assemble! Learning to Ensemble with Autoregressive Models for AMR  Parsing

Abstract: In this paper, we examine the current state-of-the-art in AMR parsing, which
relies on ensemble strategies by merging multiple graph predictions. Our
analysis reveals that the present models often violate AMR structural
constraints. To address this issue, we develop a validation method, and show
how ensemble models can exploit SMATCH metric weaknesses to obtain higher
scores, but sometimes result in corrupted graphs. Additionally, we highlight
the demanding need to compute the SMATCH score among all possible predictions.
To overcome these challenges, we propose two novel ensemble strategies based on
Transformer models, improving robustness to structural constraints, while also
reducing the computational time. Our methods provide new insights for enhancing
AMR parsers and metrics. Our code is available at
\href{this https URL}{github.com/babelscape/AMRs-Assemble}.


------------------------------------------------------------------------------

Title:
PTDRL: Parameter Tuning using Deep Reinforcement Learning

Abstract: A variety of autonomous navigation algorithms exist that allow robots to move
around in a safe and fast manner. However, many of these algorithms require
parameter re-tuning when facing new environments. In this paper, we propose
PTDRL, a parameter-tuning strategy that adaptively selects from a fixed set of
parameters those that maximize the expected reward for a given navigation
system. Our learning strategy can be used for different environments, different
platforms, and different user preferences. Specifically, we attend to the
problem of social navigation in indoor spaces, using a classical motion
planning algorithm as our navigation system and training its parameters to
optimize its behavior. Experimental results show that PTDRL can outperform
other online parameter-tuning strategies.


------------------------------------------------------------------------------

Title:
Deep learning based black spot identification on Greek road networks

Abstract: Black spot identification, a spatiotemporal phenomenon, involves analyzing
the geographical location and time-based occurrence of road accidents.
Typically, this analysis examines specific locations on road networks during
set time periods to pinpoint areas with a higher concentration of accidents,
known as black spots. By evaluating these problem areas, researchers can
uncover the underlying causes and reasons for increased collision rates, such
as road design, traffic volume, driver behavior, weather, and infrastructure.
However, challenges in identifying black spots include limited data
availability, data quality, and assessing contributing factors. Additionally,
evolving road design, infrastructure, and vehicle safety technology can affect
black spot analysis and determination. This study focused on traffic accidents
in Greek road networks to recognize black spots, utilizing data from police and
government-issued car crash reports. The study produced a publicly available
dataset called Black Spots of North Greece (BSNG) and a highly accurate
identification method.


------------------------------------------------------------------------------

Title:
Adaptive Ordered Information Extraction with Deep Reinforcement Learning

Abstract: Information extraction (IE) has been studied extensively. The existing
methods always follow a fixed extraction order for complex IE tasks with
multiple elements to be extracted in one instance such as event extraction.
However, we conduct experiments on several complex IE datasets and observe that
different extraction orders can significantly affect the extraction results for
a great portion of instances, and the ratio of sentences that are sensitive to
extraction orders increases dramatically with the complexity of the IE task.
Therefore, this paper proposes a novel adaptive ordered IE paradigm to find the
optimal element extraction order for different instances, so as to achieve the
best extraction results. We also propose an reinforcement learning (RL) based
framework to generate optimal extraction order for each instance dynamically.
Additionally, we propose a co-training framework adapted to RL to mitigate the
exposure bias during the extractor training phase. Extensive experiments
conducted on several public datasets demonstrate that our proposed method can
beat previous methods and effectively improve the performance of various IE
tasks, especially for complex ones.


------------------------------------------------------------------------------

Title:
Preserving Commonsense Knowledge from Pre-trained Language Models via  Causal Inference

Abstract: Fine-tuning has been proven to be a simple and effective technique to
transfer the learned knowledge of Pre-trained Language Models (PLMs) to
downstream tasks. However, vanilla fine-tuning easily overfits the target data
and degrades the generalization ability. Most existing studies attribute it to
catastrophic forgetting, and they retain the pre-trained knowledge
indiscriminately without identifying what knowledge is transferable. Motivated
by this, we frame fine-tuning into a causal graph and discover that the crux of
catastrophic forgetting lies in the missing causal effects from the pretrained
data. Based on the causal view, we propose a unified objective for fine-tuning
to retrieve the causality back. Intriguingly, the unified objective can be seen
as the sum of the vanilla fine-tuning objective, which learns new knowledge
from target data, and the causal objective, which preserves old knowledge from
PLMs. Therefore, our method is flexible and can mitigate negative transfer
while preserving knowledge. Since endowing models with commonsense is a
long-standing challenge, we implement our method on commonsense QA with a
proposed heuristic estimation to verify its effectiveness. In the experiments,
our method outperforms state-of-the-art fine-tuning methods on all six
commonsense QA datasets and can be implemented as a plug-in module to inflate
the performance of existing QA models.


------------------------------------------------------------------------------

Title:
Full-Duplex-Enabled Joint Communications and Sensing with Reconfigurable  Intelligent Surfaces

Abstract: The full-duplex (FD) technology has the potential to radically evolve
wireless systems, facilitating the integration of both communications and radar
functionalities into a single device, thus, enabling joint communication and
sensing (JCAS). In this paper, we present a novel approach for JCAS that
incorporates a reconfigurable intelligent surface (RIS) in the near-field of an
FD multiple-input multiple-output (MIMO) node, which is jointly optimized with
the digital beamformers to enable JSAC and efficiently handle self-interference
(SI). We propose a novel problem formulation for FD MIMO JCAS systems to
jointly minimize the total received power at the FD node's radar receiver while
maximizing the sum rate of downlink communications subject to a Cram\'{e}r-Rao
bound (CRB) constraint. In contrast to the typically used CRB in the relevant
literature, we derive a novel, more accurate, target estimation bound that
fully takes into account the RIS deployment. The considered problem is solved
using alternating optimization, which is guaranteed to converge to a local
optimum. The simulation results demonstrate that the proposed scheme achieves
significant performance improvement both for communications and sensing. It is
showcased that, jointly designing the FD MIMO beamformers and the RIS phase
configuration to be SI aware can significantly loosen the requirement for
additional SI cancellation.


------------------------------------------------------------------------------

Title:
Practical and General Backdoor Attacks against Vertical Federated  Learning

Abstract: Federated learning (FL), which aims to facilitate data collaboration across
multiple organizations without exposing data privacy, encounters potential
security risks. One serious threat is backdoor attacks, where an attacker
injects a specific trigger into the training dataset to manipulate the model's
prediction. Most existing FL backdoor attacks are based on horizontal federated
learning (HFL), where the data owned by different parties have the same
features. However, compared to HFL, backdoor attacks on vertical federated
learning (VFL), where each party only holds a disjoint subset of features and
the labels are only owned by one party, are rarely studied. The main challenge
of this attack is to allow an attacker without access to the data labels, to
perform an effective attack. To this end, we propose BadVFL, a novel and
practical approach to inject backdoor triggers into victim models without label
information. BadVFL mainly consists of two key steps. First, to address the
challenge of attackers having no knowledge of labels, we introduce a SDD module
that can trace data categories based on gradients. Second, we propose a SDP
module that can improve the attack's effectiveness by enhancing the decision
dependency between the trigger and attack target. Extensive experiments show
that BadVFL supports diverse datasets and models, and achieves over 93% attack
success rate with only 1% poisoning rate.


------------------------------------------------------------------------------

Title:
Detection of Sensor-To-Sensor Variations using Explainable AI

Abstract: With the growing concern for air quality and its impact on human health,
interest in environmental gas monitoring has increased. However,
chemi-resistive gas sensing devices are plagued by issues of sensor
reproducibility during manufacturing. This study proposes a novel approach for
detecting sensor-to-sensor variations in sensing devices using the explainable
AI (XAI) method of SHapley Additive exPlanations (SHAP). This is achieved by
identifying sensors that contribute the most to environmental gas concentration
estimation via machine learning, and measuring the similarity of feature
rankings between sensors to flag deviations or outliers. The methodology is
tested using artificial and realistic Ozone concentration profiles to train a
Gated Recurrent Unit (GRU) model. Two applications were explored in the study:
the detection of wrong behaviors of sensors in the train dataset, and the
detection of deviations in the test dataset. By training the GRU with the
pruned train dataset, we could reduce computational costs while improving the
model performance. Overall, the results show that our approach improves the
understanding of sensor behavior, successfully detects sensor deviations down
to 5-10% from the normal behavior, and leads to more efficient model
preparation and calibration. Our method provides a novel solution for
identifying deviating sensors, linking inconsistencies in hardware to
sensor-to-sensor variations in the manufacturing process on an AI model-level.


------------------------------------------------------------------------------

Title:
QoS-Aware Downlink Beamforming for Joint Transmission in Multi-Cell  Networks

Abstract: Multi-cell cooperation is an effective means to improve service quality to
cellular users. Existing work primarily focuses on interference cancellation
using all the degrees of freedom (DoF). This leads to low service quality for
some users with poor channel quality to its serving base station. This work
investigates the multi-cell beamforming design for enhancing the downlink
signal strength and mitigating interference simultaneously. We first consider
the ideal case when perfect channel state information (CSI) is available for
determining the beamforming vectors and then extend the problem to the case
with imperfect CSI. For both cases, the beamforming optimization problems are
non-convex. Assuming perfect CSI, we obtain the optimal JT beamforming vectors
based on the uplink-downlink duality. In the presence of unknown CSI errors, we
use the semidefinite relaxation (SDR) with Bernstein-type inequality to derive
the robust JT beamforming. Numerical results are presented to evaluate the
performance of the proposed JT beamforming schemes.


------------------------------------------------------------------------------

Title:
Distributed Localization and Tracking Control for Nonholonomic Agents  with Time-varying Bearing Formation

Abstract: This paper studies the bearing-based time-varying formation control problem
for unicycle-type agents without bearing rigidity conditions. In the considered
problem, only a small set of agents, named as anchors, can obtain their global
positions, and the other agents only have access to the bearing information
relative to their neighbors. To address the problem, we propose a novel scheme
integrating the distributed localization algorithm and the observer-based
formation tracking controller. The designed localization algorithm estimates
the global position by using inter-agent bearing measurements, and the
observer-based controller tracks the desired formation with the estimated
positions. A key distinction of our approach is extending the
localization-and-tracking control scheme to the bearing-based coordination of
nonholonomic systems, where the desired inter-agent bearings can be
time-varying, instead of the constant ones assumed in most of the existing
researches. The asymptotic stability of the coupled localization-and-tracking
control system is proved, and simulations are carried out to validate the
theoretical analysis.


------------------------------------------------------------------------------

Title:
B-cos Alignment for Inherently Interpretable CNNs and Vision  Transformers

Abstract: We present a new direction for increasing the interpretability of deep neural
networks (DNNs) by promoting weight-input alignment during training. For this,
we propose to replace the linear transformations in DNNs by our novel B-cos
transformation. As we show, a sequence (network) of such transformations
induces a single linear transformation that faithfully summarises the full
model computations. Moreover, the B-cos transformation is designed such that
the weights align with relevant signals during optimisation. As a result, those
induced linear transformations become highly interpretable and highlight
task-relevant features. Importantly, the B-cos transformation is designed to be
compatible with existing architectures and we show that it can easily be
integrated into virtually all of the latest state of the art models for
computer vision - e.g. ResNets, DenseNets, ConvNext models, as well as Vision
Transformers - by combining the B-cos-based explanations with normalisation and
attention layers, all whilst maintaining similar accuracy on ImageNet. Finally,
we show that the resulting explanations are of high visual quality and perform
well under quantitative interpretability metrics.


------------------------------------------------------------------------------

Title:
Vision Transformer with Attention Map Hallucination and FFN Compaction

Abstract: Vision Transformer(ViT) is now dominating many vision tasks. The drawback of
quadratic complexity of its token-wise multi-head self-attention (MHSA), is
extensively addressed via either token sparsification or dimension reduction
(in spatial or channel). However, the therein redundancy of MHSA is usually
overlooked and so is the feed-forward network (FFN). To this end, we propose
attention map hallucination and FFN compaction to fill in the blank.
Specifically, we observe similar attention maps exist in vanilla ViT and
propose to hallucinate half of the attention maps from the rest with much
cheaper operations, which is called hallucinated-MHSA (hMHSA). As for FFN, we
factorize its hidden-to-output projection matrix and leverage the
re-parameterization technique to strengthen its capability, making it
compact-FFN (cFFN). With our proposed modules, a 10$\%$-20$\%$ reduction of
floating point operations (FLOPs) and parameters (Params) is achieved for
various ViT-based backbones, including straight (DeiT), hybrid (NextViT) and
hierarchical (PVT) structures, meanwhile, the performances are quite
competitive.


------------------------------------------------------------------------------

Title:
A Semi-implicit Finite Volume Scheme for Dissipative Measure-valued  Solutions to the Barotropic Euler System

Abstract: A semi-implicit in time, entropy stable finite volume scheme for the
compressible barotropic Euler system is designed and analyzed and its weak
convergence to a dissipative measure-valued (DMV) solution [E. Feireisl et al.,
Dissipative measure-valued solutions to the compressible Navier-Stokes system,
Calc. Var. Partial Differential Equations, 2016] of the Euler system is shown.
The entropy stability is achieved by introducing a shifted velocity in the
convective fluxes of the mass and momentum balances, provided some CFL-like
condition is satisfied to ensure stability. A consistency analysis is performed
in the spirit of the Lax's equivalence theorem under some physically reasonable
boundedness assumptions. The concept of K-convergence [E. Feireisl et al.,
K-convergence as a new tool in numerical analysis, IMA J. Numer. Anal., 2020]
is used in order to obtain some strong convergence results, which are then
illustrated via rigorous numerical case studies. The convergence of the scheme
to a DMV solution, a weak solution and a strong solution of the Euler system
using the weak-strong uniqueness principle and relative entropy are presented.


------------------------------------------------------------------------------

Title:
Female mosquito detection by means of AI techniques inside release  containers in the context of a Sterile Insect Technique program

Abstract: The Sterile Insect Technique (SIT) is a biological pest control technique
based on the release into the environment of sterile males of the insect
species whose population is to be controlled. The entire SIT process involves
mass-rearing within a biofactory, sorting of the specimens by sex,
sterilization, and subsequent release of the sterile males into the
environment. The reason for avoiding the release of female specimens is
because, unlike males, females bite, with the subsequent risk of disease
transmission. In the case of Aedes mosquito biofactories for SIT, the key point
of the whole process is sex separation. This process is nowadays performed by a
combination of mechanical devices and AI-based vision systems. However, there
is still a possibility of false negatives, so a last stage of verification is
necessary before releasing them into the environment. It is known that the
sound produced by the flapping of adult male mosquitoes is different from that
produced by females, so this feature can be used to detect the presence of
females in containers prior to environmental release. This paper presents a
study for the detection of females in Aedes mosquito release vessels for SIT
programs. The containers used consist of PVC a tubular design of 8.8cm diameter
and 12.5cm height. The containers were placed in an experimental setup that
allowed the recording of the sound of mosquito flight inside of them. Each
container was filled with 250 specimens considering the cases of (i) only male
mosquitoes, (ii) only female mosquitoes, and (iii) 75% males and 25% females.
Case (i) was used for training and testing, whereas cases (ii) and (iii) were
used only for testing. Two algorithms were implemented for the detection of
female mosquitoes: an unsupervised outlier detection algorithm (iForest) and a
one-class SVM trained with male-only recordings.


------------------------------------------------------------------------------

Title:
An Error Correction Mid-term Electricity Load Forecasting Model Based on  Seasonal Decomposition

Abstract: Mid-term electricity load forecasting (LF) plays a critical role in power
system planning and operation. To address the issue of error accumulation and
transfer during the operation of existing LF models, a novel model called error
correction based LF (ECLF) is proposed in this paper, which is designed to
provide more accurate and stable LF. Firstly, time series analysis and feature
engineering act on the original data to decompose load data into three
components and extract relevant features. Then, based on the idea of stacking
ensemble, long short-term memory is employed as an error correction module to
forecast the components separately, and the forecast results are treated as new
features to be fed into extreme gradient boosting for the second-step
forecasting. Finally, the component sub-series forecast results are
reconstructed to obtain the final LF results. The proposed model is evaluated
on real-world electricity load data from two cities in China, and the
experimental results demonstrate its superior performance compared to the other
benchmark models.


------------------------------------------------------------------------------

Title:
Grammatical gender in Swedish is predictable using recurrent neural  networks

Abstract: The grammatical gender of Swedish nouns is a mystery. While there are few
rules that can indicate the gender with some certainty, it does in general not
depend on either meaning or the structure of the word. In this paper we
demonstrate the surprising fact that grammatical gender for Swedish nouns can
be predicted with high accuracy using a recurrent neural network (RNN) working
on the raw character sequence of the word, without using any contextual
information.


------------------------------------------------------------------------------

Title:
Quantitative Parameter Reconstruction from Optical Coherence Tomographic  Data

Abstract: Quantitative tissue information, like the light scattering properties, is
considered as a key player in the detection of cancerous cells in medical
diagnosis. A promising method to obtain these data is optical coherence
tomography (OCT). In this article, we will therefore discuss the refractive
index reconstruction from OCT data, employing a Gaussian beam based forward
model. We consider in particular samples with a layered structure, meaning that
the refractive index as a function of depth is well approximated by a
piece-wise constant function. For the reconstruction, we present a
layer-by-layer method where in every step the refractive index is obtained via
a discretized least squares minimization. For an approximated form of the
minimization problem, we present an existence and uniqueness result. The
applicability of the proposed method is then verified by reconstructing
refractive indices of layered media from both simulated and experimental OCT
data.


------------------------------------------------------------------------------

Title:
Spatial-Temporal Graph Learning with Adversarial Contrastive Adaptation

Abstract: Spatial-temporal graph learning has emerged as a promising solution for
modeling structured spatial-temporal data and learning region representations
for various urban sensing tasks such as crime forecasting and traffic flow
prediction. However, most existing models are vulnerable to the quality of the
generated region graph due to the inaccurate graph-structured information
aggregation schema. The ubiquitous spatial-temporal data noise and
incompleteness in real-life scenarios pose challenges in generating
high-quality region representations. To address this challenge, we propose a
new spatial-temporal graph learning model (GraphST) for enabling effective
self-supervised learning. Our proposed model is an adversarial contrastive
learning paradigm that automates the distillation of crucial multi-view
self-supervised information for robust spatial-temporal graph augmentation. We
empower GraphST to adaptively identify hard samples for better
self-supervision, enhancing the representation discrimination ability and
robustness. In addition, we introduce a cross-view contrastive learning
paradigm to model the inter-dependencies across view-specific region
representations and preserve underlying relation heterogeneity. We demonstrate
the superiority of our proposed GraphST method in various spatial-temporal
prediction tasks on real-life datasets. We release our model implementation via
the link: \url{this https URL}.


------------------------------------------------------------------------------

Title:
Multitrack Music Transcription with a Time-Frequency Perceiver

Abstract: Multitrack music transcription aims to transcribe a music audio input into
the musical notes of multiple instruments simultaneously. It is a very
challenging task that typically requires a more complex model to achieve
satisfactory result. In addition, prior works mostly focus on transcriptions of
regular instruments, however, neglecting vocals, which are usually the most
important signal source if present in a piece of music. In this paper, we
propose a novel deep neural network architecture, Perceiver TF, to model the
time-frequency representation of audio input for multitrack transcription.
Perceiver TF augments the Perceiver architecture by introducing a hierarchical
expansion with an additional Transformer layer to model temporal coherence.
Accordingly, our model inherits the benefits of Perceiver that posses better
scalability, allowing it to well handle transcriptions of many instruments in a
single model. In experiments, we train a Perceiver TF to model 12 instrument
classes as well as vocal in a multi-task learning manner. Our result
demonstrates that the proposed system outperforms the state-of-the-art
counterparts (e.g., MT3 and SpecTNT) on various public datasets.


------------------------------------------------------------------------------

Title:
Generating Oscillation Activity with Echo State Network to Mimic the  Behavior of a Simple Central Pattern Generator

Abstract: This paper presents a method for reproducing a simple central pattern
generator (CPG) using a modified Echo State Network (ESN). Conventionally, the
dynamical reservoir needs to be damped to stabilize and preserve memory.
However, we find that a reservoir that develops oscillatory activity without
any external excitation can mimic the behaviour of a simple CPG in biological
systems. We define the specific neuron ensemble required for generating
oscillations in the reservoir and demonstrate how adjustments to the leaking
rate, spectral radius, topology, and population size can increase the
probability of reproducing these oscillations. The results of the experiments,
conducted on the time series simulation tasks, demonstrate that the ESN is able
to generate the desired waveform without any input. This approach offers a
promising solution for the development of bio-inspired controllers for robotic
systems.


------------------------------------------------------------------------------

Title:
Path to Medical AGI: Unify Domain-specific Medical LLMs with the Lowest  Cost

Abstract: Medical artificial general intelligence (AGI) is an emerging field that aims
to develop systems specifically designed for medical applications that possess
the ability to understand, learn, and apply knowledge across a wide range of
tasks and domains. Large language models (LLMs) represent a significant step
towards AGI. However, training cross-domain LLMs in the medical field poses
significant challenges primarily attributed to the requirement of collecting
data from diverse domains. This task becomes particularly difficult due to
privacy restrictions and the scarcity of publicly available medical datasets.
Here, we propose Medical AGI (MedAGI), a paradigm to unify domain-specific
medical LLMs with the lowest cost, and suggest a possible path to achieve
medical AGI. With an increasing number of domain-specific professional
multimodal LLMs in the medical field being developed, MedAGI is designed to
automatically select appropriate medical models by analyzing users' questions
with our novel adaptive expert selection algorithm. It offers a unified
approach to existing LLMs in the medical field, eliminating the need for
retraining regardless of the introduction of new models. This characteristic
renders it a future-proof solution in the dynamically advancing medical domain.
To showcase the resilience of MedAGI, we conducted an evaluation across three
distinct medical domains: dermatology diagnosis, X-ray diagnosis, and analysis
of pathology pictures. The results demonstrated that MedAGI exhibited
remarkable versatility and scalability, delivering exceptional performance
across diverse domains. Our code is publicly available to facilitate further
research at this https URL


------------------------------------------------------------------------------

Title:
PowerBEV: A Powerful Yet Lightweight Framework for Instance Prediction  in Bird's-Eye View

Abstract: Accurately perceiving instances and predicting their future motion are key
tasks for autonomous vehicles, enabling them to navigate safely in complex
urban traffic. While bird's-eye view (BEV) representations are commonplace in
perception for autonomous driving, their potential in a motion prediction
setting is less explored. Existing approaches for BEV instance prediction from
surround cameras rely on a multi-task auto-regressive setup coupled with
complex post-processing to predict future instances in a spatio-temporally
consistent manner. In this paper, we depart from this paradigm and propose an
efficient novel end-to-end framework named POWERBEV, which differs in several
design choices aimed at reducing the inherent redundancy in previous methods.
First, rather than predicting the future in an auto-regressive fashion,
POWERBEV uses a parallel, multi-scale module built from lightweight 2D
convolutional networks. Second, we show that segmentation and centripetal
backward flow are sufficient for prediction, simplifying previous multi-task
objectives by eliminating redundant output modalities. Building on this output
representation, we propose a simple, flow warping-based post-processing
approach which produces more stable instance associations across time. Through
this lightweight yet powerful design, POWERBEV outperforms state-of-the-art
baselines on the NuScenes Dataset and poses an alternative paradigm for BEV
instance prediction. We made our code publicly available at:
this https URL


------------------------------------------------------------------------------

Title:
Algorithms of Sampling-Frequency-Independent Layers for Non-integer  Strides

Abstract: In this paper, we propose algorithms for handling non-integer strides in
sampling-frequency-independent (SFI) convolutional and transposed convolutional
layers. The SFI layers have been developed for handling various sampling
frequencies (SFs) by a single neural network. They are replaceable with their
non-SFI counterparts and can be introduced into various network architectures.
However, they could not handle some specific configurations when combined with
non-SFI layers. For example, an SFI extension of Conv-TasNet, a standard audio
source separation model, cannot handle some pairs of trained and target SFs
because the strides of the SFI layers become non-integers. This problem cannot
be solved by simple rounding or signal resampling, resulting in the significant
performance degradation. To overcome this problem, we propose algorithms for
handling non-integer strides by using windowed sinc interpolation. The proposed
algorithms realize the continuous-time representations of features using the
interpolation and enable us to sample instants with the desired stride.
Experimental results on music source separation showed that the proposed
algorithms outperformed the rounding- and signal-resampling-based methods at
SFs lower than the trained SF.


------------------------------------------------------------------------------

Title:
Codes and Pseudo-Geometric Designs from the Ternary $m$-Sequences with  Welch-type decimation $d=2\cdot 3^{(n-1)/2}+1$

Abstract: Pseudo-geometric designs are designs which share the same parameters as a
finite geometry design, but which are not isomorphic to that design. As far as
we know, only a very small number of pseudo-geometric designs have been
constructed and no pseudo-geometric designs with the same parameters $S\left
(2, q+1,(q^n-1)/(q-1)\right )$ as the point-line designs of the projective
spaces $\mathrm{PG}(n-1,q)$ were found. In this paper, we present a family of
ternary cyclic codes from the $m$-sequences with Welch-type decimation
$d=2\cdot 3^{(n-1)/2}+1$, and construct some infinite family of 2-designs and a
family of Steiner systems $S\left (2, 4, (3^n-1)/2\right )$ using these cyclic
codes and their duals. We show that one of these Steiner systems is
inequivalent to the point-line design of the projective space
$\mathrm{PG}(n-1,3)$ and thus is a pseudo-geometric design. Moreover, the
parameters of these cyclic codes and their shortened codes are also determined.
Some of those ternary codes are optimal or almost optimal.


------------------------------------------------------------------------------

Title:
Fine-tuning Large Enterprise Language Models via Ontological Reasoning

Abstract: Large Language Models (LLMs) exploit fine-tuning as a technique to adapt to
diverse goals, thanks to task-specific training data. Task specificity should
go hand in hand with domain orientation, that is, the specialization of an LLM
to accurately address the tasks of a given realm of interest. However, models
are usually fine-tuned over publicly available data or, at most, over ground
data from databases, ignoring business-level definitions and domain experience.
On the other hand, Enterprise Knowledge Graphs (EKGs) are able to capture and
augment such domain knowledge via ontological reasoning. With the goal of
combining LLM flexibility with the domain orientation of EKGs, we propose a
novel neurosymbolic architecture that leverages the power of ontological
reasoning to build task- and domain-specific corpora for LLM fine-tuning.


------------------------------------------------------------------------------

Title:
A multithread AES accelerator for Cyber-Physical Systems

Abstract: Computing elements of CPSs must be flexible to ensure interoperability; and
adaptive to cope with the evolving internal and external state, such as battery
level and critical tasks. Cryptography is a common task needed in CPSs to
guarantee private communication among different devices. In this work, we
propose a reconfigurable FPGA accelerator for AES workloads with different key
lengths. The accelerator architecture exploits tagged-dataflow models to
support the concurrent execution of multiple threads on the same accelerator.
This solution demonstrates to be more resource- and energy-efficient than a set
of non-reconfigurable accelerators while keeping high performance and
flexibility of execution.


------------------------------------------------------------------------------

Title:
Gender Differences in Abuse: The Case of Dutch Politicians on Twitter

Abstract: Online abuse and threats towards politicians have become a significant
concern in the Netherlands, like in many other countries across the world. This
paper analyses gender differences in abuse received by Dutch politicians on
Twitter, while taking into account the possible additional impact of ethnic
minority status. All tweets directed at party leaders throughout the entire
year of 2022 were collected. The effect of gender and ethnic minority status
were estimated for six different linguistic measures of abuse, namely,
toxicity, severe toxicity, identity attacks, profanity, insults, and threats.
Contrary to expectations, male politicians received higher levels of all forms
of abuse, with the exception of threats, for which no significant gender
difference was found. Significant interaction effects between gender and ethnic
minority status were found for a number of abuse measures. In the case of
severe toxicity, identity attacks, and profanity, female ethnic minority
politicians were more severely impacted than their ethnic majority female
colleagues, but not worse than male politicians. Finally, female ethnic
minority politicians received the highest levels of threats compared to all
groups. Given that online abuse and threats are reported to have a negative
effect on political participation and retention, these results are particularly
worrying.


------------------------------------------------------------------------------

Title:
Scalable Probabilistic Routes

Abstract: Inference and prediction of routes have become of interest over the past
decade owing to a dramatic increase in package delivery and ride-sharing
services. Given the underlying combinatorial structure and the incorporation of
probabilities, route prediction involves techniques from both formal methods
and machine learning. One promising approach for predicting routes uses
decision diagrams that are augmented with probability values. However, the
effectiveness of this approach depends on the size of the compiled decision
diagrams. The scalability of the approach is limited owing to its empirical
runtime and space complexity. In this work, our contributions are two-fold:
first, we introduce a relaxed encoding that uses a linear number of variables
with respect to the number of vertices in a road network graph to significantly
reduce the size of resultant decision diagrams. Secondly, instead of a stepwise
sampling procedure, we propose a single pass sampling-based route prediction.
In our evaluations arising from a real-world road network, we demonstrate that
the resulting system achieves around twice the quality of suggested routes
while being an order of magnitude faster compared to state-of-the-art.


------------------------------------------------------------------------------

Title:
LaDe: The First Comprehensive Last-mile Delivery Dataset from Industry

Abstract: Real-world last-mile delivery datasets are crucial for research in logistics,
supply chain management, and spatio-temporal data mining. Despite a plethora of
algorithms developed to date, no widely accepted, publicly available last-mile
delivery dataset exists to support research in this field. In this paper, we
introduce \texttt{LaDe}, the first publicly available last-mile delivery
dataset with millions of packages from the industry. LaDe has three unique
characteristics: (1) Large-scale. It involves 10,677k packages of 21k couriers
over 6 months of real-world operation. (2) Comprehensive information. It offers
original package information, such as its location and time requirements, as
well as task-event information, which records when and where the courier is
while events such as task-accept and task-finish events happen. (3) Diversity.
The dataset includes data from various scenarios, including package pick-up and
delivery, and from multiple cities, each with its unique spatio-temporal
patterns due to their distinct characteristics such as populations. We verify
LaDe on three tasks by running several classical baseline models per task. We
believe that the large-scale, comprehensive, diverse feature of LaDe can offer
unparalleled opportunities to researchers in the supply chain community, data
mining community, and beyond. The dataset homepage is publicly available at
this https URL


------------------------------------------------------------------------------

Title:
Scheduling with cardinality dependent unavailability periods

Abstract: We consider non-preemptive scheduling problems on parallel identical machines
where machines change their status from being available to being unavailable
and vice versa along the time horizon. The particular form of unavailability we
consider is when the starting time of each downtime depends upon the
cardinality of the job subset processed on that machine since the previous
downtime. We consider the problem of minimizing the makespan in such scenarios
as well as its dual problem where we have a fixed common deadline of $1$ and
the goal is to minimize the number of machines for which there is a feasible
schedule. We develop an EPTAS for the first variant and an AFPTAS for the
second variant.


------------------------------------------------------------------------------

Title:
Collaborative Optimization of Multi-microgrids System with Shared Energy  Storage Based on Multi-agent Stochastic Game and Reinforcement Learning

Abstract: Achieving the economical and stable operation of Multi-microgrids (MMG)
systems is vital. However, there are still some challenging problems to be
solved. Firstly, from the perspective of stable operation, it is necessary to
minimize the energy fluctuation of the main grid. Secondly, the characteristics
of energy conversion equipment need to be considered. Finally, privacy
protection while reducing the operating cost of an MMG system is crucial. To
address these challenges, a Data-driven strategy for MMG systems with Shared
Energy Storage (SES) is proposed. The Mixed-Attention is applied to fit the
conditions of the equipment, additionally, Multi-Agent Soft
Actor-Critic(MA-SAC) and (Multi-Agent Win or Learn Fast Policy
Hill-Climbing)MA-WoLF-PHC are proposed to solve the partially observable
dynamic stochastic game problem. By testing the operation data of the MMG
system in Northwest China, following conclusions are drawn: the R-Square (R2)
values of results reach 0.999, indicating the neural network effectively models
the nonlinear conditions. The proposed MMG system framework can reduce energy
fluctuations in the main grid by 1746.5kW in 24 hours and achieve a cost
reduction of 16.21% in the test. Finally, the superiority of the proposed
algorithms is verified through their fast convergence speed and excellent
optimization performance.


------------------------------------------------------------------------------

Title:
AdaSelection: Accelerating Deep Learning Training through Data  Subsampling

Abstract: In this paper, we introduce AdaSelection, an adaptive sub-sampling method to
identify the most informative sub-samples within each minibatch to speed up the
training of large-scale deep learning models without sacrificing model
performance. Our method is able to flexibly combines an arbitrary number of
baseline sub-sampling methods incorporating the method-level importance and
intra-method sample-level importance at each iteration. The standard practice
of ad-hoc sampling often leads to continuous training with vast amounts of data
from production environments. To improve the selection of data instances during
forward and backward passes, we propose recording a constant amount of
information per instance from these passes. We demonstrate the effectiveness of
our method by testing it across various types of inputs and tasks, including
the classification tasks on both image and language datasets, as well as
regression tasks. Compared with industry-standard baselines, AdaSelection
consistently displays superior performance.


------------------------------------------------------------------------------

Title:
Ethereum Proof-of-Stake Consensus Layer: Participation and  Decentralization

Abstract: In September 2022, Ethereum transitioned from Proof-of-Work (PoW) to
Proof-of-Stake (PoS) during 'the merge' - making it the largest PoS
cryptocurrency in terms of market capitalization. With this work, we present a
comprehensive measurement study of the current state of the Ethereum PoS
consensus layer on the beacon chain. We perform a longitudinal study over the
entire history of the beacon chain, which ranges from 1 December 2020 until 15
May 2023. Our work finds that all dips in network participation, unrelated to
network upgrades, are caused by issues with major consensus clients or service
operators controlling a large number of validators. Thus, we analyze the
decentralization of staking power over time by clustering validators to
entities. We find that the staking power is concentrated in the hands of a few
large entities. Further, we also analyze the consensus client landscape, given
that bugs in a consensus client pose a security risk to the consensus layer.
While the consensus client landscape exhibits significant concentration, with a
single client accounting for one-third of the market share throughout the
entire history of the beacon chain, we observe an improving trend.


------------------------------------------------------------------------------

Title:
Enhancing Generalization and Plasticity for Sample Efficient  Reinforcement Learning

Abstract: In Reinforcement Learning (RL), enhancing sample efficiency is crucial,
particularly in scenarios when data acquisition is costly and risky. In
principle, off-policy RL algorithms can improve sample efficiency by allowing
multiple updates per environment interaction. However, these multiple updates
often lead to overfitting, which decreases the network's ability to adapt to
new data. We conduct an empirical analysis of this challenge and find that
generalizability and plasticity constitute different roles in improving the
model's adaptability. In response, we propose a combined usage of
Sharpness-Aware Minimization (SAM) and a reset mechanism. SAM seeks wide,
smooth minima, improving generalization, while the reset mechanism, through
periodic reinitialization of the last few layers, consistently injects
plasticity into the model. Through extensive empirical studies, we demonstrate
that this combined usage improves sample efficiency and computational cost on
the Atari-100k and DeepMind Control Suite benchmarks.


------------------------------------------------------------------------------

Title:
SegT: A Novel Separated Edge-guidance Transformer Network for Polyp  Segmentation

Abstract: Accurate segmentation of colonoscopic polyps is considered a fundamental step
in medical image analysis and surgical interventions. Many recent studies have
made improvements based on the encoder-decoder framework, which can effectively
segment diverse polyps. Such improvements mainly aim to enhance local features
by using global features and applying attention methods. However, relying only
on the global information of the final encoder block can result in losing local
regional features in the intermediate layer. In addition, determining the edges
between benign regions and polyps could be a challenging task. To address the
aforementioned issues, we propose a novel separated edge-guidance transformer
(SegT) network that aims to build an effective polyp segmentation model. A
transformer encoder that learns a more robust representation than existing
CNN-based approaches was specifically applied. To determine the precise
segmentation of polyps, we utilize a separated edge-guidance module consisting
of separator and edge-guidance blocks. The separator block is a two-stream
operator to highlight edges between the background and foreground, whereas the
edge-guidance block lies behind both streams to strengthen the understanding of
the edge. Lastly, an innovative cascade fusion module was used and fused the
refined multi-level features. To evaluate the effectiveness of SegT, we
conducted experiments with five challenging public datasets, and the proposed
model achieved state-of-the-art performance.


------------------------------------------------------------------------------

Title:
Comparison of L2 Korean pronunciation error patterns from five L1  backgrounds by using automatic phonetic transcription

Abstract: This paper presents a large-scale analysis of L2 Korean pronunciation error
patterns from five different language backgrounds, Chinese, Vietnamese,
Japanese, Thai, and English, by using automatic phonetic transcription. For the
analysis, confusion matrices are generated for each L1, by aligning canonical
phone sequences and automatically transcribed phone sequences obtained from
fine-tuned Wav2Vec2 XLS-R phone recognizer. Each value in the confusion
matrices is compared to capture frequent common error patterns and to specify
patterns unique to a certain language background. Using the Foreign Speakers'
Voice Data of Korean for Artificial Intelligence Learning dataset, common error
pattern types are found to be (1) substitutions of aspirated or tense
consonants with plain consonants, (2) deletions of syllable-final consonants,
and (3) substitutions of diphthongs with monophthongs. On the other hand,
thirty-nine patterns including (1) syllable-final /l/ substitutions with /n/
for Vietnamese and (2) /\textturnm/ insertions for Japanese are discovered as
language-dependent.


------------------------------------------------------------------------------

Title:
Seamless Redundancy for High Reliability Wi-Fi

Abstract: By removing wire harness, Wi-Fi is becoming increasingly pervasive in every
aspect of our lives, in both the consumer and industrial worlds. Besides
flexibility, the recent high efficiency and extremely high throughput versions
managed to close the performance gap with Ethernet. However, it still lags
behind Ethernet for what concerns dependability. To this aim, the ultra high
reliability study group has been recently formed. This paper reports on some
preliminary ideas and proposals about the ways seamless redundancy can be
exploited to make Wi-Fi more reliable, yet retaining a good degree of backward
compatibility with existing network infrastructures.


------------------------------------------------------------------------------

Title:
Simplifying and Empowering Transformers for Large-Graph Representations

Abstract: Learning representations on large-sized graphs is a long-standing challenge
due to the inter-dependence nature involved in massive data points.
Transformers, as an emerging class of foundation encoders for graph-structured
data, have shown promising performance on small graphs due to its global
attention capable of capturing all-pair influence beyond neighboring nodes.
Even so, existing approaches tend to inherit the spirit of Transformers in
language and vision tasks, and embrace complicated models by stacking deep
multi-head attentions. In this paper, we critically demonstrate that even using
a one-layer attention can bring up surprisingly competitive performance across
node property prediction benchmarks where node numbers range from
thousand-level to billion-level. This encourages us to rethink the design
philosophy for Transformers on large graphs, where the global attention is a
computation overhead hindering the scalability. We frame the proposed scheme as
Simplified Graph Transformers (SGFormer), which is empowered by a simple
attention model that can efficiently propagate information among arbitrary
nodes in one layer. SGFormer requires none of positional encodings,
feature/graph pre-processing or augmented loss. Empirically, SGFormer
successfully scales to the web-scale graph ogbn-papers100M and yields up to
141x inference acceleration over SOTA Transformers on medium-sized graphs.
Beyond current results, we believe the proposed methodology alone enlightens a
new technical path of independent interest for building Transformers on large
graphs.


------------------------------------------------------------------------------

Title:
Categories of Response-Based, Feature-Based, and Relation-Based  Knowledge Distillation

Abstract: Deep neural networks have achieved remarkable performance for artificial
intelligence tasks. The success behind intelligent systems often relies on
large-scale models with high computational complexity and storage costs. The
over-parameterized networks are often easy to optimize and can achieve better
performance. However, it is challenging to deploy them over resource-limited
edge-devices. Knowledge Distillation (KD) aims to optimize a lightweight
network from the perspective of over-parameterized training. The traditional
offline KD transfers knowledge from a cumbersome teacher to a small and fast
student network. When a sizeable pre-trained teacher network is unavailable,
online KD can improve a group of models by collaborative or mutual learning.
Without needing extra models, Self-KD boosts the network itself using attached
auxiliary architectures. KD mainly involves knowledge extraction and
distillation strategies these two aspects. Beyond KD schemes, various KD
algorithms are widely used in practical applications, such as multi-teacher KD,
cross-modal KD, attention-based KD, data-free KD and adversarial KD. This paper
provides a comprehensive KD survey, including knowledge categories,
distillation schemes and algorithms, as well as some empirical studies on
performance comparison. Finally, we discuss the open challenges of existing KD
works and prospect the future directions.


------------------------------------------------------------------------------

Title:
Renderers are Good Zero-Shot Representation Learners: Exploring  Diffusion Latents for Metric Learning

Abstract: Can the latent spaces of modern generative neural rendering models serve as
representations for 3D-aware discriminative visual understanding tasks? We use
retrieval as a proxy for measuring the metric learning properties of the latent
spaces of Shap-E, including capturing view-independence and enabling the
aggregation of scene representations from the representations of individual
image views, and find that Shap-E representations outperform those of the
classical EfficientNet baseline representations zero-shot, and is still
competitive when both methods are trained using a contrative loss. These
findings give preliminary indication that 3D-based rendering and generative
models can yield useful representations for discriminative tasks in our
innately 3D-native world. Our code is available at
\url{this https URL}.


------------------------------------------------------------------------------

Title:
Variability of echo state network prediction horizon for partially  observed dynamical systems

Abstract: Study of dynamical systems using partial state observation is an important
problem due to its applicability to many real-world systems. We address the
problem by proposing an echo state network (ESN) framework with partial state
input with partial or full state output. The Lorenz system and Chua's
oscillator (both numerically simulated and experimental systems) are used to
check the effectiveness of our method. We demonstrate that the ESN, as an
autonomous dynamical system, is capable of making short-term predictions up to
a few Lyapunov times. However, the prediction horizon has high variability
depending on the initial condition -- an aspect that we explore in detail.
Further, using a variety of statistical metrics to compare the long-term
dynamics of the ESN predictions with numerically simulated or experimental
dynamics and observed similar results, we show that the ESN can effectively
learn the system's dynamics even when trained with noisy numerical or
experimental datasets. Thus, we demonstrate the potential of ESNs to serve as a
cheap surrogate model for predicting the dynamics of systems where complete
observations are unavailable.


------------------------------------------------------------------------------

Title:
Robust Defect Detection with Contrastive Localization

Abstract: Defect detection aims to detect and localize regions out of the normal
distribution. Previous works rely on modeling the normality to identify the
defective regions, which may lead to non-ideal generalizability. This paper
proposed a one-stage framework that detects defective patterns directly without
the modeling process. This ability is adopted through the joint efforts of
three parties: a generative adversarial network (GAN), a newly proposed scaled
pattern loss, and a dynamic masked cycle-consistent auxiliary network. Explicit
information that could indicate the position of defects is intentionally
excluded to avoid learning any direct mapping. Experimental results on the
texture class of the challenging MVTec AD dataset show that the proposed method
is 2.9\% higher than the SOTA methods in F1-Score, while substantially
outperforming SOTA methods in generalizability.


------------------------------------------------------------------------------

Title:
Partial-order Checking with Unfolding for Linear Temporal Properties

Abstract: Unfolding can tackle the path-explosion problem caused by concurrency.
Traditional unfolding generation faces an NP-complete problem when adding
events to the unfolding structure, which also exists in the case of verifying
linear temporal logic (LTL). The reason is that it is necessary to enumerate
possible concurrent event combinations after adding an event. Many
state-of-the-art methods optimally explore unfolding-based structure (called
event structure) by a tree-like structure, which should be constructed on the
event structure with complete conflict and causal relations. However, a
synchronization of a Petri net and the Buchi representation of LTL as a folded
net can not represent complete conflict and causal relations. Thus, it is
difficult to apply such a tree-like structure directly on the folded net. To
resolve this difficulty, we propose a new method, called partial-order checking
with unfolding, to verify LTL based on PDNet (program dependence net). We
define an exploration tree with a new notion of delayed transitions, which is
different from the existing tree-like structure. It improves the unfolding
generation by avoiding all possible event combinations. Then, we propose an
algorithm to simultaneously construct the exploration tree while generating the
unfolding structure, as well as checking LTL. We implement a tool PUPER for
concurrent programs with POSIX threads. It improves traditional unfolding
generations via our exploration tree-based algorithms and shows better
performance than SPIN and DiVine on the used benchmarks.


------------------------------------------------------------------------------

Title:
Efficient Parameterized Pattern Matching in Sublinear Space

Abstract: The parameterized matching problem is a variant of string matching, which is
to search for all parameterized occurrences of a pattern $P$ in a text $T$. In
considering matching algorithms, the combinatorial natures of strings,
especially periodicity, play an important role. In this paper, we analyze the
properties of periods of parameterized strings and propose a generalization of
Galil and Seiferas's exact matching algorithm (1980) into parameterized
matching, which runs in $O(\pi|T|+|P|)$ time and $O(\log{|P|}+|{\rm\Pi}|)$
space in addition to the input space, where ${\rm\Pi}$ is the parameter
alphabet and $\pi$ is the number of parameter characters appearing in $P$ plus
one.


------------------------------------------------------------------------------

Title:
Partial Hypernetworks for Continual Learning

Abstract: Hypernetworks mitigate forgetting in continual learning (CL) by generating
task-dependent weights and penalizing weight changes at a meta-model level.
Unfortunately, generating all weights is not only computationally expensive for
larger architectures, but also, it is not well understood whether generating
all model weights is necessary. Inspired by latent replay methods in CL, we
propose partial weight generation for the final layers of a model using
hypernetworks while freezing the initial layers. With this objective, we first
answer the question of how many layers can be frozen without compromising the
final performance. Through several experiments, we empirically show that the
number of layers that can be frozen is proportional to the distributional
similarity in the CL stream. Then, to demonstrate the effectiveness of
hypernetworks, we show that noisy streams can significantly impact the
performance of latent replay methods, leading to increased forgetting when
features from noisy experiences are replayed with old samples. In contrast,
partial hypernetworks are more robust to noise by maintaining accuracy on
previous experiences. Finally, we conduct experiments on the split CIFAR-100
and TinyImagenet benchmarks and compare different versions of partial
hypernetworks to latent replay methods. We conclude that partial weight
generation using hypernetworks is a promising solution to the problem of
forgetting in neural networks. It can provide an effective balance between
computation and final test accuracy in CL streams.


------------------------------------------------------------------------------

Title:
A bounded partition approach to identifying one fake coin and its type

Abstract: Fake coin problems using balance scales to identify one fake coin and its
type among n coins (n > 2) were solved by Dyson in 1946. Dyson gave adaptive
solutions with the minimum number of weighings where later weighings may be
dependent on results of past weighings. In 2003 Born et al. gave non-adaptive
solutions where all weighings are predetermined. Both solutions require the
computation of a Dyson set, which is a list of placement of each coin for each
weighing. The computation of a Dyson set requires substantial amount of time
when n gets larger. We present a bounded partition approach to the fake coin
problems without computing any Dyson set. Our approach uses bounded partition
of coins recursively until the problem size may be small enough to use at most
one weighing.


------------------------------------------------------------------------------

Title:
MB-HGCN: A Hierarchical Graph Convolutional Network for Multi-behavior  Recommendation

Abstract: Collaborative filtering-based recommender systems that rely on a single type
of behavior often encounter serious sparsity issues in real-world applications,
leading to unsatisfactory performance. Multi-behavior Recommendation (MBR) is a
method that seeks to learn user preferences, represented as vector embeddings,
from auxiliary information. By leveraging these preferences for target behavior
recommendations, MBR addresses the sparsity problem and improves the accuracy
of recommendations. In this paper, we propose MB-HGCN, a novel multi-behavior
recommendation model that uses a hierarchical graph convolutional network to
learn user and item embeddings from coarse-grained on the global level to
fine-grained on the behavior-specific level. Our model learns global embeddings
from a unified homogeneous graph constructed by the interactions of all
behaviors, which are then used as initialized embeddings for behavior-specific
embedding learning in each behavior graph. We also emphasize the distinct of
the user and item behaviorspecific embeddings and design two
simple-yet-effective strategies to aggregate the behavior-specific embeddings
for users and items, respectively. Finally, we adopt multi-task learning for
optimization. Extensive experimental results on three real-world datasets
demonstrate that our model significantly outperforms the baselines, achieving a
relative improvement of 73.93% and 74.21% for HR@10 and NDCG@10, respectively,
on the Tmall datasets.


------------------------------------------------------------------------------

Title:
Perturbation-Based Two-Stage Multi-Domain Active Learning

Abstract: In multi-domain learning (MDL) scenarios, high labeling effort is required
due to the complexity of collecting data from various domains. Active Learning
(AL) presents an encouraging solution to this issue by annotating a smaller
number of highly informative instances, thereby reducing the labeling effort.
Previous research has relied on conventional AL strategies for MDL scenarios,
which underutilize the domain-shared information of each instance during the
selection procedure. To mitigate this issue, we propose a novel
perturbation-based two-stage multi-domain active learning (P2S-MDAL) method
incorporated into the well-regarded ASP-MTL model. Specifically, P2S-MDAL
involves allocating budgets for domains and establishing regions for diversity
selection, which are further used to select the most cross-domain influential
samples in each region. A perturbation metric has been introduced to evaluate
the robustness of the shared feature extractor of the model, facilitating the
identification of potentially cross-domain influential samples. Experiments are
conducted on three real-world datasets, encompassing both texts and images. The
superior performance over conventional AL strategies shows the effectiveness of
the proposed strategy. Additionally, an ablation study has been carried out to
demonstrate the validity of each component. Finally, we outline several
intriguing potential directions for future MDAL research, thus catalyzing the
field's advancement.


------------------------------------------------------------------------------

Title:
A Preliminary Study of ChatGPT on News Recommendation: Personalization,  Provider Fairness, Fake News

Abstract: Online news platforms commonly employ personalized news recommendation
methods to assist users in discovering interesting articles, and many previous
works have utilized language model techniques to capture user interests and
understand news content. With the emergence of large language models like GPT-3
and T-5, a new recommendation paradigm has emerged, leveraging pre-trained
language models for making recommendations. ChatGPT, with its user-friendly
interface and growing popularity, has become a prominent choice for text-based
tasks. Considering the growing reliance on ChatGPT for language tasks, the
importance of news recommendation in addressing social issues, and the trend of
using language models in recommendations, this study conducts an initial
investigation of ChatGPT's performance in news recommendations, focusing on
three perspectives: personalized news recommendation, news provider fairness,
and fake news detection. ChatGPT has the limitation that its output is
sensitive to the input phrasing. We therefore aim to explore the constraints
present in the generated responses of ChatGPT for each perspective.
Additionally, we investigate whether specific prompt formats can alleviate
these constraints or if these limitations require further attention from
researchers in the future. We also surpass fixed evaluations by developing a
webpage to monitor ChatGPT's performance on weekly basis on the tasks and
prompts we investigated. Our aim is to contribute to and encourage more
researchers to engage in the study of enhancing news recommendation performance
through the utilization of large language models such as ChatGPT.


------------------------------------------------------------------------------

Title:
Controlling Type Confounding in Ad Hoc Teamwork with Instance-wise  Teammate Feedback Rectification

Abstract: Ad hoc teamwork requires an agent to cooperate with unknown teammates without
prior coordination. Many works propose to abstract teammate instances into
high-level representation of types and then pre-train the best response for
each type. However, most of them do not consider the distribution of teammate
instances within a type. This could expose the agent to the hidden risk of
\emph{type confounding}. In the worst case, the best response for an abstract
teammate type could be the worst response for all specific instances of that
type. This work addresses the issue from the lens of causal inference. We first
theoretically demonstrate that this phenomenon is due to the spurious
correlation brought by uncontrolled teammate distribution. Then, we propose our
solution, CTCAT, which disentangles such correlation through an instance-wise
teammate feedback rectification. This operation reweights the interaction of
teammate instances within a shared type to reduce the influence of type
confounding. The effect of CTCAT is evaluated in multiple domains, including
classic ad hoc teamwork tasks and real-world scenarios. Results show that CTCAT
is robust to the influence of type confounding, a practical issue that directly
hazards the robustness of our trained agents but was unnoticed in previous
works.


------------------------------------------------------------------------------

Title:
Deep Reinforcement Learning with Multitask Episodic Memory Based on  Task-Conditioned Hypernetwork

Abstract: Deep reinforcement learning algorithms are usually impeded by sampling
inefficiency, heavily depending on multiple interactions with the environment
to acquire accurate decision-making capabilities. In contrast, humans seem to
rely on their hippocampus to retrieve relevant information from past
experiences of relevant tasks, which guides their decision-making when learning
a new task, rather than exclusively depending on environmental interactions.
Nevertheless, designing a hippocampus-like module for an agent to incorporate
past experiences into established reinforcement learning algorithms presents
two challenges. The first challenge involves selecting the most relevant past
experiences for the current task, and the second is integrating such
experiences into the decision network. To address these challenges, we propose
a novel algorithm that utilizes a retrieval network based on a task-conditioned
hypernetwork, which adapts the retrieval network's parameters depending on the
task. At the same time, a dynamic modification mechanism enhances the
collaborative efforts between the retrieval and decision networks. We evaluate
the proposed algorithm on the challenging MiniGrid environment. The
experimental results demonstrate that our proposed method significantly
outperforms strong baselines.


------------------------------------------------------------------------------

Title:
FDNet: Focal Decomposed Network for Efficient, Robust and Practical Time  Series Forecasting

Abstract: This paper presents FDNet: a Focal Decomposed Network for efficient, robust
and practical time series forecasting. We break away from conventional deep
time series forecasting formulas which obtain prediction results from universal
feature maps of input sequences. In contrary, FDNet neglects universal
correlations of input elements and only extracts fine-grained local features
from input sequence. We show that: (1) Deep time series forecasting with only
fine-grained local feature maps of input sequence is feasible upon theoretical
basis. (2) By abandoning global coarse-grained feature maps, FDNet overcomes
distribution shift problem caused by changing dynamics of time series which is
common in real-world applications. (3) FDNet is not dependent on any inductive
bias of time series except basic auto-regression, making it general and
practical. Moreover, we propose focal input sequence decomposition method which
decomposes input sequence in a focal manner for efficient and robust
forecasting when facing Long Sequence Time series Input (LSTI) problem. FDNet
achieves competitive forecasting performances on six real-world benchmarks and
reduces prediction MSE by 38.4% on average compared with other thirteen SOTA
baselines. The source code is available at this https URL


------------------------------------------------------------------------------

Title:
NAR-Former V2: Rethinking Transformer for Universal Neural Network  Representation Learning

Abstract: As more deep learning models are being applied in real-world applications,
there is a growing need for modeling and learning the representations of neural
networks themselves. An efficient representation can be used to predict target
attributes of networks without the need for actual training and deployment
procedures, facilitating efficient network deployment and design. Recently,
inspired by the success of Transformer, some Transformer-based representation
learning frameworks have been proposed and achieved promising performance in
handling cell-structured models. However, graph neural network (GNN) based
approaches still dominate the field of learning representation for the entire
network. In this paper, we revisit Transformer and compare it with GNN to
analyse their different architecture characteristics. We then propose a
modified Transformer-based universal neural network representation learning
model NAR-Former V2. It can learn efficient representations from both
cell-structured networks and entire networks. Specifically, we first take the
network as a graph and design a straightforward tokenizer to encode the network
into a sequence. Then, we incorporate the inductive representation learning
capability of GNN into Transformer, enabling Transformer to generalize better
when encountering unseen architecture. Additionally, we introduce a series of
simple yet effective modifications to enhance the ability of the Transformer in
learning representation from graph structures. Our proposed method surpasses
the GNN-based method NNLP by a significant margin in latency estimation on the
NNLQP dataset. Furthermore, regarding accuracy prediction on the NASBench101
and NASBench201 datasets, our method achieves highly comparable performance to
other state-of-the-art methods.


------------------------------------------------------------------------------

Title:
WiCo: Win-win Cooperation of Bottom-up and Top-down Referring Image  Segmentation

Abstract: The top-down and bottom-up methods are two mainstreams of referring
segmentation, while both methods have their own intrinsic weaknesses. Top-down
methods are chiefly disturbed by Polar Negative (PN) errors owing to the lack
of fine-grained cross-modal alignment. Bottom-up methods are mainly perturbed
by Inferior Positive (IP) errors due to the lack of prior object information.
Nevertheless, we discover that two types of methods are highly complementary
for restraining respective weaknesses but the direct average combination leads
to harmful interference. In this context, we build Win-win Cooperation (WiCo)
to exploit complementary nature of two types of methods on both interaction and
integration aspects for achieving a win-win improvement. For the interaction
aspect, Complementary Feature Interaction (CFI) provides fine-grained
information to top-down branch and introduces prior object information to
bottom-up branch for complementary feature enhancement. For the integration
aspect, Gaussian Scoring Integration (GSI) models the gaussian performance
distributions of two branches and weightedly integrates results by sampling
confident scores from the distributions. With our WiCo, several prominent
top-down and bottom-up combinations achieve remarkable improvements on three
common datasets with reasonable extra costs, which justifies effectiveness and
generality of our method.


------------------------------------------------------------------------------

Title:
BNN-DP: Robustness Certification of Bayesian Neural Networks via Dynamic  Programming

Abstract: In this paper, we introduce BNN-DP, an efficient algorithmic framework for
analysis of adversarial robustness of Bayesian Neural Networks (BNNs). Given a
compact set of input points $T\subset \mathbb{R}^n$, BNN-DP computes lower and
upper bounds on the BNN's predictions for all the points in $T$. The framework
is based on an interpretation of BNNs as stochastic dynamical systems, which
enables the use of Dynamic Programming (DP) algorithms to bound the prediction
range along the layers of the network. Specifically, the method uses bound
propagation techniques and convex relaxations to derive a backward recursion
procedure to over-approximate the prediction range of the BNN with piecewise
affine functions. The algorithm is general and can handle both regression and
classification tasks. On a set of experiments on various regression and
classification tasks and BNN architectures, we show that BNN-DP outperforms
state-of-the-art methods by up to four orders of magnitude in both tightness of
the bounds and computational efficiency.


------------------------------------------------------------------------------

Title:
Unsupervised Open-domain Keyphrase Generation

Abstract: In this work, we study the problem of unsupervised open-domain keyphrase
generation, where the objective is a keyphrase generation model that can be
built without using human-labeled data and can perform consistently across
domains. To solve this problem, we propose a seq2seq model that consists of two
modules, namely \textit{phraseness} and \textit{informativeness} module, both
of which can be built in an unsupervised and open-domain fashion. The
phraseness module generates phrases, while the informativeness module guides
the generation towards those that represent the core concepts of the text. We
thoroughly evaluate our proposed method using eight benchmark datasets from
different domains. Results on in-domain datasets show that our approach
achieves state-of-the-art results compared with existing unsupervised models,
and overall narrows the gap between supervised and unsupervised methods down to
about 16\%. Furthermore, we demonstrate that our model performs consistently
across domains, as it overall surpasses the baselines on out-of-domain
datasets.


------------------------------------------------------------------------------

Title:
Least Square Value Iteration is Robust Under Locally Bounded  Misspecification Error

Abstract: The success of reinforcement learning heavily relies on the function
approximation of policy, value or models, where misspecification (a mismatch
between the ground-truth and best function approximators) naturally occurs
especially when the ground-truth is complex. As misspecification error does not
vanish even with infinite number of samples, designing algorithms that are
robust under misspecification is of paramount importance. Recently, it is shown
that policy-based approaches can be robust even when the policy function
approximation is under a large locally-bounded misspecification error, with
which the function class may have $\Omega(1)$ approximation error in certain
states and actions but is only small on average under a policy-induced
state-distribution; whereas it is only known that value-based approach can
effectively learn under globally-bounded misspecification error, i.e., the
approximation errors to value functions have a uniform upper bound on all
state-actions. Yet it remains an open question whether similar robustness can
be achieved with value-based approaches. In this paper, we answer this question
affirmatively by showing that the algorithm, Least-Square-Value-Iteration [Jin
et al, 2020], with carefully designed exploration bonus can achieve robustness
under local misspecification error bound. In particular, we show that algorithm
achieves a regret bound of $\widetilde{O}\left(\sqrt{d^3KH^4} + dKH^2\zeta
\right)$, where $d$ is the dimension of linear features, $H$ is the length of
the episode, $K$ is the total number of episodes, and $\zeta$ is the local
bound of the misspecification error. Moreover, we show that the algorithm can
achieve the same regret bound without knowing $\zeta$ and can be used as robust
policy evaluation oracle that can be applied to improve sample complexity in
policy-based approaches.


------------------------------------------------------------------------------

Title:
Pneumatic bellows actuated parallel platform control with adjustable  stiffness using a hybrid feed-forward and variable gain I-controller

Abstract: Redundant cascade manipulators actuated by pneumatic bellows actuators are
passively compliant, rugged and dexterous which are qualities making them
exceptionally well suited for applications in agriculture. Unfortunately
bellows actuators are notoriously difficult to precisely position. This paper
presents a novel control algorithm for the control of a parallel platform
actuated by pneumatic bellows actuators, which is serving as one module of a
cascade manipulator. The algorithm combines a feed-forward controller and a
variable gain I-controller. The feed-forward controller was designed using
experimental data and two regression steps to create a mathematical
representation of the data. The gain of the I-controller depends linearly on
the total reference error, which allows the I-controller to work in concert
with the feed-forward part of the controller. The presented algorithm was
experimentally verified and its performance was compared with two controllers,
an ANFIS controller and a constant gain PID controller, to satisfactory
results. The controller was also tested under dynamic loading conditions
showing promising results.


------------------------------------------------------------------------------

Title:
Jamp: Controlled Japanese Temporal Inference Dataset for Evaluating  Generalization Capacity of Language Models

Abstract: Natural Language Inference (NLI) tasks involving temporal inference remain
challenging for pre-trained language models (LMs). Although various datasets
have been created for this task, they primarily focus on English and do not
address the need for resources in other languages. It is unclear whether
current LMs realize the generalization capacity for temporal inference across
languages. In this paper, we present Jamp, a Japanese NLI benchmark focused on
temporal inference. Our dataset includes a range of temporal inference
patterns, which enables us to conduct fine-grained analysis. To begin the data
annotation process, we create diverse inference templates based on the formal
semantics test suites. We then automatically generate diverse NLI examples by
using the Japanese case frame dictionary and well-designed templates while
controlling the distribution of inference patterns and gold labels. We evaluate
the generalization capacities of monolingual/multilingual LMs by splitting our
dataset based on tense fragments (i.e., temporal inference patterns). Our
findings demonstrate that LMs struggle with specific linguistic phenomena, such
as habituality, indicating that there is potential for the development of more
effective NLI models across languages.


------------------------------------------------------------------------------

Title:
COLE: A Column-based Learned Storage for Blockchain Systems

Abstract: Blockchain systems suffer from high storage costs as every node needs to
store and maintain the entire blockchain data. After investigating Ethereum's
storage, we find that the storage cost mostly comes from the index, i.e.,
Merkle Patricia Trie (MPT), that is used to guarantee data integrity and
support provenance queries. To reduce the index storage overhead, an initial
idea is to leverage the emerging learned index technique, which has been shown
to have a smaller index size and more efficient query performance. However,
directly applying it to the blockchain storage results in even higher overhead
owing to the blockchain's persistence requirement and the learned index's large
node size. Meanwhile, existing learned indexes are designed for in-memory
databases, whereas blockchain systems require disk-based storage and feature
frequent data updates. To address these challenges, we propose COLE, a novel
column-based learned storage for blockchain systems. We follow the column-based
database design to contiguously store each state's historical values, which are
indexed by learned models to facilitate efficient data retrieval and provenance
queries. We develop a series of write-optimized strategies to realize COLE in
disk environments. Extensive experiments are conducted to validate the
performance of the proposed COLE system. Compared with MPT, COLE reduces the
storage size by up to 94% while improving the system throughput by 1.4X-5.4X.


------------------------------------------------------------------------------

Title:
PartSLAM: Unsupervised Part-based Scene Modeling for Fast Succinct Map  Matching

Abstract: In this paper, we explore the challenging 1-to-N map matching problem, which
exploits a compact description of map data, to improve the scalability of map
matching techniques used by various robot vision tasks. We propose a first
method explicitly aimed at fast succinct map matching, which consists only of
map-matching subtasks. These tasks include offline map matching attempts to
find a compact part-based scene model that effectively explains each map using
fewer larger parts. The tasks also include an online map matching attempt to
efficiently find correspondence between the part-based maps. Our part-based
scene modeling approach is unsupervised and uses common pattern discovery (CPD)
between the input and known reference maps. This enables a robot to learn a
compact map model without human intervention. We also present a practical
implementation that uses the state-of-the-art CPD technique of randomized
visual phrases (RVP) with a compact bounding box (BB) based part descriptor,
which consists of keypoint and descriptor BBs. The results of our challenging
map-matching experiments, which use a publicly available radish dataset, show
that the proposed approach achieves successful map matching with significant
speedup and a compact description of map data that is tens of times more
compact. Although this paper focuses on the standard 2D point-set map and the
BB-based part representation, we believe our approach is sufficiently general
to be applicable to a broad range of map formats, such as the 3D point cloud
map, as well as to general bounding volumes and other compact part
representations.


------------------------------------------------------------------------------

Title:
Impact of Dynamic Tariffs for Smart EV Charging on LV Distribution  Network Operation

Abstract: With a growing share of electric vehicles (EVs) in our distribution grids,
the need for smart charging becomes indispensable to minimise grid
reinforcement. To circumvent the associated capacity limitations, this paper
evaluates the effectiveness of different levels of network constraints and
different dynamic tariffs, including a dynamic network tariff. A detailed
optimisation model is first developed for public charging electric vehicles in
a representative Dutch low voltage (LV) distribution network, susceptible to
congestion and voltage problems by 2050 without smart charging of EVs. Later, a
detailed reflection is made to assess the influence of the modelled features on
the distribution system operator (DSO), charge point operator (CPO) costs, and
the EVs' final state-of-charge (SOC) for both mono- (V1G) and bi-directional
(V2G) charging. Results show that the dynamic network tariff outperforms other
flat tariffs by increasing valley-filling. Consequently, compared to regular
day-ahead pricing, a {significant} reduction in the frequency of congestion in
the lines is achieved. In addition, V2G ensures the joint optimum for different
stakeholders causing adequate EV user satisfaction, decreased CPO costs
compared to conventional charging and fewer violations of grid constraints for
the DSOs.


------------------------------------------------------------------------------

Title:
Machine Learning for Real-Time Anomaly Detection in Optical Networks

Abstract: This work proposes a real-time anomaly detection scheme that leverages the
multi-step ahead prediction capabilities of encoder-decoder (ED) deep learning
models with recurrent units. Specifically, an encoder-decoder is used to model
soft-failure evolution over a long future horizon (i.e., for several days
ahead) by analyzing past quality-of-transmission (QoT) observations. This
information is subsequently used for real-time anomaly detection (e.g., of
attack incidents), as the knowledge of how the QoT is expected to evolve allows
capturing unexpected network behavior. Specifically, for anomaly detection, a
statistical hypothesis testing scheme is used, alleviating the limitations of
supervised (SL) and unsupervised learning (UL) schemes, usually applied for
this purpose. Indicatively, the proposed scheme eliminates the need for labeled
anomalies, required when SL is applied, and the need for on-line analyzing
entire datasets to identify abnormal instances (i.e., UL). Overall, it is shown
that by utilizing QoT evolution information, the proposed approach can
effectively detect abnormal deviations in real-time. Importantly, it is shown
that the information concerning soft-failure evolution (i.e., QoT predictions)
is essential to accurately detect anomalies.


------------------------------------------------------------------------------

Title:
Guiding Language Models of Code with Global Context using Monitors

Abstract: Language models of code (LMs) work well when the surrounding code in the
vicinity of generation provides sufficient context. This is not true when it
becomes necessary to use types or functionality defined in another module or
library, especially those not seen during training. LMs suffer from limited
awareness of such global context and end up hallucinating, e.g., using types
defined in other files incorrectly. Recent work tries to overcome this issue by
retrieving global information to augment the local context. However, this
bloats the prompt or requires architecture modifications and additional
training.
Integrated development environments (IDEs) assist developers by bringing the
global context at their fingertips using static analysis. We extend this
assistance, enjoyed by developers, to the LMs. We propose a notion of monitors
that use static analysis in the background to guide the decoding. Unlike a
priori retrieval, static analysis is invoked iteratively during the entire
decoding process, providing the most relevant suggestions on demand. We
demonstrate the usefulness of our proposal by monitoring for type-consistent
use of identifiers whenever an LM generates code for object dereference.
To evaluate our approach, we curate PragmaticCode, a dataset of open-source
projects with their development environments. On models of varying parameter
scale, we show that monitor-guided decoding consistently improves the ability
of an LM to not only generate identifiers that match the ground truth but also
improves compilation rates and agreement with ground truth. We find that LMs
with fewer parameters, when guided with our monitor, can outperform larger LMs.
With monitor-guided decoding, SantaCoder-1.1B achieves better compilation rate
and next-identifier match than the much larger text-davinci-003 model. The
datasets and code will be released at this https URL .


------------------------------------------------------------------------------

Title:
Design of an Axial Flux Permanent Magnet Eddy Current Brake for  Application on Light Weight Motor Vehicles

Abstract: Axial flux permanent magnet designs are compact and becoming an attractive
design for electric vehicles as an auxiliary braking system. This work develops
the design of an Axial Flux Permanent Magnet Eddy current brake for application
on Light Weight Motor Vehicles as guided by industry regulations. The work
makes a link between the common Finite Element Method approach used in the
literature and the Analytical approach previously done. The design is conducted
to meet the torque requirements per wheel. The average torque over the
operating range closest to the design requirement was used as the solution.


------------------------------------------------------------------------------

Title:
SeMAIL: Eliminating Distractors in Visual Imitation via Separated Models

Abstract: Model-based imitation learning (MBIL) is a popular reinforcement learning
method that improves sample efficiency on high-dimension input sources, such as
images and videos. Following the convention of MBIL research, existing
algorithms are highly deceptive by task-irrelevant information, especially
moving distractors in videos. To tackle this problem, we propose a new
algorithm - named Separated Model-based Adversarial Imitation Learning (SeMAIL)
- decoupling the environment dynamics into two parts by task-relevant
dependency, which is determined by agent actions, and training separately. In
this way, the agent can imagine its trajectories and imitate the expert
behavior efficiently in task-relevant state space. Our method achieves
near-expert performance on various visual control tasks with complex
observations and the more challenging tasks with different backgrounds from
expert observations.


------------------------------------------------------------------------------

Title:
A neuro-symbolic approach for multimodal reference expression  comprehension

Abstract: Human-Machine Interaction (HMI) systems have gained huge interest in recent
years, with reference expression comprehension being one of the main
challenges. Traditionally human-machine interaction has been mostly limited to
speech and visual modalities. However, to allow for more freedom in
interaction, recent works have proposed the integration of additional
modalities, such as gestures in HMI systems. We consider such an HMI system
with pointing gestures and construct a table-top object picking scenario inside
a simulated virtual reality (VR) environment to collect data. Previous works
for such a task have used deep neural networks to classify the referred object,
which lacks transparency. In this work, we propose an interpretable and
compositional model, crucial to building robust HMI systems for real-world
application, based on a neuro-symbolic approach to tackle this task. Finally we
also show the generalizability of our model on unseen environments and report
the results.


------------------------------------------------------------------------------

Title:
Frame Fusion with Vehicle Motion Prediction for 3D Object Detection

Abstract: In LiDAR-based 3D detection, history point clouds contain rich temporal
information helpful for future prediction. In the same way, history detections
should contribute to future detections. In this paper, we propose a detection
enhancement method, namely FrameFusion, which improves 3D object detection
results by fusing history frames. In FrameFusion, we ''forward'' history frames
to the current frame and apply weighted Non-Maximum-Suppression on dense
bounding boxes to obtain a fused frame with merged boxes. To ''forward''
frames, we use vehicle motion models to estimate the future pose of the
bounding boxes. However, the commonly used constant velocity model fails
naturally on turning vehicles, so we explore two vehicle motion models to
address this issue. On Waymo Open Dataset, our FrameFusion method consistently
improves the performance of various 3D detectors by about $2$ vehicle level 2
APH with negligible latency and slightly enhances the performance of the
temporal fusion method MPPNet. We also conduct extensive experiments on motion
model selection.


------------------------------------------------------------------------------

Title:
Dual-view Correlation Hybrid Attention Network for Robust Holistic  Mammogram Classification

Abstract: Mammogram image is important for breast cancer screening, and typically
obtained in a dual-view form, i.e., cranio-caudal (CC) and mediolateral oblique
(MLO), to provide complementary information. However, previous methods mostly
learn features from the two views independently, which violates the clinical
knowledge and ignores the importance of dual-view correlation. In this paper,
we propose a dual-view correlation hybrid attention network (DCHA-Net) for
robust holistic mammogram classification. Specifically, DCHA-Net is carefully
designed to extract and reinvent deep features for the two views, and meanwhile
to maximize the underlying correlations between them. A hybrid attention
module, consisting of local relation and non-local attention blocks, is
proposed to alleviate the spatial misalignment of the paired views in the
correlation maximization. A dual-view correlation loss is introduced to
maximize the feature similarity between corresponding strip-like regions with
equal distance to the chest wall, motivated by the fact that their features
represent the same breast tissues, and thus should be highly-correlated.
Experimental results on two public datasets, i.e., INbreast and CBIS-DDSM,
demonstrate that DCHA-Net can well preserve and maximize feature correlations
across views, and thus outperforms the state-of-the-arts for classifying a
whole mammogram as malignant or not.


------------------------------------------------------------------------------

Title:
Visually-Guided Sound Source Separation with Audio-Visual Predictive  Coding

Abstract: The framework of visually-guided sound source separation generally consists
of three parts: visual feature extraction, multimodal feature fusion, and sound
signal processing. An ongoing trend in this field has been to tailor involved
visual feature extractor for informative visual guidance and separately devise
module for feature fusion, while utilizing U-Net by default for sound analysis.
However, such divide-and-conquer paradigm is parameter inefficient and,
meanwhile, may obtain suboptimal performance as jointly optimizing and
harmonizing various model components is challengeable. By contrast, this paper
presents a novel approach, dubbed audio-visual predictive coding (AVPC), to
tackle this task in a parameter efficient and more effective manner. The
network of AVPC features a simple ResNet-based video analysis network for
deriving semantic visual features, and a predictive coding-based sound
separation network that can extract audio features, fuse multimodal
information, and predict sound separation masks in the same architecture. By
iteratively minimizing the prediction error between features, AVPC integrates
audio and visual information recursively, leading to progressively improved
performance. In addition, we develop a valid self-supervised learning strategy
for AVPC via co-predicting two audio-visual representations of the same sound
source. Extensive evaluations demonstrate that AVPC outperforms several
baselines in separating musical instrument sounds, while reducing the model
size significantly. Code is available at:
this https URL


------------------------------------------------------------------------------

Title:
A HRNet-based Rehabilitation Monitoring System

Abstract: The rehabilitation treatment helps to heal minor sports and occupational
injuries. In a traditional rehabilitation process, a therapist will assign
certain actions to a patient to perform in between hospital visits, and it will
rely on the patient to remember actions correctly and the schedule to perform
them. Unfortunately, many patients forget to perform actions or fail to recall
actions in detail. As a consequence, the rehabilitation treatment is hampered
or, in the worst case, the patient may suffer from additional injury caused by
performing incorrect actions. To resolve these issues, we propose a HRNet-based
rehabilitation monitoring system, which can remind a patient when to perform
the actions and display the actions for the patient to follow via the patient's
smartphone. In addition, it helps the therapist to monitor the progress of the
rehabilitation for the patient. Our system consists of an iOS app and several
components at the server side. The app is in charge of displaying and
collecting action videos. The server computes the similarity score between the
therapist's actions and the patient's in the videos to keep track of the number
of repetitions of each action. Theses stats will be shown to both of the
patient and therapist. The extensive experiments show that the F1-Score of the
similarity calculation is as high as 0.9 and the soft accuracy of the number of
repetitions is higher than 90%.


------------------------------------------------------------------------------

Title:
Leveraging The Edge-to-Cloud Continuum for Scalable Machine Learning on  Decentralized Data

Abstract: With mobile, IoT and sensor devices becoming pervasive in our life and recent
advances in Edge Computational Intelligence (e.g., Edge AI/ML), it became
evident that the traditional methods for training AI/ML models are becoming
obsolete, especially with the growing concerns over privacy and security. This
work tries to highlight the key challenges that prohibit Edge AI/ML from seeing
wide-range adoption in different sectors, especially for large-scale scenarios.
Therefore, we focus on the main challenges acting as adoption barriers for the
existing methods and propose a design with a drastic shift from the current
ill-suited approaches. The new design is envisioned to be model-centric in
which the trained models are treated as a commodity driving the exchange
dynamics of collaborative learning in decentralized settings. It is expected
that this design will provide a decentralized framework for efficient
collaborative learning at scale.


------------------------------------------------------------------------------

Title:
Maximum Entropy Heterogeneous-Agent Mirror Learning

Abstract: Multi-agent reinforcement learning (MARL) has been shown effective for
cooperative games in recent years. However, existing state-of-the-art methods
face challenges related to sample inefficiency, brittleness regarding
hyperparameters, and the risk of converging to a suboptimal Nash Equilibrium.
To resolve these issues, in this paper, we propose a novel theoretical
framework, named Maximum Entropy Heterogeneous-Agent Mirror Learning (MEHAML),
that leverages the maximum entropy principle to design maximum entropy MARL
actor-critic algorithms. We prove that algorithms derived from the MEHAML
framework enjoy the desired properties of the monotonic improvement of the
joint maximum entropy objective and the convergence to quantal response
equilibrium (QRE). The practicality of MEHAML is demonstrated by developing a
MEHAML extension of the widely used RL algorithm, HASAC (for soft
actor-critic), which shows significant improvements in exploration and
robustness on three challenging benchmarks: Multi-Agent MuJoCo, StarCraftII,
and Google Research Football. Our results show that HASAC outperforms strong
baseline methods such as HATD3, HAPPO, QMIX, and MAPPO, thereby establishing
the new state of the art. See our project page at
this https URL


------------------------------------------------------------------------------

Title:
Conditional Text Image Generation with Diffusion Models

Abstract: Current text recognition systems, including those for handwritten scripts and
scene text, have relied heavily on image synthesis and augmentation, since it
is difficult to realize real-world complexity and diversity through collecting
and annotating enough real text images. In this paper, we explore the problem
of text image generation, by taking advantage of the powerful abilities of
Diffusion Models in generating photo-realistic and diverse image samples with
given conditions, and propose a method called Conditional Text Image Generation
with Diffusion Models (CTIG-DM for short). To conform to the characteristics of
text images, we devise three conditions: image condition, text condition, and
style condition, which can be used to control the attributes, contents, and
styles of the samples in the image generation process. Specifically, four text
image generation modes, namely: (1) synthesis mode, (2) augmentation mode, (3)
recovery mode, and (4) imitation mode, can be derived by combining and
configuring these three conditions. Extensive experiments on both handwritten
and scene text demonstrate that the proposed CTIG-DM is able to produce image
samples that simulate real-world complexity and diversity, and thus can boost
the performance of existing text recognizers. Besides, CTIG-DM shows its
appealing potential in domain adaptation and generating images containing
Out-Of-Vocabulary (OOV) words.


------------------------------------------------------------------------------

Title:
UniG3D: A Unified 3D Object Generation Dataset

Abstract: The field of generative AI has a transformative impact on various areas,
including virtual reality, autonomous driving, the metaverse, gaming, and
robotics. Among these applications, 3D object generation techniques are of
utmost importance. This technique has unlocked fresh avenues in the realm of
creating, customizing, and exploring 3D objects. However, the quality and
diversity of existing 3D object generation methods are constrained by the
inadequacies of existing 3D object datasets, including issues related to text
quality, the incompleteness of multi-modal data representation encompassing 2D
rendered images and 3D assets, as well as the size of the dataset. In order to
resolve these issues, we present UniG3D, a unified 3D object generation dataset
constructed by employing a universal data transformation pipeline on Objaverse
and ShapeNet datasets. This pipeline converts each raw 3D model into
comprehensive multi-modal data representation <text, image, point cloud, mesh>
by employing rendering engines and multi-modal models. These modules ensure the
richness of textual information and the comprehensiveness of data
representation. Remarkably, the universality of our pipeline refers to its
ability to be applied to any 3D dataset, as it only requires raw 3D data. The
selection of data sources for our dataset is based on their scale and quality.
Subsequently, we assess the effectiveness of our dataset by employing Point-E
and SDFusion, two widely recognized methods for object generation, tailored to
the prevalent 3D representations of point clouds and signed distance functions.
Our dataset is available at: this https URL


------------------------------------------------------------------------------

Title:
Data-Heterogeneous Hierarchical Federated Learning with Mobility

Abstract: Federated learning enables distributed training of machine learning (ML)
models across multiple devices in a privacy-preserving manner. Hierarchical
federated learning (HFL) is further proposed to meet the requirements of both
latency and coverage. In this paper, we consider a data-heterogeneous HFL
scenario with mobility, mainly targeting vehicular networks. We derive the
convergence upper bound of HFL with respect to mobility and data heterogeneity,
and analyze how mobility impacts the performance of HFL. While mobility is
considered as a challenge from a communication point of view, our goal here is
to exploit mobility to improve the learning performance by mitigating data
heterogeneity. Simulation results verify the analysis and show that mobility
can indeed improve the model accuracy by up to 15.1\% when training a
convolutional neural network on the CIFAR-10 dataset using HFL.


------------------------------------------------------------------------------

Title:
Virtual Human Generative Model: Masked Modeling Approach for Learning  Human Characteristics

Abstract: Identifying the relationship between healthcare attributes, lifestyles, and
personality is vital for understanding and improving physical and mental
conditions. Machine learning approaches are promising for modeling their
relationships and offering actionable suggestions. In this paper, we propose
Virtual Human Generative Model (VHGM), a machine learning model for estimating
attributes about healthcare, lifestyles, and personalities. VHGM is a deep
generative model trained with masked modeling to learn the joint distribution
of attributes conditioned on known ones. Using heterogeneous tabular datasets,
VHGM learns more than 1,800 attributes efficiently. We numerically evaluate the
performance of VHGM and its training techniques. As a proof-of-concept of VHGM,
we present several applications demonstrating user scenarios, such as virtual
measurements of healthcare attributes and hypothesis verifications of
lifestyles.


------------------------------------------------------------------------------

Title:
Distributed Marker Representation for Ambiguous Discourse Markers and  Entangled Relations

Abstract: Discourse analysis is an important task because it models intrinsic semantic
structures between sentences in a document. Discourse markers are natural
representations of discourse in our daily language. One challenge is that the
markers as well as pre-defined and human-labeled discourse relations can be
ambiguous when describing the semantics between sentences. We believe that a
better approach is to use a contextual-dependent distribution over the markers
to express discourse information. In this work, we propose to learn a
Distributed Marker Representation (DMR) by utilizing the (potentially)
unlimited discourse marker data with a latent discourse sense, thereby bridging
markers with sentence pairs. Such representations can be learned automatically
from data without supervision, and in turn provide insights into the data
itself. Experiments show the SOTA performance of our DMR on the implicit
discourse relation recognition task and strong interpretability. Our method
also offers a valuable tool to understand complex ambiguity and entanglement
among discourse markers and manually defined discourse relations.


------------------------------------------------------------------------------

Title:
GPU-Accelerated Verification of Machine Learning Models for Power  Systems

Abstract: Computational tools for rigorously verifying the performance of large-scale
machine learning (ML) models have progressed significantly in recent years. The
most successful solvers employ highly specialized, GPU-accelerated branch and
bound routines. Such tools are crucial for the successful deployment of machine
learning applications in safety-critical systems, such as power systems.
Despite their successes, however, barriers prevent out-of-the-box application
of these routines to power system problems. This paper addresses this issue in
two key ways. First, for the first time to our knowledge, we enable the
simultaneous verification of multiple verification problems (e.g., checking for
the violation of all line flow constraints simultaneously and not by solving
individual verification problems). For that, we introduce an exact
transformation that converts the "worst-case" violation across a set of
potential violations to a series of ReLU-based layers that augment the original
neural network. This allows verifiers to interpret them directly. Second, power
system ML models often must be verified to satisfy power flow constraints. We
propose a dualization procedure which encodes linear equality and inequality
constraints directly into the verification problem; and in a manner which is
mathematically consistent with the specialized verification tools. To
demonstrate these innovations, we verify problems associated with data-driven
security constrained DC-OPF solvers. We build and test our first set of
innovations using the $\alpha,\beta$-CROWN solver, and we benchmark against
Gurobi 10.0. Our contributions achieve a speedup that can exceed 100x and allow
higher degrees of verification flexibility.


------------------------------------------------------------------------------

Title:
Object Topological Character Acquisition by Inductive Learning

Abstract: Understanding the shape and structure of objects is undoubtedly extremely
important for object recognition, but the most common pattern recognition
method currently used is machine learning, which often requires a large number
of training data. The problem is that this kind of object-oriented learning
lacks a priori knowledge. The amount of training data and the complexity of
computations are very large, and it is hard to extract explicit knowledge after
learning. This is typically called "knowing how without knowing why". We
adopted a method of inductive learning, hoping to derive conceptual knowledge
of the shape of an object and its formal representation based on a small number
of positive examples. It is clear that implementing object recognition is not
based on simple physical features such as colors, edges, textures, etc., but on
their common geometry, such as topologies, which are stable, persistent, and
essential to recognition. In this paper, a formal representation of topological
structure based on object's skeleton (RTS) was proposed and the induction
process of "seeking common ground" is realized. This research helps promote the
method of object recognition from empiricism to rationalism.


------------------------------------------------------------------------------

Title:
Short-Term Voltage Security Constrained UC to Prevent Trip Faults in  High Wind Power Penetrated Power Systems

Abstract: For high wind power-penetrated power systems, the multiple renewable energy
station short-circuit ratio (MRSCR) is often insufficient due to weak grid
structures. Additionally, transient voltage sag/overvoltage issues may cause
trip faults of wind turbines (WTs). Due to the time delay in WTs' controllers,
it is difficult for WTs alone to meet the reactive power demands in different
stages of the transient process. Some synchronous machines (SMs) must be
retained through unit commitment (UC) scheduling to improve MRSCR and prevent
trip faults of WTs. The MRSCR and short-term voltage security constrained-UC
model is a mixed integer nonlinear programming (MINLP) problem with
differential algebraic equations (DAEs) and symbolic matrix inversion, which is
intractable to solve. Based on the dynamic characteristics of different
devices, the original model is simplified as a general MINLP model without
DAEs. Then, generalized Benders decomposition is applied to improve the
solution efficiency. The relaxed MRSCR constraints are formulated in the master
problem to improve the convergence, and the precise MRSCR constraints are
formulated in the subproblems to consider the impact of voltage profiles. Case
studies based on several benchmark systems and a provincial power grid verify
the validity and efficiency of the proposed method


------------------------------------------------------------------------------

Title:
Referenceless User Controllable Semantic Image Synthesis

Abstract: Despite recent progress in semantic image synthesis, complete control over
image style remains a challenging problem. Existing methods require reference
images to feed style information into semantic layouts, which indicates that
the style is constrained by the given image. In this paper, we propose a model
named RUCGAN for user controllable semantic image synthesis, which utilizes a
singular color to represent the style of a specific semantic region. The
proposed network achieves reference-free semantic image synthesis by injecting
color as user-desired styles into each semantic layout, and is able to
synthesize semantic images with unusual colors. Extensive experimental results
on various challenging datasets show that the proposed method outperforms
existing methods, and we further provide an interactive UI to demonstrate the
advantage of our approach for style controllability.


------------------------------------------------------------------------------

Title:
Knowledge Transfer for Dynamic Multi-objective Optimization with a  Changing Number of Objectives

Abstract: Different from most other dynamic multi-objective optimization problems
(DMOPs), DMOPs with a changing number of objectives usually result in expansion
or contraction of the Pareto front or Pareto set manifold. Knowledge transfer
has been used for solving DMOPs, since it can transfer useful information from
solving one problem instance to solve another related problem instance.
However, we show that the state-of-the-art transfer algorithm for DMOPs with a
changing number of objectives lacks sufficient diversity when the fitness
landscape and Pareto front shape present nonseparability, deceptiveness or
other challenging features. Therefore, we propose a knowledge transfer dynamic
multi-objective evolutionary algorithm (KTDMOEA) to enhance population
diversity after changes by expanding/contracting the Pareto set in response to
an increase/decrease in the number of objectives. This enables a solution set
with good convergence and diversity to be obtained after optimization.
Comprehensive studies using 13 DMOP benchmarks with a changing number of
objectives demonstrate that our proposed KTDMOEA is successful in enhancing
population diversity compared to state-of-the-art algorithms, improving
optimization especially in fast changing environments.


------------------------------------------------------------------------------

Title:
Evolving Strategies for Competitive Multi-Agent Search

Abstract: While evolutionary computation is well suited for automatic discovery in
engineering, it can also be used to gain insight into how humans and
organizations could perform more effectively. Using a real-world problem of
innovation search in organizations as the motivating example, this article
first formalizes human creative problem solving as competitive multiagent
search (CMAS). CMAS is different from existing single-agent and team search
problems in that the agents interact through knowledge of other agents'
searches and through the dynamic changes in the search landscape that result
from these searches. The main hypothesis is that evolutionary computation can
be used to discover effective strategies for CMAS; this hypothesis is verified
in a series of experiments on the NK model, i.e. partially correlated and
tunably rugged fitness landscapes. Different specialized strategies are evolved
for each different competitive environment, and also general strategies that
perform well across environments. These strategies are more effective and more
complex than hand-designed strategies and a strategy based on traditional tree
search. Using a novel spherical visualization of such landscapes, insight is
gained about how successful strategies work, e.g. by tracking positive changes
in the landscape. The article thus provides a possible framework for studying
various human creative activities as competitive multi-agent search in the
future.


------------------------------------------------------------------------------

Title:
Bidder Selection Problem in Position Auctions via Poisson Approximation

Abstract: We consider Bidder Selection Problem (BSP) in position auctions motivated by
practical concerns of online advertising platforms. In this problem, the
platform sells ad slots via an auction to a large pool of $n$ potential buyers
with independent values drawn from known prior distributions. The seller can
only invite a fraction of $k<n$ advertisers to the auction due to communication
and computation restrictions. She wishes to maximize either the social welfare
or her revenue by selecting the set of invited bidders.
We study BSP in a classic multi-winner model of position auctions for welfare
and revenue objectives using the optimal (respectively, VCG mechanism, or
Myerson's auction) format for the selected set of bidders. We propose a novel
Poisson-Chernoff relaxation of the problem that immediately implies that 1) BSP
is polynomial time solvable up to a vanishingly small error as the problem size
$k$ grows; 2) PTAS for position auctions after combining our relaxation with
the trivial brute force algorithm; the algorithm is in fact an Efficient PTAS
(EPTAS) under a mild assumption $k\ge\log n$ with much better running time than
previous PTASes for single-item auction. Our approach yields simple and
practically relevant algorithms unlike all previous complex PTASes, which had
at least doubly exponential dependency of their running time on $\varepsilon$.
In contrast, our algorithms are even faster than popular algorithms such as
greedy for submodular maximization. Furthermore, we did extensive numerical
experiments, which demonstrate high efficiency and practical applicability of
our solution. Our experiments corroborate the experimental findings of [Mehta,
Nadav, Psomas, Rubinstein 2020] that many simple heuristics perform
surprisingly well, which indicates importance of using small $\varepsilon$ for
the BSP and practical irrelevance of all previous PTAS approaches.


------------------------------------------------------------------------------

Title:
Meta-Learning for Airflow Simulations with Graph Neural Networks

Abstract: The field of numerical simulation is of significant importance for the design
and management of real-world systems, with partial differential equations
(PDEs) being a commonly used mathematical modeling tool. However, solving PDEs
remains still a challenge, as commonly used traditional numerical solvers often
require high computational costs. As a result, data-driven methods leveraging
machine learning (more particularly Deep Learning) algorithms have been
increasingly proposed to learn models that can predict solutions to complex
PDEs, such as those arising in computational fluid dynamics (CFD). However,
these methods are known to suffer from poor generalization performance on
out-of-distribution (OoD) samples, highlighting the need for more efficient
approaches. To this end, we present a meta-learning approach to enhance the
performance of learned models on OoD samples. Specifically, we set the airflow
simulation in CFD over various airfoils as a meta-learning problem, where each
set of examples defined on a single airfoil shape is treated as a separate
task. Through the use of model-agnostic meta-learning (MAML), we learn a
meta-learner capable of adapting to new tasks, i.e., previously unseen airfoil
shapes, using only a small amount of task-specific data. We experimentally
demonstrate the efficiency of the proposed approach for improving the OoD
generalization performance of learned models while maintaining efficiency.


------------------------------------------------------------------------------

Title:
Understanding and Characterizing Cryptocurrency Free Giveaway and  Arbitrage Bot Scams In the Wild

Abstract: This paper presents a large-scale analysis of two prevalent cryptocurrency
scams disseminated through Twitter and YouTube. The first scam involves free
giveaway schemes where scammers publish fake giveaway websites to deceive
victims and steal funds. The second scam revolves around arbitrage bots and
publishes videos to entice victims into executing malicious smart contracts. To
collect and analyze these scams in the wild, we developed a fully automated
scam detection system called \textit{CryptoScamHunter}, which collects data
from Twitter and YouTube and employs Nature-Language-Processing (NLP) models to
automatically identify scams and extract the associated cryptocurrency address.
By deploying \textit{CryptoScamHunter} over 11 months spanning from June 2022
to May 2023, we detected 95,111 free giveaway scam lists on Twitter and 10,442
arbitrage bot scam videos on YouTube that were disseminated through thousands
of social network accounts and have reached millions of users. Through analysis
of the scam creator accounts, we discovered that the scammers combined
different strategies to spread each scam, including compromising popular
accounts and registering spam accounts. Our findings indicate that 28.7% to
43.9% of these spam accounts remain active as of this writing. Furthermore,
from the identified scams, we extracted 327 URLs associated with giveaway
scams, 808 malicious contracts in arbitrage bot scams, and 429 scam
cryptocurrency addresses. By analyzing the transaction history of the extracted
scam addresses, we estimated that over 9,717 victims fell prey to these scams,
resulting in a loss of up to 3.8 million USD.
Overall, this study sheds light on the tactics, scale, and impact of
cryptocurrency scams on social media and blockchain platforms, emphasizing the
urgent need for effective detection and prevention mechanisms to protect users
from these fraudulent activities.


------------------------------------------------------------------------------

Title:
UniSG^GA: A 3D scenegraph powered by Geometric Algebra unifying  geometry, behavior and GNNs towards generative AI

Abstract: This work presents the introduction of UniSG^GA, a novel integrated
scenegraph structure, that to incorporates behavior and geometry data on a 3D
scene. It is specifically designed to seamlessly integrate Graph Neural
Networks (GNNs) and address the challenges associated with transforming a 3D
scenegraph (3D-SG) during generative tasks. To effectively capture and preserve
the topological relationships between objects in a simplified way, within the
graph representation, we propose UniSG^GA, that seamlessly integrates Geometric
Algebra (GA) forms. This novel approach enhances the overall performance and
capability of GNNs in handling generative and predictive tasks, opening up new
possibilities and aiming to lay the foundation for further exploration and
development of graph-based generative AI models that can effectively
incorporate behavior data for enhanced scene generation and synthesis.


------------------------------------------------------------------------------

Title:
Developing Effective Educational Chatbots with ChatGPT prompts: Insights  from Preliminary Tests in a Case Study on Social Media Literacy

Abstract: Educational chatbots come with a promise of interactive and personalized
learning experiences, yet their development has been limited by the restricted
free interaction capabilities of available platforms and the difficulty of
encoding knowledge in a suitable format. Recent advances in language learning
models with zero-shot learning capabilities, such as ChatGPT, suggest a new
possibility for developing educational chatbots using a prompt-based approach.
We present a case study with a simple system that enables mixed-turn chatbot
interactions and we discuss the insights and preliminary guidelines obtained
from initial tests. We examine ChatGPT's ability to pursue multiple
interconnected learning objectives, adapt the educational activity to users'
characteristics, such as culture, age, and level of education, and its ability
to use diverse educational strategies and conversational styles. Although the
results are encouraging, challenges are posed by the limited history maintained
for the conversation and the highly structured form of responses by ChatGPT, as
well as their variability, which can lead to an unexpected switch of the
chatbot's role from a teacher to a therapist. We provide some initial
guidelines to address these issues and to facilitate the development of
effective educational chatbots.


------------------------------------------------------------------------------

Title:
Towards Stability of Autoregressive Neural Operators

Abstract: Neural operators have proven to be a promising approach for modeling
spatiotemporal systems in the physical sciences. However, training these models
for large systems can be quite challenging as they incur significant
computational and memory expense -- these systems are often forced to rely on
autoregressive time-stepping of the neural network to predict future temporal
states. While this is effective in managing costs, it can lead to uncontrolled
error growth over time and eventual instability. We analyze the sources of this
autoregressive error growth using prototypical neural operator models for
physical systems and explore ways to mitigate it. We introduce architectural
and application-specific improvements that allow for careful control of
instability-inducing operations within these models without inflating the
compute/memory expense. We present results on several scientific systems that
include Navier-Stokes fluid flow, rotating shallow water, and a high-resolution
global weather forecasting system. We demonstrate that applying our design
principles to prototypical neural networks leads to significantly lower errors
in long-range forecasts with 800\% longer forecasts without qualitative signs
of divergence compared to the original models for these systems. We open-source
our
\href{this https URL}{code}
for reproducibility.


------------------------------------------------------------------------------

Title:
MA-BBOB: Many-Affine Combinations of BBOB Functions for Evaluating  AutoML Approaches in Noiseless Numerical Black-Box Optimization Contexts

Abstract: Extending a recent suggestion to generate new instances for numerical
black-box optimization benchmarking by interpolating pairs of the
well-established BBOB functions from the COmparing COntinuous Optimizers (COCO)
platform, we propose in this work a further generalization that allows multiple
affine combinations of the original instances and arbitrarily chosen locations
of the global optima. We demonstrate that the MA-BBOB generator can help fill
the instance space, while overall patterns in algorithm performance are
preserved. By combining the landscape features of the problems with the
performance data, we pose the question of whether these features are as useful
for algorithm selection as previous studies suggested. MA-BBOB is built on the
publicly available IOHprofiler platform, which facilitates standardized
experimentation routines, provides access to the interactive IOHanalyzer module
for performance analysis and visualization, and enables comparisons with the
rich and growing data collection available for the (MA-)BBOB functions.


------------------------------------------------------------------------------

Title:
Enhanced Masked Image Modeling for Analysis of Dental Panoramic  Radiographs

Abstract: The computer-assisted radiologic informative report has received increasing
research attention to facilitate diagnosis and treatment planning for dental
care providers. However, manual interpretation of dental images is limited,
expensive, and time-consuming. Another barrier in dental imaging is the limited
number of available images for training, which is a challenge in the era of
deep learning. This study proposes a novel self-distillation (SD) enhanced
self-supervised learning on top of the masked image modeling (SimMIM)
Transformer, called SD-SimMIM, to improve the outcome with a limited number of
dental radiographs. In addition to the prediction loss on masked patches,
SD-SimMIM computes the self-distillation loss on the visible patches. We apply
SD-SimMIM on dental panoramic X-rays for teeth numbering, detection of dental
restorations and orthodontic appliances, and instance segmentation tasks. Our
results show that SD-SimMIM outperforms other self-supervised learning methods.
Furthermore, we augment and improve the annotation of an existing dataset of
panoramic X-rays.


------------------------------------------------------------------------------

Title:
A Metadata-Based Ecosystem to Improve the FAIRness of Research Software

Abstract: The reuse of research software is central to research efficiency and academic
exchange. The application of software enables researchers with varied
backgrounds to reproduce, validate, and expand upon study findings.
Furthermore, the analysis of open source code aids in the comprehension,
comparison, and integration of approaches. Often, however, no further use
occurs because relevant software cannot be found or is incompatible with
existing research processes. This results in repetitive software development,
which impedes the advancement of individual researchers and entire research
communities. In this article, the DataDesc ecosystem is presented, an approach
to describing data models of software interfaces with detailed and
machine-actionable metadata. In addition to a specialized metadata schema, an
exchange format and support tools for easy collection and the automated
publishing of software documentation are introduced. This approach practically
increases the FAIRness, i.e., findability, accessibility, interoperability, and
so the reusability of research software, as well as effectively promotes its
impact on research.


------------------------------------------------------------------------------

Title:
INDCOR white paper 3: Interactive Digital Narratives and Interaction

Abstract: The nature of interaction within Interactive Digital Narrative (IDN) is
inherently complex. This is due, in part, to the wide range of potential
interaction modes through which IDNs can be conceptualised, produced and
deployed and the complex dynamics this might entail. The purpose of this
whitepaper is to provide IDN practitioners with the essential knowledge on the
nature of interaction in IDNs and allow them to make informed design decisions
that lead to the incorporation of complexity thinking throughout the design
pipeline, the implementation of the work, and the ways its audience perceives
it. This white paper is concerned with the complexities of authoring,
delivering and processing dynamic interactive contents from the perspectives of
both creators and audiences. This white paper is part of a series of
publications by the INDCOR COST Action 18230 (Interactive Narrative Design for
Complexity Representations), which all clarify how IDNs representing complexity
can be understood and applied (INDCOR WP 0 - 5, 2023).


------------------------------------------------------------------------------

Title:
Gender Bias in Transformer Models: A comprehensive survey

Abstract: Gender bias in artificial intelligence (AI) has emerged as a pressing concern
with profound implications for individuals' lives. This paper presents a
comprehensive survey that explores gender bias in Transformer models from a
linguistic perspective. While the existence of gender bias in language models
has been acknowledged in previous studies, there remains a lack of consensus on
how to effectively measure and evaluate this bias. Our survey critically
examines the existing literature on gender bias in Transformers, shedding light
on the diverse methodologies and metrics employed to assess bias. Several
limitations in current approaches to measuring gender bias in Transformers are
identified, encompassing the utilization of incomplete or flawed metrics,
inadequate dataset sizes, and a dearth of standardization in evaluation
methods. Furthermore, our survey delves into the potential ramifications of
gender bias in Transformers for downstream applications, including dialogue
systems and machine translation. We underscore the importance of fostering
equity and fairness in these systems by emphasizing the need for heightened
awareness and accountability in developing and deploying language technologies.
This paper serves as a comprehensive overview of gender bias in Transformer
models, providing novel insights and offering valuable directions for future
research in this critical domain.


------------------------------------------------------------------------------

Title:
Agnostically Learning Single-Index Models using Omnipredictors

Abstract: We give the first result for agnostically learning Single-Index Models (SIMs)
with arbitrary monotone and Lipschitz activations. All prior work either held
only in the realizable setting or required the activation to be known.
Moreover, we only require the marginal to have bounded second moments, whereas
all prior work required stronger distributional assumptions (such as
anticoncentration or boundedness). Our algorithm is based on recent work by
[GHK$^+$23] on omniprediction using predictors satisfying calibrated
multiaccuracy. Our analysis is simple and relies on the relationship between
Bregman divergences (or matching losses) and $\ell_p$ distances. We also
provide new guarantees for standard algorithms like GLMtron and logistic
regression in the agnostic setting.


------------------------------------------------------------------------------

Title:
Human vs Machine: Comparison of Student-generated and AI-generated  Educational Content

Abstract: As an increasing number of students move to online learning platforms that
deliver personalized learning experiences, there is a great need for the
production of high-quality educational content. Large language models (LLMs)
appear to offer a promising solution to the rapid creation of learning
materials at scale, reducing the burden on instructors. In this study, we
investigated the potential for LLMs to produce learning resources in an
introductory programming context, by comparing the quality of the resources
generated by an LLM with those created by students as part of a learnersourcing
activity. Using a blind evaluation, students rated the correctness and
helpfulness of resources generated by AI and their peers, after both were
initially provided with identical exemplars. Our results show that the quality
of AI-generated resources, as perceived by students, is equivalent to the
quality of resources generated by their peers. This suggests that AI-generated
resources may serve as viable supplementary material in certain contexts.
Resources generated by LLMs tend to closely mirror the given exemplars, whereas
student-generated resources exhibit greater variety in terms of content length
and specific syntax features used. The study highlights the need for further
research exploring different types of learning resources and a broader range of
subject areas, and understanding the long-term impact of AI-generated resources
on learning outcomes.


------------------------------------------------------------------------------

Title:
Identifiable causal inference with noisy treatment and no side  information

Abstract: In some causal inference scenarios, the treatment (i.e. cause) variable is
measured inaccurately, for instance in epidemiology or econometrics. Failure to
correct for the effect of this measurement error can lead to biased causal
effect estimates. Previous research has not studied methods that address this
issue from a causal viewpoint while allowing for complex nonlinear dependencies
and without assuming access to side information. For such as scenario, this
paper proposes a model that assumes a continuous treatment variable which is
inaccurately measured. Building on existing results for measurement error
models, we prove that our model's causal effect estimates are identifiable,
even without knowledge of the measurement error variance or other side
information. Our method relies on a deep latent variable model where Gaussian
conditionals are parameterized by neural networks, and we develop an amortized
importance-weighted variational objective for training the model. Empirical
results demonstrate the method's good performance with unknown measurement
error. More broadly, our work extends the range of applications where reliable
causal inference can be conducted.


------------------------------------------------------------------------------

Title:
O numeričkom rješavanju Cauchyjevog problema Runge-Kutta  metodama na Shishkinovoj mreži

Abstract: In this paper, the numerical solution of the singular-perturbation Cauchy
problem by Runge-Kutta methods on the Shishkin grid is discussed. Numerical
solutions of the observed problem were obtained using two explicit and one
implicit Runge-Kutta method on the simplest layer-adaptive network. Finally,
the obtained results were compared.


------------------------------------------------------------------------------

Title:
CompanyKG: A Large-Scale Heterogeneous Graph for Company Similarity  Quantification

Abstract: In the investment industry, it is often essential to carry out fine-grained
company similarity quantification for a range of purposes, including market
mapping, competitor analysis, and mergers and acquisitions. We propose and
publish a knowledge graph, named CompanyKG, to represent and learn diverse
company features and relations. Specifically, 1.17 million companies are
represented as nodes enriched with company description embeddings; and 15
different inter-company relations result in 51.06 million weighted edges. To
enable a comprehensive assessment of methods for company similarity
quantification, we have devised and compiled three evaluation tasks with
annotated test sets: similarity prediction, competitor retrieval and similarity
ranking. We present extensive benchmarking results for 11 reproducible
predictive methods categorized into three groups: node-only, edge-only, and
node+edge. To the best of our knowledge, CompanyKG is the first large-scale
heterogeneous graph dataset originating from a real-world investment platform,
tailored for quantifying inter-company similarity.


------------------------------------------------------------------------------

Title:
A Study on Quantifying Sim2Real Image Gap in Autonomous Driving  Simulations Using Lane Segmentation Attention Map Similarity

Abstract: Autonomous driving simulations require highly realistic images. Our
preliminary study found that when the CARLA Simulator image was made more like
reality by using DCLGAN, the performance of the lane recognition model improved
to levels comparable to real-world driving. It was also confirmed that the
vehicle's ability to return to the center of the lane after deviating from it
improved significantly. However, there is currently no agreed-upon metric for
quantitatively evaluating the realism of simulation images. To address this
issue, based on the idea that FID (Fr\'echet Inception Distance) measures the
feature vector distribution distance using a pre-trained model, this paper
proposes a metric that measures the similarity of simulation road images using
the attention map from the self-attention distillation process of ENet-SAD.
Finally, this paper verified the suitability of the measurement method by
applying it to the image of the CARLA map that implemented a realworld
autonomous driving test road.


------------------------------------------------------------------------------

Title:
DropCompute: simple and more robust distributed synchronous training via  compute variance reduction

Abstract: Background: Distributed training is essential for large scale training of
deep neural networks (DNNs). The dominant methods for large scale DNN training
are synchronous (e.g. All-Reduce), but these require waiting for all workers in
each step. Thus, these methods are limited by the delays caused by straggling
workers. Results: We study a typical scenario in which workers are straggling
due to variability in compute time. We find an analytical relation between
compute time properties and scalability limitations, caused by such straggling
workers. With these findings, we propose a simple yet effective decentralized
method to reduce the variation among workers and thus improve the robustness of
synchronous training. This method can be integrated with the widely used
All-Reduce. Our findings are validated on large-scale training tasks using 200
Gaudi Accelerators.


------------------------------------------------------------------------------

Title:
Dynamic Cell Modeling of Li-Ion Polymer Batteries for Precise SOC  Estimation in Power-Needy Autonomous Electric Vehicles

Abstract: This paper presents findings on dynamic cell modeling for state-of-charge
(SOC) estimation in an autonomous electric vehicle (AEV). The studied cells are
Lithium-Ion Polymer-based with a nominal capacity of around 8Ah, optimized for
power-needy applications. The AEV operates in a harsh environment with rate
requirements up to +/-25C and highly dynamic rate profiles, unlike
portable-electronic applications with constant power output and fractional C
rates. SOC estimation methods effective in portable electronics may not suffice
for the AEV. Accurate SOC estimation necessitates a precise cell model. The
proposed SOC estimation method utilizes a detailed Kalman-filtering approach.
The cell model must include SOC as a state in the model state vector. Multiple
cell models are presented, starting with a simple one employing "Coulomb
counting" as the state equation and Shepherd's rule as the output equation,
lacking prediction of cell relaxation dynamics. An improved model incorporates
filter states to account for relaxation and other dynamics in closed-circuit
cell voltage, yielding better performance. The best overall results are
achieved with a method combining nonlinear autoregressive filtering and dynamic
radial basis function networks. The paper includes lab test results comparing
physical cells with model predictions. The most accurate models obtained have
an RMS estimation error lower than the quantization noise floor expected in the
battery-management-system design. Importantly, these models enable precise SOC
estimation, allowing the vehicle controller to utilize the battery pack's full
operating range without overcharging or undercharging concerns.


------------------------------------------------------------------------------

Title:
Improving Generalizability of Graph Anomaly Detection Models via Data  Augmentation

Abstract: Graph anomaly detection (GAD) is a vital task since even a few anomalies can
pose huge threats to benign users. Recent semi-supervised GAD methods, which
can effectively leverage the available labels as prior knowledge, have achieved
superior performances than unsupervised methods. In practice, people usually
need to identify anomalies on new (sub)graphs to secure their business, but
they may lack labels to train an effective detection model. One natural idea is
to directly adopt a trained GAD model to the new (sub)graph for testing.
However, we find that existing semi-supervised GAD methods suffer from poor
generalization issue, i.e., well-trained models could not perform well on an
unseen area (i.e., not accessible in training) of the same graph. It may cause
great troubles. In this paper, we base on the phenomenon and propose a general
and novel research problem of generalized graph anomaly detection that aims to
effectively identify anomalies on both the training-domain graph and unseen
testing graph to eliminate potential dangers. Nevertheless, it is a challenging
task since only limited labels are available, and the normal background may
differ between training and testing data. Accordingly, we propose a data
augmentation method named \textit{AugAN} (\uline{Aug}mentation for
\uline{A}nomaly and \uline{N}ormal distributions) to enrich training data and
boost the generalizability of GAD models. Experiments verify the effectiveness
of our method in improving model generalizability.


------------------------------------------------------------------------------

Title:
LiDAR-Based Place Recognition For Autonomous Driving: A Survey

Abstract: LiDAR-based place recognition (LPR) plays a pivotal role in autonomous
driving, which assists Simultaneous Localization and Mapping (SLAM) systems in
reducing accumulated errors and achieving reliable localization. However,
existing reviews predominantly concentrate on visual place recognition (VPR)
methods. Despite notable advancements in LPR in recent years, there is yet a
systematic review dedicated to this field to the best of our knowledge. This
paper bridges the gap by providing a comprehensive review of place recognition
methods employing LiDAR sensors, thus facilitating and encouraging further
research. We commence by delving into the problem formulation of place
recognition and exploring existing challenges, describing relations to previous
surveys. Subsequently, we conduct an in-depth review of related research, which
offers detailed classifications, strengths and weaknesses, and architectures.
Finally, we summarize existing datasets, commonly used evaluation metrics, and
comprehensive evaluation results from various methods on public datasets. This
paper can serve as a valuable tutorial for newcomers entering the realm of
place recognition and researchers interested in long-term robot localization.
We pledge to maintain an up-to-date project on our website
this https URL


------------------------------------------------------------------------------

Title:
Concept-Based Visual Analysis of Dynamic Textual Data

Abstract: Analyzing how interrelated ideas flow within and between multiple social
groups helps understand the propagation of information, ideas, and thoughts on
social media. The existing dynamic text analysis work on idea flow analysis is
mostly based on the topic model. Therefore, when analyzing the reasons behind
the flow of ideas, people have to check the textual data of the ideas, which is
annoying because of the huge amount and complex structures of these texts. To
solve this problem, we propose a concept-based dynamic visual text analytics
method, which illustrates how the content of the ideas change and helps users
analyze the root cause of the idea flow. We use concepts to summarize the
content of the ideas and show the flow of concepts with the flow lines. To
ensure the stability of the flow lines, a constrained t-SNE projection
algorithm is used to display the change of concepts over time and the
correlation between them. In order to better convey the anomalous change of the
concepts, we propose a method to detect the time periods with anomalous change
of concepts based on anomaly detection and highlight them. A qualitative
evaluation and a case study on real-world Twitter datasets demonstrate the
correctness and effectiveness of our visual analytics method.


------------------------------------------------------------------------------

Title:
Score-based Data Assimilation

Abstract: Data assimilation, in its most comprehensive form, addresses the Bayesian
inverse problem of identifying plausible state trajectories that explain noisy
or incomplete observations of stochastic dynamical systems. Various approaches
have been proposed to solve this problem, including particle-based and
variational methods. However, most algorithms depend on the transition dynamics
for inference, which becomes intractable for long time horizons or for
high-dimensional systems with complex dynamics, such as oceans or atmospheres.
In this work, we introduce score-based data assimilation for trajectory
inference. We learn a score-based generative model of state trajectories based
on the key insight that the score of an arbitrarily long trajectory can be
decomposed into a series of scores over short segments. After training,
inference is carried out using the score model, in a non-autoregressive manner
by generating all states simultaneously. Quite distinctively, we decouple the
observation model from the training procedure and use it only at inference to
guide the generative process, which enables a wide range of zero-shot
observation scenarios. We present theoretical and empirical evidence supporting
the effectiveness of our method.


------------------------------------------------------------------------------

Title:
On Distribution Dependent Sub-Logarithmic Query Time of Learned Indexing

Abstract: A fundamental problem in data management is to find the elements in an array
that match a query. Recently, learned indexes are being extensively used to
solve this problem, where they learn a model to predict the location of the
items in the array. They are empirically shown to outperform non-learned
methods (e.g., B-trees or binary search that answer queries in $O(\log n)$
time) by orders of magnitude. However, success of learned indexes has not been
theoretically justified. Only existing attempt shows the same query time of
$O(\log n)$, but with a constant factor improvement in space complexity over
non-learned methods, under some assumptions on data distribution. In this
paper, we significantly strengthen this result, showing that under mild
assumptions on data distribution, and the same space complexity as non-learned
methods, learned indexes can answer queries in $O(\log\log n)$ expected query
time. We also show that allowing for slightly larger but still near-linear
space overhead, a learned index can achieve $O(1)$ expected query time. Our
results theoretically prove learned indexes are orders of magnitude faster than
non-learned methods, theoretically grounding their empirical success.


------------------------------------------------------------------------------

Title:
Optimizing Stateful Dataflow with Local Rewrites

Abstract: Optimizing a stateful dataflow language is a challenging task. There are
strict correctness constraints for preserving properties expected by downstream
consumers, a large space of possible optimizations, and complex analyses that
must reason about the behavior of the program over time. Classic compiler
techniques with specialized optimization passes yield unpredictable performance
and have complex correctness proofs. But with e-graphs, we can dramatically
simplify the process of building a correct optimizer while yielding more
consistent results! In this short paper, we discuss our early work using
e-graphs to develop an optimizer for a the Hydroflow dataflow language. Our
prototype demonstrates that composing simple, easy-to-prove rewrite rules is
sufficient to match techniques in hand-optimized systems.


------------------------------------------------------------------------------

Title:
Fast Conditional Mixing of MCMC Algorithms for Non-log-concave  Distributions

Abstract: MCMC algorithms offer empirically efficient tools for sampling from a target
distribution $\pi(x) \propto \exp(-V(x))$. However, on the theory side, MCMC
algorithms suffer from slow mixing rate when $\pi(x)$ is non-log-concave. Our
work examines this gap and shows that when Poincar\'e-style inequality holds on
a subset $\mathcal{X}$ of the state space, the conditional distribution of MCMC
iterates over $\mathcal{X}$ mixes fast to the true conditional distribution.
This fast mixing guarantee can hold in cases when global mixing is provably
slow. We formalize the statement and quantify the conditional mixing rate. We
further show that conditional mixing can have interesting implications for
sampling from mixtures of Gaussians, parameter estimation for Gaussian mixture
models and Gibbs-sampling with well-connected local minima.


------------------------------------------------------------------------------

Title:
Optimism and Adaptivity in Policy Optimization

Abstract: We work towards a unifying paradigm for accelerating policy optimization
methods in reinforcement learning (RL) through \emph{optimism} \&
\emph{adaptivity}. Leveraging the deep connection between policy iteration and
policy gradient methods, we recast seemingly unrelated policy optimization
algorithms as the repeated application of two interleaving steps (i) an
\emph{optimistic policy improvement operator} maps a prior policy $\pi_t$ to a
hypothesis $\pi_{t+1}$ using a \emph{gradient ascent prediction}, followed by
(ii) a \emph{hindsight adaptation} of the optimistic prediction based on a
partial evaluation of the performance of $\pi_{t+1}$. We use this shared lens
to jointly express other well-known algorithms, including soft and optimistic
policy iteration, natural actor-critic methods, model-based policy improvement
based on forward search, and meta-learning algorithms. By doing so, we shed
light on collective theoretical properties related to acceleration via optimism
\& adaptivity. Building on these insights, we design an \emph{adaptive \&
optimistic policy gradient} algorithm via meta-gradient learning, and
empirically highlight several design choices pertaining to optimism, in an
illustrative task.


------------------------------------------------------------------------------

Title:
STHG: Spatial-Temporal Heterogeneous Graph Learning for Advanced  Audio-Visual Diarization

Abstract: This report introduces our novel method named STHG for the Audio-Visual
Diarization task of the Ego4D Challenge 2023. Our key innovation is that we
model all the speakers in a video using a single, unified heterogeneous graph
learning framework. Unlike previous approaches that require a separate
component solely for the camera wearer, STHG can jointly detect the speech
activities of all people including the camera wearer. Our final method obtains
61.1% DER on the test set of Ego4D, which significantly outperforms all the
baselines as well as last year's winner. Our submission achieved 1st place in
the Ego4D Challenge 2023. We additionally demonstrate that applying the
off-the-shelf speech recognition system to the diarized speech segments by STHG
produces a competitive performance on the Speech Transcription task of this
challenge.


------------------------------------------------------------------------------

Title:
On the Global Convergence of Natural Actor-Critic with Two-layer Neural  Network Parametrization

Abstract: Actor-critic algorithms have shown remarkable success in solving
state-of-the-art decision-making problems. However, despite their empirical
effectiveness, their theoretical underpinnings remain relatively unexplored,
especially with neural network parametrization. In this paper, we delve into
the study of a natural actor-critic algorithm that utilizes neural networks to
represent the critic. Our aim is to establish sample complexity guarantees for
this algorithm, achieving a deeper understanding of its performance
characteristics. To achieve that, we propose a Natural Actor-Critic algorithm
with 2-Layer critic parametrization (NAC2L). Our approach involves estimating
the $Q$-function in each iteration through a convex optimization problem. We
establish that our proposed approach attains a sample complexity of
$\tilde{\mathcal{O}}\left(\frac{1}{\epsilon^{4}(1-\gamma)^{4}}\right)$. In
contrast, the existing sample complexity results in the literature only hold
for a tabular or linear MDP. Our result, on the other hand, holds for countable
state spaces and does not require a linear or low-rank structure on the MDP.


------------------------------------------------------------------------------

Title:
Transferring Neural Potentials For High Order Dependency Parsing

Abstract: High order dependency parsing leverages high order features such as siblings
or grandchildren to improve state of the art accuracy of current first order
dependency parsers. The present paper uses biaffine scores to provide an
estimate of the arc scores and is then propagated into a graphical model. The
inference inside the graphical model is solved using dual decomposition. The
present algorithm propagates biaffine neural scores to the graphical model and
by leveraging dual decomposition inference, the overall circuit is trained
end-to-end to transfer first order informations to the high order informations.


------------------------------------------------------------------------------

Title:
Graph Ladling: Shockingly Simple Parallel GNN Training without  Intermediate Communication

Abstract: Graphs are omnipresent and GNNs are a powerful family of neural networks for
learning over graphs. Despite their popularity, scaling GNNs either by
deepening or widening suffers from prevalent issues of unhealthy gradients,
over-smoothening, information squashing, which often lead to sub-standard
performance. In this work, we are interested in exploring a principled way to
scale GNNs capacity without deepening or widening, which can improve its
performance across multiple small and large graphs. Motivated by the recent
intriguing phenomenon of model soups, which suggest that fine-tuned weights of
multiple large-language pre-trained models can be merged to a better minima, we
argue to exploit the fundamentals of model soups to mitigate the aforementioned
issues of memory bottleneck and trainability during GNNs scaling. More
specifically, we propose not to deepen or widen current GNNs, but instead
present a data-centric perspective of model soups tailored for GNNs, i.e., to
build powerful GNNs by dividing giant graph data to build independently and
parallelly trained multiple comparatively weaker GNNs without any intermediate
communication, and combining their strength using a greedy interpolation soup
procedure to achieve state-of-the-art performance. Moreover, we provide a wide
variety of model soup preparation techniques by leveraging state-of-the-art
graph sampling and graph partitioning approaches that can handle large graph
data structures. Our extensive experiments across many real-world small and
large graphs, illustrate the effectiveness of our approach and point towards a
promising orthogonal direction for GNN scaling. Codes are available at:
\url{this https URL}.


------------------------------------------------------------------------------

Title:
On Refactoring Quantum Programs

Abstract: Refactoring is a crucial technique for improving the efficiency and
maintainability of software by restructuring its internal design while
preserving its external behavior. While classical programs have benefited from
various refactoring methods, the field of quantum programming lacks dedicated
refactoring techniques. The distinct properties of quantum computing, such as
quantum superposition, entanglement, and the no-cloning principle, necessitate
specialized refactoring techniques. This paper bridges this gap by presenting a
comprehensive set of refactorings specifically designed for quantum programs.
Each refactoring is carefully designed and explained to ensure the effective
restructuring of quantum programs. Additionally, we highlight the importance of
tool support in automating the refactoring process for quantum programs.
Although our study focuses on the quantum programming language Q\#, our
approach is applicable to other quantum programming languages, offering a
general solution for enhancing the maintainability and efficiency of quantum
software.


------------------------------------------------------------------------------

Title:
Decongestion by Representation: Learning to Improve Economic Welfare in  Marketplaces

Abstract: Congestion is a common failure mode of markets, where consumers compete
inefficiently on the same subset of goods (e.g., chasing the same small set of
properties on a vacation rental platform). The typical economic story is that
prices solve this problem by balancing supply and demand in order to decongest
the market. But in modern online marketplaces, prices are typically set in a
decentralized way by sellers, with the power of a platform limited to
controlling representations -- the information made available about products.
This motivates the present study of decongestion by representation, where a
platform uses this power to learn representations that improve social welfare
by reducing congestion. The technical challenge is twofold: relying only on
revealed preferences from users' past choices, rather than true valuations; and
working with representations that determine which features to reveal and are
inherently combinatorial. We tackle both by proposing a differentiable proxy of
welfare that can be trained end-to-end on consumer choice data. We provide
theory giving sufficient conditions for when decongestion promotes welfare, and
present experiments on both synthetic and real data shedding light on our
setting and approach.


------------------------------------------------------------------------------

Title:
Language-Guided Generation of Physically Realistic Robot Motion and  Control

Abstract: We aim to control a robot to physically behave in the real world following
any high-level language command like "cartwheel" or "kick. " Although human
motion datasets exist, this task remains particularly challenging since
generative models can produce physically unrealistic motions, which will be
more severe for robots due to different body structures and physical
properties. In addition, to control a physical robot to perform a desired
motion, a control policy must be learned. We develop LAnguage-Guided mOtion
cONtrol (LAGOON), a multi-phase method to generate physically realistic robot
motions under language commands. LAGOON first leverages a pre-trained model to
generate human motion from a language command. Then an RL phase is adopted to
train a control policy in simulation to mimic the generated human motion.
Finally, with domain randomization, we show that our learned policy can be
successfully deployed to a quadrupedal robot, leading to a robot dog that can
stand up and wave its front legs in the real world to mimic the behavior of a
hand-waving human.


------------------------------------------------------------------------------

Title:
MARBLE: Music Audio Representation Benchmark for Universal Evaluation

Abstract: In the era of extensive intersection between art and Artificial Intelligence
(AI), such as image generation and fiction co-creation, AI for music remains
relatively nascent, particularly in music understanding. This is evident in the
limited work on deep music representations, the scarcity of large-scale
datasets, and the absence of a universal and community-driven benchmark. To
address this issue, we introduce the Music Audio Representation Benchmark for
universaL Evaluation, termed MARBLE. It aims to provide a benchmark for various
Music Information Retrieval (MIR) tasks by defining a comprehensive taxonomy
with four hierarchy levels, including acoustic, performance, score, and
high-level description. We then establish a unified protocol based on 14 tasks
on 8 public-available datasets, providing a fair and standard assessment of
representations of all open-sourced pre-trained models developed on music
recordings as baselines. Besides, MARBLE offers an easy-to-use, extendable, and
reproducible suite for the community, with a clear statement on copyright
issues on datasets. Results suggest recently proposed large-scale pre-trained
musical language models perform the best in most tasks, with room for further
improvement. The leaderboard and toolkit repository are published at
this https URL to promote future music AI research.


------------------------------------------------------------------------------

Title:
Experimental Study of Transport Layer Protocols for Wireless Networked  Control Systems

Abstract: In Wireless Networked Control Systems (WNCSs), the feedback control loops are
closed over a wireless communication network. The proliferation of WNCSs
requires efficient network resource management mechanisms since the control
performance is significantly affected by the impairments caused by network
limitations. In conventional communication networks, the amount of transmitted
data is one of the key performance indicators. In contrast, in WNCSs, the
efficiency of the network is measured by its ability to facilitate control
applications, and the data transmission rate should be limited to avoid network
congestion. In this work, we consider an experimental setup where multiple
control loops share a wireless communication network. Our testbed comprises up
to five control loops that include Zolertia Re-Mote devices implementing IEEE
802.15.4 standard. We propose a novel relevance- and network-aware transport
layer (TL) scheme for WNCSs. The proposed scheme admits the most important
measurements for the control process into the network while taking current
network conditions into account. Moreover, we propose a mechanism for the
scheme parameters adaptation in dynamic scenarios with unknown network
statistics. Unlike the conventional TL mechanisms failing to provide adequate
control performance due to either congestion in the network or inefficient
utilization of available resources, our method prevents network congestion
while keeping the control performance high. We argue that relevance- and
network-awareness are critical components of network protocol design to avoid
control performance degradation in practice.


------------------------------------------------------------------------------

Title:
A Survey on User-Space Storage and Its Implementations

Abstract: The storage stack in the traditional operating system is primarily optimized
towards improving the CPU utilization and hiding the long I/O latency imposed
by the slow I/O devices such as hard disk drivers (HDDs). However, the emerging
storage media experience significant technique shifts in the past decade, which
exhibit high bandwidth and low latency. These high-performance storage devices,
unfortunately, suffer from the huge overheads imposed by the system software
including the long storage stack and the frequent context switch between the
user and kernel modes. Many researchers have investigated huge efforts in
addressing this challenge by constructing a direct software path between a user
process and the underlying storage devices. We revisit such novel designs in
the prior work and present a survey in this paper. Specifically, we classify
the former research into three categories according to their commonalities. We
then present the designs of each category based on the timeline and analyze
their uniqueness and contributions. This paper also reviews the applications
that exploit the characteristics of theses designs. Given that the user-space
storage is a growing research field, we believe this paper can be an
inspiration for future researchers, who are interested in the user-space
storage system designs.


------------------------------------------------------------------------------

Title:
Weakly Supervised Regression with Interval Targets

Abstract: This paper investigates an interesting weakly supervised regression setting
called regression with interval targets (RIT). Although some of the previous
methods on relevant regression settings can be adapted to RIT, they are not
statistically consistent, and thus their empirical performance is not
guaranteed. In this paper, we provide a thorough study on RIT. First, we
proposed a novel statistical model to describe the data generation process for
RIT and demonstrate its validity. Second, we analyze a simple selection method
for RIT, which selects a particular value in the interval as the target value
to train the model. Third, we propose a statistically consistent limiting
method for RIT to train the model by limiting the predictions to the interval.
We further derive an estimation error bound for our limiting method. Finally,
extensive experiments on various datasets demonstrate the effectiveness of our
proposed method.


------------------------------------------------------------------------------

Title:
Generation of Radiology Findings in Chest X-Ray by Leveraging  Collaborative Knowledge

Abstract: Among all the sub-sections in a typical radiology report, the Clinical
Indications, Findings, and Impression often reflect important details about the
health status of a patient. The information included in Impression is also
often covered in Findings. While Findings and Impression can be deduced by
inspecting the image, Clinical Indications often require additional context.
The cognitive task of interpreting medical images remains the most critical and
often time-consuming step in the radiology workflow. Instead of generating an
end-to-end radiology report, in this paper, we focus on generating the Findings
from automated interpretation of medical images, specifically chest X-rays
(CXRs). Thus, this work focuses on reducing the workload of radiologists who
spend most of their time either writing or narrating the Findings. Unlike past
research, which addresses radiology report generation as a single-step image
captioning task, we have further taken into consideration the complexity of
interpreting CXR images and propose a two-step approach: (a) detecting the
regions with abnormalities in the image, and (b) generating relevant text for
regions with abnormalities by employing a generative large language model
(LLM). This two-step approach introduces a layer of interpretability and aligns
the framework with the systematic reasoning that radiologists use when
reviewing a CXR.


------------------------------------------------------------------------------

Title:
Lighthouses and Global Graph Stabilization: Active SLAM for Low-compute,  Narrow-FoV Robots

Abstract: Autonomous exploration to build a map of an unknown environment is a
fundamental robotics problem. However, the quality of the map directly
influences the quality of subsequent robot operation. Instability in a
simultaneous localization and mapping (SLAM) system can lead to poorquality
maps and subsequent navigation failures during or after exploration. This
becomes particularly noticeable in consumer robotics, where compute budget and
limited field-of-view are very common. In this work, we propose (i) the concept
of lighthouses: panoramic views with high visual information content that can
be used to maintain the stability of the map locally in their neighborhoods and
(ii) the final stabilization strategy for global pose graph stabilization. We
call our novel exploration strategy SLAM-aware exploration (SAE) and evaluate
its performance on real-world home environments.


------------------------------------------------------------------------------

Title:
Isabelle Formalisation of Original Representation Theorems

Abstract: In a recent paper, new theorems linking apparently unrelated mathematical
objects (event structures from concurrency theory and full graphs arising in
computational biology) were discovered by cross-site data mining on huge
databases, and building on existing Isabelle-verified event structures
enumeration algorithms. Given the origin and newness of such theorems, their
formal verification is particularly desirable. This paper presents such a
verification via Isabelle/HOL definitions and theorems, and exposes the
technical challenges found in the process. The introduced formalisation
completes the verification of Isabelle-verified event structure enumeration
algorithms into a fully verified framework to link event structures to full
graphs.


------------------------------------------------------------------------------

Title:
Advancing Biomedicine with Graph Representation Learning: Recent  Progress, Challenges, and Future Directions

Abstract: Graph representation learning (GRL) has emerged as a pivotal field that has
contributed significantly to breakthroughs in various fields, including
biomedicine. The objective of this survey is to review the latest advancements
in GRL methods and their applications in the biomedical field. We also
highlight key challenges currently faced by GRL and outline potential
directions for future research.


------------------------------------------------------------------------------

Title:
Balanced Energy Regularization Loss for Out-of-distribution Detection

Abstract: In the field of out-of-distribution (OOD) detection, a previous method that
use auxiliary data as OOD data has shown promising performance. However, the
method provides an equal loss to all auxiliary data to differentiate them from
inliers. However, based on our observation, in various tasks, there is a
general imbalance in the distribution of the auxiliary OOD data across classes.
We propose a balanced energy regularization loss that is simple but generally
effective for a variety of tasks. Our balanced energy regularization loss
utilizes class-wise different prior probabilities for auxiliary data to address
the class imbalance in OOD data. The main concept is to regularize auxiliary
samples from majority classes, more heavily than those from minority classes.
Our approach performs better for OOD detection in semantic segmentation,
long-tailed image classification, and image classification than the prior
energy regularization loss. Furthermore, our approach achieves state-of-the-art
performance in two tasks: OOD detection in semantic segmentation and
long-tailed image classification. Code is available at
this https URL


------------------------------------------------------------------------------

Title:
Online Map Vectorization for Autonomous Driving: A Rasterization  Perspective

Abstract: Vectorized high-definition (HD) map is essential for autonomous driving,
providing detailed and precise environmental information for advanced
perception and planning. However, current map vectorization methods often
exhibit deviations, and the existing evaluation metric for map vectorization
lacks sufficient sensitivity to detect these deviations. To address these
limitations, we propose integrating the philosophy of rasterization into map
vectorization. Specifically, we introduce a new rasterization-based evaluation
metric, which has superior sensitivity and is better suited to real-world
autonomous driving scenarios. Furthermore, we propose MapVR (Map Vectorization
via Rasterization), a novel framework that applies differentiable rasterization
to vectorized outputs and then performs precise and geometry-aware supervision
on rasterized HD maps. Notably, MapVR designs tailored rasterization strategies
for various geometric shapes, enabling effective adaptation to a wide range of
map elements. Experiments show that incorporating rasterization into map
vectorization greatly enhances performance with no extra computational cost
during inference, leading to more accurate map perception and ultimately
promoting safer autonomous driving.


------------------------------------------------------------------------------

Title:
Evaluating Graph Neural Networks for Link Prediction: Current Pitfalls  and New Benchmarking

Abstract: Link prediction attempts to predict whether an unseen edge exists based on
only a portion of edges of a graph. A flurry of methods have been introduced in
recent years that attempt to make use of graph neural networks (GNNs) for this
task. Furthermore, new and diverse datasets have also been created to better
evaluate the effectiveness of these new models. However, multiple pitfalls
currently exist that hinder our ability to properly evaluate these new methods.
These pitfalls mainly include: (1) Lower than actual performance on multiple
baselines, (2) A lack of a unified data split and evaluation metric on some
datasets, and (3) An unrealistic evaluation setting that uses easy negative
samples. To overcome these challenges, we first conduct a fair comparison
across prominent methods and datasets, utilizing the same dataset and
hyperparameter search settings. We then create a more practical evaluation
setting based on a Heuristic Related Sampling Technique (HeaRT), which samples
hard negative samples via multiple heuristics. The new evaluation setting helps
promote new challenges and opportunities in link prediction by aligning the
evaluation with real-world situations. Our implementation and data are
available at this https URL


------------------------------------------------------------------------------

Title:
An Empirical Study of Untangling Patterns of Two-Class Dependency Cycles

Abstract: Dependency cycles pose a significant challenge to software quality and
maintainability. However, there is limited understanding of how practitioners
resolve dependency cycles in real-world scenarios. This paper presents an
empirical study investigating the recurring patterns employed by software
developers to resolve dependency cycles between two classes in practice. We
analyzed the data from 18 open-source projects across different domains and
manually inspected hundreds of cycle untangling cases. Our findings reveal that
developers tend to employ five recurring patterns to address dependency cycles.
The chosen patterns are not only determined by dependency relations between
cyclic classes, but also highly related to their design context, i.e., how
cyclic classes depend on or are depended by their neighbor classes. Through
this empirical study, we also discovered three common mistakes developers
usually made during cycles' handling. These recurring patterns and common
mistakes observed in dependency cycles' practice can serve as a taxonomy to
improve developers' awareness and also be used as learning materials for
students in software engineering and inexperienced developers. Our results also
suggest that, in addition to considering the internal structure of dependency
cycles, automatic tools need to consider the design context of cycles to
provide better support for refactoring dependency cycles.


------------------------------------------------------------------------------

Title:
Federated Learning Based Distributed Localization of False Data  Injection Attacks on Smart Grids

Abstract: Data analysis and monitoring on smart grids are jeopardized by attacks on
cyber-physical systems. False data injection attack (FDIA) is one of the
classes of those attacks that target the smart measurement devices by injecting
malicious data. The employment of machine learning techniques in the detection
and localization of FDIA is proven to provide effective results. Training of
such models requires centralized processing of sensitive user data that may not
be plausible in a practical scenario. By employing federated learning for the
detection of FDIA attacks, it is possible to train a model for the detection
and localization of the attacks while preserving the privacy of sensitive user
data. However, federated learning introduces new problems such as the
personalization of the detectors in each node. In this paper, we propose a
federated learning-based scheme combined with a hybrid deep neural network
architecture that exploits the local correlations between the connected power
buses by employing graph neural networks as well as the temporal patterns in
the data by using LSTM layers. The proposed mechanism offers flexible and
efficient training of an FDIA detector in a distributed setup while preserving
the privacy of the clients. We validate the proposed architecture by extensive
simulations on the IEEE 57, 118, and 300 bus systems and real electricity load
data.


------------------------------------------------------------------------------

Title:
MISMATCH: Fine-grained Evaluation of Machine-generated Text with  Mismatch Error Types

Abstract: With the growing interest in large language models, the need for evaluating
the quality of machine text compared to reference (typically human-generated)
text has become focal attention. Most recent works focus either on
task-specific evaluation metrics or study the properties of machine-generated
text captured by the existing metrics. In this work, we propose a new
evaluation scheme to model human judgments in 7 NLP tasks, based on the
fine-grained mismatches between a pair of texts. Inspired by the recent efforts
in several NLP tasks for fine-grained evaluation, we introduce a set of 13
mismatch error types such as spatial/geographic errors, entity errors, etc, to
guide the model for better prediction of human judgments. We propose a neural
framework for evaluating machine texts that uses these mismatch error types as
auxiliary tasks and re-purposes the existing single-number evaluation metrics
as additional scalar features, in addition to textual features extracted from
the machine and reference texts. Our experiments reveal key insights about the
existing metrics via the mismatch errors. We show that the mismatch errors
between the sentence pairs on the held-out datasets from 7 NLP tasks align well
with the human evaluation.


------------------------------------------------------------------------------

Title:
Instant Soup: Cheap Pruning Ensembles in A Single Pass Can Draw Lottery  Tickets from Large Models

Abstract: Large pre-trained transformers have been receiving explosive attention in the
past few years, due to their wide adaptability for numerous downstream
applications via fine-tuning, but their exponentially increasing parameter
counts are becoming a primary hurdle to even just fine-tune them without
industry-standard hardware. Recently, Lottery Ticket Hypothesis (LTH) and its
variants, have been exploited to prune these large pre-trained models
generating subnetworks that can achieve similar performance as their dense
counterparts, but LTH pragmatism is enormously inhibited by repetitive full
training and pruning routine of iterative magnitude pruning (IMP) which worsens
with increasing model size. Motivated by the recent observations of model
soups, which suggest that fine-tuned weights of multiple models can be merged
to a better minima, we propose Instant Soup Pruning (ISP) to generate lottery
ticket quality subnetworks, using a fraction of the original IMP cost by
replacing the expensive intermediate pruning stages of IMP with computationally
efficient weak mask generation and aggregation routine. More specifically,
during the mask generation stage, ISP takes a small handful of iterations using
varying training protocols and data subsets to generate many weak and noisy
subnetworks, and superpose them to average out the noise creating a
high-quality denoised subnetwork. Our extensive experiments and ablation on two
popular large-scale pre-trained models: CLIP (unexplored in pruning till date)
and BERT across multiple benchmark vision and language datasets validate the
effectiveness of ISP compared to several state-of-the-art pruning methods.
Codes are available at: \url{this https URL}


------------------------------------------------------------------------------

Title:
"Is Reporting Worth the Sacrifice of Revealing What I Have Sent?":  Privacy Considerations When Reporting on End-to-End Encrypted Platforms

Abstract: User reporting is an essential component of content moderation on many online
platforms -- in particular, on end-to-end encrypted (E2EE) messaging platforms
where platform operators cannot proactively inspect message contents. However,
users' privacy concerns when considering reporting may impede the effectiveness
of this strategy in regulating online harassment. In this paper, we conduct
interviews with 16 users of E2EE platforms to understand users' mental models
of how reporting works and their resultant privacy concerns and considerations
surrounding reporting. We find that users expect platforms to store rich
longitudinal reporting datasets, recognizing both their promise for better
abuse mitigation and the privacy risk that platforms may exploit or fail to
protect them. We also find that users have preconceptions about the respective
capabilities and risks of moderators at the platform versus community level --
for instance, users trust platform moderators more to not abuse their power but
think community moderators have more time to attend to reports. These
considerations, along with perceived effectiveness of reporting and how to
provide sufficient evidence while maintaining privacy, shape how users decide
whether, to whom, and how much to report. We conclude with design implications
for a more privacy-preserving reporting system on E2EE messaging platforms.


------------------------------------------------------------------------------

Title:
Learn to Enhance the Negative Information in Convolutional Neural  Network

Abstract: This paper proposes a learnable nonlinear activation mechanism specifically
for convolutional neural network (CNN) termed as LENI, which learns to enhance
the negative information in CNNs. In sharp contrast to ReLU which cuts off the
negative neurons and suffers from the issue of ''dying ReLU'', LENI enjoys the
capacity to reconstruct the dead neurons and reduce the information loss.
Compared to improved ReLUs, LENI introduces a learnable approach to process the
negative phase information more properly. In this way, LENI can enhance the
model representational capacity significantly while maintaining the original
advantages of ReLU. As a generic activation mechanism, LENI possesses the
property of portability and can be easily utilized in any CNN models through
simply replacing the activation layers with LENI block. Extensive experiments
validate that LENI can improve the performance of various baseline models on
various benchmark datasets by a clear margin (up to 1.24% higher top-1 accuracy
on ImageNet-1k) with negligible extra parameters. Further experiments show that
LENI can act as a channel compensation mechanism, offering competitive or even
better performance but with fewer learned parameters than baseline models. In
addition, LENI introduces the asymmetry to the model structure which
contributes to the enhancement of representational capacity. Through
visualization experiments, we validate that LENI can retain more information
and learn more representations.


------------------------------------------------------------------------------

Title:
Stabilizing GANs' Training with Brownian Motion Controller

Abstract: The training process of generative adversarial networks (GANs) is unstable
and does not converge globally. In this paper, we examine the stability of GANs
from the perspective of control theory and propose a universal higher-order
noise-based controller called Brownian Motion Controller (BMC). Starting with
the prototypical case of Dirac-GANs, we design a BMC to retrieve precisely the
same but reachable optimal equilibrium. We theoretically prove that the
training process of DiracGANs-BMC is globally exponential stable and derive
bounds on the rate of convergence. Then we extend our BMC to normal GANs and
provide implementation instructions on GANs-BMC. Our experiments show that our
GANs-BMC effectively stabilizes GANs' training under StyleGANv2-ada frameworks
with a faster rate of convergence, a smaller range of oscillation, and better
performance in terms of FID score.


------------------------------------------------------------------------------

Title:
Seen to Unseen: Exploring Compositional Generalization of  Multi-Attribute Controllable Dialogue Generation

Abstract: Existing controllable dialogue generation work focuses on the
single-attribute control and lacks generalization capability to
out-of-distribution multiple attribute combinations. In this paper, we explore
the compositional generalization for multi-attribute controllable dialogue
generation where a model can learn from seen attribute values and generalize to
unseen combinations. We propose a prompt-based disentangled controllable
dialogue generation model, DCG. It learns attribute concept composition by
generating attribute-oriented prompt vectors and uses a disentanglement loss to
disentangle different attributes for better generalization. Besides, we design
a unified reference-free evaluation framework for multiple attributes with
different levels of granularities. Experiment results on two benchmarks prove
the effectiveness of our method and the evaluation metric.


------------------------------------------------------------------------------

Title:
MOSPC: MOS Prediction Based on Pairwise Comparison

Abstract: As a subjective metric to evaluate the quality of synthesized speech, Mean
opinion score~(MOS) usually requires multiple annotators to score the same
speech. Such an annotation approach requires a lot of manpower and is also
time-consuming. MOS prediction model for automatic evaluation can significantly
reduce labor cost. In previous works, it is difficult to accurately rank the
quality of speech when the MOS scores are close. However, in practical
applications, it is more important to correctly rank the quality of synthesis
systems or sentences than simply predicting MOS scores. Meanwhile, as each
annotator scores multiple audios during annotation, the score is probably a
relative value based on the first or the first few speech scores given by the
annotator. Motivated by the above two points, we propose a general framework
for MOS prediction based on pair comparison (MOSPC), and we utilize C-Mixup
algorithm to enhance the generalization performance of MOSPC. The experiments
on BVCC and VCC2018 show that our framework outperforms the baselines on most
of the correlation coefficient metrics, especially on the metric KTAU related
to quality ranking. And our framework also surpasses the strong baseline in
ranking accuracy on each fine-grained segment. These results indicate that our
framework contributes to improving the ranking accuracy of speech quality.


------------------------------------------------------------------------------

Title:
Efficiently Measuring the Cognitive Ability of LLMs: An Adaptive Testing  Perspective

Abstract: Large language models (LLMs), like ChatGPT, have shown some human-like
cognitive abilities. For comparing these abilities of different models, several
benchmarks (i.e. sets of standard test questions) from different fields (e.g.,
Literature, Biology and Psychology) are often adopted and the test results
under traditional metrics such as accuracy, recall and F1, are reported.
However, such way for evaluating LLMs can be inefficient and inaccurate from
the cognitive science perspective. Inspired by Computerized Adaptive Testing
(CAT) used in psychometrics, we propose an adaptive testing framework for LLM
evaluation. Rather than using a standard test set and simply reporting
accuracy, this approach dynamically adjusts the characteristics of the test
questions, such as difficulty, based on the model's performance. This allows
for a more accurate estimation of the model's abilities, using fewer questions.
More importantly, it allows LLMs to be compared with humans easily, which is
essential for NLP models that aim for human-level ability. Our diagnostic
reports have found that ChatGPT often behaves like a ``careless student'',
prone to slip and occasionally guessing the questions. We conduct a
fine-grained diagnosis and rank the latest 6 instruction-tuned LLMs from three
aspects of Subject Knowledge, Mathematical Reasoning, and Programming, where
GPT4 can outperform other models significantly and reach the cognitive ability
of middle-level students. Different tests for different models using efficient
adaptive testing -- we believe this has the potential to become a new norm in
evaluating large language models.


------------------------------------------------------------------------------

Title:
Boosting the Performance of Degraded Reads in RS-coded Distributed  Storage Systems

Abstract: Reed-Solomon (RS) codes have been increasingly adopted by distributed storage
systems in place of replication,because they provide the same level of
availability with much lower storage overhead. However, a key drawback of those
RS-coded distributed storage systems is the poor latency of degraded reads,
which can be incurred by data failures or hot spots,and are not rare in
production environments. To address this issue, we propose a novel parallel
reconstruction solution called APLS. APLS leverages all surviving source nodes
to send the data needed by degraded reads and chooses light-loaded starter
nodes to receive the reconstructed data of those degraded reads. Hence, the
latency of the degraded reads can be improved.Prototyping-based experiments are
conducted to compare APLS with ECPipe, the state-of-the-art solution of
improving the latency of degraded reads. The experimental results demonstrate
that APLS effectively reduces the latency, particularly under heavy or medium
workloads.


------------------------------------------------------------------------------

Title:
MATNet: Multi-Level Fusion and Self-Attention Transformer-Based Model  for Multivariate Multi-Step Day-Ahead PV Generation Forecasting

Abstract: The integration of renewable energy sources (RES) into modern power systems
has become increasingly important due to climate change and macroeconomic and
geopolitical instability. Among the RES, photovoltaic (PV) energy is rapidly
emerging as one of the world's most promising. However, its widespread adoption
poses challenges related to its inherently uncertain nature that can lead to
imbalances in the electrical system. Therefore, accurate forecasting of PV
production can help resolve these uncertainties and facilitate the integration
of PV into modern power systems. Currently, PV forecasting methods can be
divided into two main categories: physics-based and data-based strategies, with
AI-based models providing state-of-the-art performance in PV power forecasting.
However, while these AI-based models can capture complex patterns and
relationships in the data, they ignore the underlying physical prior knowledge
of the phenomenon. Therefore, we propose MATNet, a novel self-attention
transformer-based architecture for multivariate multi-step day-ahead PV power
generation forecasting. It consists of a hybrid approach that combines the AI
paradigm with the prior physical knowledge of PV power generation of
physics-based methods. The model is fed with historical PV data and historical
and forecast weather data through a multi-level joint fusion approach. The
effectiveness of the proposed model is evaluated using the Ausgrid benchmark
dataset with different regression performance metrics. The results show that
our proposed architecture significantly outperforms the current
state-of-the-art methods with an RMSE equal to 0.0460. These findings
demonstrate the potential of MATNet in improving forecasting accuracy and
suggest that it could be a promising solution to facilitate the integration of
PV energy into the power grid.


------------------------------------------------------------------------------

Title:
Iterative Hierarchy and Ranking Process (IHRP): A Novel Effective  Hierarchy Method for Densely Connected Systems and Case Study in Student  Performance Assessment

Abstract: In real-life decision-making problems, determining the influences of the
factors on the decision attribute is one of the primary tasks. To affect the
decision attribute most, finding a proper hierarchy among the factors and
determining their importance values in the system becomes quite important.
Interpretive structural modeling (ISM) is a widely used hierarchy-building
method that mines factor inter-influences based on expert opinions. This paper
discusses one of the main drawbacks of the conventional ISM method in systems
where the factors are densely interrelated. We refer to such systems as "dense
systems". We propose a novel iterative hierarchy-building technique, called
'Iterative Hierarchy and Ranking Process'(IHRP) which performs effectively in
such dense systems. To take the vagueness of the expert opinions into account,
intuitionistic fuzzy linguistics has been used in the research work. In this
paper, we propose a two-stage calculation of the relative importance of the
factors in the system based on their hierarchical positions and rank the
factors accordingly. We have performed a case study on student performance
assessment by taking up novel Indian high-school administrative factors' data
collected by surveying the experts in this field. A comparative study has been
conducted in terms of the correlation of the factor ranking achieved by the
proposed method and conventional ISM method with that of standard outranking
methods like TOPSIS, and VIKOR. Our proposed IHRP framework achieves an 85-95%
correlation compared to a 50-60% correlation for the conventional ISM method.
This proves the effectiveness of the proposed method in determining a better
hierarchy than the conventional method, especially in dense systems.


------------------------------------------------------------------------------

Title:
An Architectural Design Decision Model for Resilient IoT Application

Abstract: The Internet of Things is a paradigm that refers to the ubiquitous presence
around us of physical objects equipped with sensing, networking, and processing
capabilities that allow them to cooperate with their environment to reach
common goals. However, any threat affecting the availability of IoT
applications can be crucial financially and for the safety of the physical
integrity of users. This feature calls for IoT applications that remain
operational and efficiently handle possible threats. However, designing an IoT
application that can handle threats is challenging for stakeholders due to the
high susceptibility to threats of IoT applications and the lack of modeling
mechanisms that contemplate resilience as a first-class representation. In this
paper, an architectural Design Decision Model for Resilient IoT applications is
presented to reduce the difficulty of stakeholders in designing resilient IoT
applications. Our approach is illustrated and demonstrates the value through
the modeling of a case.


------------------------------------------------------------------------------

Title:
A novel explicit design method for complex thin-walled structures based  on embedded solid moving morphable components

Abstract: In this article, a novel explicit approach for designing complex thin-walled
structures based on the Moving Morphable Component (MMC) method is proposed,
which provides a unified framework to systematically address various design
issues, including topology optimization, reinforced-rib layout optimization,
and sandwich structure design problems. The complexity of thin-walled
structures mainly comes from flexible geometries and the variation of
thickness. On the one hand, the geometric complexity of thin-walled structures
leads to the difficulty in automatically describing material distribution
(e.g., reinforced ribs). On the other hand, thin-walled structures with
different thicknesses require various hypotheses (e.g., Kirchhoff-Love shell
theory and Reissner-Mindlin shell theory) to ensure the precision of structural
responses. Whereas for cases that do not fit the shell hypothesis, the
precision loss of response solutions is nonnegligible in the optimization
process since the accumulation of errors will cause entirely different designs.
Hence, the current article proposes a novel embedded solid component to tackle
these challenges. The geometric constraints that make the components fit to the
curved thin-walled structure are whereby satisfied. Compared with traditional
strategies, the proposed method is free from the limit of shell assumptions of
structural analysis and can achieve optimized designs with clear load
transmission paths at the cost of few design variables and degrees of freedom
for finite element analysis (FEA). Finally, we apply the proposed method to
several representative examples to demonstrate its effectiveness, efficiency,
versatility, and potential to handle complex industrial structures.


------------------------------------------------------------------------------

Title:
Generalized spectrum of second order differential operators: 3D problems

Abstract: Generalized spectra of differential operators can be related to spectra of
preconditioned discretized operators. Obtaining (estimates of) the eigenvalues
of the preconditioned discretized operators may lead to better estimating of
the quality of preconditioners. In this short paper, we answer the open
question posted in the recent paper Generalized spectrum of second order
differential operators, authored by Gergelits, Nielsen, and Strako\v s. The
proof we present allows us to fully extend characterizing the generalized
spectra of $\nabla\cdot K\nabla u=\lambda\triangle u$ to problems of dimension
three or higher.


------------------------------------------------------------------------------

Title:
IF2Net: Innately Forgetting-Free Networks for Continual Learning

Abstract: Continual learning can incrementally absorb new concepts without interfering
with previously learned knowledge. Motivated by the characteristics of neural
networks, in which information is stored in weights on connections, we
investigated how to design an Innately Forgetting-Free Network (IF2Net) for
continual learning context. This study proposed a straightforward yet effective
learning paradigm by ingeniously keeping the weights relative to each seen task
untouched before and after learning a new task. We first presented the novel
representation-level learning on task sequences with random weights. This
technique refers to tweaking the drifted representations caused by
randomization back to their separate task-optimal working states, but the
involved weights are frozen and reused (opposite to well-known layer-wise
updates of weights). Then, sequential decision-making without forgetting can be
achieved by projecting the output weight updates into the parsimonious
orthogonal space, making the adaptations not disturb old knowledge while
maintaining model plasticity. IF2Net allows a single network to inherently
learn unlimited mapping rules without telling task identities at test time by
integrating the respective strengths of randomization and orthogonalization. We
validated the effectiveness of our approach in the extensive theoretical
analysis and empirical study.


------------------------------------------------------------------------------

Title:
Predicting Risk of Dementia with Survival Machine Learning and  Statistical Methods: Results on the English Longitudinal Study of Ageing  Cohort

Abstract: Machine learning models that aim to predict dementia onset usually follow the
classification methodology ignoring the time until an event happens. This study
presents an alternative, using survival analysis within the context of machine
learning techniques. Two survival method extensions based on machine learning
algorithms of Random Forest and Elastic Net are applied to train, optimise, and
validate predictive models based on the English Longitudinal Study of Ageing
ELSA cohort. The two survival machine learning models are compared with the
conventional statistical Cox proportional hazard model, proving their superior
predictive capability and stability on the ELSA data, as demonstrated by
computationally intensive procedures such as nested cross-validation and Monte
Carlo validation. This study is the first to apply survival machine learning to
the ELSA data, and demonstrates in this case the superiority of AI based
predictive modelling approaches over the widely employed Cox statistical
approach in survival analysis. Implications, methodological considerations, and
future research directions are discussed.


------------------------------------------------------------------------------

Title:
FP-IRL: Fokker-Planck-based Inverse Reinforcement Learning -- A  Physics-Constrained Approach to Markov Decision Processes

Abstract: Inverse Reinforcement Learning (IRL) is a compelling technique for revealing
the rationale underlying the behavior of autonomous agents. IRL seeks to
estimate the unknown reward function of a Markov decision process (MDP) from
observed agent trajectories. However, IRL needs a transition function, and most
algorithms assume it is known or can be estimated in advance from data. It
therefore becomes even more challenging when such transition dynamics is not
known a-priori, since it enters the estimation of the policy in addition to
determining the system's evolution. When the dynamics of these agents in the
state-action space is described by stochastic differential equations (SDE) in
It^{o} calculus, these transitions can be inferred from the mean-field theory
described by the Fokker-Planck (FP) equation. We conjecture there exists an
isomorphism between the time-discrete FP and MDP that extends beyond the
minimization of free energy (in FP) and maximization of the reward (in MDP). We
identify specific manifestations of this isomorphism and use them to create a
novel physics-aware IRL algorithm, FP-IRL, which can simultaneously infer the
transition and reward functions using only observed trajectories. We employ
variational system identification to infer the potential function in FP, which
consequently allows the evaluation of reward, transition, and policy by
leveraging the conjecture. We demonstrate the effectiveness of FP-IRL by
applying it to a synthetic benchmark and a biological problem of cancer cell
dynamics, where the transition function is inaccessible.


------------------------------------------------------------------------------

Title:
GlyphNet: Homoglyph domains dataset and detection using attention-based  Convolutional Neural Networks

Abstract: Cyber attacks deceive machines into believing something that does not exist
in the first place. However, there are some to which even humans fall prey. One
such famous attack that attackers have used over the years to exploit the
vulnerability of vision is known to be a Homoglyph attack. It employs a primary
yet effective mechanism to create illegitimate domains that are hard to
differentiate from legit ones. Moreover, as the difference is pretty
indistinguishable for a user to notice, they cannot stop themselves from
clicking on these homoglyph domain names. In many cases, that results in either
information theft or malware attack on their systems. Existing approaches use
simple, string-based comparison techniques applied in primary language-based
tasks. Although they are impactful to some extent, they usually fail because
they are not robust to different types of homoglyphs and are computationally
not feasible because of their time requirement proportional to the string
length. Similarly, neural network-based approaches are employed to determine
real domain strings from fake ones. Nevertheless, the problem with both methods
is that they require paired sequences of real and fake domain strings to work
with, which is often not the case in the real world, as the attacker only sends
the illegitimate or homoglyph domain to the vulnerable user. Therefore,
existing approaches are not suitable for practical scenarios in the real world.
In our work, we created GlyphNet, an image dataset that contains 4M domains,
both real and homoglyphs. Additionally, we introduce a baseline method for a
homoglyph attack detection system using an attention-based convolutional Neural
Network. We show that our model can reach state-of-the-art accuracy in
detecting homoglyph attacks with a 0.93 AUC on our dataset.


------------------------------------------------------------------------------

Title:
Towards Real-time Scalable Dense Mapping using Robot-centric Implicit  Representation

Abstract: Real-time and high-quailty dense mapping is essential for robots to perform
fine tasks. However, most existing methods can not achieve both speed and
quality. Recent works have shown that implicit neural representations of 3D
scenes can produce remarkable results, but they are limited to small scenes and
lack real-time performance. To address these limitations, we propose a
real-time scalable mapping method using robot-centric implicit representation.
We train implicit features with a multi-resolution local map and decode them as
signed distance values through a shallow neural network. We maintain the
learned features in a scalable manner using a global map that consists of a
hash table and a submap set. We exploit the characteristics of the local map to
achieve highly efficient training and mitigate the catastrophic forgetting
problem in incremental implicit mapping. Extensive experiments validate that
our method outperforms existing methods in reconstruction quality, real-time
performance, and applicability. The code of our system will be available at
\url{this https URL}.


------------------------------------------------------------------------------

Title:
Optical Integrated Sensing and Communication for Cooperative Mobile  Robotics Design and Experiments

Abstract: Integrated Sensing and Communication (ISAC) is an emerging technology that
integrates wireless sensing and communication into a single system,
transforming many applications, including cooperative mobile robotics. However,
in scenarios where radio communications are unavailable, alternative approaches
are needed. In this paper, we propose a new optical ISAC (OISAC) scheme for
cooperative mobile robots by integrating camera sensing and screen-camera
communication (SCC). Unlike previous throughput-oriented SCC designs that work
with stationary SCC links, our OISAC scheme is designed for real-time control
of mobile robots. It addresses new problems such as image blur and long image
display delay. As a case study, we consider the leader-follower formation
control problem, an essential part of cooperative mobile robotics. The proposed
OISAC scheme enables the follower robot to simultaneously acquire the
information shared by the leader and sense the relative pose to the leader
using only RGB images captured by its onboard camera. We then design a new
control law that can leverage all the information acquired by the camera to
achieve stable and accurate formations. We design and conduct real-world
experiments involving uniform and nonuniform motions to evaluate the proposed
system and demonstrate the advantages of applying OISAC over a benchmark
approach that uses extended Kalman filtering (EKF) to estimate the leader's
states. Our results show that the proposed OISAC-augmented leader-follower
formation system achieves better performance in terms of accuracy, stability,
and robustness.


------------------------------------------------------------------------------

Title:
Point-Cloud Completion with Pretrained Text-to-image Diffusion Models

Abstract: Point-cloud data collected in real-world applications are often incomplete.
Data is typically missing due to objects being observed from partial
viewpoints, which only capture a specific perspective or angle. Additionally,
data can be incomplete due to occlusion and low-resolution sampling. Existing
completion approaches rely on datasets of predefined objects to guide the
completion of noisy and incomplete, point clouds. However, these approaches
perform poorly when tested on Out-Of-Distribution (OOD) objects, that are
poorly represented in the training dataset. Here we leverage recent advances in
text-guided image generation, which lead to major breakthroughs in text-guided
shape generation. We describe an approach called SDS-Complete that uses a
pre-trained text-to-image diffusion model and leverages the text semantics of a
given incomplete point cloud of an object, to obtain a complete surface
representation. SDS-Complete can complete a variety of objects using test-time
optimization without expensive collection of 3D information. We evaluate SDS
Complete on incomplete scanned objects, captured by real-world depth sensors
and LiDAR scanners. We find that it effectively reconstructs objects that are
absent from common datasets, reducing Chamfer loss by 50% on average compared
with current methods. Project page: this https URL


------------------------------------------------------------------------------

Title:
Typo-Robust Representation Learning for Dense Retrieval

Abstract: Dense retrieval is a basic building block of information retrieval
applications. One of the main challenges of dense retrieval in real-world
settings is the handling of queries containing misspelled words. A popular
approach for handling misspelled queries is minimizing the representations
discrepancy between misspelled queries and their pristine ones. Unlike the
existing approaches, which only focus on the alignment between misspelled and
pristine queries, our method also improves the contrast between each misspelled
query and its surrounding queries. To assess the effectiveness of our proposed
method, we compare it against the existing competitors using two benchmark
datasets and two base encoders. Our method outperforms the competitors in all
cases with misspelled queries. Our code and models are available at
this https URL com/panuthept/DST-DenseRetrieval.


------------------------------------------------------------------------------

Title:
Multilingual Multiword Expression Identification Using Lateral  Inhibition and Domain Adaptation

Abstract: Correctly identifying multiword expressions (MWEs) is an important task for
most natural language processing systems since their misidentification can
result in ambiguity and misunderstanding of the underlying text. In this work,
we evaluate the performance of the mBERT model for MWE identification in a
multilingual context by training it on all 14 languages available in version
1.2 of the PARSEME corpus. We also incorporate lateral inhibition and language
adversarial training into our methodology to create language-independent
embeddings and improve its capabilities in identifying multiword expressions.
The evaluation of our models shows that the approach employed in this work
achieves better results compared to the best system of the PARSEME 1.2
competition, MTLB-STRUCT, on 11 out of 14 languages for global MWE
identification and on 12 out of 14 languages for unseen MWE identification.
Additionally, averaged across all languages, our best approach outperforms the
MTLB-STRUCT system by 1.23% on global MWE identification and by 4.73% on unseen
global MWE identification.


------------------------------------------------------------------------------

Title:
A Universal Semantic-Geometric Representation for Robotic Manipulation

Abstract: Robots rely heavily on sensors, especially RGB and depth cameras, to perceive
and interact with the world. RGB cameras record 2D images with rich semantic
information while missing precise spatial information. On the other side, depth
cameras offer critical 3D geometry data but capture limited semantics.
Therefore, integrating both modalities is crucial for learning representations
for robotic perception and control. However, current research predominantly
focuses on only one of these modalities, neglecting the benefits of
incorporating both. To this end, we present Semantic-Geometric Representation
(SGR), a universal perception module for robotics that leverages the rich
semantic information of large-scale pre-trained 2D models and inherits the
merits of 3D spatial reasoning. Our experiments demonstrate that SGR empowers
the agent to successfully complete a diverse range of simulated and real-world
robotic manipulation tasks, outperforming state-of-the-art methods
significantly in both single-task and multi-task settings. Furthermore, SGR
possesses the unique capability to generalize to novel semantic attributes,
setting it apart from the other methods.


------------------------------------------------------------------------------

Title:
Differentially Private Histogram, Predecessor, and Set Cardinality under  Continual Observation

Abstract: Differential privacy is the de-facto privacy standard in data analysis. The
classic model of differential privacy considers the data to be static. The
dynamic setting, called differential privacy under continual observation,
captures many applications more realistically. In this work we consider several
natural dynamic data structure problems under continual observation, where we
want to maintain information about a changing data set such that we can answer
certain sets of queries at any given time while satisfying
$\epsilon$-differential privacy. The problems we consider include (a)
maintaining a histogram and various extensions of histogram queries such as
quantile queries, (b) maintaining a predecessor search data structure of a
dynamically changing set in a given ordered universe, and (c) maintaining the
cardinality of a dynamically changing set. For (a) we give new error bounds
parameterized in the maximum output of any query $c_{\max}$: our algorithm
gives an upper bound of $O(d\log^2dc_{\max}+\log T)$ for computing histogram,
the maximum and minimum column sum, quantiles on the column sums, and related
queries. The bound holds for unknown $c_{\max}$ and $T$. For (b), we give a
general reduction to orthogonal range counting. Further, we give an improvement
for the case where only insertions are allowed. We get a data structure which
for a given query, returns an interval that contains the predecessor, and at
most $O(\log^2 u \sqrt{\log T})$ more elements, where $u$ is the size of the
universe. The bound holds for unknown $T$. Lastly, for (c), we give a
parameterized upper bound of $O(\min(d,\sqrt{K\log T}))$, where $K$ is an upper
bound on the number of updates. We show a matching lower bound. Finally, we
show how to extend the bound for (c) for unknown $K$ and $T$.


------------------------------------------------------------------------------

Title:
Deep Reinforcement Learning for Flipper Control of Tracked Robots

Abstract: The autonomous control of flippers plays an important role in enhancing the
intelligent operation of tracked robots within complex environments. While
existing methods mainly rely on hand-crafted control models, in this paper, we
introduce a novel approach that leverages deep reinforcement learning (DRL)
techniques for autonomous flipper control in complex terrains. Specifically, we
propose a new DRL network named AT-D3QN, which ensures safe and smooth flipper
control for tracked robots. It comprises two modules, a feature extraction and
fusion module for extracting and integrating robot and environment state
features, and a deep Q-Learning control generation module for incorporating
expert knowledge to obtain a smooth and efficient control strategy. To train
the network, a novel reward function is proposed, considering both learning
efficiency and passing smoothness. A simulation environment is constructed
using the Pymunk physics engine for training. We then directly apply the
trained model to a more realistic Gazebo simulation for quantitative analysis.
The consistently high performance of the proposed approach validates its
superiority over manual teleoperation.


------------------------------------------------------------------------------

Title:
Do as I can, not as I get: Topology-aware multi-hop reasoning on  multi-modal knowledge graphs

Abstract: Multi-modal knowledge graph (MKG) includes triplets that consist of entities
and relations and multi-modal auxiliary data. In recent years, multi-hop
multi-modal knowledge graph reasoning (MMKGR) based on reinforcement learning
(RL) has received extensive attention because it addresses the intrinsic
incompleteness of MKG in an interpretable manner. However, its performance is
limited by empirically designed rewards and sparse relations. In addition, this
method has been designed for the transductive setting where test entities have
been seen during training, and it works poorly in the inductive setting where
test entities do not appear in the training set. To overcome these issues, we
propose TMR (Topology-aware Multi-hop Reasoning), which can conduct MKG
reasoning under inductive and transductive settings. Specifically, TMR mainly
consists of two components. (1) The topology-aware inductive representation
captures information from the directed relations of unseen entities, and
aggregates query-related topology features in an attentive manner to generate
the fine-grained entity-independent features. (2) After completing multi-modal
feature fusion, the relation-augment adaptive RL conducts multi-hop reasoning
by eliminating manual rewards and dynamically adding actions. Finally, we
construct new MKG datasets with different scales for inductive reasoning
evaluation. Experimental results demonstrate that TMP outperforms
state-of-the-art MKGR methods under both inductive and transductive settings.


------------------------------------------------------------------------------

Title:
BOBA: A Parallel Lightweight Graph Reordering Algorithm with Heavyweight  Implications

Abstract: We describe a simple parallel-friendly lightweight graph reordering algorithm
for COO graphs (edge lists). Our
``Batched Order By Attachment'' (BOBA) algorithm is linear in the number of
edges in terms of reads and linear in the number of vertices for writes through
to main memory. It is highly parallelizable on GPUs\@. We show that, compared
to a randomized baseline, the ordering produced gives improved locality of
reference in sparse matrix-vector multiplication (SpMV) as well as other graph
algorithms. Moreover, it can substantially speed up the conversion from a COO
representation to the compressed format CSR, a very common workflow. Thus, it
can give \emph{end-to-end} speedups even in SpMV\@. Unlike other lightweight
approaches, this reordering does not rely on explicitly knowing the degrees of
the vertices, and indeed its runtime is comparable to that of computing
degrees. Instead, it uses the structure and edge distribution inherent in the
input edge list, making it a candidate for default use in a pragmatic graph
creation pipeline. This algorithm is suitable for road-type networks as well as
scale-free. It improves cache locality on both CPUs and GPUs, achieving hit
rates similar to the heavyweight techniques (e.g., for SpMV, 7--52\% and
11--67\% in the L1 and L2 caches, respectively). Compared to randomly labeled
graphs, BOBA-reordered graphs achieve end-to-end speedups of up to 3.45. The
reordering time is approximately one order of magnitude faster than existing
lightweight techniques and up to 2.5 orders of magnitude faster than
heavyweight techniques.


------------------------------------------------------------------------------

Title:
Personalized Elastic Embedding Learning for On-Device Recommendation

Abstract: To address privacy concerns and reduce network latency, there has been a
recent trend of compressing cumbersome recommendation models trained on the
cloud and deploying compact recommender models to resource-limited devices for
real-time recommendation. Existing solutions generally overlook device
heterogeneity and user heterogeneity. They either require all devices to share
the same compressed model or the devices with the same resource budget to share
the same model. However, even users with the same devices may have different
preferences. In addition, they assume the available resources (e.g., memory)
for the recommender on a device are constant, which is not reflective of
reality. In light of device and user heterogeneities as well as dynamic
resource constraints, this paper proposes a Personalized Elastic Embedding
Learning framework (PEEL) for on-device recommendation, which generates
personalized embeddings for devices with various memory budgets in once-for-all
manner, efficiently adapting to new or dynamic budgets, and effectively
addressing user preference diversity by assigning personalized embeddings for
different groups of users. Specifically, it pretrains using user-item
interaction instances to generate the global embedding table and cluster users
into groups. Then, it refines the embedding tables with local interaction
instances within each group. Personalized elastic embedding is generated from
the group-wise embedding blocks and their weights that indicate the
contribution of each embedding block to the local recommendation performance.
PEEL efficiently generates personalized elastic embeddings by selecting
embedding blocks with the largest weights, making it adaptable to dynamic
memory budgets. Extensive experiments are conducted on two public datasets, and
the results show that PEEL yields superior performance on devices with
heterogeneous and dynamic memory budgets.


------------------------------------------------------------------------------

Title:
Old and New Minimalism: a Hopf algebra comparison

Abstract: In this paper we compare some old formulations of Minimalism, in particular
Stabler's computational minimalism, and Chomsky's new formulation of Merge and
Minimalism, from the point of view of their mathematical description in terms
of Hopf algebras. We show that the newer formulation has a clear advantage
purely in terms of the underlying mathematical structure. More precisely, in
the case of Stabler's computational minimalism, External Merge can be described
in terms of a partially defined operated algebra with binary operation, while
Internal Merge determines a system of right-ideal coideals of the Loday-Ronco
Hopf algebra and corresponding right-module coalgebra quotients. This
mathematical structure shows that Internal and External Merge have
significantly different roles in the old formulations of Minimalism, and they
are more difficult to reconcile as facets of a single algebraic operation, as
desirable linguistically. On the other hand, we show that the newer formulation
of Minimalism naturally carries a Hopf algebra structure where Internal and
External Merge directly arise from the same operation. We also compare, at the
level of algebraic properties, the externalization model of the new Minimalism
with proposals for assignments of planar embeddings based on heads of trees.


------------------------------------------------------------------------------

Title:
A Smoothed FPTAS for Equilibria in Congestion Games

Abstract: We present a fully polynomial-time approximation scheme (FPTAS) for computing
equilibria in congestion games, under \emph{smoothed} running-time analysis.
More precisely, we prove that if the resource costs of a congestion game are
randomly perturbed by independent noises, whose density is at most $\phi$, then
\emph{any} sequence of $(1+\varepsilon)$-improving dynamics will reach an
$(1+\varepsilon)$-approximate pure Nash equilibrium (PNE) after an expected
number of steps which is strongly polynomial in $\frac{1}{\varepsilon}$,
$\phi$, and the size of the game's description. Our results establish a sharp
contrast to the traditional worst-case analysis setting, where it is known that
better-response dynamics take exponentially long to converge to
$\alpha$-approximate PNE, for any constant factor $\alpha\geq 1$. As a matter
of fact, computing $\alpha$-approximate PNE in congestion games is PLS-hard.
We demonstrate how our analysis can be applied to various different models of
congestion games including general, step-function, and polynomial cost, as well
as fair cost-sharing games (where the resource costs are decreasing). It is
important to note that our bounds do not depend explicitly on the cardinality
of the players' strategy sets, and thus the smoothed FPTAS is readily
applicable to network congestion games as well.


------------------------------------------------------------------------------

Title:
PIMMiner: A High-performance PIM Architecture-aware Graph Mining  Framework

Abstract: Graph mining applications, such as subgraph pattern matching and mining, are
widely used in real-world domains such as bioinformatics, social network
analysis, and computer vision. Such applications are considered a new class of
data-intensive applications that generate massive irregular computation
workloads and memory accesses, which degrade the performance significantly.
Leveraging emerging hardware, such as process-in-memory (PIM) technology, could
potentially accelerate such applications. In this paper, we propose PIMMiner, a
high-performance PIM architecture graph mining framework. We first identify
that current PIM architecture cannot be fully utilized by graph mining
applications. Next, we propose a set of optimizations and interfaces that
enhance the locality, and internal bandwidth utilization and reduce remote bank
accesses and load imbalance through cohesive algorithm and architecture
co-designs. We compare PIMMiner with several state-of-the-art graph mining
frameworks and show that PIMMiner is able to outperform all of them
significantly.


------------------------------------------------------------------------------

Title:
Residual Spatial Fusion Network for RGB-Thermal Semantic Segmentation

Abstract: Semantic segmentation plays an important role in widespread applications such
as autonomous driving and robotic sensing. Traditional methods mostly use RGB
images which are heavily affected by lighting conditions, \eg, darkness. Recent
studies show thermal images are robust to the night scenario as a compensating
modality for segmentation. However, existing works either simply fuse
RGB-Thermal (RGB-T) images or adopt the encoder with the same structure for
both the RGB stream and the thermal stream, which neglects the modality
difference in segmentation under varying lighting conditions. Therefore, this
work proposes a Residual Spatial Fusion Network (RSFNet) for RGB-T semantic
segmentation. Specifically, we employ an asymmetric encoder to learn the
compensating features of the RGB and the thermal images. To effectively fuse
the dual-modality features, we generate the pseudo-labels by saliency detection
to supervise the feature learning, and develop the Residual Spatial Fusion
(RSF) module with structural re-parameterization to learn more promising
features by spatially fusing the cross-modality features. RSF employs a
hierarchical feature fusion to aggregate multi-level features, and applies the
spatial weights with the residual connection to adaptively control the
multi-spectral feature fusion by the confidence gate. Extensive experiments
were carried out on two benchmarks, \ie, MFNet database and PST900 database.
The results have shown the state-of-the-art segmentation performance of our
method, which achieves a good balance between accuracy and speed.


------------------------------------------------------------------------------

Title:
Managing health insurance using blockchain technology

Abstract: Health insurance plays a significant role in ensuring quality healthcare. In
response to the escalating costs of the medical industry, the demand for health
insurance is soaring. Additionally, those with health insurance are more likely
to receive preventative care than those without health insurance. However, from
granting health insurance to delivering services to insured individuals, the
health insurance industry faces numerous obstacles. Fraudulent actions, false
claims, a lack of transparency and data privacy, reliance on human effort and
dishonesty from consumers, healthcare professionals, or even the insurer party
itself, are the most common and important hurdles towards success. Given these
constraints, this chapter briefly covers the most immediate concerns in the
health insurance industry and provides insight into how blockchain technology
integration can contribute to resolving these issues. This chapter finishes by
highlighting existing limitations as well as potential future directions.


------------------------------------------------------------------------------

Title:
Fair Causal Feature Selection

Abstract: Causal feature selection has recently received increasing attention in
machine learning. Existing causal feature selection algorithms select unique
causal features of a class variable as the optimal feature subset. However, a
class variable usually has multiple states, and it is unfair to select the same
causal features for different states of a class variable. To address this
problem, we employ the class-specific mutual information to evaluate the causal
information carried by each state of the class attribute, and theoretically
analyze the unique relationship between each state and the causal features.
Based on this, a Fair Causal Feature Selection algorithm (FairCFS) is proposed
to fairly identifies the causal features for each state of the class variable.
Specifically, FairCFS uses the pairwise comparisons of class-specific mutual
information and the size of class-specific mutual information values from the
perspective of each state, and follows a divide-and-conquer framework to find
causal features. The correctness and application condition of FairCFS are
theoretically proved, and extensive experiments are conducted to demonstrate
the efficiency and superiority of FairCFS compared to the state-of-the-art
approaches.


------------------------------------------------------------------------------

Title:
Rapid Image Labeling via Neuro-Symbolic Learning

Abstract: The success of Computer Vision (CV) relies heavily on manually annotated
data. However, it is prohibitively expensive to annotate images in key domains
such as healthcare, where data labeling requires significant domain expertise
and cannot be easily delegated to crowd workers. To address this challenge, we
propose a neuro-symbolic approach called Rapid, which infers image labeling
rules from a small amount of labeled data provided by domain experts and
automatically labels unannotated data using the rules. Specifically, Rapid
combines pre-trained CV models and inductive logic learning to infer the
logic-based labeling rules. Rapid achieves a labeling accuracy of 83.33% to
88.33% on four image labeling tasks with only 12 to 39 labeled samples. In
particular, Rapid significantly outperforms finetuned CV models in two highly
specialized tasks. These results demonstrate the effectiveness of Rapid in
learning from small data and its capability to generalize among different
tasks. Code and our dataset are publicly available at
this https URL


------------------------------------------------------------------------------

Title:
A new approach based on quadratic forms to attack the McEliece  cryptosystem

Abstract: We bring in here a novel algebraic approach for attacking the McEliece
cryptosystem. It consists in introducing a subspace of matrices representing
quadratic forms. Those are associated with quadratic relationships for the
component-wise product in the dual of the code used in the cryptosystem.
Depending on the characteristic of the code field, this space of matrices
consists only of symmetric matrices or skew-symmetric matrices. This matrix
space is shown to contain unusually low-rank matrices (rank $2$ or $3$
depending on the characteristic) which reveal the secret polynomial structure
of the code. Finding such matrices can then be used to recover the secret key
of the scheme. We devise a dedicated approach in characteristic $2$ consisting
in using a Gr\"obner basis modeling that a skew-symmetric matrix is of rank
$2$. This allows to analyze the complexity of solving the corresponding
algebraic system with Gr\"obner bases techniques. This computation behaves
differently when applied to the skew-symmetric matrix space associated with a
random code rather than with a Goppa or an alternant code. This gives a
distinguisher of the latter code family. We give a bound on its complexity
which turns out to interpolate nicely between polynomial and exponential
depending on the code parameters. A distinguisher for alternant/Goppa codes was
already known [FGO+11]. It is of polynomial complexity but works only in a
narrow parameter regime. This new distinguisher is also polynomial for the
parameter regime necessary for [FGO+11] but contrarily to the previous one is
able to operate for virtually all code parameters relevant to cryptography.
Moreover, we use this matrix space to find a polynomial time attack of the
McEliece cryptosystem provided that the Goppa code is distinguishable by the
method of [FGO+11] and its degree is less than $q-1$, where $q$ is the alphabet
size of the code.


------------------------------------------------------------------------------

Title:
SRL-ORCA: A Socially Aware Multi-Agent Mapless Navigation Algorithm In  Complex Dynamic Scenes

Abstract: For real-world navigation, it is important to endow robots with the
capabilities to navigate safely and efficiently in a complex environment with
both dynamic and non-convex static obstacles. However, achieving path-finding
in non-convex complex environments without maps as well as enabling multiple
robots to follow social rules for obstacle avoidance remains challenging
problems. In this letter, we propose a socially aware robot mapless navigation
algorithm, namely Safe Reinforcement Learning-Optimal Reciprocal Collision
Avoidance (SRL-ORCA). This is a multi-agent safe reinforcement learning
algorithm by using ORCA as an external knowledge to provide a safety guarantee.
This algorithm further introduces traffic norms of human society to improve
social comfort and achieve cooperative avoidance by following human social
customs. The result of experiments shows that SRL-ORCA learns strategies to
obey specific traffic rules. Compared to DRL, SRL-ORCA shows a significant
improvement in navigation success rate in different complex scenarios mixed
with the application of the same training network. SRL-ORCA is able to cope
with non-convex obstacle environments without falling into local minimal
regions and has a 14.1\% improvement in path quality (i.e., the average time to
target) compared to ORCA. Videos are available at this https URL


------------------------------------------------------------------------------

Title:
Evolutionary Verbalizer Search for Prompt-based Few Shot Text  Classification

Abstract: Recent advances for few-shot text classification aim to wrap textual inputs
with task-specific prompts to cloze questions. By processing them with a masked
language model to predict the masked tokens and using a verbalizer that
constructs the mapping between predicted words and target labels. This approach
of using pre-trained language models is called prompt-based tuning, which could
remarkably outperform conventional fine-tuning approach in the low-data
scenario. As the core of prompt-based tuning, the verbalizer is usually
handcrafted with human efforts or suboptimally searched by gradient descent. In
this paper, we focus on automatically constructing the optimal verbalizer and
propose a novel evolutionary verbalizer search (EVS) algorithm, to improve
prompt-based tuning with the high-performance verbalizer. Specifically,
inspired by evolutionary algorithm (EA), we utilize it to automatically evolve
various verbalizers during the evolutionary procedure and select the best one
after several iterations. Extensive few-shot experiments on five text
classification datasets show the effectiveness of our method.


------------------------------------------------------------------------------

Title:
OpenDataVal: a Unified Benchmark for Data Valuation

Abstract: Assessing the quality and impact of individual data points is critical for
improving model performance and mitigating undesirable biases within the
training dataset. Several data valuation algorithms have been proposed to
quantify data quality, however, there lacks a systemic and standardized
benchmarking system for data valuation. In this paper, we introduce
OpenDataVal, an easy-to-use and unified benchmark framework that empowers
researchers and practitioners to apply and compare various data valuation
algorithms. OpenDataVal provides an integrated environment that includes (i) a
diverse collection of image, natural language, and tabular datasets, (ii)
implementations of nine different state-of-the-art data valuation algorithms,
and (iii) a prediction model API that can import any models in scikit-learn.
Furthermore, we propose four downstream machine learning tasks for evaluating
the quality of data values. We perform benchmarking analysis using OpenDataVal,
quantifying and comparing the efficacy of state-of-the-art data valuation
approaches. We find that no single algorithm performs uniformly best across all
tasks, and an appropriate algorithm should be employed for a user's downstream
task. OpenDataVal is publicly available at this https URL with
comprehensive documentation. Furthermore, we provide a leaderboard where
researchers can evaluate the effectiveness of their own data valuation
algorithms.


------------------------------------------------------------------------------

Title:
Universal quantification makes automatic structures hard to decide

Abstract: Automatic structures are structures whose universe and relations can be
represented as regular languages. It follows from the standard closure
properties of regular languages that the first-order theory of an automatic
structure is decidable. While existential quantifiers can be eliminated in
linear time by application of a homomorphism, universal quantifiers are
commonly eliminated via the identity $\forall\,x\,.\,\Phi \equiv \neg
(\exists\,x\,.\,\neg \Phi)$. If $\Phi$ is represented in the standard way as an
NFA, a priori this approach results in a doubly exponential blow-up. However,
the recent literature has shown that there are classes of automatic structures
for which universal quantifiers can be eliminated by different means without
this blow-up by treating them as first-class citizens and not resorting to
double complementation. While existing lower bounds for some classes of
automatic structures show that a singly exponential blow-up is unavoidable when
eliminating a universal quantifier, it is not known whether there may be better
approaches that avoid the na\"ive doubly exponential blow-up, perhaps at least
in restricted settings.
In this paper, we answer this question negatively and show that there is a
family of NFA representing automatic relations for which the minimal NFA
recognising the language after eliminating a single universal quantifier is
doubly exponential, and deciding whether this language is empty is
ExpSpace-complete.


------------------------------------------------------------------------------

Title:
Adversaries with Limited Information in the Friedkin--Johnsen Model

Abstract: In recent years, online social networks have been the target of adversaries
who seek to introduce discord into societies, to undermine democracies and to
destabilize communities. Often the goal is not to favor a certain side of a
conflict but to increase disagreement and polarization. To get a mathematical
understanding of such attacks, researchers use opinion-formation models from
sociology, such as the Friedkin--Johnsen model, and formally study how much
discord the adversary can produce when altering the opinions for only a small
set of users. In this line of work, it is commonly assumed that the adversary
has full knowledge about the network topology and the opinions of all users.
However, the latter assumption is often unrealistic in practice, where user
opinions are not available or simply difficult to estimate accurately.
To address this concern, we raise the following question: Can an attacker sow
discord in a social network, even when only the network topology is known? We
answer this question affirmatively. We present approximation algorithms for
detecting a small set of users who are highly influential for the disagreement
and polarization in the network. We show that when the adversary radicalizes
these users and if the initial disagreement/polarization in the network is not
very high, then our method gives a constant-factor approximation on the setting
when the user opinions are known. To find the set of influential users, we
provide a novel approximation algorithm for a variant of MaxCut in graphs with
positive and negative edge weights. We experimentally evaluate our methods,
which have access only to the network topology, and we find that they have
similar performance as methods that have access to the network topology and all
user opinions. We further present an NP-hardness proof, which was an open
question by Chen and Racz [IEEE Trans. Netw. Sci. Eng., 2021].


------------------------------------------------------------------------------

Title:
Edge Intelligence Over the Air: Two Faces of Interference in Federated  Learning

Abstract: Federated edge learning is envisioned as the bedrock of enabling intelligence
in next-generation wireless networks, but the limited spectral resources often
constrain its scalability. In light of this challenge, a line of recent
research suggested integrating analog over-the-air computations into federated
edge learning systems, to exploit the superposition property of electromagnetic
waves for fast aggregation of intermediate parameters and achieve (almost)
unlimited scalability. Over-the-air computations also benefit the system in
other aspects, such as low hardware cost, reduced access latency, and enhanced
privacy protection. Despite these advantages, the interference introduced by
wireless communications also influences various aspects of the model training
process, while its importance is not well recognized yet. This article provides
a comprehensive overview of the positive and negative effects of interference
on over-the-air computation-based edge learning systems. The potential open
issues and research trends are also discussed.


------------------------------------------------------------------------------

Title:
Trauma lurking in the shadows: A Reddit case study of mental health  issues in online posts about Childhood Sexual Abuse

Abstract: Childhood Sexual Abuse (CSA) is a menace to society and has long-lasting
effects on the mental health of the survivors. From time to time CSA survivors
are haunted by various mental health issues in their lifetime. Proper care and
attention towards CSA survivors facing mental health issues can drastically
improve the mental health conditions of CSA survivors. Previous works
leveraging online social media (OSM) data for understanding mental health
issues haven't focused on mental health issues in individuals with CSA
background. Our work fills this gap by studying Reddit posts related to CSA to
understand their mental health issues. Mental health issues such as depression,
anxiety, and Post-Traumatic Stress Disorder (PTSD) are most commonly observed
in posts with CSA background. Observable differences exist between posts
related to mental health issues with and without CSA background. Keeping this
difference in mind, for identifying mental health issues in posts with CSA
exposure we develop a two-stage framework. The first stage involves classifying
posts with and without CSA background and the second stage involves recognizing
mental health issues in posts that are classified as belonging to CSA
background. The top model in the first stage is able to achieve accuracy and
f1-score (macro) of 96.26% and 96.24%. and in the second stage, the top model
reports hamming score of 67.09%. Content Warning: Reader discretion is
recommended as our study tackles topics such as child sexual abuse,
molestation, etc.


------------------------------------------------------------------------------

Title:
Summarization from Leaderboards to Practice: Choosing A Representation  Backbone and Ensuring Robustness

Abstract: Academic literature does not give much guidance on how to build the best
possible customer-facing summarization system from existing research
components. Here we present analyses to inform the selection of a system
backbone from popular models; we find that in both automatic and human
evaluation, BART performs better than PEGASUS and T5. We also find that when
applied cross-domain, summarizers exhibit considerably worse performance. At
the same time, a system fine-tuned on heterogeneous domains performs well on
all domains and will be most suitable for a broad-domain summarizer. Our work
highlights the need for heterogeneous domain summarization benchmarks. We find
considerable variation in system output that can be captured only with human
evaluation and are thus unlikely to be reflected in standard leaderboards with
only automatic evaluation.


------------------------------------------------------------------------------

Title:
2D-Shapley: A Framework for Fragmented Data Valuation

Abstract: Data valuation -- quantifying the contribution of individual data sources to
certain predictive behaviors of a model -- is of great importance to enhancing
the transparency of machine learning and designing incentive systems for data
sharing. Existing work has focused on evaluating data sources with the shared
feature or sample space. How to valuate fragmented data sources of which each
only contains partial features and samples remains an open question. We start
by presenting a method to calculate the counterfactual of removing a fragment
from the aggregated data matrix. Based on the counterfactual calculation, we
further propose 2D-Shapley, a theoretical framework for fragmented data
valuation that uniquely satisfies some appealing axioms in the fragmented data
context. 2D-Shapley empowers a range of new use cases, such as selecting useful
data fragments, providing interpretation for sample-wise data values, and
fine-grained data issue diagnosis.


------------------------------------------------------------------------------

Title:
Fast Fourier Inception Networks for Occluded Video Prediction

Abstract: Video prediction is a pixel-level task that generates future frames by
employing the historical frames. There often exist continuous complex motions,
such as object overlapping and scene occlusion in video, which poses great
challenges to this task. Previous works either fail to well capture the
long-term temporal dynamics or do not handle the occlusion masks. To address
these issues, we develop the fully convolutional Fast Fourier Inception
Networks for video prediction, termed \textit{FFINet}, which includes two
primary components, \ie, the occlusion inpainter and the spatiotemporal
translator. The former adopts the fast Fourier convolutions to enlarge the
receptive field, such that the missing areas (occlusion) with complex geometric
structures are filled by the inpainter. The latter employs the stacked Fourier
transform inception module to learn the temporal evolution by group
convolutions and the spatial movement by channel-wise Fourier convolutions,
which captures both the local and the global spatiotemporal features. This
encourages generating more realistic and high-quality future frames. To
optimize the model, the recovery loss is imposed to the objective, \ie,
minimizing the mean square error between the ground-truth frame and the
recovery frame. Both quantitative and qualitative experimental results on five
benchmarks, including Moving MNIST, TaxiBJ, Human3.6M, Caltech Pedestrian, and
KTH, have demonstrated the superiority of the proposed approach. Our code is
available at GitHub.


------------------------------------------------------------------------------

Title:
Tailoring Machine Learning for Process Mining

Abstract: Machine learning models are routinely integrated into process mining
pipelines to carry out tasks like data transformation, noise reduction, anomaly
detection, classification, and prediction. Often, the design of such models is
based on some ad-hoc assumptions about the corresponding data distributions,
which are not necessarily in accordance with the non-parametric distributions
typically observed with process data. Moreover, the learning procedure they
follow ignores the constraints concurrency imposes to process data. Data
encoding is a key element to smooth the mismatch between these assumptions but
its potential is poorly exploited. In this paper, we argue that a deeper
insight into the issues raised by training machine learning models with process
data is crucial to ground a sound integration of process mining and machine
learning. Our analysis of such issues is aimed at laying the foundation for a
methodology aimed at correctly aligning machine learning with process mining
requirements and stimulating the research to elaborate in this direction.


------------------------------------------------------------------------------

Title:
Object counting from aerial remote sensing images: application to  wildlife and marine mammals

Abstract: Anthropogenic activities pose threats to wildlife and marine fauna, prompting
the need for efficient animal counting methods. This research study utilizes
deep learning techniques to automate counting tasks. Inspired by previous
studies on crowd and animal counting, a UNet model with various backbones is
implemented, which uses Gaussian density maps for training, bypassing the need
of training a detector. The new model is applied to the task of counting
dolphins and elephants in aerial images. Quantitative evaluation shows
promising results, with the EfficientNet-B5 backbone achieving the best
performance for African elephants and the ResNet18 backbone for dolphins. The
model accurately locates animals despite complex image background conditions.
By leveraging artificial intelligence, this research contributes to wildlife
conservation efforts and enhances coexistence between humans and wildlife
through efficient object counting without detection from aerial remote sensing.


------------------------------------------------------------------------------

Title:
Understanding Certified Training with Interval Bound Propagation

Abstract: As robustness verification methods are becoming more precise, training
certifiably robust neural networks is becoming ever more relevant. To this end,
certified training methods compute and then optimize an upper bound on the
worst-case loss over a robustness specification. Curiously, training methods
based on the imprecise interval bound propagation (IBP) consistently outperform
those leveraging more precise bounding methods. Still, we lack an understanding
of the mechanisms making IBP so successful.
In this work, we thoroughly investigate these mechanisms by leveraging a
novel metric measuring the tightness of IBP bounds. We first show theoretically
that, for deep linear models, tightness decreases with width and depth at
initialization, but improves with IBP training, given sufficient network width.
We, then, derive sufficient and necessary conditions on weight matrices for IBP
bounds to become exact and demonstrate that these impose strong regularization,
explaining the empirically observed trade-off between robustness and accuracy
in certified training.
Our extensive experimental evaluation validates our theoretical predictions
for ReLU networks, including that wider networks improve performance, yielding
state-of-the-art results. Interestingly, we observe that while all IBP-based
training methods lead to high tightness, this is neither sufficient nor
necessary to achieve high certifiable robustness. This hints at the existence
of new training methods that do not induce the strong regularization required
for tight IBP bounds, leading to improved robustness and standard accuracy.


------------------------------------------------------------------------------

Title:
Persian Semantic Role Labeling Using Transfer Learning and BERT-Based  Models

Abstract: Semantic role labeling (SRL) is the process of detecting the
predicate-argument structure of each predicate in a sentence. SRL plays a
crucial role as a pre-processing step in many NLP applications such as topic
and concept extraction, question answering, summarization, machine translation,
sentiment analysis, and text mining. Recently, in many languages, unified SRL
dragged lots of attention due to its outstanding performance, which is the
result of overcoming the error propagation problem. However, regarding the
Persian language, all previous works have focused on traditional methods of SRL
leading to a drop in accuracy and imposing expensive feature extraction steps
in terms of financial resources, time and energy consumption. In this work, we
present an end-to-end SRL method that not only eliminates the need for feature
extraction but also outperforms existing methods in facing new samples in
practical situations. The proposed method does not employ any auxiliary
features and shows more than 16 (83.16) percent improvement in accuracy against
previous methods in similar circumstances.


------------------------------------------------------------------------------

Title:
CAV-based Traffic Control to Mitigate the Impact of Congestion from  Bottlenecks- A Linear-Quadratic-Regulator-based Approach and Microsimulation  Study

Abstract: This work investigates traffic control via controlled connected and automated
vehicles (CAVs) using novel controllers derived from the linear-quadratic
regulator (LQR) theory. CAV-platoons are modeled as moving bottlenecks
impacting the surrounding traffic with their speeds as control inputs. An
iterative controller algorithm based on the LQR theory is proposed along with a
variant that allows for penalizing abrupt changes in platoons speeds. The
controllers use the Lighthill-Whitham-Richards (LWR) model implemented using an
extended cell transmission model (CTM) which considers the capacity drop
phenomenon for a realistic representation of traffic in congestion. The impact
of various parameters of the proposed controller on the control performance is
analyzed. The effectiveness of the proposed traffic control algorithms is
tested using a traffic control example and compared with existing
proportional-integral (PI)- and model predictive control (MPC)- based
controllers from the literature. A case study using the TransModeler traffic
microsimulation software is conducted to test the usability of the proposed
controller as well as existing controllers in a realistic setting and derive
qualitative insights. It is observed that the proposed controller works well in
both settings to mitigate the impact of the jam caused by a fixed bottleneck.
The computation time required by the controller is also small making it
suitable for real-time control.


------------------------------------------------------------------------------

Title:
AI Clinics on Mobile (AICOM): Universal AI Doctors for the Underserved  and Hard-to-Reach

Abstract: This paper introduces Artificial Intelligence Clinics on Mobile (AICOM), an
open-source project devoted to answering the United Nations Sustainable
Development Goal 3 (SDG3) on health, which represents a universal recognition
that health is fundamental to human capital and social and economic
development. The core motivation for the AICOM project is the fact that over
80% of the people in the least developed countries (LDCs) own a mobile phone,
even though less than 40% of these people have internet access. Hence, through
enabling AI-based disease diagnostics and screening capability on affordable
mobile phones without connectivity will be a critical first step to addressing
healthcare access problems. The technologies developed in the AICOM project
achieve exactly this goal, and we have demonstrated the effectiveness of AICOM
on monkeypox screening tasks. We plan to continue expanding and open-sourcing
the AICOM platform, aiming for it to evolve into an universal AI doctor for the
Underserved and Hard-to-Reach.


------------------------------------------------------------------------------

Title:
MA-NeRF: Motion-Assisted Neural Radiance Fields for Face Synthesis from  Sparse Images

Abstract: We address the problem of photorealistic 3D face avatar synthesis from sparse
images. Existing Parametric models for face avatar reconstruction struggle to
generate details that originate from inputs. Meanwhile, although current
NeRF-based avatar methods provide promising results for novel view synthesis,
they fail to generalize well for unseen expressions. We improve from NeRF and
propose a novel framework that, by leveraging the parametric 3DMM models, can
reconstruct a high-fidelity drivable face avatar and successfully handle the
unseen expressions. At the core of our implementation are structured
displacement feature and semantic-aware learning module. Our structured
displacement feature will introduce the motion prior as an additional
constraints and help perform better for unseen expressions, by constructing
displacement volume. Besides, the semantic-aware learning incorporates
multi-level prior, e.g., semantic embedding, learnable latent code, to lift the
performance to a higher level. Thorough experiments have been doen both
quantitatively and qualitatively to demonstrate the design of our framework,
and our method achieves much better results than the current state-of-the-arts.


------------------------------------------------------------------------------

Title:
The RL Perceptron: Generalisation Dynamics of Policy Learning in High  Dimensions

Abstract: Reinforcement learning (RL) algorithms have proven transformative in a range
of domains. To tackle real-world domains, these systems often use neural
networks to learn policies directly from pixels or other high-dimensional
sensory input. By contrast, much theory of RL has focused on discrete state
spaces or worst-case analysis, and fundamental questions remain about the
dynamics of policy learning in high-dimensional settings. Here, we propose a
solvable high-dimensional model of RL that can capture a variety of learning
protocols, and derive its typical dynamics as a set of closed-form ordinary
differential equations (ODEs). We derive optimal schedules for the learning
rates and task difficulty - analogous to annealing schemes and curricula during
training in RL - and show that the model exhibits rich behaviour, including
delayed learning under sparse rewards; a variety of learning regimes depending
on reward baselines; and a speed-accuracy trade-off driven by reward
stringency. Experiments on variants of the Procgen game "Bossfight" and Arcade
Learning Environment game "Pong" also show such a speed-accuracy trade-off in
practice. Together, these results take a step towards closing the gap between
theory and practice in high-dimensional RL.


------------------------------------------------------------------------------

Title:
Bid Optimization for Offsite Display Ad Campaigns on eCommerce

Abstract: Online retailers often use third-party demand-side-platforms (DSPs) to
conduct offsite advertising and reach shoppers across the Internet on behalf of
their advertisers. The process involves the retailer participating in instant
auctions with real-time bidding for each ad slot of their interest. In this
paper, we introduce a bid optimization system that leverages the dimensional
bidding function provided by most well-known DSPs for Walmart offsite display
ad campaigns. The system starts by automatically searching for the optimal
segmentation of the ad requests space based on their characteristics such as
geo location, time, ad format, serving website, device type, etc. Then, it
assesses the quality of impressions observed from each dimension based on
revenue signals driven by the campaign effect. During the campaign, the system
iteratively approximates the bid landscape based on the data observed and
calculates the bid adjustments for each dimension. Finally, a higher bid
adjustment factor is applied to dimensions with potentially higher revenue over
ad spend (ROAS), and vice versa. The initial A/B test results of the proposed
optimization system has shown its effectiveness of increasing the ROAS and
conversion rate while reducing the effective cost per mille for ad serving.


------------------------------------------------------------------------------

Title:
Image Harmonization with Diffusion Model

Abstract: Image composition in image editing involves merging a foreground image with a
background image to create a composite. Inconsistent lighting conditions
between the foreground and background often result in unrealistic composites.
Image harmonization addresses this challenge by adjusting illumination and
color to achieve visually appealing and consistent outputs. In this paper, we
present a novel approach for image harmonization by leveraging diffusion
models. We conduct a comparative analysis of two conditional diffusion models,
namely Classifier-Guidance and Classifier-Free. Our focus is on addressing the
challenge of adjusting illumination and color in foreground images to create
visually appealing outputs that seamlessly blend with the background. Through
this research, we establish a solid groundwork for future investigations in the
realm of diffusion model-based image harmonization.


------------------------------------------------------------------------------

Title:
Distributed Accelerated Projection-Based Consensus Decomposition

Abstract: With the development of machine learning and Big Data, the concepts of linear
and non-linear optimization techniques are becoming increasingly valuable for
many quantitative disciplines. Problems of that nature are typically solved
using distinctive optimization algorithms, iterative methods, or heuristics. A
new variant of the Accelerated Projection-Based Consensus (APC) iterative
method is proposed, which is faster than its classical version while handling
large sparse matrices in distributed settings. The algorithm is proposed, and
its description and implementation in a high-level programming language are
presented. Convergence tests measuring acceleration factors based on real-world
datasets are done, and their results are promising. The results of this
research can be used as an alternative to solving numerical optimization
problems.


------------------------------------------------------------------------------

Title:
Structure-Sensitive Graph Dictionary Embedding for Graph Classification

Abstract: Graph structure expression plays a vital role in distinguishing various
graphs. In this work, we propose a Structure-Sensitive Graph Dictionary
Embedding (SS-GDE) framework to transform input graphs into the embedding space
of a graph dictionary for the graph classification task. Instead of a plain use
of a base graph dictionary, we propose the variational graph dictionary
adaptation (VGDA) to generate a personalized dictionary (named adapted graph
dictionary) for catering to each input graph. In particular, for the
adaptation, the Bernoulli sampling is introduced to adjust substructures of
base graph keys according to each input, which increases the expression
capacity of the base dictionary tremendously. To make cross-graph measurement
sensitive as well as stable, multi-sensitivity Wasserstein encoding is proposed
to produce the embeddings by designing multi-scale attention on optimal
transport. To optimize the framework, we introduce mutual information as the
objective, which further deduces to variational inference of the adapted graph
dictionary. We perform our SS-GDE on multiple datasets of graph classification,
and the experimental results demonstrate the effectiveness and superiority over
the state-of-the-art methods.


------------------------------------------------------------------------------

Title:
UniMC: A Unified Framework for Long-Term Memory Conversation via  Relevance Representation Learning

Abstract: Open-domain long-term memory conversation can establish long-term intimacy
with humans, and the key is the ability to understand and memorize long-term
dialogue history information. Existing works integrate multiple models for
modelling through a pipeline, which ignores the coupling between different
stages. In this paper, we propose a Unified framework for Long-term Memory
Conversations (UniMC), which increases the connection between different stages
by learning relevance representation. Specifically, we decompose the main task
into three subtasks based on probability graphs: 1) conversation summarization,
2) memory retrieval, 3) memory-augmented generation. Each subtask involves
learning a representation for calculating the relevance between the query and
memory, which is modelled by inserting a special token at the beginning of the
decoder input. The relevance representation learning strengthens the connection
across subtasks through parameter sharing and joint training. Extensive
experimental results show that the proposed method consistently improves over
strong baselines and yields better dialogue consistency and engagingness.


------------------------------------------------------------------------------

Title:
Universal Information Extraction with Meta-Pretrained Self-Retrieval

Abstract: Universal Information Extraction~(Universal IE) aims to solve different
extraction tasks in a uniform text-to-structure generation manner. Such a
generation procedure tends to struggle when there exist complex information
structures to be extracted. Retrieving knowledge from external knowledge bases
may help models to overcome this problem but it is impossible to construct a
knowledge base suitable for various IE tasks. Inspired by the fact that large
amount of knowledge are stored in the pretrained language models~(PLM) and can
be retrieved explicitly, in this paper, we propose MetaRetriever to retrieve
task-specific knowledge from PLMs to enhance universal IE. As different IE
tasks need different knowledge, we further propose a Meta-Pretraining Algorithm
which allows MetaRetriever to quicktly achieve maximum task-specific retrieval
performance when fine-tuning on downstream IE tasks. Experimental results show
that MetaRetriever achieves the new state-of-the-art on 4 IE tasks, 12 datasets
under fully-supervised, low-resource and few-shot scenarios.


------------------------------------------------------------------------------

Title:
GenPose: Generative Category-level Object Pose Estimation via Diffusion  Models

Abstract: Object pose estimation plays a vital role in embodied AI and computer vision,
enabling intelligent agents to comprehend and interact with their surroundings.
Despite the practicality of category-level pose estimation, current approaches
encounter challenges with partially observed point clouds, known as the
multihypothesis issue. In this study, we propose a novel solution by reframing
categorylevel object pose estimation as conditional generative modeling,
departing from traditional point-to-point regression. Leveraging score-based
diffusion models, we estimate object poses by sampling candidates from the
diffusion model and aggregating them through a two-step process: filtering out
outliers via likelihood estimation and subsequently mean-pooling the remaining
candidates. To avoid the costly integration process when estimating the
likelihood, we introduce an alternative method that trains an energy-based
model from the original score-based model, enabling end-to-end likelihood
estimation. Our approach achieves state-of-the-art performance on the REAL275
dataset, surpassing 50% and 60% on strict 5d2cm and 5d5cm metrics,
respectively. Furthermore, our method demonstrates strong generalizability to
novel categories sharing similar symmetric properties without fine-tuning and
can readily adapt to object pose tracking tasks, yielding comparable results to
the current state-of-the-art baselines.


------------------------------------------------------------------------------

Title:
Linearly-scalable learning of smooth low-dimensional patterns with  permutation-aided entropic dimension reduction

Abstract: In many data science applications, the objective is to extract
appropriately-ordered smooth low-dimensional data patterns from
high-dimensional data sets. This is challenging since common sorting algorithms
are primarily aiming at finding monotonic orderings in low-dimensional data,
whereas typical dimension reduction and feature extraction algorithms are not
primarily designed for extracting smooth low-dimensional data patterns. We show
that when selecting the Euclidean smoothness as a pattern quality criterium,
both of these problems (finding the optimal 'crisp' data permutation and
extracting the sparse set of permuted low-dimensional smooth patterns) can be
efficiently solved numerically as one unsupervised entropy-regularized
iterative optimization problem. We formulate and prove the conditions for
monotonicity and convergence of this linearly-scalable (in dimension) numerical
procedure, with the iteration cost scaling of $\mathcal{O}(DT^2)$, where $T$ is
the size of the data statistics and $D$ is a feature space dimension. The
efficacy of the proposed method is demonstrated through the examination of
synthetic examples as well as a real-world application involving the
identification of smooth bankruptcy risk minimizing transition patterns from
high-dimensional economical data. The results showcase that the statistical
properties of the overall time complexity of the method exhibit linear scaling
in the dimensionality $D$ within the specified confidence intervals.


------------------------------------------------------------------------------

Title:
Algorithms for Multiple Drone-Delivery Scheduling Problem (MDSP)

Abstract: The Multiple Drone-Delivery Scheduling Problem (MDSP) is a scheduling problem
that optimizes the maximum reward earned by a set of $m$ drones executing a
sequence of deliveries on a truck delivery route. The current best-known
approximation algorithm for the problem is a $\frac{1}{4}$-approximation
algorithm developed by Jana and Mandal (2022). In this paper, we propose exact
and approximation algorithms for the general MDSP, as well as a unit-cost
variant. We first propose a greedy algorithm which we show to be a
$\frac{1}{3}$-approximation algorithm for the general MDSP problem formulation,
provided the number of conflicting intervals is less than the number of drones.
We then introduce a unit-cost variant of MDSP and we devise an exact dynamic
programming algorithm that runs in polynomial time when the number of drones
$m$ can be assumed to be a constant.


------------------------------------------------------------------------------

Title:
QCNeXt: A Next-Generation Framework For Joint Multi-Agent Trajectory  Prediction

Abstract: Estimating the joint distribution of on-road agents' future trajectories is
essential for autonomous driving. In this technical report, we propose a
next-generation framework for joint multi-agent trajectory prediction called
QCNeXt. First, we adopt the query-centric encoding paradigm for the task of
joint multi-agent trajectory prediction. Powered by this encoding scheme, our
scene encoder is equipped with permutation equivariance on the set elements,
roto-translation invariance in the space dimension, and translation invariance
in the time dimension. These invariance properties not only enable accurate
multi-agent forecasting fundamentally but also empower the encoder with the
capability of streaming processing. Second, we propose a multi-agent DETR-like
decoder, which facilitates joint multi-agent trajectory prediction by modeling
agents' interactions at future time steps. For the first time, we show that a
joint prediction model can outperform marginal prediction models even on the
marginal metrics, which opens up new research opportunities in trajectory
prediction. Our approach ranks 1st on the Argoverse 2 multi-agent motion
forecasting benchmark, winning the championship of the Argoverse Challenge at
the CVPR 2023 Workshop on Autonomous Driving.


------------------------------------------------------------------------------

Title:
An analysis of Universal Differential Equations for data-driven  discovery of Ordinary Differential Equations

Abstract: In the last decade, the scientific community has devolved its attention to
the deployment of data-driven approaches in scientific research to provide
accurate and reliable analysis of a plethora of phenomena. Most notably,
Physics-informed Neural Networks and, more recently, Universal Differential
Equations (UDEs) proved to be effective both in system integration and
identification. However, there is a lack of an in-depth analysis of the
proposed techniques. In this work, we make a contribution by testing the UDE
framework in the context of Ordinary Differential Equations (ODEs) discovery.
In our analysis, performed on two case studies, we highlight some of the issues
arising when combining data-driven approaches and numerical solvers, and we
investigate the importance of the data collection process. We believe that our
analysis represents a significant contribution in investigating the
capabilities and limitations of Physics-informed Machine Learning frameworks.


------------------------------------------------------------------------------

Title:
Benchmarking Deep Learning Architectures for Urban Vegetation Points  Segmentation

Abstract: Vegetation is crucial for sustainable and resilient cities providing various
ecosystem services and well-being of humans. However, vegetation is under
critical stress with rapid urbanization and expanding infrastructure
footprints. Consequently, mapping of this vegetation is essential in the urban
environment. Recently, deep learning for point cloud semantic segmentation has
shown significant progress. Advanced models attempt to obtain state-of-the-art
performance on benchmark datasets, comprising multiple classes and representing
real world scenarios. However, class specific segmentation with respect to
vegetation points has not been explored. Therefore, selection of a deep
learning model for vegetation points segmentation is ambiguous. To address this
problem, we provide a comprehensive assessment of point-based deep learning
models for semantic segmentation of vegetation class. We have selected four
representative point-based models, namely PointCNN, KPConv (omni-supervised),
RandLANet and SCFNet. These models are investigated on three different
datasets, specifically Chandigarh, Toronto3D and Kerala, which are
characterized by diverse nature of vegetation, varying scene complexity and
changing per-point features. PointCNN achieves the highest mIoU on the
Chandigarh (93.32%) and Kerala datasets (85.68%) while KPConv (omni-supervised)
provides the highest mIoU on the Toronto3D dataset (91.26%). The paper develops
a deeper insight, hitherto not reported, into the working of these models for
vegetation segmentation and outlines the ingredients that should be included in
a model specifically for vegetation segmentation. This paper is a step towards
the development of a novel architecture for vegetation points segmentation.


------------------------------------------------------------------------------

Title:
A Machine Learning Approach for Predicting Deterioration in Alzheimer's  Disease

Abstract: This paper explores deterioration in Alzheimers Disease using Machine
Learning. Subjects were split into two datasets based on baseline diagnosis
(Cognitively Normal, Mild Cognitive Impairment), with outcome of deterioration
at final visit (a binomial essentially yes/no categorisation) using data from
the Alzheimers Disease Neuroimaging Initiative (demographics, genetics, CSF,
imaging, and neuropsychological testing etc). Six machine learning models,
including gradient boosting, were built, and evaluated on these datasets using
a nested crossvalidation procedure, with the best performing models being put
through repeated nested cross-validation at 100 iterations. We were able to
demonstrate good predictive ability using CART predicting which of those in the
cognitively normal group deteriorated and received a worse diagnosis (AUC =
0.88). For the mild cognitive impairment group, we were able to achieve good
predictive ability for deterioration with Elastic Net (AUC = 0.76).


------------------------------------------------------------------------------

Title:
Multi-Task Offloading via Graph Neural Networks in Heterogeneous  Multi-access Edge Computing

Abstract: In the rapidly evolving field of Heterogeneous Multi-access Edge Computing
(HMEC), efficient task offloading plays a pivotal role in optimizing system
throughput and resource utilization. However, existing task offloading methods
often fall short of adequately modeling the dependency topology relationships
between offloaded tasks, which limits their effectiveness in capturing the
complex interdependencies of task features. To address this limitation, we
propose a task offloading mechanism based on Graph Neural Networks (GNN). Our
modeling approach takes into account factors such as task characteristics,
network conditions, and available resources at the edge, and embeds these
captured features into the graph structure. By utilizing GNNs, our mechanism
can capture and analyze the intricate relationships between task features,
enabling a more comprehensive understanding of the underlying dependency
topology. Through extensive evaluations in heterogeneous networks, our proposed
algorithm improves 18.6\%-53.8\% over greedy and approximate algorithms in
optimizing system throughput and resource utilization. Our experiments showcase
the advantage of considering the intricate interplay of task features using
GNN-based modeling.


------------------------------------------------------------------------------

Title:
Breaking On-device Training Memory Wall: A Systematic Survey

Abstract: On-device training has become an increasingly popular approach to machine
learning, enabling models to be trained directly on mobile and edge devices.
However, a major challenge in this area is the limited memory available on
these devices, which can severely restrict the size and complexity of the
models that can be trained. In this systematic survey, we aim to explore the
current state-of-the-art techniques for breaking on-device training memory
walls, focusing on methods that can enable larger and more complex models to be
trained on resource-constrained devices.
Specifically, we first analyze the key factors that contribute to the
phenomenon of memory walls encountered during on-device training. Then, we
present a comprehensive literature review of on-device training, which
addresses the issue of memory limitations. Finally, we summarize on-device
training and highlight the open problems for future research.
By providing a comprehensive overview of these techniques and their
effectiveness in breaking memory walls, we hope to help researchers and
practitioners in this field navigate the rapidly evolving landscape of
on-device training.


------------------------------------------------------------------------------

Title:
On the Complexity of Co-secure Dominating Set Problem

Abstract: A set $D \subseteq V$ of a graph $G=(V, E)$ is a dominating set of $G$ if
every vertex $v\in V\setminus D$ is adjacent to at least one vertex in $D.$ A
set $S \subseteq V$ is a co-secure dominating set (CSDS) of a graph $G$ if $S$
is a dominating set of $G$ and for each vertex $u \in S$ there exists a vertex
$v \in V\setminus S$ such that $uv \in E$ and $(S\setminus \{u\}) \cup \{v\}$
is a dominating set of $G$. The minimum cardinality of a co-secure dominating
set of $G$ is the co-secure domination number and it is denoted by
$\gamma_{cs}(G)$. Given a graph $G=(V, E)$, the minimum co-secure dominating
set problem (Min Co-secure Dom) is to find a co-secure dominating set of
minimum cardinality. In this paper, we strengthen the inapproximability result
of Min Co-secure Dom for general graphs by showing that this problem can not be
approximated within a factor of $(1- \epsilon)\ln |V|$ for perfect elimination
bipartite graphs and star convex bipartite graphs unless P=NP. On the positive
side, we show that Min Co-secure Dom can be approximated within a factor of
$O(\ln |V|)$ for any graph $G$ with $\delta(G)\geq 2$. For $3$-regular and
$4$-regular graphs, we show that Min Co-secure Dom is approximable within a
factor of $\dfrac{8}{3}$ and $\dfrac{10}{3}$, respectively. Furthermore, we
prove that Min Co-secure Dom is APX-complete for $3$-regular graphs.


------------------------------------------------------------------------------

Title:
A normal form for bases of finite-dimensional vector spaces

Abstract: Most algorithms constructing bases of finite-dimensional vector spaces return
basis vectors which, apart from orthogonality, do not show any special
properties. While every basis is sufficient to define the vector space, not all
bases are equally suited to unravel properties of the problem to be solved. In
this paper a normal form for bases of finite-dimensional vector spaces is
introduced which may prove very useful in the context of understanding the
structure of the problem in which the basis appears in a step towards the
solution. This normal form may be viewed as a new normal form for matrices of
full column rank.


------------------------------------------------------------------------------

Title:
The CUFF, Clenching Upper-limb Force Feedback wearable device: design,  characterization and validation

Abstract: This paper presents the design, characterization and validation of a wearable
haptic device able to convey skin stretch, force feedback, and a combination of
both, to the user's arm. In this work, we carried out physical and perceptual
characterization with eleven able-bodied participants as well as two
experiments of discrimination and manipulation task hiring a total of 32
participants. In both the experiments the CUFF was used in conjunction with the
Pisa/IIT SoftHand. The first experiment was a discrimination task where the
subjects had to recognize the dimension and the softness between pair of
cylinder. in the second experiment the subjects were asked to control the
robotic hand for grasping objects. After the experiments the subjects underwent
to a subjective evaluation of the device. Results of the experiments and
questionnaire showed the effectiveness of the proposed device. Thank to its
versatility and structure, the device could be a viable solution for
teleoperation application, guidance and rehabilitation tasks, including
prosthesis applications.


------------------------------------------------------------------------------

Title:
MultiWave: Multiresolution Deep Architectures through Wavelet  Decomposition for Multivariate Time Series Prediction

Abstract: The analysis of multivariate time series data is challenging due to the
various frequencies of signal changes that can occur over both short and long
terms. Furthermore, standard deep learning models are often unsuitable for such
datasets, as signals are typically sampled at different rates. To address these
issues, we introduce MultiWave, a novel framework that enhances deep learning
time series models by incorporating components that operate at the intrinsic
frequencies of signals. MultiWave uses wavelets to decompose each signal into
subsignals of varying frequencies and groups them into frequency bands. Each
frequency band is handled by a different component of our model. A gating
mechanism combines the output of the components to produce sparse models that
use only specific signals at specific frequencies. Our experiments demonstrate
that MultiWave accurately identifies informative frequency bands and improves
the performance of various deep learning models, including LSTM, Transformer,
and CNN-based models, for a wide range of applications. It attains top
performance in stress and affect detection from wearables. It also increases
the AUC of the best-performing model by 5% for in-hospital COVID-19 mortality
prediction from patient blood samples and for human activity recognition from
accelerometer and gyroscope data. We show that MultiWave consistently
identifies critical features and their frequency components, thus providing
valuable insights into the applications studied.


------------------------------------------------------------------------------

Title:
Structured Thoughts Automaton: First Formalized Execution Model for  Auto-Regressive Language Models

Abstract: In recent months, Language Models (LMs) have become a part of daily
discourse, with focus on OpenAI and the potential of Artificial General
Intelligence (AGI). Furthermore, the leaking of LLama's weights to the public
has led to an influx of innovations demonstrating the impressive capabilities
of generative LMs. While we believe that AGI is still a distant goal, we
recognize the potential of LMs in solving tasks such as searching complex
documents, compiling reports with basic analysis, and providing assistance in
problem-solving. In this paper, we propose formalizing the execution model of
language models. We investigate current execution models, to find that this
formalism has received little attention, and present our contribution: the
first formalized execution model for LMs. We introduce a new algorithm for
sampling the predictions of LMs, which we use to build a reliable and
inspectable execution model. We introduce a low-level language to write
"cognitive program" for this execution model. We hope to shed light on the need
for execution models for LMs and encourage further research in this area.


------------------------------------------------------------------------------

Title:
Enlighten Anything: When Segment Anything Model Meets Low-Light Image  Enhancement

Abstract: Image restoration is a low-level visual task, and most CNN methods are
designed as black boxes, lacking transparency and intrinsic aesthetics. Many
unsupervised approaches ignore the degradation of visible information in
low-light scenes, which will seriously affect the aggregation of complementary
information and also make the fusion algorithm unable to produce satisfactory
fusion results under extreme conditions. In this paper, we propose
Enlighten-anything, which is able to enhance and fuse the semantic intent of
SAM segmentation with low-light images to obtain fused images with good visual
perception. The generalization ability of unsupervised learning is greatly
improved, and experiments on LOL dataset are conducted to show that our method
improves 3db in PSNR over baseline and 8 in SSIM. Zero-shot learning of SAM
introduces a powerful aid for unsupervised low-light enhancement. The source
code of Enlighten Anything can be obtained from
this https URL


------------------------------------------------------------------------------

Title:
Genes in Intelligent Agents

Abstract: Training intelligent agents in Reinforcement Learning (RL) is much more
time-consuming than animal learning. This is because agents learn from scratch,
but animals learn with genes inherited from ancestors and are born with some
innate abilities. Inspired by genes in animals, here we conceptualize the gene
in intelligent agents and introduce Genetic Reinforcement Learning (GRL), a
computational framework to represent, evaluate, and evolve genes (in agents).
Leveraging GRL we identify genes and demonstrate several advantages of genes.
First, we find that genes take the form of the fragment of agents' neural
networks and can be inherited across generations. Second, we validate that
genes bring better and stabler learning ability to agents, since genes condense
knowledge from ancestors and bring agent with innate abilities. Third, we
present evidence of Lamarckian evolution in intelligent agents. The continuous
encoding of knowledge into genes across generations facilitates the evolution
of genes. Overall, our work promotes a novel paradigm to train agents by
incorporating genes.


------------------------------------------------------------------------------

Title:
ALP: Action-Aware Embodied Learning for Perception

Abstract: Current methods in training and benchmarking vision models exhibit an
over-reliance on passive, curated datasets. Although models trained on these
datasets have shown strong performance in a wide variety of tasks such as
classification, detection, and segmentation, they fundamentally are unable to
generalize to an ever-evolving world due to constant out-of-distribution shifts
of input data. Therefore, instead of training on fixed datasets, can we
approach learning in a more human-centric and adaptive manner? In this paper,
we introduce \textbf{A}ction-aware Embodied \textbf{L}earning for
\textbf{P}erception (ALP), an embodied learning framework that incorporates
action information into representation learning through a combination of
optimizing policy gradients through reinforcement learning and inverse dynamics
prediction objectives. Our method actively explores complex 3D environments to
both learn generalizable task-agnostic representations as well as collect
downstream training data. We show that ALP outperforms existing baselines in
object detection and semantic segmentation. In addition, we show that by
training on actively collected data more relevant to the environment and task,
our method generalizes more robustly to downstream tasks compared to models
pre-trained on fixed datasets such as ImageNet.


------------------------------------------------------------------------------

Title:
Dual Adaptive Representation Alignment for Cross-domain Few-shot  Learning

Abstract: Few-shot learning aims to recognize novel queries with limited support
samples by learning from base knowledge. Recent progress in this setting
assumes that the base knowledge and novel query samples are distributed in the
same domains, which are usually infeasible for realistic applications. Toward
this issue, we propose to address the cross-domain few-shot learning problem
where only extremely few samples are available in target domains. Under this
realistic setting, we focus on the fast adaptation capability of meta-learners
by proposing an effective dual adaptive representation alignment approach. In
our approach, a prototypical feature alignment is first proposed to recalibrate
support instances as prototypes and reproject these prototypes with a
differentiable closed-form solution. Therefore feature spaces of learned
knowledge can be adaptively transformed to query spaces by the cross-instance
and cross-prototype relations. Besides the feature alignment, we further
present a normalized distribution alignment module, which exploits prior
statistics of query samples for solving the covariant shifts among the support
and query samples. With these two modules, a progressive meta-learning
framework is constructed to perform the fast adaptation with extremely few-shot
samples while maintaining its generalization capabilities. Experimental
evidence demonstrates our approach achieves new state-of-the-art results on 4
CDFSL benchmarks and 4 fine-grained cross-domain benchmarks.


------------------------------------------------------------------------------

Title:
Bootstrapped Representations in Reinforcement Learning

Abstract: In reinforcement learning (RL), state representations are key to dealing with
large or continuous state spaces. While one of the promises of deep learning
algorithms is to automatically construct features well-tuned for the task they
try to solve, such a representation might not emerge from end-to-end training
of deep RL agents. To mitigate this issue, auxiliary objectives are often
incorporated into the learning process and help shape the learnt state
representation. Bootstrapping methods are today's method of choice to make
these additional predictions. Yet, it is unclear which features these
algorithms capture and how they relate to those from other auxiliary-task-based
approaches. In this paper, we address this gap and provide a theoretical
characterization of the state representation learnt by temporal difference
learning (Sutton, 1988). Surprisingly, we find that this representation differs
from the features learned by Monte Carlo and residual gradient algorithms for
most transition structures of the environment in the policy evaluation setting.
We describe the efficacy of these representations for policy evaluation, and
use our theoretical analysis to design new auxiliary learning rules. We
complement our theoretical results with an empirical comparison of these
learning rules for different cumulant functions on classic domains such as the
four-room domain (Sutton et al, 1999) and Mountain Car (Moore, 1990).


------------------------------------------------------------------------------

Title:
Privacy-Enhancing Technologies for Financial Data Sharing

Abstract: Today, financial institutions (FIs) store and share consumers' financial data
for various reasons such as offering loans, processing payments, and protecting
against fraud and financial crime. Such sharing of sensitive data have been
subject to data breaches in the past decade.
While some regulations (e.g., GDPR, FCRA, and CCPA) help to prevent
institutions from freely sharing clients' sensitive information, some
regulations (e.g., BSA 1970) require FIs to share certain financial data with
government agencies to combat financial crime. This creates an inherent tension
between the privacy and the integrity of financial transactions. In the past
decade, significant progress has been made in building efficient
privacy-enhancing technologies that allow computer systems and networks to
validate encrypted data automatically.
In this paper, we investigate some of these technologies to identify the
benefits and limitations of each, in particular, for use in data sharing among
FIs. As a case study, we look into the emerging area of Central Bank Digital
Currencies (CBDCs) and how privacy-enhancing technologies can be integrated
into the CBDC architecture. Our study, however, is not limited to CBDCs and can
be applied to other financial scenarios with tokenized bank deposits such as
cross-border payments, real-time settlements, and card payments.


------------------------------------------------------------------------------

Title:
Multi-Scale Simulation of Complex Systems: A Perspective of Integrating  Knowledge and Data

Abstract: Complex system simulation has been playing an irreplaceable role in
understanding, predicting, and controlling diverse complex systems. In the past
few decades, the multi-scale simulation technique has drawn increasing
attention for its remarkable ability to overcome the challenges of complex
system simulation with unknown mechanisms and expensive computational costs. In
this survey, we will systematically review the literature on multi-scale
simulation of complex systems from the perspective of knowledge and data.
Firstly, we will present background knowledge about simulating complex system
simulation and the scales in complex systems. Then, we divide the main
objectives of multi-scale modeling and simulation into five categories by
considering scenarios with clear scale and scenarios with unclear scale,
respectively. After summarizing the general methods for multi-scale simulation
based on the clues of knowledge and data, we introduce the adopted methods to
achieve different objectives. Finally, we introduce the applications of
multi-scale simulation in typical matter systems and social systems.


------------------------------------------------------------------------------

Title:
Achilles' Heels: Vulnerable Record Identification in Synthetic Data  Publishing

Abstract: Synthetic data is seen as the most promising solution to share
individual-level data while preserving privacy. Shadow modeling-based
membership inference attacks (MIAs) have become the standard approach to
evaluate the privacy risk of synthetic data. While very effective, they require
a large number of datasets to be created and models trained to evaluate the
risk posed by a single record. The privacy risk of a dataset is thus currently
evaluated by running MIAs on a handful of records selected using ad-hoc
methods. We here propose what is, to the best of our knowledge, the first
principled vulnerable record identification technique for synthetic data
publishing, leveraging the distance to a record's closest neighbors. We show
our method to strongly outperform previous ad-hoc methods across datasets and
generators. We also show evidence of our method to be robust to the choice of
MIA and to specific choice of parameters. Finally, we show it to accurately
identify vulnerable records when synthetic data generators are made
differentially private. The choice of vulnerable records is as important as
more accurate MIAs when evaluating the privacy of synthetic data releases,
including from a legal perspective. We here propose a simple yet highly
effective method to do so. We hope our method will enable practitioners to
better estimate the risk posed by synthetic data publishing and researchers to
fairly compare ever improving MIAs on synthetic data.


------------------------------------------------------------------------------

Title:
FuzzyLogic.jl: a Flexible Library for Efficient and Productive Fuzzy  Inference

Abstract: This paper introduces \textsc{FuzzyLogic.jl}, a Julia library to perform
fuzzy inference. The library is fully open-source and released under a
permissive license. The core design principles of the library are:
user-friendliness, flexibility, efficiency and interoperability. Particularly,
our library is easy to use, allows to specify fuzzy systems in an expressive
yet concise domain specific language, has several visualization tools, supports
popular inference systems like Mamdani, Sugeno and Type-2 systems, can be
easily expanded with custom user settings or algorithms and can perform fuzzy
inference efficiently. It also allows reading fuzzy models from other formats
such as Matlab .fis, FCL or FML. In this paper, we describe the library main
features and benchmark it with a few examples, showing it achieves significant
speedup compared to the Matlab fuzzy toolbox.


------------------------------------------------------------------------------

Title:
Magnificent Minified Models

Abstract: This paper concerns itself with the task of taking a large trained neural
network and 'compressing' it to be smaller by deleting parameters or entire
neurons, with minimal decreases in the resulting model accuracy. We compare
various methods of parameter and neuron selection: dropout-based neuron damage
estimation, neuron merging, absolute-value based selection, random selection,
OBD (Optimal Brain Damage). We also compare a variation on the classic OBD
method that slightly outperformed all other parameter and neuron selection
methods in our tests with substantial pruning, which we call OBD-SD. We compare
these methods against quantization of parameters. We also compare these
techniques (all applied to a trained neural network), with neural networks
trained from scratch (random weight initialization) on various pruned
architectures. Our results are only barely consistent with the Lottery Ticket
Hypothesis, in that fine-tuning a parameter-pruned model does slightly better
than retraining a similarly pruned model from scratch with randomly initialized
weights. For neuron-level pruning, retraining from scratch did much better in
our experiments.


------------------------------------------------------------------------------

Title:
Predicting Alzheimers Disease Diagnosis Risk over Time with Survival  Machine Learning on the ADNI Cohort

Abstract: The rise of Alzheimers Disease worldwide has prompted a search for efficient
tools which can be used to predict deterioration in cognitive decline leading
to dementia. In this paper, we explore the potential of survival machine
learning as such a tool for building models capable of predicting not only
deterioration but also the likely time to deterioration. We demonstrate good
predictive ability (0.86 C-Index), lending support to its use in clinical
investigation and prediction of Alzheimers Disease risk.


------------------------------------------------------------------------------

Title:
Orientation Optimization Based on Topological Derivatives in Cooperation  with Multi-Material Topology Optimization Based on Extended Level Set Method

Abstract: This paper provides an orientation angle optimization method for the design
of fiber-reinforced composite materials using topology optimization. The
orientation angle optimization is based on a topological derivative, which
measures the sensitivity of an objective function with respect to a topological
change of anisotropic materials. The sensitivity is incorporated into a new
gradient-based optimization algorithm. This method allows us to avoid local
optima and seek a global optimal solution. We provide some numerical examples
and verify the effectiveness of the proposed method.


------------------------------------------------------------------------------

Title:
Meta-Personalizing Vision-Language Models to Find Named Instances in  Video

Abstract: Large-scale vision-language models (VLM) have shown impressive results for
language-guided search applications. While these models allow category-level
queries, they currently struggle with personalized searches for moments in a
video where a specific object instance such as ``My dog Biscuit'' appears. We
present the following three contributions to address this problem. First, we
describe a method to meta-personalize a pre-trained VLM, i.e., learning how to
learn to personalize a VLM at test time to search in video. Our method extends
the VLM's token vocabulary by learning novel word embeddings specific to each
instance. To capture only instance-specific features, we represent each
instance embedding as a combination of shared and learned global category
features. Second, we propose to learn such personalization without explicit
human supervision. Our approach automatically identifies moments of named
visual instances in video using transcripts and vision-language similarity in
the VLM's embedding space. Finally, we introduce This-Is-My, a personal video
instance retrieval benchmark. We evaluate our approach on This-Is-My and
DeepFashion2 and show that we obtain a 15% relative improvement over the state
of the art on the latter dataset.


------------------------------------------------------------------------------

Title:
Notes on "Bounds on BDD-Based Bucket Elimination''

Abstract: This paper concerns Boolean satisfiability (SAT) solvers based on Ordered
Binary Decision Diagrams (BDDs), especially those that can generate proofs of
unsatisfiability. Mengel (arXiv:2306.00886) has presented a theoretical
analysis that a BDD-based SAT solver can generate a proof of unsatisfiability
for the pigeonhole problem (PHP$_n$) in polynomial time, even when the problem
is encoded in the standard ``direct'' form. His approach is based on bucket
elimination, using different orderings for the variables in the BDDs than in
the buckets. We show experimentally that these proofs scale as $O(n^5)$. We
also confirm the exponential scaling that occurs when the same variable
ordering is used for the BDDs as for the buckets.


------------------------------------------------------------------------------

Title:
MO-VLN: A Multi-Task Benchmark for Open-set Zero-Shot  Vision-and-Language Navigation

Abstract: Given a natural language, a general robot has to comprehend the instruction
and find the target object or location based on visual observations even in
unexplored environments. Most agents rely on massive diverse training data to
achieve better generalization, which requires expensive labor. These agents
often focus on common objects and fewer tasks, thus are not intelligent enough
to handle different types of instructions. To facilitate research in open-set
vision-and-language navigation, we propose a benchmark named MO-VLN, aiming at
testing the effectiveness and generalization of the agent in the multi-task
setting. First, we develop a 3D simulator rendered by realistic scenarios using
Unreal Engine 5, containing more realistic lights and details. The simulator
contains three scenes, i.e., cafe, restaurant, and nursing house, of high value
in the industry. Besides, our simulator involves multiple uncommon objects,
such as takeaway cup and medical adhesive tape, which are more complicated
compared with existing environments. Inspired by the recent success of large
language models (e.g., ChatGPT, Vicuna), we construct diverse high-quality data
of instruction type without human annotation. Our benchmark MO-VLN provides
four tasks: 1) goal-conditioned navigation given a specific object category
(e.g., "fork"); 2) goal-conditioned navigation given simple instructions (e.g.,
"Search for and move towards a tennis ball"); 3) step-by-step instruction
following; 4) finding abstract object based on high-level instruction (e.g., "I
am thirsty").


------------------------------------------------------------------------------

Title:
Output Voltage Response Improvement and Ripple Reduction Control for  Input-parallel Output-parallel High-Power DC Supply

Abstract: A three-phase isolated AC-DC-DC power supply is widely used in the industrial
field due to its attractive features such as high-power density, modularity for
easy expansion and electrical isolation. In high-power application scenarios,
it can be realized by multiple AC-DC-DC modules with Input-Parallel
Output-Parallel (IPOP) mode. However, it has the problems of slow output
voltage response and large ripple in some special applications, such as
electrophoresis and electroplating. This paper investigates an improved
Adaptive Linear Active Disturbance Rejection Control (A-LADRC) with flexible
adjustment capability of the bandwidth parameter value for the high-power DC
supply to improve the output voltage response speed. To reduce the DC supply
ripple, a control strategy is designed for a single module to adaptively adjust
the duty cycle compensation according to the output feedback value. When
multiple modules are connected in parallel, a Hierarchical Delay Current
Sharing Control (HDCSC) strategy for centralized controllers is proposed to
make the peaks and valleys of different modules offset each other. Finally, the
proposed method is verified by designing a 42V/12000A high-power DC supply, and
the results demonstrate that the proposed method is effective in improving the
system output voltage response speed and reducing the voltage ripple, which has
significant practical engineering application value.


------------------------------------------------------------------------------

Title:
Algorithm MGB to solve highly nonlinear elliptic PDEs in $\tilde{O}(n)$  FLOPS

Abstract: We introduce Algorithm MGB (Multi Grid Barrier) for solving highly nonlinear
convex Euler-Lagrange equations. This class of problems includes many highly
nonlinear partial differential equations, such as $p$-Laplacians. We prove
that, if certain regularity hypotheses are satisfied, then our algorithm
converges in $\tilde{O}(1)$ damped Newton iterations, or $\tilde{O}(n)$ FLOPS,
where the tilde indicates that we neglect some polylogarithmic terms. This the
first algorithm whose running time is proven optimal in the big-$\tilde{O}$
sense. Previous algorithms for the $p$-Laplacian required $\tilde{O}(\sqrt{n})$
damped Newton iterations or more.


------------------------------------------------------------------------------

Title:
Error analysis of an effective numerical scheme for a temporal  multiscale plaque growth problem

Abstract: In this work, we propose a simple numerical scheme based on a fast
front-tracking approach for solving a fluid-structure interaction (FSI) problem
of plaque growth in blood vessels. A rigorous error analysis is carried out for
the temporal semi-discrete scheme to show that it is first-order accurate for
all macro time step $\Delta T$, micro time step $\Delta t$ and scale parameter
$\epsilon$. A numerical example is presented to verify the theoretical results
and demonstrate the excellent performance of the proposed multiscale algorithm.


------------------------------------------------------------------------------

Title:
Building the Bridge of Schrödinger: A Continuous Entropic Optimal  Transport Benchmark

Abstract: Over the last several years, there has been a significant progress in
developing neural solvers for the Schr\"odinger Bridge (SB) problem and
applying them to generative modeling. This new research field is justifiably
fruitful as it is interconnected with the practically well-performing diffusion
models and theoretically-grounded entropic optimal transport (EOT). Still the
area lacks non-trivial tests allowing a researcher to understand how well do
the methods solve SB or its equivalent continuous EOT problem. We fill this gap
and propose a novel way to create pairs of probability distributions for which
the ground truth OT solution in known by the construction. Our methodology is
generic and works for a wide range of OT formulations, in particular, it covers
the EOT which is equivalent to SB (the main interest of our study). This
development allows us to create continuous benchmark distributions with the
known EOT and SB solution on high-dimensional spaces such as spaces of images.
As an illustration, we use these benchmark pairs to test how well do existing
neural EOT/SB solvers actually compute the EOT solution. The benchmark is
available via the link: this https URL


------------------------------------------------------------------------------

Title:
MachMap: End-to-End Vectorized Solution for Compact HD-Map Construction

Abstract: This report introduces the 1st place winning solution for the Autonomous
Driving Challenge 2023 - Online HD-map Construction. By delving into the
vectorization pipeline, we elaborate an effective architecture, termed as
MachMap, which formulates the task of HD-map construction as the point
detection paradigm in the bird-eye-view space with an end-to-end manner.
Firstly, we introduce a novel map-compaction scheme into our framework, leading
to reducing the number of vectorized points by 93% without any expression
performance degradation. Build upon the above process, we then follow the
general query-based paradigm and propose a strong baseline with integrating a
powerful CNN-based backbone like InternImage, a temporal-based instance decoder
and a well-designed point-mask coupling head. Additionally, an extra optional
ensemble stage is utilized to refine model predictions for better performance.
Our MachMap-tiny with IN-1K initialization achieves a mAP of 79.1 on the
Argoverse2 benchmark and the further improved MachMap-huge reaches the best mAP
of 83.5, outperforming all the other online HD-map construction approaches on
the final leaderboard with a distinct performance margin (> 9.8 mAP at least).


------------------------------------------------------------------------------

Title:
Superconducting Heater Cryotron-Based Reconfigurable Logic Towards  Cryogenic IC Camouflaging

Abstract: Superconducting electronics are among the most promising alternatives to
conventional CMOS technology thanks to the ultra-fast speed and ultra-high
energy efficiency of the superconducting devices. Having a cryogenic control
processor is also a crucial requirement for scaling the existing quantum
computers up to thousands of qubits. Despite showing outstanding speed and
energy efficiency, Josephson junction-based circuits suffer from several
challenges such as flux trapping leading to limited scalability, difficulty in
driving high impedances, and so on. Three-terminal cryotron devices have been
proposed to solve these issues which can drive high impedances (>100 k{\Omega})
and are free from any flux trapping issue. In this work, we develop a
reconfigurable logic circuit using a heater cryotron (hTron). In conventional
approaches, the number of devices to perform a logic operation typically
increases with the number of inputs. However, here, we demonstrate a single
hTron device-based logic circuit that can be reconfigured to perform 1-input
copy and NOT, 2-input AND and OR, and 3-input majority logic operations by
choosing suitable biasing conditions. Consequently, we can perform any
processing task with a much smaller number of devices. Also, since we can
perform different logic operations with the same circuit (same layout), we can
develop a camouflaged system where all the logic gates will have the same
layout. Therefore, this proposed circuit will ensure enhanced hardware security
against reverse engineering attacks.


------------------------------------------------------------------------------

Title:
Gradient-type subspace iteration methods for the symmetric eigenvalue  problem

Abstract: This paper explores variants of the subspace iteration algorithm for
computing approximate invariant subspaces. The standard subspace iteration
approach is revisited and new variants that exploit gradient-type techniques
combined with a Grassmann manifold viewpoint are developed. A gradient method
as well as a conjugate gradient technique are described.
Convergence of the gradient-based algorithm is analyzed and a few numerical
experiments are reported, indicating that the proposed algorithms are sometimes
superior to a standard Chebyshev-based subspace iteration when compared in
terms of number of matrix vector products, but do not require estimating
optimal parameters. An important contribution of this paper to achieve this
good performance is the accurate and efficient implementation of an exact line
search. In addition, new convergence proofs are presented for the
non-accelerated gradient method that includes a locally exponential convergence
if started in a $\mathcal{O(\sqrt{\delta})}$ neighbourhood of the dominant
subspace with spectral gap $\delta$.


------------------------------------------------------------------------------

Title:
OpenGSL: A Comprehensive Benchmark for Graph Structure Learning

Abstract: Graph Neural Networks (GNNs) have emerged as the de facto standard for
representation learning on graphs, owing to their ability to effectively
integrate graph topology and node attributes. However, the inherent suboptimal
nature of node connections, resulting from the complex and contingent formation
process of graphs, presents significant challenges in modeling them
effectively. To tackle this issue, Graph Structure Learning (GSL), a family of
data-centric learning approaches, has garnered substantial attention in recent
years. The core concept behind GSL is to jointly optimize the graph structure
and the corresponding GNN models. Despite the proposal of numerous GSL methods,
the progress in this field remains unclear due to inconsistent experimental
protocols, including variations in datasets, data processing techniques, and
splitting strategies. In this paper, we introduce OpenGSL, the first
comprehensive benchmark for GSL, aimed at addressing this gap. OpenGSL enables
a fair comparison among state-of-the-art GSL methods by evaluating them across
various popular datasets using uniform data processing and splitting
strategies. Through extensive experiments, we observe that existing GSL methods
do not consistently outperform vanilla GNN counterparts. However, we do observe
that the learned graph structure demonstrates a strong generalization ability
across different GNN backbones, despite its high computational and space
requirements. We hope that our open-sourced library will facilitate rapid and
equitable evaluation and inspire further innovative research in the field of
GSL. The code of the benchmark can be found in
this https URL


------------------------------------------------------------------------------

Title:
OpenSBT: A Modular Framework for Search-based Testing of Automated  Driving Systems

Abstract: Search-based software testing (SBT) is an effective and efficient approach
for testing automated driving systems (ADS). However, testing pipelines for ADS
testing are particularly challenging as they involve integrating complex
driving simulation platforms and establishing communication protocols and APIs
with the desired search algorithm. This complexity prevents a wide adoption of
SBT and thorough empirical comparative experiments with different simulators
and search approaches. We present OpenSBT, an open-source, modular and
extensible framework to facilitate the SBT of ADS. With OpenSBT, it is possible
to integrate simulators with an embedded system under test, search algorithms
and fitness functions for testing. We describe the architecture and show the
usage of our framework by applying different search algorithms for testing
Automated Emergency Braking Systems in CARLA as well in the high-fidelity
Prescan simulator in collaboration with our industrial partner DENSO. OpenSBT
is available at this https URL


------------------------------------------------------------------------------

Title:
Vision-Language Models can Identify Distracted Driver Behavior from  Naturalistic Videos

Abstract: Recognizing the activities, causing distraction, in real-world driving
scenarios is critical for ensuring the safety and reliability of both drivers
and pedestrians on the roadways. Conventional computer vision techniques are
typically data-intensive and require a large volume of annotated training data
to detect and classify various distracted driving behaviors, thereby limiting
their efficiency and scalability. We aim to develop a generalized framework
that showcases robust performance with access to limited or no annotated
training data. Recently, vision-language models have offered large-scale
visual-textual pretraining that can be adapted to task-specific learning like
distracted driving activity recognition. Vision-language pretraining models,
such as CLIP, have shown significant promise in learning natural
language-guided visual representations. This paper proposes a CLIP-based driver
activity recognition approach that identifies driver distraction from
naturalistic driving images and videos. CLIP's vision embedding offers
zero-shot transfer and task-based finetuning, which can classify distracted
activities from driving video data. Our results show that this framework offers
state-of-the-art performance on zero-shot transfer and video-based CLIP for
predicting the driver's state on two public datasets. We propose both
frame-based and video-based frameworks developed on top of the CLIP's visual
representation for distracted driving detection and classification task and
report the results.


------------------------------------------------------------------------------

Title:
Bkd-FedGNN: A Benchmark for Classification Backdoor Attacks on Federated  Graph Neural Network

Abstract: Federated Graph Neural Network (FedGNN) has recently emerged as a rapidly
growing research topic, as it integrates the strengths of graph neural networks
and federated learning to enable advanced machine learning applications without
direct access to sensitive data. Despite its advantages, the distributed nature
of FedGNN introduces additional vulnerabilities, particularly backdoor attacks
stemming from malicious participants. Although graph backdoor attacks have been
explored, the compounded complexity introduced by the combination of GNNs and
federated learning has hindered a comprehensive understanding of these attacks,
as existing research lacks extensive benchmark coverage and in-depth analysis
of critical factors. To address these limitations, we propose Bkd-FedGNN, a
benchmark for backdoor attacks on FedGNN. Specifically, Bkd-FedGNN decomposes
the graph backdoor attack into trigger generation and injection steps, and
extending the attack to the node-level federated setting, resulting in a
unified framework that covers both node-level and graph-level classification
tasks. Moreover, we thoroughly investigate the impact of multiple critical
factors in backdoor attacks on FedGNN. These factors are categorized into
global-level and local-level factors, including data distribution, the number
of malicious attackers, attack time, overlapping rate, trigger size, trigger
type, trigger position, and poisoning rate. Finally, we conduct comprehensive
evaluations on 13 benchmark datasets and 13 critical factors, comprising 1,725
experimental configurations for node-level and graph-level tasks from six
domains. These experiments encompass over 8,000 individual tests, allowing us
to provide a thorough evaluation and insightful observations that advance our
understanding of backdoor attacks on FedGNN.The Bkd-FedGNN benchmark is
publicly available at this https URL


------------------------------------------------------------------------------

Title:
Dynamic Size Message Scheduling for Multi-Agent Communication under  Limited Bandwidth

Abstract: Communication plays a vital role in multi-agent systems, fostering
collaboration and coordination. However, in real-world scenarios where
communication is bandwidth-limited, existing multi-agent reinforcement learning
(MARL) algorithms often provide agents with a binary choice: either
transmitting a fixed number of bytes or no information at all. This limitation
hinders the ability to effectively utilize the available bandwidth. To overcome
this challenge, we present the Dynamic Size Message Scheduling (DSMS) method,
which introduces a finer-grained approach to scheduling by considering the
actual size of the information to be exchanged. Our contribution lies in
adaptively adjusting message sizes using Fourier transform-based compression
techniques, enabling agents to tailor their messages to match the allocated
bandwidth while striking a balance between information loss and transmission
efficiency. Receiving agents can reliably decompress the messages using the
inverse Fourier transform. Experimental results demonstrate that DSMS
significantly improves performance in multi-agent cooperative tasks by
optimizing the utilization of bandwidth and effectively balancing information
value.


------------------------------------------------------------------------------

Title:
Practical Sliding Window Recoder: Design, Analysis, and Usecases

Abstract: Network coding has been widely used as a technology to ensure efficient and
reliable communication. The ability to recode packets at the intermediate nodes
is a major benefit of network coding implementations. This allows the
intermediate nodes to choose a different code rate and fine-tune the outgoing
transmission to the channel conditions, decoupling the requirement for the
source node to compensate for cumulative losses over a multi-hop network. Block
network coding solutions already have practical recoders but an on-the-fly
recoder for sliding window network coding has not been studied in detail. In
this paper, we present the implementation details of a practical recoder for
sliding window network coding for the first time along with a comprehensive
performance analysis of a multi-hop network using the recoder. The sliding
window recoder ensures that the network performs closest to its capacity and
that each node can use its outgoing links efficiently.


------------------------------------------------------------------------------

Title:
Enhancing Visual Domain Adaptation with Source Preparation

Abstract: Robotic Perception in diverse domains such as low-light scenarios, where new
modalities like thermal imaging and specialized night-vision sensors are
increasingly employed, remains a challenge. Largely, this is due to the limited
availability of labeled data. Existing Domain Adaptation (DA) techniques, while
promising to leverage labels from existing well-lit RGB images, fail to
consider the characteristics of the source domain itself. We holistically
account for this factor by proposing Source Preparation (SP), a method to
mitigate source domain biases. Our Almost Unsupervised Domain Adaptation (AUDA)
framework, a label-efficient semi-supervised approach for robotic scenarios --
employs Source Preparation (SP), Unsupervised Domain Adaptation (UDA) and
Supervised Alignment (SA) from limited labeled data. We introduce
CityIntensified, a novel dataset comprising temporally aligned image pairs
captured from a high-sensitivity camera and an intensifier camera for semantic
segmentation and object detection in low-light settings. We demonstrate the
effectiveness of our method in semantic segmentation, with experiments showing
that SP enhances UDA across a range of visual domains, with improvements up to
40.64% in mIoU over baseline, while making target models more robust to
real-world shifts within the target domain. We show that AUDA is a
label-efficient framework for effective DA, significantly improving target
domain performance with only tens of labeled samples from the target domain.


------------------------------------------------------------------------------

Title:
Opportunities and Challenges for ChatGPT and Large Language Models in  Biomedicine and Health

Abstract: ChatGPT has drawn considerable attention from both the general public and
domain experts with its remarkable text generation capabilities. This has
subsequently led to the emergence of diverse applications in the field of
biomedicine and health. In this work, we examine the diverse applications of
large language models (LLMs), such as ChatGPT, in biomedicine and health.
Specifically we explore the areas of biomedical information retrieval, question
answering, medical text summarization, information extraction, and medical
education, and investigate whether LLMs possess the transformative power to
revolutionize these tasks or whether the distinct complexities of biomedical
domain presents unique challenges. Following an extensive literature survey, we
find that significant advances have been made in the field of text generation
tasks, surpassing the previous state-of-the-art methods. For other
applications, the advances have been modest. Overall, LLMs have not yet
revolutionized the biomedicine, but recent rapid progress indicates that such
methods hold great potential to provide valuable means for accelerating
discovery and improving health. We also find that the use of LLMs, like
ChatGPT, in the fields of biomedicine and health entails various risks and
challenges, including fabricated information in its generated responses, as
well as legal and privacy concerns associated with sensitive patient data. We
believe this first-of-its-kind survey can provide a comprehensive overview to
biomedical researchers and healthcare practitioners on the opportunities and
challenges associated with using ChatGPT and other LLMs for transforming
biomedicine and health.


------------------------------------------------------------------------------

Title:
Reorganizing Educational Institutional Domain using Faceted Ontological  Principles

Abstract: The purpose of this work is to find out how different library classification
systems and linguistic ontologies arrange a particular domain of interest and
what are the limitations for information retrieval. We use knowledge
representation techniques and languages for construction of a domain specific
ontology. This ontology would help not only in problem solving, but it would
demonstrate the ease with which complex queries can be handled using principles
of domain ontology, thereby facilitating better information retrieval.


------------------------------------------------------------------------------

Title:
On Orderings of Probability Vectors and Unsupervised Performance  Estimation

Abstract: Unsupervised performance estimation, or evaluating how well models perform on
unlabeled data is a difficult task. Recently, a method was proposed by Garg et
al. [2022] which performs much better than previous methods. Their method
relies on having a score function, satisfying certain properties, to map
probability vectors outputted by the classifier to the reals, but it is an open
problem which score function is best. We explore this problem by first showing
that their method fundamentally relies on the ordering induced by this score
function. Thus, under monotone transformations of score functions, their method
yields the same estimate. Next, we show that in the binary classification
setting, nearly all common score functions - the $L^\infty$ norm; the $L^2$
norm; negative entropy; and the $L^2$, $L^1$, and Jensen-Shannon distances to
the uniform vector - all induce the same ordering over probability vectors.
However, this does not hold for higher dimensional settings. We conduct
numerous experiments on well-known NLP data sets and rigorously explore the
performance of different score functions. We conclude that the $L^\infty$ norm
is the most appropriate.


------------------------------------------------------------------------------

Title:
Self-Supervised Learning for Time Series Analysis: Taxonomy, Progress,  and Prospects

Abstract: Self-supervised learning (SSL) has recently achieved impressive performance
on various time series tasks. The most prominent advantage of SSL is that it
reduces the dependence on labeled data. Based on the pre-training and
fine-tuning strategy, even a small amount of labeled data can achieve high
performance. Compared with many published self-supervised surveys on computer
vision and natural language processing, a comprehensive survey for time series
SSL is still missing. To fill this gap, we review current state-of-the-art SSL
methods for time series data in this article. To this end, we first
comprehensively review existing surveys related to SSL and time series, and
then provide a new taxonomy of existing time series SSL methods. We summarize
these methods into three categories: generative-based, contrastive-based, and
adversarial-based. All methods can be further divided into ten subcategories.
To facilitate the experiments and validation of time series SSL methods, we
also summarize datasets commonly used in time series forecasting,
classification, anomaly detection, and clustering tasks. Finally, we present
the future directions of SSL for time series analysis.


------------------------------------------------------------------------------

Title:
Vanishing Bias Heuristic-guided Reinforcement Learning Algorithm

Abstract: Reinforcement Learning has achieved tremendous success in the many Atari
games. In this paper we explored with the lunar lander environment and
implemented classical methods including Q-Learning, SARSA, MC as well as tiling
coding. We also implemented Neural Network based methods including DQN, Double
DQN, Clipped DQN. On top of these, we proposed a new algorithm called Heuristic
RL which utilizes heuristic to guide the early stage training while alleviating
the introduced human bias. Our experiments showed promising results for our
proposed methods in the lunar lander environment.


------------------------------------------------------------------------------

Title:
Musico-acoustic Depictions of Laminar and Turbulent Flows in Ligeti  Piano Etude No. 9 and a Novel Method of Analysis

Abstract: The relationship between musical material and physical phenomena has become a
topic in the musicological literature over the last several decades,
particularly concerning elements of the musical system itself, and
constructions found in the work of contemporary classical composers such as
Gyorgy Ligeti and Iannis Xenakis. Most scholars, who adopt this approach,
explore the physical phenomena of fractals in the analysis of musical works,
but fluid mechanical frameworks, such as laminar and turbulent flows, offer a
new avenue to be explored. In this paper I will propose a novel method of
musical analysis for examining musical structures in terms of fluid-like
behaviour such that Ligeti etude no. 9 serves as a model, whereby the metaphors
of laminar and turbulent flows take precedence. The methodological design
includes the utility of converting terms (by proposing correlations between
physical concepts and the acoustic properties of music), theoretical frameworks
for musicological application, and scatter plots, which provide central
analytic support to demonstrating the fluid-like tendencies in musical
materials, for they capture a formal development over time.


------------------------------------------------------------------------------

Title:
Automatic Deduction Path Learning via Reinforcement Learning with  Environmental Correction

Abstract: Automatic bill payment is an important part of business operations in fintech
companies. The practice of deduction was mainly based on the total amount or
heuristic search by dividing the bill into smaller parts to deduct as much as
possible. This article proposes an end-to-end approach of automatically
learning the optimal deduction paths (deduction amount in order), which reduces
the cost of manual path design and maximizes the amount of successful
deduction. Specifically, in view of the large search space of the paths and the
extreme sparsity of historical successful deduction records, we propose a deep
hierarchical reinforcement learning approach which abstracts the action into a
two-level hierarchical space: an upper agent that determines the number of
steps of deductions each day and a lower agent that decides the amount of
deduction at each step. In such a way, the action space is structured via prior
knowledge and the exploration space is reduced. Moreover, the inherited
information incompleteness of the business makes the environment just partially
observable. To be precise, the deducted amounts indicate merely the lower
bounds of the available account balance. To this end, we formulate the problem
as a partially observable Markov decision problem (POMDP) and employ an
environment correction algorithm based on the characteristics of the business.
In the world's largest electronic payment business, we have verified the
effectiveness of this scheme offline and deployed it online to serve millions
of users.


------------------------------------------------------------------------------

Title:
Edge Learning for 6G-enabled Internet of Things: A Comprehensive Survey  of Vulnerabilities, Datasets, and Defenses

Abstract: The ongoing deployment of the fifth generation (5G) wireless networks
constantly reveals limitations concerning its original concept as a key driver
of Internet of Everything (IoE) applications. These 5G challenges are behind
worldwide efforts to enable future networks, such as sixth generation (6G)
networks, to efficiently support sophisticated applications ranging from
autonomous driving capabilities to the Metaverse. Edge learning is a new and
powerful approach to training models across distributed clients while
protecting the privacy of their data. This approach is expected to be embedded
within future network infrastructures, including 6G, to solve challenging
problems such as resource management and behavior prediction. This survey
article provides a holistic review of the most recent research focused on edge
learning vulnerabilities and defenses for 6G-enabled IoT. We summarize the
existing surveys on machine learning for 6G IoT security and machine
learning-associated threats in three different learning modes: centralized,
federated, and distributed. Then, we provide an overview of enabling emerging
technologies for 6G IoT intelligence. Moreover, we provide a holistic survey of
existing research on attacks against machine learning and classify threat
models into eight categories, including backdoor attacks, adversarial examples,
combined attacks, poisoning attacks, Sybil attacks, byzantine attacks,
inference attacks, and dropping attacks. In addition, we provide a
comprehensive and detailed taxonomy and a side-by-side comparison of the
state-of-the-art defense methods against edge learning vulnerabilities.
Finally, as new attacks and defense technologies are realized, new research and
future overall prospects for 6G-enabled IoT are discussed.


------------------------------------------------------------------------------

Title:
Enhancing the Prediction of Emotional Experience in Movies using Deep  Neural Networks: The Significance of Audio and Language

Abstract: Our paper focuses on making use of deep neural network models to accurately
predict the range of human emotions experienced during watching movies. In this
certain setup, there exist three clear-cut input modalities that considerably
influence the experienced emotions: visual cues derived from RGB video frames,
auditory components encompassing sounds, speech, and music, and linguistic
elements encompassing actors' dialogues. Emotions are commonly described using
a two-factor model including valence (ranging from happy to sad) and arousal
(indicating the intensity of the emotion). In this regard, a Plethora of works
have presented a multitude of models aiming to predict valence and arousal from
video content. However, non of these models contain all three modalities, with
language being consistently eliminated across all of them. In this study, we
comprehensively combine all modalities and conduct an analysis to ascertain the
importance of each in predicting valence and arousal. Making use of pre-trained
neural networks, we represent each input modality in our study. In order to
process visual input, we employ pre-trained convolutional neural networks to
recognize scenes[1], objects[2], and actions[3,4]. For audio processing, we
utilize a specialized neural network designed for handling sound-related tasks,
namely SoundNet[5]. Finally, Bidirectional Encoder Representations from
Transformers (BERT) models are used to extract linguistic features[6] in our
analysis. We report results on the COGNIMUSE dataset[7], where our proposed
model outperforms the current state-of-the-art approaches. Surprisingly, our
findings reveal that language significantly influences the experienced arousal,
while sound emerges as the primary determinant for predicting valence. In
contrast, the visual modality exhibits the least impact among all modalities in
predicting emotions.


------------------------------------------------------------------------------

Title:
Dual Node and Edge Fairness-Aware Graph Partition

Abstract: Fair graph partition of social networks is a crucial step toward ensuring
fair and non-discriminatory treatments in unsupervised user analysis. Current
fair partition methods typically consider node balance, a notion pursuing a
proportionally balanced number of nodes from all demographic groups, but ignore
the bias induced by imbalanced edges in each cluster. To address this gap, we
propose a notion edge balance to measure the proportion of edges connecting
different demographic groups in clusters. We analyze the relations between node
balance and edge balance, then with line graph transformations, we propose a
co-embedding framework to learn dual node and edge fairness-aware
representations for graph partition. We validate our framework through several
social network datasets and observe balanced partition in terms of both nodes
and edges along with good utility. Moreover, we demonstrate our fair partition
can be used as pseudo labels to facilitate graph neural networks to behave
fairly in node classification and link prediction tasks.


------------------------------------------------------------------------------

Title:
Data Selection for Fine-tuning Large Language Models Using Transferred  Shapley Values

Abstract: Although Shapley values have been shown to be highly effective for
identifying harmful training instances, dataset size and model complexity
constraints limit the ability to apply Shapley-based data valuation to
fine-tuning large pre-trained language models. To address this, we propose
TS-DShapley, an algorithm that reduces computational cost of Shapley-based data
valuation through: 1) an efficient sampling-based method that aggregates
Shapley values computed from subsets for valuation of the entire training set,
and 2) a value transfer method that leverages value information extracted from
a simple classifier trained using representations from the target language
model. Our experiments applying TS-DShapley to select data for fine-tuning
BERT-based language models on benchmark natural language understanding (NLU)
datasets show that TS-DShapley outperforms existing data selection methods.
Further, TS-DShapley can filter fine-tuning data to increase language model
performance compared to training with the full fine-tuning dataset.


------------------------------------------------------------------------------

Title:
Adaptive Surface Meshes from Harmonic Maps

Abstract: We present a novel shape-approximating anisotropic re-meshing algorithm as a
geometric generalization of the adaptive moving mesh method. Conventional
moving mesh methods reduce the interpolation error of a mesh that discretizes a
given function over a planar domain. Our algorithm, in contrast, optimizes the
mesh's approximation of a curved surface; surfaces can be represented in
various formats, such as a signed distance field. The optimization is achieved
by continuously flowing the mesh without altering its topology, making the
implementation simpler compared to other adaptive surface meshing techniques.
The resulting optimal mesh can be interpreted as a harmonic map with respect to
a metric using the shape operator. Furthermore, our approach can be tailored to
target height fields by utilizing isotropic geometry.


------------------------------------------------------------------------------

Title:
LLMVA-GEBC: Large Language Model with Video Adapter for Generic Event  Boundary Captioning

Abstract: Our winning entry for the CVPR 2023 Generic Event Boundary Captioning (GEBC)
competition is detailed in this paper. Unlike conventional video captioning
tasks, GEBC demands that the captioning model possess an understanding of
immediate changes in status around the designated video boundary, making it a
difficult task. This paper proposes an effective model LLMVA-GEBC (Large
Language Model with Video Adapter for Generic Event Boundary Captioning): (1)
We utilize a pretrained LLM for generating human-like captions with high
quality. (2) To adapt the model to the GEBC task, we take the video Q-former as
an adapter and train it with the frozen visual feature extractors and LLM. Our
proposed method achieved a 76.14 score on the test set and won the first place
in the challenge. Our code is available at
this https URL .


------------------------------------------------------------------------------

Title:
Snowman: A Million-scale Chinese Commonsense Knowledge Graph Distilled  from Foundation Model

Abstract: Constructing commonsense knowledge graphs (CKGs) has attracted wide research
attention due to its significant importance in cognitive intelligence.
Nevertheless, existing CKGs are typically oriented to English, limiting the
research in non-English languages. Meanwhile, the emergence of foundation
models like ChatGPT and GPT-4 has shown promising intelligence with the help of
reinforcement learning from human feedback. Under the background, in this
paper, we utilize foundation models to construct a Chinese CKG, named Snowman.
Specifically, we distill different types of commonsense head items from
ChatGPT, and continue to use it to collect tail items with respect to the head
items and pre-defined relations. Based on the preliminary analysis, we find the
negative commonsense knowledge distilled by ChatGPT achieves lower human
acceptance compared to other knowledge. Therefore, we design a simple yet
effective self-instruct filtering strategy to filter out invalid negative
commonsense. Overall, the constructed Snowman covers more than ten million
Chinese commonsense triples, making it the largest Chinese CKG. Moreover, human
studies show the acceptance of Snowman achieves 90.6\%, indicating the
high-quality triples distilled by the cutting-edge foundation model. We also
conduct experiments on commonsense knowledge models to show the usability and
effectiveness of our Snowman.


------------------------------------------------------------------------------

Title:
NFTs to MARS: Multi-Attention Recommender System for NFTs

Abstract: Recommender systems have become essential tools for enhancing user
experiences across various domains. While extensive research has been conducted
on recommender systems for movies, music, and e-commerce, the rapidly growing
and economically significant Non-Fungible Token (NFT) market remains
underexplored. The unique characteristics and increasing prominence of the NFT
market highlight the importance of developing tailored recommender systems to
cater to its specific needs and unlock its full potential. In this paper, we
examine the distinctive characteristics of NFTs and propose the first
recommender system specifically designed to address NFT market challenges. In
specific, we develop a Multi-Attention Recommender System for NFTs (NFT-MARS)
with three key characteristics: (1) graph attention to handle sparse user-item
interactions, (2) multi-modal attention to incorporate feature preference of
users, and (3) multi-task learning to consider the dual nature of NFTs as both
artwork and financial assets. We demonstrate the effectiveness of NFT-MARS
compared to various baseline models using the actual transaction data of NFTs
collected directly from blockchain for four of the most popular NFT
collections. The source code and data are available at
this https URL


------------------------------------------------------------------------------

Title:
Identifying Nearest Fog Nodes With Network Coordinate Systems

Abstract: Identifying the closest fog node is crucial for mobile clients to benefit
from fog computing. Relying on geographical location alone us insufficient for
this as it ignores real observed client access latency.
In this paper, we analyze the performance of the Meridian and Vivaldi network
coordinate systems in identifying nearest fog nodes. To that end, we simulate a
dense fog environment with mobile clients. We find that while network
coordinate systems really find fog nodes in close network proximity, a purely
latency-oriented identification approach ignores the larger problem of
balancing load across fog nodes.


------------------------------------------------------------------------------

Title:
M3PT: A Multi-Modal Model for POI Tagging

Abstract: POI tagging aims to annotate a point of interest (POI) with some informative
tags, which facilitates many services related to POIs, including search,
recommendation, and so on. Most of the existing solutions neglect the
significance of POI images and seldom fuse the textual and visual features of
POIs, resulting in suboptimal tagging performance. In this paper, we propose a
novel Multi-Modal Model for POI Tagging, namely M3PT, which achieves enhanced
POI tagging through fusing the target POI's textual and visual features, and
the precise matching between the multi-modal representations. Specifically, we
first devise a domain-adaptive image encoder (DIE) to obtain the image
embeddings aligned to their gold tags' semantics. Then, in M3PT's text-image
fusion module (TIF), the textual and visual representations are fully fused
into the POIs' content embeddings for the subsequent matching. In addition, we
adopt a contrastive learning strategy to further bridge the gap between the
representations of different modalities. To evaluate the tagging models'
performance, we have constructed two high-quality POI tagging datasets from the
real-world business scenario of Ali Fliggy. Upon the datasets, we conducted the
extensive experiments to demonstrate our model's advantage over the baselines
of uni-modality and multi-modality, and verify the effectiveness of important
components in M3PT, including DIE, TIF and the contrastive learning strategy.


------------------------------------------------------------------------------

Title:
Towards social generative AI for education: theory, practices and ethics

Abstract: This paper explores educational interactions involving humans and artificial
intelligences not as sequences of prompts and responses, but as a social
process of conversation and exploration. In this conception, learners
continually converse with AI language models within a dynamic computational
medium of internet tools and resources. Learning happens when this distributed
system sets goals, builds meaning from data, consolidates understanding,
reconciles differences, and transfers knowledge to new domains. Building social
generative AI for education will require development of powerful AI systems
that can converse with each other as well as humans, construct external
representations such as knowledge maps, access and contribute to internet
resources, and act as teachers, learners, guides and mentors. This raises
fundamental problems of ethics. Such systems should be aware of their
limitations, their responsibility to learners and the integrity of the
internet, and their respect for human teachers and experts. We need to consider
how to design and constrain social generative AI for education.


------------------------------------------------------------------------------

Title:
Towards an induction principle for nested data types

Abstract: A well-known problem in the theory of dependent types is how to handle
so-called nested data types. These data types are difficult to program and to
reason about in total dependently typed languages such as Agda and Coq. In
particular, it is not easy to derive a canonical induction principle for such
types. Working towards a solution to this problem, we introduce dependently
typed folds for nested data types. Using the nested data type Bush as a guiding
example, we show how to derive its dependently typed fold and induction
principle. We also discuss the relationship between dependently typed folds and
the more traditional higher-order folds.


------------------------------------------------------------------------------

Title:
Interpolating Item and User Fairness in Recommendation Systems

Abstract: Online platforms employ recommendation systems to enhance customer engagement
and drive revenue. However, in a multi-sided platform where the platform
interacts with diverse stakeholders such as sellers (items) and customers
(users), each with their own desired outcomes, finding an appropriate middle
ground becomes a complex operational challenge. In this work, we investigate
the ``price of fairness'', which captures the platform's potential compromises
when balancing the interests of different stakeholders. Motivated by this, we
propose a fair recommendation framework where the platform maximizes its
revenue while interpolating between item and user fairness constraints. We
further examine the fair recommendation problem in a more realistic yet
challenging online setting, where the platform lacks knowledge of user
preferences and can only observe binary purchase decisions. To address this, we
design a low-regret online optimization algorithm that preserves the platform's
revenue while achieving fairness for both items and users. Finally, we
demonstrate the effectiveness of our framework and proposed method via a case
study on MovieLens data.


------------------------------------------------------------------------------

Title:
Assigning AI: Seven Approaches for Students, with Prompts

Abstract: This paper examines the transformative role of Large Language Models (LLMs)
in education and their potential as learning tools, despite their inherent
risks and limitations. The authors propose seven approaches for utilizing AI in
classrooms: AI-tutor, AI-coach, AI-mentor, AI-teammate, AI-tool, AI-simulator,
and AI-student, each with distinct pedagogical benefits and risks. The aim is
to help students learn with and about AI, with practical strategies designed to
mitigate risks such as complacency about the AI's output, errors, and biases.
These strategies promote active oversight, critical assessment of AI outputs,
and complementarity of AI's capabilities with the students' unique insights. By
challenging students to remain the "human in the loop," the authors aim to
enhance learning outcomes while ensuring that AI serves as a supportive tool
rather than a replacement. The proposed framework offers a guide for educators
navigating the integration of AI-assisted learning in classrooms


------------------------------------------------------------------------------

Title:
Neural Fast Full-Rank Spatial Covariance Analysis for Blind Source  Separation

Abstract: This paper describes an efficient unsupervised learning method for a neural
source separation model that utilizes a probabilistic generative model of
observed multichannel mixtures proposed for blind source separation (BSS). For
this purpose, amortized variational inference (AVI) has been used for directly
solving the inverse problem of BSS with full-rank spatial covariance analysis
(FCA). Although this unsupervised technique called neural FCA is in principle
free from the domain mismatch problem, it is computationally demanding due to
the full rankness of the spatial model in exchange for robustness against
relatively short reverberations. To reduce the model complexity without
sacrificing performance, we propose neural FastFCA based on the
jointly-diagonalizable yet full-rank spatial model. Our neural separation model
introduced for AVI alternately performs neural network blocks and single steps
of an efficient iterative algorithm called iterative source steering. This
alternating architecture enables the separation model to quickly separate the
mixture spectrogram by leveraging both the deep neural network and the
multichannel optimization algorithm. The training objective with AVI is derived
to maximize the marginalized likelihood of the observed mixtures. The
experiment using mixture signals of two to four sound sources shows that neural
FastFCA outperforms conventional BSS methods and reduces the computational time
to about 2% of that for the neural FCA.


------------------------------------------------------------------------------

Title:
TOBY: A Tool for Exploring Data in Academic Survey Papers

Abstract: This paper describes TOBY, a visualization tool that helps a user explore the
contents of an academic survey paper. The visualization consists of four
components: a hierarchical view of taxonomic data in the survey, a document
similarity view in the space of taxonomic classes, a network view of citations,
and a new paper recommendation tool. In this paper, we will discuss these
features in the context of three separate deployments of the tool.


------------------------------------------------------------------------------

Title:
Federated Few-shot Learning

Abstract: Federated Learning (FL) enables multiple clients to collaboratively learn a
machine learning model without exchanging their own local data. In this way,
the server can exploit the computational power of all clients and train the
model on a larger set of data samples among all clients. Although such a
mechanism is proven to be effective in various fields, existing works generally
assume that each client preserves sufficient data for training. In practice,
however, certain clients may only contain a limited number of samples (i.e.,
few-shot samples). For example, the available photo data taken by a specific
user with a new mobile device is relatively rare. In this scenario, existing FL
efforts typically encounter a significant performance drop on these clients.
Therefore, it is urgent to develop a few-shot model that can generalize to
clients with limited data under the FL scenario. In this paper, we refer to
this novel problem as federated few-shot learning. Nevertheless, the problem
remains challenging due to two major reasons: the global data variance among
clients (i.e., the difference in data distributions among clients) and the
local data insufficiency in each client (i.e., the lack of adequate local data
for training). To overcome these two challenges, we propose a novel federated
few-shot learning framework with two separately updated models and dedicated
training strategies to reduce the adverse impact of global data variance and
local data insufficiency. Extensive experiments on four prevalent datasets that
cover news articles and images validate the effectiveness of our framework
compared with the state-of-the-art baselines. Our code is provided at
this https URL


------------------------------------------------------------------------------

Title:
Conformal Language Modeling

Abstract: We propose a novel approach to conformal prediction for generative language
models (LMs). Standard conformal prediction produces prediction sets -- in
place of single predictions -- that have rigorous, statistical performance
guarantees. LM responses are typically sampled from the model's predicted
distribution over the large, combinatorial output space of natural language.
Translating this process to conformal prediction, we calibrate a stopping rule
for sampling different outputs from the LM that get added to a growing set of
candidates until we are confident that the output set is sufficient. Since some
samples may be low-quality, we also simultaneously calibrate and apply a
rejection rule for removing candidates from the output set to reduce noise.
Similar to conformal prediction, we prove that the sampled set returned by our
procedure contains at least one acceptable answer with high probability, while
still being empirically precise (i.e., small) on average. Furthermore, within
this set of candidate responses, we show that we can also accurately identify
subsets of individual components -- such as phrases or sentences -- that are
each independently correct (e.g., that are not "hallucinations"), again with
statistical guarantees. We demonstrate the promise of our approach on multiple
tasks in open-domain question answering, text summarization, and radiology
report generation using different LM variants.


------------------------------------------------------------------------------

Title:
Query2GMM: Learning Representation with Gaussian Mixture Model for  Reasoning over Knowledge Graphs

Abstract: Logical query answering over Knowledge Graphs (KGs) is a fundamental yet
complex task. A promising approach to achieve this is to embed queries and
entities jointly into the same embedding space. Research along this line
suggests that using multi-modal distribution to represent answer entities is
more suitable than uni-modal distribution, as a single query may contain
multiple disjoint answer subsets due to the compositional nature of multi-hop
queries and the varying latent semantics of relations. However, existing
methods based on multi-modal distribution roughly represent each subset without
capturing its accurate cardinality, or even degenerate into uni-modal
distribution learning during the reasoning process due to the lack of an
effective similarity measure. To better model queries with diversified answers,
we propose Query2GMM for answering logical queries over knowledge graphs. In
Query2GMM, we present the GMM embedding to represent each query using a
univariate Gaussian Mixture Model (GMM). Each subset of a query is encoded by
its cardinality, semantic center and dispersion degree, allowing for precise
representation of multiple subsets. Then we design specific neural networks for
each operator to handle the inherent complexity that comes with multi-modal
distribution while alleviating the cascading errors. Last, we define a new
similarity measure to assess the relationships between an entity and a query's
multi-answer subsets, enabling effective multi-modal distribution learning for
reasoning. Comprehensive experimental results show that Query2GMM outperforms
the best competitor by an absolute average of $5.5\%$. The source code is
available at \url{this https URL}.


------------------------------------------------------------------------------

Title:
General Spatial Photonic Ising Machine Based on Interaction Matrix  Eigendecomposition Method

Abstract: The spatial photonic Ising machine has achieved remarkable advancements in
solving combinatorial optimization problems. However, it still remains a huge
challenge to flexibly mapping an arbitrary problem to Ising model. In this
paper, we propose a general spatial photonic Ising machine based on interaction
matrix eigendecomposition method. Arbitrary interaction matrix can be
configured in the two-dimensional Fourier transformation based spatial photonic
Ising model by using values generated by matrix eigendecomposition. The error
in the structural representation of the Hamiltonian decreases substantially
with the growing number of eigenvalues utilized to form the Ising machine. In
combination with the optimization algorithm, as low as 65% of the eigenvalues
is required by intensity modulation to guarantee the best probability of
optimal solution for a 20-vertex graph Max-cut problem, and this probability
decreases to below 20% for zero best chance. Our work provides a viable
approach for spatial photonic Ising machines to solve arbitrary combinatorial
optimization problems with the help of multi-dimensional optical property.


------------------------------------------------------------------------------

Title:
Othered, Silenced and Scapegoated: Understanding the Situated Security  of Marginalised Populations in Lebanon

Abstract: In this paper we explore the digital security experiences of marginalised
populations in Lebanon such as LGBTQI+ identifying people, refugees and women.
We situate our work in the post-conflict Lebanese context, which is shaped by
sectarian divides, failing governance and economic collapse. We do so through
an ethnographically informed study conducted in Beirut, Lebanon, in July 2022
and through interviews with 13 people with Lebanese digital and human rights
expertise. Our research highlights how LGBTQI+ identifying people and refugees
are scapegoated for the failings of the Lebanese government, while women who
speak out against such failings are silenced. We show how government-supported
incitements of violence aimed at transferring blame from the political
leadership to these groups lead to amplified digital security risks for already
at-risk populations. Positioning our work in broader sociological
understandings of security, we discuss how the Lebanese context impacts
identity and ontological security. We conclude by proposing to design for and
with positive security in post-conflict settings.


------------------------------------------------------------------------------

Title:
AI Driven Near Real-time Locational Marginal Pricing Method: A  Feasibility and Robustness Study

Abstract: Accurate price predictions are essential for market participants in order to
optimize their operational schedules and bidding strategies, especially in the
current context where electricity prices become more volatile and less
predictable using classical approaches. Locational Marginal Pricing (LMP)
pricing mechanism is used in many modern power markets, where the traditional
approach utilizes optimal power flow (OPF) solvers. However, for large
electricity grids this process becomes prohibitively time-consuming and
computationally intensive. Machine learning solutions could provide an
efficient tool for LMP prediction, especially in energy markets with
intermittent sources like renewable energy. The study evaluates the performance
of popular machine learning and deep learning models in predicting LMP on
multiple electricity grids. The accuracy and robustness of these models in
predicting LMP is assessed considering multiple scenarios. The results show
that machine learning models can predict LMP 4-5 orders of magnitude faster
than traditional OPF solvers with 5-6\% error rate, highlighting the potential
of machine learning models in LMP prediction for large-scale power models with
the help of hardware solutions like multi-core CPUs and GPUs in modern HPC
clusters.


------------------------------------------------------------------------------

Title:
Generate to Understand for Representation

Abstract: In recent years, a significant number of high-quality pretrained models have
emerged, greatly impacting Natural Language Understanding (NLU), Natural
Language Generation (NLG), and Text Representation tasks. Traditionally, these
models are pretrained on custom domain corpora and finetuned for specific
tasks, resulting in high costs related to GPU usage and labor. Unfortunately,
recent trends in language modeling have shifted towards enhancing performance
through scaling, further exacerbating the associated costs.
Introducing GUR: a pretraining framework that combines language modeling and
contrastive learning objectives in a single training step. We select similar
text pairs based on their Longest Common Substring (LCS) from raw unlabeled
documents and train the model using masked language modeling and unsupervised
contrastive learning. The resulting model, GUR, achieves impressive results
without any labeled training data, outperforming all other pretrained baselines
as a retriever at the recall benchmark in a zero-shot setting. Additionally,
GUR maintains its language modeling ability, as demonstrated in our ablation
experiment. Our code is available at \url{this https URL}.


------------------------------------------------------------------------------

Title:
KEST: Kernel Distance Based Efficient Self-Training for Improving  Controllable Text Generation

Abstract: Self-training (ST) has come to fruition in language understanding tasks by
producing pseudo labels, which reduces the labeling bottleneck of language
model fine-tuning. Nevertheless, in facilitating semi-supervised controllable
language generation, ST faces two key challenges. First, augmented by
self-generated pseudo text, generation models tend to over-exploit the
previously learned text distribution, suffering from mode collapse and poor
generation diversity. Second, generating pseudo text in each iteration is
time-consuming, severely decelerating the training process. In this work, we
propose KEST, a novel and efficient self-training framework to handle these
problems. KEST utilizes a kernel-based loss, rather than standard cross
entropy, to learn from the soft pseudo text produced by a shared
non-autoregressive generator. We demonstrate both theoretically and empirically
that KEST can benefit from more diverse pseudo text in an efficient manner,
which allows not only refining and exploiting the previously fitted
distribution but also enhanced exploration towards a larger potential text
space, providing a guarantee of improved performance. Experiments on three
controllable generation tasks demonstrate that KEST significantly improves
control accuracy while maintaining comparable text fluency and generation
diversity against several strong baselines.


------------------------------------------------------------------------------

Title:
DreamCatcher: Revealing the Language of the Brain with fMRI using GPT  Embedding

Abstract: The human brain possesses remarkable abilities in visual processing,
including image recognition and scene summarization. Efforts have been made to
understand the cognitive capacities of the visual brain, but a comprehensive
understanding of the underlying mechanisms still needs to be discovered.
Advancements in brain decoding techniques have led to sophisticated approaches
like fMRI-to-Image reconstruction, which has implications for cognitive
neuroscience and medical imaging. However, challenges persist in fMRI-to-image
reconstruction, such as incorporating global context and contextual
information. In this article, we propose fMRI captioning, where captions are
generated based on fMRI data to gain insight into the neural correlates of
visual perception. This research presents DreamCatcher, a novel framework for
fMRI captioning. DreamCatcher consists of the Representation Space Encoder
(RSE) and the RevEmbedding Decoder, which transform fMRI vectors into a latent
space and generate captions, respectively. We evaluated the framework through
visualization, dataset training, and testing on subjects, demonstrating strong
performance. fMRI-based captioning has diverse applications, including
understanding neural mechanisms, Human-Computer Interaction, and enhancing
learning and training processes.


------------------------------------------------------------------------------

Title:
A New Perspective for Shuttlecock Hitting Event Detection

Abstract: This article introduces a novel approach to shuttlecock hitting event
detection. Instead of depending on generic methods, we capture the hitting
action of players by reasoning over a sequence of images. To learn the features
of hitting events in a video clip, we specifically utilize a deep learning
model known as SwingNet. This model is designed to capture the relevant
characteristics and patterns associated with the act of hitting in badminton.
By training SwingNet on the provided video clips, we aim to enable the model to
accurately recognize and identify the instances of hitting events based on
their distinctive features. Furthermore, we apply the specific video processing
technique to extract the prior features from the video, which significantly
reduces the learning difficulty for the model. The proposed method not only
provides an intuitive and user-friendly approach but also presents a fresh
perspective on the task of detecting badminton hitting events. The source code
will be available at
this https URL


------------------------------------------------------------------------------

Title:
FABLE : Fabric Anomaly Detection Automation Process

Abstract: Unsupervised anomaly in industry has been a concerning topic and a stepping
stone for high performance industrial automation process. The vast majority of
industry-oriented methods focus on learning from good samples to detect anomaly
notwithstanding some specific industrial scenario requiring even less specific
training and therefore a generalization for anomaly detection. The obvious use
case is the fabric anomaly detection, where we have to deal with a really wide
range of colors and types of textile and a stoppage of the production line for
training could not be considered. In this paper, we propose an automation
process for industrial fabric texture defect detection with a
specificity-learning process during the domain-generalized anomaly detection.
Combining the ability to generalize and the learning process offer a fast and
precise anomaly detection and segmentation. The main contributions of this
paper are the following: A domain-generalization texture anomaly detection
method achieving the state-of-the-art performances, a fast specific training on
good samples extracted by the proposed method, a self-evaluation method based
on custom defect creation and an automatic detection of already seen fabric to
prevent re-training.


------------------------------------------------------------------------------

Title:
Neural Priming for Sample-Efficient Adaptation

Abstract: We propose Neural Priming, a technique for adapting large pretrained models
to distribution shifts and downstream tasks given few or no labeled examples.
Presented with class names or unlabeled test samples, Neural Priming enables
the model to recall and conditions its parameters on relevant data seen
throughout pretraining, thereby priming it for the test distribution. Neural
Priming can be performed at test time in even for pretraining datasets as large
as LAION-2B. Performing lightweight updates on the recalled data significantly
improves accuracy across a variety of distribution shift and transfer learning
benchmarks. Concretely, in the zero-shot setting, we see a 2.45 improvement in
accuracy on ImageNet and 3.81 accuracy improvement on average across standard
transfer learning benchmarks. Further, using our test time inference scheme, we
see a 1.41 accuracy improvement on ImageNetV2. These results demonstrate the
effectiveness of Neural Priming in addressing the common challenge of limited
labeled data and changing distributions. Code is available at
github.com/RAIVNLab/neural-priming.


------------------------------------------------------------------------------

Title:
SSE: A Metric for Evaluating Search System Explainability

Abstract: Explainable Information Retrieval (XIR) is a growing research area focused on
enhancing transparency and trustworthiness of the complex decision-making
processes taking place in modern information retrieval systems. While there has
been progress in developing XIR systems, empirical evaluation tools to assess
the degree of explainability attained by such systems are lacking. To close
this gap and gain insights into the true merit of XIR systems, we extend
existing insights from a factor analysis of search explainability to introduce
SSE (Search System Explainability), an evaluation metric for XIR search
systems. Through a crowdsourced user study, we demonstrate SSE's ability to
distinguish between explainable and non-explainable systems, showing that
systems with higher scores indeed indicate greater interpretability.
Additionally, we observe comparable perceived temporal demand and performance
levels between non-native and native English speakers. We hope that aside from
these concrete contributions to XIR, this line of work will serve as a
blueprint for similar explainability evaluation efforts in other domains of
machine learning and natural language processing.


------------------------------------------------------------------------------

Title:
An Interdisciplinary Survey on Origin-destination Flows Modeling: Theory  and Techniques

Abstract: Origin-destination~(OD) flow modeling is an extensively researched subject
across multiple disciplines, such as the investigation of travel demand in
transportation and spatial interaction modeling in geography. However,
researchers from different fields tend to employ their own unique research
paradigms and lack interdisciplinary communication, preventing the
cross-fertilization of knowledge and the development of novel solutions to
challenges. This article presents a systematic interdisciplinary survey that
comprehensively and holistically scrutinizes OD flows from utilizing
fundamental theory to studying the mechanism of population mobility and solving
practical problems with engineering techniques, such as computational models.
Specifically, regional economics, urban geography, and sociophysics are adept
at employing theoretical research methods to explore the underlying mechanisms
of OD flows. They have developed three influential theoretical models: the
gravity model, the intervening opportunities model, and the radiation model.
These models specifically focus on examining the fundamental influences of
distance, opportunities, and population on OD flows, respectively. In the
meantime, fields such as transportation, urban planning, and computer science
primarily focus on addressing four practical problems: OD prediction, OD
construction, OD estimation, and OD forecasting. Advanced computational models,
such as deep learning models, have gradually been introduced to address these
problems more effectively. Finally, based on the existing research, this survey
summarizes current challenges and outlines future directions for this topic.
Through this survey, we aim to break down the barriers between disciplines in
OD flow-related research, fostering interdisciplinary perspectives and modes of
thinking.


------------------------------------------------------------------------------

Title:
Text-Driven Foley Sound Generation With Latent Diffusion Model

Abstract: Foley sound generation aims to synthesise the background sound for multimedia
content. Previous models usually employ a large development set with labels as
input (e.g., single numbers or one-hot vector). In this work, we propose a
diffusion model based system for Foley sound generation with text conditions.
To alleviate the data scarcity issue, our model is initially pre-trained with
large-scale datasets and fine-tuned to this task via transfer learning using
the contrastive language-audio pertaining (CLAP) technique. We have observed
that the feature embedding extracted by the text encoder can significantly
affect the performance of the generation model. Hence, we introduce a trainable
layer after the encoder to improve the text embedding produced by the encoder.
In addition, we further refine the generated waveform by generating multiple
candidate audio clips simultaneously and selecting the best one, which is
determined in terms of the similarity score between the embedding of the
candidate clips and the embedding of the target text label. Using the proposed
method, our system ranks ${1}^{st}$ among the systems submitted to DCASE
Challenge 2023 Task 7. The results of the ablation studies illustrate that the
proposed techniques significantly improve sound generation performance. The
codes for implementing the proposed system are available online.


------------------------------------------------------------------------------

Title:
Revealing the structure of language model capabilities

Abstract: Building a theoretical understanding of the capabilities of large language
models (LLMs) is vital for our ability to predict and explain the behavior of
these systems. Here, we investigate the structure of LLM capabilities by
extracting latent capabilities from patterns of individual differences across a
varied population of LLMs. Using a combination of Bayesian and frequentist
factor analysis, we analyzed data from 29 different LLMs across 27 cognitive
tasks. We found evidence that LLM capabilities are not monolithic. Instead,
they are better explained by three well-delineated factors that represent
reasoning, comprehension and core language modeling. Moreover, we found that
these three factors can explain a high proportion of the variance in model
performance. These results reveal a consistent structure in the capabilities of
different LLMs and demonstrate the multifaceted nature of these capabilities.
We also found that the three abilities show different relationships to model
properties such as model size and instruction tuning. These patterns help
refine our understanding of scaling laws and indicate that changes to a model
that improve one ability might simultaneously impair others. Based on these
findings, we suggest that benchmarks could be streamlined by focusing on tasks
that tap into each broad model ability.


------------------------------------------------------------------------------

Title:
Stacking of Hyperparameter Tuned Models for Tagging Coding Problems

Abstract: Coding problems are problems that require a solution in the form of a
computer program. Coding problems are popular among students and professionals
as it enhances their skills and career opportunities. An AI system that would
help those who practice coding problems would be highly useful and there is a
huge potential for such a system. In this work, we propose a model which uses
stacking of hyperparameter tuned boosting models to achieve impressive metric
scores of 77.8% accuracy and 0.815 PR-AUC on the dataset that was scraped from
Codeforces and Leetcode. We open source the dataset and the models developed
for this work.


------------------------------------------------------------------------------

Title:
Multi-Label Meta Weighting for Long-Tailed Dynamic Scene Graph  Generation

Abstract: This paper investigates the problem of scene graph generation in videos with
the aim of capturing semantic relations between subjects and objects in the
form of $\langle$subject, predicate, object$\rangle$ triplets. Recognizing the
predicate between subject and object pairs is imbalanced and multi-label in
nature, ranging from ubiquitous interactions such as spatial relationships (\eg
\emph{in front of}) to rare interactions such as \emph{twisting}. In
widely-used benchmarks such as Action Genome and VidOR, the imbalance ratio
between the most and least frequent predicates reaches 3,218 and 3,408,
respectively, surpassing even benchmarks specifically designed for long-tailed
recognition. Due to the long-tailed distributions and label co-occurrences,
recent state-of-the-art methods predominantly focus on the most frequently
occurring predicate classes, ignoring those in the long tail. In this paper, we
analyze the limitations of current approaches for scene graph generation in
videos and identify a one-to-one correspondence between predicate frequency and
recall performance. To make the step towards unbiased scene graph generation in
videos, we introduce a multi-label meta-learning framework to deal with the
biased predicate distribution. Our meta-learning framework learns a meta-weight
network for each training sample over all possible label losses. We evaluate
our approach on the Action Genome and VidOR benchmarks by building upon two
current state-of-the-art methods for each benchmark. The experiments
demonstrate that the multi-label meta-weight network improves the performance
for predicates in the long tail without compromising performance for head
classes, resulting in better overall performance and favorable
generalizability. Code: \url{this https URL}.


------------------------------------------------------------------------------

Title:
Artificial Intelligence for Technical Debt Management in Software  Development

Abstract: Technical debt is a well-known challenge in software development, and its
negative impact on software quality, maintainability, and performance is widely
recognized. In recent years, artificial intelligence (AI) has proven to be a
promising approach to assist in managing technical debt. This paper presents a
comprehensive literature review of existing research on the use of AI powered
tools for technical debt avoidance in software development. In this literature
review we analyzed 15 related research papers which covers various AI-powered
techniques, such as code analysis and review, automated testing, code
refactoring, predictive maintenance, code generation, and code documentation,
and explores their effectiveness in addressing technical debt. The review also
discusses the benefits and challenges of using AI for technical debt
management, provides insights into the current state of research, and
highlights gaps and opportunities for future research. The findings of this
review suggest that AI has the potential to significantly improve technical
debt management in software development, and that existing research provides
valuable insights into how AI can be leveraged to address technical debt
effectively and efficiently. However, the review also highlights several
challenges and limitations of current approaches, such as the need for
high-quality data and ethical considerations and underscores the importance of
further research to address these issues. The paper provides a comprehensive
overview of the current state of research on AI for technical debt avoidance
and offers practical guidance for software development teams seeking to
leverage AI in their development processes to mitigate technical debt
effectively


------------------------------------------------------------------------------

Title:
A Comprehensive Modeling Approach for Crop Yield Forecasts using  AI-based Methods and Crop Simulation Models

Abstract: Numerous solutions for yield estimation are either based on data-driven
models, or on crop-simulation models (CSMs). Researchers tend to build
data-driven models using nationwide crop information databases provided by
agencies such as the USDA. On the opposite side of the spectrum, CSMs require
fine data that may be hard to generalize from a handful of fields. In this
paper, we propose a comprehensive approach for yield forecasting that combines
data-driven solutions, crop simulation models, and model surrogates to support
multiple user-profiles and needs when dealing with crop management
decision-making. To achieve this goal, we have developed a solution to
calibrate CSMs at scale, a surrogate model of a CSM assuring faster execution,
and a neural network-based approach that performs efficient risk assessment in
such settings. Our data-driven modeling approach outperforms previous works
with yield correlation predictions close to 91\%. The crop simulation modeling
architecture achieved 6% error; the proposed crop simulation model surrogate
performs predictions almost 100 times faster than the adopted crop simulator
with similar accuracy levels.


------------------------------------------------------------------------------

Title:
Measuring IT Carbon Footprint: What is the Current Status Actually?

Abstract: Despite the new Corporate Sustainability Reporting Directive from the
European Union, which presses large enterprises to be more transparent about
their GHG emissions, and though large technology- or advisory firms might
peddle otherwise, there are plenty of challenges ahead when it comes to
measuring GHG emissions from IT activities in the first place. This paper
categories those challenges into 4 categories, and explains the current status,
shortcomings and potential future research directions. These categories are:
measuring software energy consumption, server overhead energy consumption,
Energy Mix and emissions from embodied carbon. Next to that, various non-profit
and open-source initiatives are introduced as well as a mathematical framework,
based on CPU consumption, that can act as a rule-of-thumb for quick and
effortless assessments.


------------------------------------------------------------------------------

Title:
Multi-scale Spatial-temporal Interaction Network for Video Anomaly  Detection

Abstract: Video anomaly detection (VAD) is an essential yet challenge task in signal
processing. Since certain anomalies cannot be detected by analyzing temporal or
spatial information alone, the interaction between two types of information is
considered crucial for VAD. However, current dual-stream architectures either
limit interaction between the two types of information to the bottleneck of
autoencoder or incorporate background pixels irrelevant to anomalies into the
interaction. To this end, we propose a multi-scale spatial-temporal interaction
network (MSTI-Net) for VAD. First, to pay particular attention to objects and
reconcile the significant semantic differences between the two information, we
propose an attention-based spatial-temporal fusion module (ASTM) as a
substitute for the conventional direct fusion. Furthermore, we inject multi
ASTM-based connections between the appearance and motion pathways of a dual
stream network to facilitate spatial-temporal interaction at all possible
scales. Finally, the regular information learned from multiple scales is
recorded in memory to enhance the differentiation between anomalies and normal
events during the testing phase. Solid experimental results on three standard
datasets validate the effectiveness of our approach, which achieve AUCs of
96.8% for UCSD Ped2, 87.6% for CUHK Avenue, and 73.9% for the ShanghaiTech
dataset.


------------------------------------------------------------------------------

Title:
NBMOD: Find It and Grasp It in Noisy Background

Abstract: Grasping objects is a fundamental yet important capability of robots, and
many tasks such as sorting and picking rely on this skill. The prerequisite for
stable grasping is the ability to correctly identify suitable grasping
positions. However, finding appropriate grasping points is challenging due to
the diverse shapes, varying density distributions, and significant differences
between the barycenter of various objects. In the past few years, researchers
have proposed many methods to address the above-mentioned issues and achieved
very good results on publicly available datasets such as the Cornell dataset
and the Jacquard dataset. The problem is that the backgrounds of Cornell and
Jacquard datasets are relatively simple - typically just a whiteboard, while in
real-world operational environments, the background could be complex and noisy.
Moreover, in real-world scenarios, robots usually only need to grasp fixed
types of objects. To address the aforementioned issues, we proposed a
large-scale grasp detection dataset called NBMOD: Noisy Background Multi-Object
Dataset for grasp detection, which consists of 31,500 RGB-D images of 20
different types of fruits. Accurate prediction of angles has always been a
challenging problem in the detection task of oriented bounding boxes. This
paper presents a Rotation Anchor Mechanism (RAM) to address this issue.
Considering the high real-time requirement of robotic systems, we propose a
series of lightweight architectures called RA-GraspNet (GraspNet with Rotation
Anchor): RARA (network with Rotation Anchor and Region Attention), RAST
(network with Rotation Anchor and Semi Transformer), and RAGT (network with
Rotation Anchor and Global Transformer) to tackle this problem. Among them, the
RAGT-3/3 model achieves an accuracy of 99% on the NBMOD dataset. The NBMOD and
our code are available at this https URL


------------------------------------------------------------------------------

Title:
ZeRO++: Extremely Efficient Collective Communication for Giant Model  Training

Abstract: Zero Redundancy Optimizer (ZeRO) has been used to train a wide range of large
language models on massive GPUs clusters due to its ease of use, efficiency,
and good scalability. However, when training on low-bandwidth clusters, or at
scale which forces batch size per GPU to be small, ZeRO's effective throughput
is limited because of high communication volume from gathering weights in
forward pass, backward pass, and averaging gradients. This paper introduces
three communication volume reduction techniques, which we collectively refer to
as ZeRO++, targeting each of the communication collectives in ZeRO. First is
block-quantization based all-gather. Second is data remapping that trades-off
communication for more memory. Third is a novel all-to-all based quantized
gradient averaging paradigm as replacement of reduce-scatter collective, which
preserves accuracy despite communicating low precision data. Collectively,
ZeRO++ reduces communication volume of ZeRO by 4x, enabling up to 2.16x better
throughput at 384 GPU scale.


------------------------------------------------------------------------------

Title:
AD-AutoGPT: An Autonomous GPT for Alzheimer's Disease Infodemiology

Abstract: In this pioneering study, inspired by AutoGPT, the state-of-the-art
open-source application based on the GPT-4 large language model, we develop a
novel tool called AD-AutoGPT which can conduct data collection, processing, and
analysis about complex health narratives of Alzheimer's Disease in an
autonomous manner via users' textual prompts. We collated comprehensive data
from a variety of news sources, including the Alzheimer's Association, BBC,
Mayo Clinic, and the National Institute on Aging since June 2022, leading to
the autonomous execution of robust trend analyses, intertopic distance maps
visualization, and identification of salient terms pertinent to Alzheimer's
Disease. This approach has yielded not only a quantifiable metric of relevant
discourse but also valuable insights into public focus on Alzheimer's Disease.
This application of AD-AutoGPT in public health signifies the transformative
potential of AI in facilitating a data-rich understanding of complex health
narratives like Alzheimer's Disease in an autonomous manner, setting the
groundwork for future AI-driven investigations in global health landscapes.


------------------------------------------------------------------------------

Title:
Joint Path planning and Power Allocation of a Cellular-Connected UAV  using Apprenticeship Learning via Deep Inverse Reinforcement Learning

Abstract: This paper investigates an interference-aware joint path planning and power
allocation mechanism for a cellular-connected unmanned aerial vehicle (UAV) in
a sparse suburban environment. The UAV's goal is to fly from an initial point
and reach a destination point by moving along the cells to guarantee the
required quality of service (QoS). In particular, the UAV aims to maximize its
uplink throughput and minimize the level of interference to the ground user
equipment (UEs) connected to the neighbor cellular BSs, considering the
shortest path and flight resource limitation. Expert knowledge is used to
experience the scenario and define the desired behavior for the sake of the
agent (i.e., UAV) training. To solve the problem, an apprenticeship learning
method is utilized via inverse reinforcement learning (IRL) based on both
Q-learning and deep reinforcement learning (DRL). The performance of this
method is compared to learning from a demonstration technique called behavioral
cloning (BC) using a supervised learning approach. Simulation and numerical
results show that the proposed approach can achieve expert-level performance.
We also demonstrate that, unlike the BC technique, the performance of our
proposed approach does not degrade in unseen situations.


------------------------------------------------------------------------------

Title:
Smoothing the Rough Edges: Evaluating Automatically Generated  Multi-Lattice Transitions

Abstract: Additive manufacturing is advantageous for producing lightweight components
while addressing complex design requirements. This capability has been
bolstered by the introduction of unit lattice cells and the gradation of those
cells. In cases where loading varies throughout a part, it may be beneficial to
use multiple, distinct lattice cell types, resulting in multi-lattice
structures. In such structures, abrupt transitions between unit cell topologies
may cause stress concentrations, making the boundary between unit cell types a
primary failure point. Thus, these regions require careful design in order to
ensure the overall functionality of the part. Although computational design
approaches have been proposed, smooth transition regions are still difficult to
achieve, especially between lattices of drastically different topologies. This
work demonstrates and assesses a method for using variational autoencoders to
automate the creation of transitional lattice cells, examining the factors that
contribute to smooth transitions. Through computational experimentation, it was
found that the smoothness of transition regions was strongly predicted by how
closely the endpoints were in the latent space, whereas the number of
transition intervals was not a sole predictor.


------------------------------------------------------------------------------

Title:
Calculating the matrix profile from noisy data

Abstract: The matrix profile (MP) is a data structure computed from a time series which
encodes the data required to locate motifs and discords, corresponding to
recurring patterns and outliers respectively. When the time series contains
noisy data then the conventional approach is to pre-filter it in order to
remove noise but this cannot apply in unsupervised settings where patterns and
outliers are not annotated. The resilience of the algorithm used to generate
the MP when faced with noisy data remains unknown. We measure the similarities
between the MP from original time series data with MPs generated from the same
data with noisy data added under a range of parameter settings including adding
duplicates and adding irrelevant data. We use three real world data sets drawn
from diverse domains for these experiments Based on dissimilarities between the
MPs, our results suggest that MP generation is resilient to a small amount of
noise being introduced into the data but as the amount of noise increases this
resilience disappears


------------------------------------------------------------------------------

Title:
Differentiable Instruction Optimization for Cross-Task Generalization

Abstract: Instruction tuning has been attracting much attention to achieve
generalization ability across a wide variety of tasks. Although various types
of instructions have been manually created for instruction tuning, it is still
unclear what kind of instruction is optimal to obtain cross-task generalization
ability. This work presents instruction optimization, which optimizes training
instructions with respect to generalization ability. Rather than manually
tuning instructions, we introduce learnable instructions and optimize them with
gradient descent by leveraging bilevel optimization. Experimental results show
that the learned instruction enhances the diversity of instructions and
improves the generalization ability compared to using only manually created
instructions.


------------------------------------------------------------------------------

Title:
Democratizing Chatbot Debugging: A Computational Framework for  Evaluating and Explaining Inappropriate Chatbot Responses

Abstract: Evaluating and understanding the inappropriateness of chatbot behaviors can
be challenging, particularly for chatbot designers without technical
backgrounds. To democratize the debugging process of chatbot misbehaviors for
non-technical designers, we propose a framework that leverages dialogue act
(DA) modeling to automate the evaluation and explanation of chatbot response
inappropriateness. The framework first produces characterizations of
context-aware DAs based on discourse analysis theory and real-world
human-chatbot transcripts. It then automatically extracts features to identify
the appropriateness level of a response and can explain the causes of the
inappropriate response by examining the DA mismatch between the response and
its conversational context. Using interview chatbots as a testbed, our
framework achieves comparable classification accuracy with higher
explainability and fewer computational resources than the deep learning
baseline, making it the first step in utilizing DAs for chatbot response
appropriateness evaluation and explanation.


------------------------------------------------------------------------------

Title:
Catastrophic Forgetting in the Context of Model Updates

Abstract: A large obstacle to deploying deep learning models in practice is the process
of updating models post-deployment (ideally, frequently). Deep neural networks
can cost many thousands of dollars to train. When new data comes in the
pipeline, you can train a new model from scratch (randomly initialized weights)
on all existing data. Instead, you can take an existing model and fine-tune
(continue to train) it on new data. The former is costly and slow. The latter
is cheap and fast, but catastrophic forgetting generally causes the new model
to 'forget' how to classify older data well. There are a plethora of
complicated techniques to keep models from forgetting their past learnings.
Arguably the most basic is to mix in a small amount of past data into the new
data during fine-tuning: also known as 'data rehearsal'. In this paper, we
compare various methods of limiting catastrophic forgetting and conclude that
if you can maintain access to a portion of your past data (or tasks), data
rehearsal is ideal in terms of overall accuracy across all time periods, and
performs even better when combined with methods like Elastic Weight
Consolidation (EWC). Especially when the amount of past data (past 'tasks') is
large compared to new data, the cost of updating an existing model is far
cheaper and faster than training a new model from scratch.


------------------------------------------------------------------------------

Title:
Domain-specific ChatBots for Science using Embeddings

Abstract: Large language models (LLMs) have emerged as powerful machine-learning
systems capable of handling a myriad of tasks. Tuned versions of these systems
have been turned into chatbots that can respond to user queries on a vast
diversity of topics, providing informative and creative replies. However, their
application to physical science research remains limited owing to their
incomplete knowledge in these areas, contrasted with the needs of rigor and
sourcing in science domains. Here, we demonstrate how existing methods and
software tools can be easily combined to yield a domain-specific chatbot. The
system ingests scientific documents in existing formats, and uses text
embedding lookup to provide the LLM with domain-specific contextual information
when composing its reply. We similarly demonstrate that existing image
embedding methods can be used for search and retrieval across publication
figures. These results confirm that LLMs are already suitable for use by
physical scientists in accelerating their research efforts.


------------------------------------------------------------------------------

Title:
Learning-Augmented Decentralized Online Convex Optimization in Networks

Abstract: This paper studies decentralized online convex optimization in a networked
multi-agent system and proposes a novel algorithm, Learning-Augmented
Decentralized Online optimization (LADO), for individual agents to select
actions only based on local online information. LADO leverages a baseline
policy to safeguard online actions for worst-case robustness guarantees, while
staying close to the machine learning (ML) policy for average performance
improvement. In stark contrast with the existing learning-augmented online
algorithms that focus on centralized settings, LADO achieves strong robustness
guarantees in a decentralized setting. We also prove the average cost bound for
LADO, revealing the tradeoff between average performance and worst-case
robustness and demonstrating the advantage of training the ML policy by
explicitly considering the robustness requirement.


------------------------------------------------------------------------------

Title:
Learning Spanning Forests Optimally using CUT Queries in Weighted  Undirected Graphs

Abstract: In this paper we describe a randomized algorithm which returns a maximal
spanning forest of an unknown {\em weighted} undirected graph making $O(n)$
$\mathsf{CUT}$ queries in expectation. For weighted graphs, this is optimal due
to a result in [Auza and Lee, 2021] which shows an $\Omega(n)$ lower bound for
zero-error randomized algorithms. %To our knowledge, it is the only regime of
this problem where we have upper and lower bounds tight up to constants. These
questions have been extensively studied in the past few years, especially due
to the problem's connections to symmetric submodular function minimization. We
also describe a simple polynomial time deterministic algorithm that makes
$O(\frac{n\log n}{\log\log n})$ queries on undirected unweighted graphs and
returns a maximal spanning forest, thereby (slightly) improving upon the
state-of-the-art.


------------------------------------------------------------------------------

Title:
Multi-task 3D building understanding with multi-modal pretraining

Abstract: This paper explores various learning strategies for 3D building type
classification and part segmentation on the BuildingNet dataset. ULIP with
PointNeXt and PointNeXt segmentation are extended for the classification and
segmentation task on BuildingNet dataset. The best multi-task PointNeXt-s model
with multi-modal pretraining achieves 59.36 overall accuracy for 3D building
type classification, and 31.68 PartIoU for 3D building part segmentation on
validation split. The final PointNeXt XL model achieves 31.33 PartIoU and 22.78
ShapeIoU on test split for BuildingNet-Points segmentation, which significantly
improved over PointNet++ model reported from BuildingNet paper, and it won the
1st place in the BuildingNet challenge at CVPR23 StruCo3D workshop.


------------------------------------------------------------------------------

Title:
Systematic Architectural Design of Scale Transformed Attention Condenser  DNNs via Multi-Scale Class Representational Response Similarity Analysis

Abstract: Self-attention mechanisms are commonly included in a convolutional neural
networks to achieve an improved efficiency performance balance. However, adding
self-attention mechanisms adds additional hyperparameters to tune for the
application at hand. In this work we propose a novel type of DNN analysis
called Multi-Scale Class Representational Response Similarity Analysis
(ClassRepSim) which can be used to identify specific design interventions that
lead to more efficient self-attention convolutional neural network
architectures. Using insights grained from ClassRepSim we propose the Spatial
Transformed Attention Condenser (STAC) module, a novel attention-condenser
based self-attention module. We show that adding STAC modules to ResNet style
architectures can result in up to a 1.6% increase in top-1 accuracy compared to
vanilla ResNet models and up to a 0.5% increase in top-1 accuracy compared to
SENet models on the ImageNet64x64 dataset, at the cost of up to 1.7% increase
in FLOPs and 2x the number of parameters. In addition, we demonstrate that
results from ClassRepSim analysis can be used to select an effective
parameterization of the STAC module resulting in competitive performance
compared to an extensive parameter search.


------------------------------------------------------------------------------

Title:
CStream: Parallel Data Stream Compression on Multicore Edge Devices

Abstract: In the burgeoning realm of Internet of Things (IoT) applications on edge
devices, data stream compression has become increasingly pertinent. The
integration of added compression overhead and limited hardware resources on
these devices calls for a nuanced software-hardware co-design. This paper
introduces CStream, a pioneering framework crafted for parallelizing stream
compression on multicore edge devices. CStream grapples with the distinct
challenges of delivering a high compression ratio, high throughput, low
latency, and low energy consumption. Notably, CStream distinguishes itself by
accommodating an array of stream compression algorithms, a variety of hardware
architectures and configurations, and an innovative set of parallelization
strategies, some of which are proposed herein for the first time. Our
evaluation showcases the efficacy of a thoughtful co-design involving a lossy
compression algorithm, asymmetric multicore processors, and our novel,
hardware-conscious parallelization strategies. This approach achieves a 2.8x
compression ratio with only marginal information loss, 4.3x throughput, 65%
latency reduction and 89% energy consumption reduction, compared to designs
lacking such strategic integration.


------------------------------------------------------------------------------

Title:
Geometric Mechanics of Contact-Switching Systems

Abstract: Discrete and periodic contact switching is a key characteristic of steady
state legged locomotion. This paper introduces a framework for modeling and
analyzing this contact-switching behavior through the framework of geometric
mechanics on a toy robot model that can make continuous limb swings and
discrete contact switches. The kinematics of this model forms a hybrid shape
space and by extending the generalized Stokes' theorem to compute discrete
curvature functions called stratified panels, we determine average locomotion
generated by gaits spanning multiple contact modes. Using this tool, we also
demonstrate the ability to optimize gaits based on system's locomotion
constraints and perform gait reduction on a complex gait spanning multiple
contact modes to highlight the scalability to multilegged systems.


------------------------------------------------------------------------------

Title:
CLARA: Classifying and Disambiguating User Commands for Reliable  Interactive Robotic Agents

Abstract: In this paper, we focus on inferring whether the given user command is clear,
ambiguous, or infeasible in the context of interactive robotic agents utilizing
large language models (LLMs). To tackle this problem, we first present an
uncertainty estimation method for LLMs to classify whether the command is
certain (i.e., clear) or not (i.e., ambiguous or infeasible). Once the command
is classified as uncertain, we further distinguish it between ambiguous or
infeasible commands leveraging LLMs with situational aware context in a
zero-shot manner. For ambiguous commands, we disambiguate the command by
interacting with users via question generation with LLMs. We believe that
proper recognition of the given commands could lead to a decrease in
malfunction and undesired actions of the robot, enhancing the reliability of
interactive robot agents. We present a dataset for robotic situational
awareness, consisting pair of high-level commands, scene descriptions, and
labels of command type (i.e., clear, ambiguous, or infeasible). We validate the
proposed method on the collected dataset, pick-and-place tabletop simulation.
Finally, we demonstrate the proposed approach in real-world human-robot
interaction experiments, i.e., handover scenarios.


------------------------------------------------------------------------------

Title:
DCdetector: Dual Attention Contrastive Representation Learning for Time  Series Anomaly Detection

Abstract: Time series anomaly detection is critical for a wide range of applications.
It aims to identify deviant samples from the normal sample distribution in time
series. The most fundamental challenge for this task is to learn a
representation map that enables effective discrimination of anomalies.
Reconstruction-based methods still dominate, but the representation learning
with anomalies might hurt the performance with its large abnormal loss. On the
other hand, contrastive learning aims to find a representation that can clearly
distinguish any instance from the others, which can bring a more natural and
promising representation for time series anomaly detection. In this paper, we
propose DCdetector, a multi-scale dual attention contrastive representation
learning model. DCdetector utilizes a novel dual attention asymmetric design to
create the permutated environment and pure contrastive loss to guide the
learning process, thus learning a permutation invariant representation with
superior discrimination abilities. Extensive experiments show that DCdetector
achieves state-of-the-art results on multiple time series anomaly detection
benchmark datasets. Code is publicly available at
this https URL


------------------------------------------------------------------------------

Title:
The Ontology for Agents, Systems and Integration of Services: OASIS  version 2

Abstract: Semantic representation is a key enabler for several application domains, and
the multi-agent systems realm makes no exception. Among the methods for
semantically representing agents, one has been essentially achieved by taking a
behaviouristic vision, through which one can describe how they operate and
engage with their peers. The approach essentially aims at defining the
operational capabilities of agents through the mental states related with the
achievement of tasks. The OASIS ontology -- An Ontology for Agent, Systems, and
Integration of Services, presented in 2019 -- pursues the behaviouristic
approach to deliver a semantic representation system and a communication
protocol for agents and their commitments. This paper reports on the main
modeling choices concerning the representation of agents in OASIS 2, the latest
major upgrade of OASIS, and the achievement reached by the ontology since it
was first introduced, in particular in the context of ontologies for
blockchains.


------------------------------------------------------------------------------

Title:
Artificial Intelligence for Emergency Response

Abstract: Emergency response management (ERM) is a challenge faced by communities
across the globe. First responders must respond to various incidents, such as
fires, traffic accidents, and medical emergencies. They must respond quickly to
incidents to minimize the risk to human life. Consequently, considerable
attention has been devoted to studying emergency incidents and response in the
last several decades. In particular, data-driven models help reduce human and
financial loss and improve design codes, traffic regulations, and safety
measures. This tutorial paper explores four sub-problems within emergency
response: incident prediction, incident detection, resource allocation, and
resource dispatch. We aim to present mathematical formulations for these
problems and broad frameworks for each problem. We also share open-source
(synthetic) data from a large metropolitan area in the USA for future work on
data-driven emergency response.


------------------------------------------------------------------------------

Title:
Semi-supervised Relation Extraction via Data Augmentation and  Consistency-training

Abstract: Due to the semantic complexity of the Relation extraction (RE) task,
obtaining high-quality human labelled data is an expensive and noisy process.
To improve the sample efficiency of the models, semi-supervised learning (SSL)
methods aim to leverage unlabelled data in addition to learning from limited
labelled data points. Recently, strong data augmentation combined with
consistency-based semi-supervised learning methods have advanced the state of
the art in several SSL tasks. However, adapting these methods to the RE task
has been challenging due to the difficulty of data augmentation for RE. In this
work, we leverage the recent advances in controlled text generation to perform
high quality data augmentation for the RE task. We further introduce small but
significant changes to model architecture that allows for generation of more
training data by interpolating different data points in their latent space.
These data augmentations along with consistency training result in very
competitive results for semi-supervised relation extraction on four benchmark
datasets.


------------------------------------------------------------------------------

Title:
The pop song generator: designing an online course to teach  collaborative, creative AI

Abstract: This article describes and evaluates a new online AI-creativity course. The
course is based around three near-state-of-the-art AI models combined into a
pop song generating system. A fine-tuned GPT-2 model writes lyrics, Music-VAE
composes musical scores and instrumentation and Diffsinger synthesises a
singing voice. We explain the decisions made in designing the course which is
based on Piagetian, constructivist 'learning-by-doing'. We present details of
the five-week course design with learning objectives, technical concepts, and
creative and technical activities. We explain how we overcame technical
challenges to build a complete pop song generator system, consisting of Python
scripts, pre-trained models, and Javascript code that runs in a dockerised
Linux container via a web-based IDE. A quantitative analysis of student
activity provides evidence on engagement and a benchmark for future
improvements. A qualitative analysis of a workshop with experts validated the
overall course design, it suggested the need for a stronger creative brief and
ethical and legal content.


------------------------------------------------------------------------------

Title:
Current Trends in Digital Twin Development, Maintenance, and Operation:  An Interview Study

Abstract: Digital twins (DT) are often defined as a pairing of a physical entity and a
corresponding virtual entity mimicking certain aspects of the former depending
on the use-case. In recent years, this concept has facilitated numerous
use-cases ranging from design to validation and predictive maintenance of large
and small high-tech systems. Although growing in popularity in both industry
and academia, digital twins and the methodologies for developing and
maintaining them differ vastly. To better understand these differences and
similarities, we performed a semi-structured interview research study with 19
professionals from industry and academia who are closely associated with
different lifecycle stages of the corresponding digital twins. In this paper,
we present our analysis and findings from this study, which is based on eight
research questions (RQ). We present our findings per research question. In
general, we identified an overall lack of uniformity in terms of the
understanding of digital twins and used tools, techniques, and methodologies
for their development and maintenance. Furthermore, considering that digital
twins are software intensive systems, we recognize a significant growth
potential for adopting more software engineering practices, processes, and
expertise in various stages of a digital twin's lifecycle.


------------------------------------------------------------------------------

Title:
DsMtGCN: A Direction-sensitive Multi-task framework for Knowledge Graph  Completion

Abstract: To solve the inherent incompleteness of knowledge graphs (KGs), numbers of
knowledge graph completion (KGC) models have been proposed to predict missing
links from known triples. Among those, several works have achieved more
advanced results via exploiting the structure information on KGs with Graph
Convolutional Networks (GCN). However, we observe that entity embeddings
aggregated from neighbors in different directions are just simply averaged to
complete single-tasks by existing GCN based models, ignoring the specific
requirements of forward and backward sub-tasks. In this paper, we propose a
Direction-sensitive Multi-task GCN (DsMtGCN) to make full use of the direction
information, the multi-head self-attention is applied to specifically combine
embeddings in different directions based on various entities and sub-tasks, the
geometric constraints are imposed to adjust the distribution of embeddings, and
the traditional binary cross-entropy loss is modified to reflect the triple
uncertainty. Moreover, the competitive experiments results on several benchmark
datasets verify the effectiveness of our model.


------------------------------------------------------------------------------

Title:
Ladder: A software to label images, detect objects and deploy models  recurrently for object detection

Abstract: Object Detection (OD) is a computer vision technology that can locate and
classify objects in images and videos, which has the potential to significantly
improve efficiency in precision agriculture. To simplify OD application
process, we developed Ladder - a software that provides users with a friendly
graphic user interface (GUI) that allows for efficient labelling of training
datasets, training OD models, and deploying the trained model. Ladder was
designed with an interactive recurrent framework that leverages predictions
from a pre-trained OD model as the initial image labeling. After adding human
labels, the newly labeled images can be added into the training data to retrain
the OD model. With the same GUI, users can also deploy well-trained OD models
by loading the model weight file to detect new images. We used Ladder to
develop a deep learning model to access wheat stripe rust in RGB (red, green,
blue) images taken by an Unmanned Aerial Vehicle (UAV). Ladder employs OD to
directly evaluate different severity levels of wheat stripe rust in field
images, eliminating the need for photo stitching process for UAVs-based images.
The accuracy for low, medium and high severity scores were 72%, 50% and 80%,
respectively. This case demonstrates how Ladder empowers OD in precision
agriculture and crop breeding.


------------------------------------------------------------------------------

Title:
Identification of leaky Lamb waves for waveguides sandwiched between  elastic half-spaces using the Spectral Collocation Method

Abstract: In non-destructive evaluation guided wave inspections, the elastic structure
to be inspected is often embedded within other elastic media and the ensuing
leaky waves are complex and non-trivial to characterise; we consider the
canonical example of an elastic waveguide surrounded by other elastic materials
that demonstrates the fundamental issues with characterising the leaky waves in
such systems. Due to the complex wavenumber solutions required to represent
them, leaky waves pose significant challenges to existing numerical methods,
while methods that spatially discretise the field to retrieve them suffer from
the exponential growth of their amplitude far into the surrounding media. We
present a spectral collocation method yielding an accurate and efficient
identification of these modes, leaking into elastic half-spaces. We discretise
the elastic domains and, depending on the exterior bulk wavespeeds, select
appropriate mappings of the discretised domain to complex paths, in which the
numerical solution decays and the physics of the problem are preserved. By
iterating through all possible radiation cases, the full set of dispersion and
attenuation curves are successfully retrieved and validated, where possible,
against the commercially available software DISPERSE. As an independent
validation, dispersion curves are obtained from finite element simulations of
time-dependent waves using Fourier analysis.


------------------------------------------------------------------------------

Title:
Deep Learning Guided Autonomous Surgery: Guiding Small Needles into  Sub-Millimeter Scale Blood Vessels

Abstract: We propose a general strategy for autonomous guidance and insertion of a
needle into a retinal blood vessel. The main challenges underpinning this task
are the accurate placement of the needle-tip on the target vein and a careful
needle insertion maneuver to avoid double-puncturing the vein, while dealing
with challenging kinematic constraints and depth-estimation uncertainty.
Following how surgeons perform this task purely based on visual feedback, we
develop a system which relies solely on \emph{monocular} visual cues by
combining data-driven kinematic and contact estimation, visual-servoing, and
model-based optimal control. By relying on both known kinematic models, as well
as deep-learning based perception modules, the system can localize the surgical
needle tip and detect needle-tissue interactions and venipuncture events. The
outputs from these perception modules are then combined with a motion planning
framework that uses visual-servoing and optimal control to cannulate the target
vein, while respecting kinematic constraints that consider the safety of the
procedure. We demonstrate that we can reliably and consistently perform needle
insertion in the domain of retinal surgery, specifically in performing retinal
vein cannulation. Using cadaveric pig eyes, we demonstrate that our system can
navigate to target veins within 22$\mu m$ XY accuracy and perform the entire
procedure in less than 35 seconds on average, and all 24 trials performed on 4
pig eyes were successful. Preliminary comparison study against a human operator
show that our system is consistently more accurate and safer, especially during
safety-critical needle-tissue interactions. To the best of the authors'
knowledge, this work accomplishes a first demonstration of autonomous retinal
vein cannulation at a clinically-relevant setting using animal tissues.


------------------------------------------------------------------------------

Title:
Learning Space-Time Semantic Correspondences

Abstract: We propose a new task of space-time semantic correspondence prediction in
videos. Given a source video, a target video, and a set of space-time
key-points in the source video, the task requires predicting a set of keypoints
in the target video that are the semantic correspondences of the provided
source keypoints. We believe that this task is important for fine-grain video
understanding, potentially enabling applications such as activity coaching,
sports analysis, robot imitation learning, and more. Our contributions in this
paper are: (i) proposing a new task and providing annotations for space-time
semantic correspondences on two existing benchmarks: Penn Action and Pouring;
and (ii) presenting a comprehensive set of baselines and experiments to gain
insights about the new problem. Our main finding is that the space-time
semantic correspondence prediction problem is best approached jointly in space
and time rather than in their decomposed sub-problems: time alignment and
spatial correspondences.


------------------------------------------------------------------------------

Title:
ActiveGLAE: A Benchmark for Deep Active Learning with Transformers

Abstract: Deep active learning (DAL) seeks to reduce annotation costs by enabling the
model to actively query instance annotations from which it expects to learn the
most. Despite extensive research, there is currently no standardized evaluation
protocol for transformer-based language models in the field of DAL. Diverse
experimental settings lead to difficulties in comparing research and deriving
recommendations for practitioners. To tackle this challenge, we propose the
ActiveGLAE benchmark, a comprehensive collection of data sets and evaluation
guidelines for assessing DAL. Our benchmark aims to facilitate and streamline
the evaluation process of novel DAL strategies. Additionally, we provide an
extensive overview of current practice in DAL with transformer-based language
models. We identify three key challenges - data set selection, model training,
and DAL settings - that pose difficulties in comparing query strategies. We
establish baseline results through an extensive set of experiments as a
reference point for evaluating future work. Based on our findings, we provide
guidelines for researchers and practitioners.


------------------------------------------------------------------------------

Title:
In-Process Global Interpretation for Graph Learning via Distribution  Matching

Abstract: Graphs neural networks (GNNs) have emerged as a powerful graph learning model
due to their superior capacity in capturing critical graph patterns. To gain
insights about the model mechanism for interpretable graph learning, previous
efforts focus on post-hoc local interpretation by extracting the data pattern
that a pre-trained GNN model uses to make an individual prediction. However,
recent works show that post-hoc methods are highly sensitive to model
initialization and local interpretation can only explain the model prediction
specific to a particular instance. In this work, we address these limitations
by answering an important question that is not yet studied: how to provide
global interpretation of the model training procedure? We formulate this
problem as in-process global interpretation, which targets on distilling
high-level and human-intelligible patterns that dominate the training procedure
of GNNs. We further propose Graph Distribution Matching (GDM) to synthesize
interpretive graphs by matching the distribution of the original and
interpretive graphs in the feature space of the GNN as its training proceeds.
These few interpretive graphs demonstrate the most informative patterns the
model captures during training. Extensive experiments on graph classification
datasets demonstrate multiple advantages of the proposed method, including high
explanation accuracy, time efficiency and the ability to reveal class-relevant
structure.


------------------------------------------------------------------------------

Title:
Spatial-SpinDrop: Spatial Dropout-based Binary Bayesian Neural Network  with Spintronics Implementation

Abstract: Recently, machine learning systems have gained prominence in real-time,
critical decision-making domains, such as autonomous driving and industrial
automation. Their implementations should avoid overconfident predictions
through uncertainty estimation. Bayesian Neural Networks (BayNNs) are
principled methods for estimating predictive uncertainty. However, their
computational costs and power consumption hinder their widespread deployment in
edge AI. Utilizing Dropout as an approximation of the posterior distribution,
binarizing the parameters of BayNNs, and further to that implementing them in
spintronics-based computation-in-memory (CiM) hardware arrays provide can be a
viable solution. However, designing hardware Dropout modules for convolutional
neural network (CNN) topologies is challenging and expensive, as they may
require numerous Dropout modules and need to use spatial information to drop
certain elements. In this paper, we introduce MC-SpatialDropout, a spatial
dropout-based approximate BayNNs with spintronics emerging devices. Our method
utilizes the inherent stochasticity of spintronic devices for efficient
implementation of the spatial dropout module compared to existing
implementations. Furthermore, the number of dropout modules per network layer
is reduced by a factor of $9\times$ and energy consumption by a factor of
$94.11\times$, while still achieving comparable predictive performance and
uncertainty estimates compared to related works.


------------------------------------------------------------------------------

Title:
Active Policy Improvement from Multiple Black-box Oracles

Abstract: Reinforcement learning (RL) has made significant strides in various complex
domains. However, identifying an effective policy via RL often necessitates
extensive exploration. Imitation learning aims to mitigate this issue by using
expert demonstrations to guide exploration. In real-world scenarios, one often
has access to multiple suboptimal black-box experts, rather than a single
optimal oracle. These experts do not universally outperform each other across
all states, presenting a challenge in actively deciding which oracle to use and
in which state. We introduce MAPS and MAPS-SE, a class of policy improvement
algorithms that perform imitation learning from multiple suboptimal oracles. In
particular, MAPS actively selects which of the oracles to imitate and improve
their value function estimates, and MAPS-SE additionally leverages an active
state exploration criterion to determine which states one should explore. We
provide a comprehensive theoretical analysis and demonstrate that MAPS and
MAPS-SE enjoy sample efficiency advantage over the state-of-the-art policy
improvement algorithms. Empirical results show that MAPS-SE significantly
accelerates policy optimization via state-wise imitation learning from multiple
oracles across a broad spectrum of control tasks in the DeepMind Control Suite.
Our code is publicly available at: this https URL


------------------------------------------------------------------------------

Title:
Understanding Revision Behavior in Adaptive Writing Support Systems for  Education

Abstract: Revision behavior in adaptive writing support systems is an important and
relatively new area of research that can improve the design and effectiveness
of these tools, and promote students' self-regulated learning (SRL).
Understanding how these tools are used is key to improving them to better
support learners in their writing and learning processes. In this paper, we
present a novel pipeline with insights into the revision behavior of students
at scale. We leverage a data set of two groups using an adaptive writing
support tool in an educational setting. With our novel pipeline, we show that
the tool was effective in promoting revision among the learners. Depending on
the writing feedback, we were able to analyze different strategies of learners
when revising their texts, we found that users of the exemplary case improved
over time and that females tend to be more efficient. Our research contributes
a pipeline for measuring SRL behaviors at scale in writing tasks (i.e.,
engagement or revision behavior) and informs the design of future adaptive
writing support systems for education, with the goal of enhancing their
effectiveness in supporting student writing. The source code is available at
this https URL


------------------------------------------------------------------------------

Title:
Thrilled by Your Progress! Large Language Models (GPT-4) No Longer  Struggle to Pass Assessments in Higher Education Programming Courses

Abstract: This paper studies recent developments in large language models' (LLM)
abilities to pass assessments in introductory and intermediate Python
programming courses at the postsecondary level. The emergence of ChatGPT
resulted in heated debates of its potential uses (e.g., exercise generation,
code explanation) as well as misuses in programming classes (e.g., cheating).
Recent studies show that while the technology performs surprisingly well on
diverse sets of assessment instruments employed in typical programming classes
the performance is usually not sufficient to pass the courses. The release of
GPT-4 largely emphasized notable improvements in the capabilities related to
handling assessments originally designed for human test-takers. This study is
the necessary analysis in the context of this ongoing transition towards mature
generative AI systems. Specifically, we report the performance of GPT-4,
comparing it to the previous generations of GPT models, on three Python courses
with assessments ranging from simple multiple-choice questions (no code
involved) to complex programming projects with code bases distributed into
multiple files (599 exercises overall). Additionally, we analyze the
assessments that were not handled well by GPT-4 to understand the current
limitations of the model, as well as its capabilities to leverage feedback
provided by an auto-grader. We found that the GPT models evolved from
completely failing the typical programming class' assessments (the original
GPT-3) to confidently passing the courses with no human involvement (GPT-4).
While we identified certain limitations in GPT-4's handling of MCQs and coding
exercises, the rate of improvement across the recent generations of GPT models
strongly suggests their potential to handle almost any type of assessment
widely used in higher education programming courses. These findings could be
leveraged by educators and institutions to adapt the design of programming
assessments as well as to fuel the necessary discussions into how programming
classes should be updated to reflect the recent technological developments.
This study provides evidence that programming instructors need to prepare for a
world in which there is an easy-to-use widely accessible technology that can be
utilized by learners to collect passing scores, with no effort whatsoever, on
what today counts as viable programming knowledge and skills assessments.


------------------------------------------------------------------------------

Title:
Adaptive Strategies in Non-convex Optimization

Abstract: An algorithm is said to be adaptive to a certain parameter (of the problem)
if it does not need a priori knowledge of such a parameter but performs
competitively to those that know it. This dissertation presents our work on
adaptive algorithms in following scenarios: 1. In the stochastic optimization
setting, we only receive stochastic gradients and the level of noise in
evaluating them greatly affects the convergence rate. Tuning is typically
required when without prior knowledge of the noise scale in order to achieve
the optimal rate. Considering this, we designed and analyzed noise-adaptive
algorithms that can automatically ensure (near)-optimal rates under different
noise scales without knowing it. 2. In training deep neural networks, the
scales of gradient magnitudes in each coordinate can scatter across a very wide
range unless normalization techniques, like BatchNorm, are employed. In such
situations, algorithms not addressing this problem of gradient scales can
behave very poorly. To mitigate this, we formally established the advantage of
scale-free algorithms that adapt to the gradient scales and presented its real
benefits in empirical experiments. 3. Traditional analyses in non-convex
optimization typically rely on the smoothness assumption. Yet, this condition
does not capture the properties of some deep learning objective functions,
including the ones involving Long Short-Term Memory networks and Transformers.
Instead, they satisfy a much more relaxed condition, with potentially unbounded
smoothness. Under this condition, we show that a generalized SignSGD algorithm
can theoretically match the best-known convergence rates obtained by SGD with
gradient clipping but does not need explicit clipping at all, and it can
empirically match the performance of Adam and beat others. Moreover, it can
also be made to automatically adapt to the unknown relaxed smoothness.


------------------------------------------------------------------------------

Title:
Convolutional and Deep Learning based techniques for Time Series Ordinal  Classification

Abstract: Time Series Classification (TSC) covers the supervised learning problem where
input data is provided in the form of series of values observed through
repeated measurements over time, and whose objective is to predict the category
to which they belong. When the class values are ordinal, classifiers that take
this into account can perform better than nominal classifiers. Time Series
Ordinal Classification (TSOC) is the field covering this gap, yet unexplored in
the literature. There are a wide range of time series problems showing an
ordered label structure, and TSC techniques that ignore the order relationship
discard useful information. Hence, this paper presents a first benchmarking of
TSOC methodologies, exploiting the ordering of the target labels to boost the
performance of current TSC state-of-the-art. Both convolutional- and deep
learning-based methodologies (among the best performing alternatives for
nominal TSC) are adapted for TSOC. For the experiments, a selection of 18
ordinal problems from two well-known archives has been made. In this way, this
paper contributes to the establishment of the state-of-the-art in TSOC. The
results obtained by ordinal versions are found to be significantly better than
current nominal TSC techniques in terms of ordinal performance metrics,
outlining the importance of considering the ordering of the labels when dealing
with this kind of problems.


------------------------------------------------------------------------------

Title:
Acoustic Identification of Ae. aegypti Mosquitoes using Smartphone Apps  and Residual Convolutional Neural Networks

Abstract: In this paper, we advocate in favor of smartphone apps as low-cost,
easy-to-deploy solution for raising awareness among the population on the
proliferation of Aedes aegypti mosquitoes. Nevertheless, devising such a
smartphone app is challenging, for many reasons, including the required
maturity level of techniques for identifying mosquitoes based on features that
can be captured using smartphone resources. In this paper, we identify a set of
(non-exhaustive) requirements that smartphone apps must meet to become an
effective tooling in the fight against Ae. aegypti, and advance the
state-of-the-art with (i) a residual convolutional neural network for
classifying Ae. aegypti mosquitoes from their wingbeat sound, (ii) a
methodology for reducing the influence of background noise in the
classification process, and (iii) a dataset for benchmarking solutions for
detecting Ae. aegypti mosquitoes from wingbeat sound recordings. From the
analysis of accuracy and recall, we provide evidence that convolutional neural
networks have potential as a cornerstone for tracking mosquito apps for
smartphones.


------------------------------------------------------------------------------

Title:
Large Language Models for Telecom: The Next Big Thing?

Abstract: The evolution of generative artificial intelligence (GenAI) constitutes a
turning point in reshaping the future of technology in different aspects.
Wireless networks in particular, with the blooming of self-evolving networks,
represent a rich field for exploiting GenAI and reaping several benefits that
can fundamentally change the way how wireless networks are designed and
operated nowadays. To be specific, large language models (LLMs), a subfield of
GenAI, are envisioned to open up a new era of autonomous wireless networks, in
which a multimodal large model trained over various Telecom data, can be
fine-tuned to perform several downstream tasks, eliminating the need for
dedicated AI models for each task and paving the way for the realization of
artificial general intelligence (AGI)-empowered wireless networks. In this
article, we aim to unfold the opportunities that can be reaped from integrating
LLMs into the Telecom domain. In particular, we aim to put a forward-looking
vision on a new realm of possibilities and applications of LLMs in future
wireless networks, defining directions for designing, training, testing, and
deploying Telecom LLMs, and reveal insights on the associated theoretical and
practical challenges.


------------------------------------------------------------------------------

Title:
GLIMMER: generalized late-interaction memory reranker

Abstract: Memory-augmentation is a powerful approach for efficiently incorporating
external information into language models, but leads to reduced performance
relative to retrieving text. Recent work introduced LUMEN, a memory-retrieval
hybrid that partially pre-computes memory and updates memory representations on
the fly with a smaller live encoder.
We propose GLIMMER, which improves on this approach through 1) exploiting
free access to the powerful memory representations by applying a shallow
reranker on top of memory to drastically improve retrieval quality at low cost,
and 2) incorporating multi-task training to learn a general and higher quality
memory and live encoder. GLIMMER achieves strong gains in performance at faster
speeds compared to LUMEN and FiD on the KILT benchmark of knowledge-intensive
tasks.


------------------------------------------------------------------------------

Title:
A Shift In Artistic Practices through Artificial Intelligence

Abstract: The explosion of content generated by Artificial Intelligence models has
initiated a cultural shift in arts, music, and media, where roles are changing,
values are shifting, and conventions are challenged. The readily available,
vast dataset of the internet has created an environment for AI models to be
trained on any content on the web. With AI models shared openly, and used by
many, globally, how does this new paradigm shift challenge the status quo in
artistic practices? What kind of changes will AI technology bring into music,
arts, and new media?


------------------------------------------------------------------------------

Title:
Approximate Model Counting: Is SAT Oracle More Powerful than NP Oracle?

Abstract: Given a Boolean formula $\phi$ over $n$ variables, the problem of model
counting is to compute the number of solutions of $\phi$. Model counting is a
fundamental problem in computer science with wide-ranging applications. Owing
to the \#P-hardness of the problems, Stockmeyer initiated the study of the
complexity of approximate counting. Stockmeyer showed that $\log n$ calls to an
NP oracle are necessary and sufficient to achieve $(\varepsilon,\delta)$
guarantees. The hashing-based framework proposed by Stockmeyer has been very
influential in designing practical counters over the past decade, wherein the
SAT solver substitutes the NP oracle calls in practice. It is well known that
an NP oracle does not fully capture the behavior of SAT solvers, as SAT solvers
are also designed to provide satisfying assignments when a formula is
satisfiable, without additional overhead. Accordingly, the notion of SAT oracle
has been proposed to capture the behavior of SAT solver wherein given a Boolean
formula, an SAT oracle returns a satisfying assignment if the formula is
satisfiable or returns unsatisfiable otherwise. Since the practical
state-of-the-art approximate counting techniques use SAT solvers, a natural
question is whether an SAT oracle is more powerful than an NP oracle in the
context of approximate model counting.
The primary contribution of this work is to study the relative power of the
NP oracle and SAT oracle in the context of approximate model counting. The
previous techniques proposed in the context of an NP oracle are weak to provide
strong bounds in the context of SAT oracle since, in contrast to an NP oracle
that provides only one bit of information, a SAT oracle can provide $n$ bits of
information. We therefore develop a new methodology to achieve the main result:
a SAT oracle is no more powerful than an NP oracle in the context of
approximate model counting.


------------------------------------------------------------------------------

Title:
Optimizer's Information Criterion: Dissecting and Correcting Bias in  Data-Driven Optimization

Abstract: In data-driven optimization, the sample performance of the obtained decision
typically incurs an optimistic bias against the true performance, a phenomenon
commonly known as the Optimizer's Curse and intimately related to overfitting
in machine learning. Common techniques to correct this bias, such as
cross-validation, require repeatedly solving additional optimization problems
and are therefore computationally expensive. We develop a general bias
correction approach, building on what we call Optimizer's Information Criterion
(OIC), that directly approximates the first-order bias and does not require
solving any additional optimization problems. Our OIC generalizes the
celebrated Akaike Information Criterion to evaluate the objective performance
in data-driven optimization, which crucially involves not only model fitting
but also its interplay with the downstream optimization. As such it can be used
for decision selection instead of only model selection. We apply our approach
to a range of data-driven optimization formulations comprising empirical and
parametric models, their regularized counterparts, and furthermore contextual
optimization. Finally, we provide numerical validation on the superior
performance of our approach under synthetic and real-world datasets.


------------------------------------------------------------------------------

Title:
Deep Learning Guided Autonomous Retinal Surgery using a Robotic Arm,  Microscopy, and iOCT Imaging

Abstract: Recent technological advancements in retinal surgery has led to the modern
operating room consisting of a surgical robot, microscope, and intraoperative
optical coherence tomography (iOCT). The integration of these tools raises the
fundamental question of how to effectively combine them to enable surgical
autonomy. In this work, we address this question by developing a unified
framework that enables real-time autonomous surgical workflows utilizing the
aforementioned devices. To achieve this, we make the following contributions:
(1) we develop a novel imaging system that integrates microscopy and iOCT in
real-time, accomplished by dynamically tracking the surgical instrument via a
small iOCT scanning region (e.g. B-scan), which was not previously possible;
(2) implementing various convolutional neural networks (CNN) that automatically
segment and detect task-relevant information for surgical autonomy; (3)
enabling surgeons to intuitively select goal waypoints within both the
microscope and iOCT views through simple mouse-click interactions; (4)
integrating model predictive control (MPC) for real-time trajectory generation
that respects kinematic constraints to ensure patient safety. We show the
utility of our system by tackling subretinal injection (SI), a challenging
procedure that involves inserting a microneedle below the retinal tissue for
targeted drug delivery, a task surgeons find challenging due to requiring
tens-of-micrometers of accuracy and precise depth perception. We validate our
system by conducting 30 successful SI trials on pig eyes, achieving needle
insertion accuracy of $26 \pm 12 \mu m$ to various subretinal goals and
duration of $55 \pm 10.8$ seconds. Preliminary comparisons to a human operator
performing SI in robot-assisted mode highlight the enhanced safety of our
system.


------------------------------------------------------------------------------

Title:
FutureTOD: Teaching Future Knowledge to Pre-trained Language Model for  Task-Oriented Dialogue

Abstract: Pre-trained language models based on general text enable huge success in the
NLP scenario. But the intrinsical difference of linguistic patterns between
general text and task-oriented dialogues makes existing pre-trained language
models less useful in practice. Current dialogue pre-training methods rely on a
contrastive framework and face the challenges of both selecting true positives
and hard negatives. In this paper, we propose a novel dialogue pre-training
model, FutureTOD, which distills future knowledge to the representation of the
previous dialogue context using a self-training framework. Our intuition is
that a good dialogue representation both learns local context information and
predicts future information. Extensive experiments on diverse downstream
dialogue tasks demonstrate the effectiveness of our model, especially the
generalization, robustness, and learning discriminative dialogue
representations capabilities.


------------------------------------------------------------------------------

Title:
Matrix Diagonalization as a Board Game: Teaching an Eigensolver the  Fastest Path to Solution

Abstract: Matrix diagonalization is at the cornerstone of numerous fields of scientific
computing. Diagonalizing a matrix to solve an eigenvalue problem requires a
sequential path of iterations that eventually reaches a sufficiently converged
and accurate solution for all the eigenvalues and eigenvectors. This typically
translates into a high computational cost. Here we demonstrate how
reinforcement learning, using the AlphaZero framework, can accelerate Jacobi
matrix diagonalizations by viewing the selection of the fastest path to
solution as a board game. To demonstrate the viability of our approach we apply
the Jacobi diagonalization algorithm to symmetric Hamiltonian matrices that
appear in quantum chemistry calculations. We find that a significant
acceleration can often be achieved. Our findings highlight the opportunity to
use machine learning as a promising tool to improve the performance of
numerical linear algebra.


------------------------------------------------------------------------------

Title:
EM-Network: Oracle Guided Self-distillation for Sequence Learning

Abstract: We introduce EM-Network, a novel self-distillation approach that effectively
leverages target information for supervised sequence-to-sequence (seq2seq)
learning. In contrast to conventional methods, it is trained with oracle
guidance, which is derived from the target sequence. Since the oracle guidance
compactly represents the target-side context that can assist the sequence model
in solving the task, the EM-Network achieves a better prediction compared to
using only the source input. To allow the sequence model to inherit the
promising capability of the EM-Network, we propose a new self-distillation
strategy, where the original sequence model can benefit from the knowledge of
the EM-Network in a one-stage manner. We conduct comprehensive experiments on
two types of seq2seq models: connectionist temporal classification (CTC) for
speech recognition and attention-based encoder-decoder (AED) for machine
translation. Experimental results demonstrate that the EM-Network significantly
advances the current state-of-the-art approaches, improving over the best prior
work on speech recognition and establishing state-of-the-art performance on
WMT'14 and IWSLT'14.


------------------------------------------------------------------------------

Title:
Pseudorandom unitaries are neither real nor sparse nor noise-robust

Abstract: Pseudorandom quantum states (PRSs) and pseudorandom unitaries (PRUs) possess
the dual nature of being efficiently constructible while appearing completely
random to any efficient quantum algorithm. In this study, we establish
fundamental bounds on pseudorandomness. We show that PRSs and PRUs exist only
when the probability that an error occurs is negligible, ruling out their
generation on noisy intermediate-scale and early fault-tolerant quantum
computers. Additionally, we derive lower bounds on the imaginarity and
coherence of PRSs and PRUs, rule out the existence of sparse or real PRUs, and
show that PRUs are more difficult to generate than PRSs. Our work also
establishes rigorous bounds on the efficiency of property testing,
demonstrating the exponential complexity in distinguishing real quantum states
from imaginary ones, in contrast to the efficient measurability of unitary
imaginarity. Furthermore, we prove lower bounds on the testing of coherence.
Lastly, we show that the transformation from a complex to a real model of
quantum computation is inefficient, in contrast to the reverse process, which
is efficient. Overall, our results establish fundamental limits on property
testing and provide valuable insights into quantum pseudorandomness.


------------------------------------------------------------------------------

Title:
Neighborhood-based Hard Negative Mining for Sequential Recommendation

Abstract: Negative sampling plays a crucial role in training successful sequential
recommendation models. Instead of merely employing random negative sample
selection, numerous strategies have been proposed to mine informative negative
samples to enhance training and performance. However, few of these approaches
utilize structural information. In this work, we observe that as training
progresses, the distributions of node-pair similarities in different groups
with varying degrees of neighborhood overlap change significantly, suggesting
that item pairs in distinct groups may possess different negative
relationships. Motivated by this observation, we propose a Graph-based Negative
sampling approach based on Neighborhood Overlap (GNNO) to exploit structural
information hidden in user behaviors for negative mining. GNNO first constructs
a global weighted item transition graph using training sequences. Subsequently,
it mines hard negative samples based on the degree of overlap with the target
item on the graph. Furthermore, GNNO employs curriculum learning to control the
hardness of negative samples, progressing from easy to difficult. Extensive
experiments on three Amazon benchmarks demonstrate GNNO's effectiveness in
consistently enhancing the performance of various state-of-the-art models and
surpassing existing negative sampling strategies. The code will be released at
\url{this https URL}.


------------------------------------------------------------------------------

Title:
High-order Moment Closure Models with Random Batch Method for Efficient  Computation of Multiscale Turbulent Systems

Abstract: We propose a high-order stochastic-statistical moment closure model for
efficient ensemble prediction of leading-order statistical moments and
probability density functions in multiscale complex turbulent systems. The
statistical moment equations are closed by a precise calibration of the
high-order feedbacks using ensemble solutions of the consistent stochastic
equations, suitable for modeling complex phenomena including non-Gaussian
statistics and extreme events. To address challenges associated with closely
coupled spatio-temporal scales in turbulent states and expensive large ensemble
simulation for high-dimensional systems, we introduce efficient computational
strategies using the random batch method (RBM). This approach significantly
reduces the required ensemble size while accurately capturing essential
high-order structures. Only a small batch of small-scale fluctuation modes is
used for each time update of the samples, and exact convergence to the full
model statistics is ensured through frequent resampling of the batches during
time evolution. Furthermore, we develop a reduced-order model to handle systems
with really high dimension by linking the large number of small-scale
fluctuation modes to ensemble samples of dominant leading modes. The
effectiveness of the proposed models is validated by numerical experiments on
the one-layer and two-layer Lorenz '96 systems, which exhibit representative
chaotic features and various statistical regimes. The full and reduced-order
RBM models demonstrate uniformly high skill in capturing the time evolution of
crucial leading-order statistics, non-Gaussian probability distributions, while
achieving significantly lower computational cost compared to direct Monte-Carlo
approaches.


------------------------------------------------------------------------------

Title:
Process Voltage Temperature Variability Estimation of Tunneling Current  for Band-to-Band-Tunneling based Neuron

Abstract: Compact and energy-efficient Synapse and Neurons are essential to realize the
full potential of neuromorphic computing. In addition, a low variability is
indeed needed for neurons in Deep neural networks for higher accuracy. Further,
process (P), voltage (V), and temperature (T) variation (PVT) are essential
considerations for low-power circuits as performance impact and compensation
complexities are added costs. Recently, band-to-band tunneling (BTBT) neuron
has been demonstrated to operate successfully in a network to enable a Liquid
State Machine. A comparison of the PVT with competing modes of operation (e.g.,
BTBT vs. sub-threshold and above threshold) of the same transistor is a
critical factor in assessing performance. In this work, we demonstrate the PVT
variation impact in the BTBT regime and benchmark the operation against the
subthreshold slope (SS) and ON-regime (ION) of partially depleted-Silicon on
Insulator MOSFET. It is shown that the On-state regime offers the lowest
variability but dissipates higher power. Hence, not usable for low-power
sources. Among the BTBT and SS regimes, which can enable the low-power neuron,
the BTBT regime has shown ~3x variability reduction ({\sigma}_I_D/{\mu}_I_D)
than the SS regime, considering the cumulative PVT variability. The improvement
is due to the well-known weaker P, V, and T dependence of BTBT vs. SS. We show
that the BTBT variation is uncorrelated with mutually correlated SS & ION
operation - indicating its different origin from the mechanism and location
perspectives. Hence, the BTBT regime is promising for low-current, low-power,
and low device-to-device variability neuron operation.


------------------------------------------------------------------------------

Title:
Individual Treatment Effects in Extreme Regimes

Abstract: Understanding individual treatment effects in extreme regimes is important
for characterizing risks associated with different interventions. This is
hindered by the fact that extreme regime data may be hard to collect, as it is
scarcely observed in practice. In addressing this issue, we propose a new
framework for estimating the individual treatment effect in extreme regimes
(ITE$_2$). Specifically, we quantify this effect by the changes in the tail
decay rates of potential outcomes in the presence or absence of the treatment.
Subsequently, we establish conditions under which ITE$_2$ may be calculated and
develop algorithms for its computation. We demonstrate the efficacy of our
proposed method on various synthetic and semi-synthetic datasets.


------------------------------------------------------------------------------

Title:
A Pairing Enhancement Approach for Aspect Sentiment Triplet Extraction

Abstract: Aspect Sentiment Triplet Extraction (ASTE) aims to extract the triplet of an
aspect term, an opinion term, and their corresponding sentiment polarity from
the review texts. Due to the complexity of language and the existence of
multiple aspect terms and opinion terms in a single sentence, current models
often confuse the connections between an aspect term and the opinion term
describing it. To address this issue, we propose a pairing enhancement approach
for ASTE, which incorporates contrastive learning during the training stage to
inject aspect-opinion pairing knowledge into the triplet extraction model.
Experimental results demonstrate that our approach performs well on four ASTE
datasets (i.e., 14lap, 14res, 15res and 16res) compared to several related
classical and state-of-the-art triplet extraction methods. Moreover, ablation
studies conduct an analysis and verify the advantage of contrastive learning
over other pairing enhancement approaches.


------------------------------------------------------------------------------

Title:
A Practical Entity Linking System for Tables in Scientific Literature

Abstract: Entity linking is an important step towards constructing knowledge graphs
that facilitate advanced question answering over scientific documents,
including the retrieval of relevant information included in tables within these
documents. This paper introduces a general-purpose system for linking entities
to items in the Wikidata knowledge base. It describes how we adapt this system
for linking domain-specific entities, especially for those entities embedded
within tables drawn from COVID-19-related scientific literature. We describe
the setup of an efficient offline instance of the system that enables our
entity-linking approach to be more feasible in practice. As part of a broader
approach to infer the semantic meaning of scientific tables, we leverage the
structural and semantic characteristics of the tables to improve overall entity
linking performance.


------------------------------------------------------------------------------

Title:
Investigating Reproducibility at Interspeech Conferences: A Longitudinal  and Comparative Perspective

Abstract: Reproducibility is a key aspect for scientific advancement across
disciplines, and reducing barriers for open science is a focus area for the
theme of Interspeech 2023. Availability of source code is one of the indicators
that facilitates reproducibility. However, less is known about the rates of
reproducibility at Interspeech conferences in comparison to other conferences
in the field. In order to fill this gap, we have surveyed 27,717 papers at
seven conferences across speech and language processing disciplines. We find
that despite having a close number of accepted papers to the other conferences,
Interspeech has up to 40% less source code availability. In addition to
reporting the difficulties we have encountered during our research, we also
provide recommendations and possible directions to increase reproducibility for
further studies.


------------------------------------------------------------------------------

Title:
Document Layout Annotation: Database and Benchmark in the Domain of  Public Affairs

Abstract: Every day, thousands of digital documents are generated with useful
information for companies, public organizations, and citizens. Given the
impossibility of processing them manually, the automatic processing of these
documents is becoming increasingly necessary in certain sectors. However, this
task remains challenging, since in most cases a text-only based parsing is not
enough to fully understand the information presented through different
components of varying significance. In this regard, Document Layout Analysis
(DLA) has been an interesting research field for many years, which aims to
detect and classify the basic components of a document. In this work, we used a
procedure to semi-automatically annotate digital documents with different
layout labels, including 4 basic layout blocks and 4 text categories. We apply
this procedure to collect a novel database for DLA in the public affairs
domain, using a set of 24 data sources from the Spanish Administration. The
database comprises 37.9K documents with more than 441K document pages, and more
than 8M labels associated to 8 layout block units. The results of our
experiments validate the proposed text labeling procedure with accuracy up to
99%.


------------------------------------------------------------------------------

Title:
Promises and Perils of Mining Software Package Ecosystem Data

Abstract: The use of third-party packages is becoming increasingly popular and has led
to the emergence of large software package ecosystems with a maze of
inter-dependencies. Since the reliance on these ecosystems enables developers
to reduce development effort and increase productivity, it has attracted the
interest of researchers: understanding the infrastructure and dynamics of
package ecosystems has given rise to approaches for better code reuse,
automated updates, and the avoidance of vulnerabilities, to name a few
examples. But the reality of these ecosystems also poses challenges to software
engineering researchers, such as: How do we obtain the complete network of
dependencies along with the corresponding versioning information? What are the
boundaries of these package ecosystems? How do we consistently detect
dependencies that are declared but not used? How do we consistently identify
developers within a package ecosystem? How much of the ecosystem do we need to
understand to analyse a single component? How well do our approaches generalise
across different programming languages and package ecosystems? In this chapter,
we review promises and perils of mining the rich data related to software
package ecosystems available to software engineering researchers.


------------------------------------------------------------------------------

Title:
Pseudo session-based recommendation with hierarchical embedding and  session attributes

Abstract: Recently, electronic commerce (EC) websites have been unable to provide an
identification number (user ID) for each transaction data entry because of
privacy issues. Because most recommendation methods assume that all data are
assigned a user ID, they cannot be applied to the data without user IDs.
Recently, session-based recommendation (SBR) based on session information,
which is short-term behavioral information of users, has been studied. A
general SBR uses only information about the item of interest to make a
recommendation (e.g., item ID for an EC site). Particularly in the case of EC
sites, the data recorded include the name of the item being purchased, the
price of the item, the category hierarchy, and the gender and region of the
user. In this study, we define a pseudo--session for the purchase history data
of an EC site without user IDs and session IDs. Finally, we propose an SBR with
a co-guided heterogeneous hypergraph and globalgraph network plus, called
CoHHGN+. The results show that our CoHHGN+ can recommend items with higher
performance than other methods.


------------------------------------------------------------------------------

Title:
Legal and ethical considerations regarding the use of ChatGPT in  education

Abstract: Artificial intelligence has evolved enormously over the last two decades,
becoming mainstream in different scientific domains including education, where
so far, it is mainly utilized to enhance administrative and intelligent
tutoring systems services and academic support. ChatGPT, an artificial
intelligence-based chatbot, developed by OpenAI and released in November 2022,
has rapidly gained attention from the entire international community for its
impressive performance in generating comprehensive, systematic, and informative
human-like responses to user input through natural language processing.
Inevitably, it has also rapidly posed several challenges, opportunities, and
potential issues and concerns raised regarding its use across various
scientific disciplines. This paper aims to discuss the legal and ethical
implications arising from this new technology, identify potential use cases,
and enrich our understanding of Generative AI, such as ChatGPT, and its
capabilities in education.


------------------------------------------------------------------------------

Title:
Neural Astrophysical Wind Models

Abstract: The bulk kinematics and thermodynamics of hot supernovae-driven galactic
winds is critically dependent on both the amount of swept up cool clouds and
non-spherical collimated flow geometry. However, accurately parameterizing
these physics is difficult because their functional forms are often unknown,
and because the coupled non-linear flow equations contain singularities. We
show that deep neural networks embedded as individual terms in the governing
coupled ordinary differential equations (ODEs) can robustly discover both of
these physics, without any prior knowledge of the true function structure, as a
supervised learning task. We optimize a loss function based on the Mach number,
rather than the explicitly solved-for 3 conserved variables, and apply a
penalty term towards near-diverging solutions. The same neural network
architecture is used for learning both the hidden mass-loading and surface area
expansion rates. This work further highlights the feasibility of neural ODEs as
a promising discovery tool with mechanistic interpretability for non-linear
inverse problems.


------------------------------------------------------------------------------

Title:
Low-complexity Multidimensional DCT Approximations

Abstract: In this paper, we introduce low-complexity multidimensional discrete cosine
transform (DCT) approximations. Three dimensional DCT (3D DCT) approximations
are formalized in terms of high-order tensor theory. The formulation is
extended to higher dimensions with arbitrary lengths. Several multiplierless
$8\times 8\times 8$ approximate methods are proposed and the computational
complexity is discussed for the general multidimensional case. The proposed
methods complexity cost was assessed, presenting considerably lower arithmetic
operations when compared with the exact 3D DCT. The proposed approximations
were embedded into 3D DCT-based video coding scheme and a modified quantization
step was introduced. The simulation results showed that the approximate 3D DCT
coding methods offer almost identical output visual quality when compared with
exact 3D DCT scheme. The proposed 3D approximations were also employed as a
tool for visual tracking. The approximate 3D DCT-based proposed system performs
similarly to the original exact 3D DCT-based method. In general, the suggested
methods showed competitive performance at a considerably lower computational
cost.


------------------------------------------------------------------------------

Title:
The Life and Death of Software Ecosystems

Abstract: Software ecosystems have gained a lot of attention in recent times. Industry
and developers gather around technologies and collaborate to their advancement;
when the boundaries of such an effort go beyond certain amount of projects, we
are witnessing the appearance of Free/Libre and Open Source Software (FLOSS)
ecosystems.
In this chapter, we explore two aspects that contribute to a healthy
ecosystem, related to the attraction (and detraction) and the death of
ecosystems. To function and survive, ecosystems need to attract people, get
them on-boarded and retain them. In Section One we explore possibilities with
provocative research questions for attracting and detracting contributors (and
users): the lifeblood of FLOSS ecosystems. Then in the Section Two, we focus on
the death of systems, exploring some presumed to be dead systems and their
state in the afterlife.


------------------------------------------------------------------------------

Title:
Unlocking Insights into Business Trajectories with Transformer-based  Spatio-temporal Data Analysis

Abstract: The world of business is constantly evolving and staying ahead of the curve
requires a deep understanding of market trends and performance. This article
addresses this requirement by modeling business trajectories using news
articles data.


------------------------------------------------------------------------------

Title:
Ethical Considerations Towards Protestware

Abstract: A key drawback to using a Open Source third-party library is the risk of
introducing malicious attacks. In recently times, these threats have taken a
new form, when maintainers turn their Open Source libraries into protestware.
This is defined as software containing political messages delivered through
these libraries, which can either be malicious or benign. Since developers are
willing to freely open-up their software to these libraries, much trust and
responsibility are placed on the maintainers to ensure that the library does
what it promises to do. This paper takes a look into the possible scenarios
where developers might consider turning their Open Source Software into
protestware, using an ethico-philosophical lens. Using different frameworks
commonly used in AI ethics, we explore the different dilemmas that may result
in protestware. Additionally, we illustrate how an open-source maintainer's
decision to protest is influenced by different stakeholders (viz., their
membership in the OSS community, their personal views, financial motivations,
social status, and moral viewpoints), making protestware a multifaceted and
intricate matter.


------------------------------------------------------------------------------

Title:
Theoretical Analysis on the Efficiency of Interleaved Comparisons

Abstract: This study presents a theoretical analysis on the efficiency of interleaving,
an efficient online evaluation method for rankings. Although interleaving has
already been applied to production systems, the source of its high efficiency
has not been clarified in the literature. Therefore, this study presents a
theoretical analysis on the efficiency of interleaving methods. We begin by
designing a simple interleaving method similar to ordinary interleaving
methods. Then, we explore a condition under which the interleaving method is
more efficient than A/B testing and find that this is the case when users leave
the ranking depending on the item's relevance, a typical assumption made in
click models. Finally, we perform experiments based on numerical analysis and
user simulation, demonstrating that the theoretical results are consistent with
the empirical results.


------------------------------------------------------------------------------

Title:
Improving visual image reconstruction from human brain activity using  latent diffusion models via multiple decoded inputs

Abstract: The integration of deep learning and neuroscience has been advancing rapidly,
which has led to improvements in the analysis of brain activity and the
understanding of deep learning models from a neuroscientific perspective. The
reconstruction of visual experience from human brain activity is an area that
has particularly benefited: the use of deep learning models trained on large
amounts of natural images has greatly improved its quality, and approaches that
combine the diverse information contained in visual experiences have
proliferated rapidly in recent years. In this technical paper, by taking
advantage of the simple and generic framework that we proposed (Takagi and
Nishimoto, CVPR 2023), we examine the extent to which various additional
decoding techniques affect the performance of visual experience reconstruction.
Specifically, we combined our earlier work with the following three techniques:
using decoded text from brain activity, nonlinear optimization for structural
image reconstruction, and using decoded depth information from brain activity.
We confirmed that these techniques contributed to improving accuracy over the
baseline. We also discuss what researchers should consider when performing
visual reconstruction using deep generative models trained on large datasets.
Please check our webpage at
this https URL Code is also
available at this https URL


------------------------------------------------------------------------------

Title:
Increasing paths in random temporal graphs

Abstract: We consider random temporal graphs, a version of the classical
Erd\H{o}s--R\'enyi random graph G(n,p) where additionally, each edge has a
distinct random time stamp, and connectivity is constrained to sequences of
edges with increasing time stamps. We study the asymptotics for the distances
in such graphs, mostly in the regime of interest where np is of order log n. We
establish the first order asymptotics for the lengths of increasing paths: the
lengths of the shortest and longest paths between typical vertices, the maxima
of these lengths from a given vertex, as well as the maxima between any two
vertices; this covers the (temporal) diameter.


------------------------------------------------------------------------------

Title:
A unified immersed finite element error analysis for one-dimensional  interface problems

Abstract: It has been noted that the traditional scaling argument cannot be directly
applied to the error analysis of immersed finite elements (IFE) because, in
general, the spaces on the reference element associated with the IFE spaces on
different interface elements via the standard affine mapping are not the same.
By analyzing a mapping from the involved Sobolev space to the IFE space, this
article is able to extend the scaling argument framework to the error
estimation for the approximation capability of a class of IFE spaces in one
spatial dimension. As demonstrations of the versatility of this unified error
analysis framework, the manuscript applies the proposed scaling argument to
obtain optimal IFE error estimates for a typical first-order linear hyperbolic
interface problem, a second-order elliptic interface problem, and the
fourth-order Euler-Bernoulli beam interface problem, respectively.


------------------------------------------------------------------------------

Title:
Improved Web Accessibility Evaluation of Open Learning Contents for  Individuals with Learning Disabilities

Abstract: Web content should be accessible to normal and disabled communities on
electronic devices. The Web Accessibility Initiative (WAI) has created standard
guidelines called Web Content Accessibility Guidelines (WCAG). Mobile Web Best
Practice (MWBP) is also proposed by WAI for accessibility of websites on
desktop computers and mobile devices like smartphones, tablets, iPads, iPhones,
and iPods. Educational Resources that provide free licensed learning content
are used to test the WCAG. The disabled community also has equal rights to gain
access to these learning materials through electronic devices. The main purpose
of this research is to evaluate these selected open educational learning
materials for individuals with only learning disabilities. This research
provides several recommendations to improve the accessibility level of the
Learning Management Systems. Future research includes developing a more
accessible learning management system with minimized or no accessibility
errors. Disability includes physical impairments, mental disorders, lack of
cognition, learning and emotional disability. Some individuals have multiple
disorders. Learning disabilities are one of them. People have difficulty
learning because of an unknown factor or low intelligence quotient (IQ).


------------------------------------------------------------------------------

Title:
Last-Iterate Convergent Policy Gradient Primal-Dual Methods for  Constrained MDPs

Abstract: We study the problem of computing an optimal policy of an infinite-horizon
discounted constrained Markov decision process (constrained MDP). Despite the
popularity of Lagrangian-based policy search methods used in practice, the
oscillation of policy iterates in these methods has not been fully understood,
bringing out issues such as violation of constraints and sensitivity to
hyper-parameters. To fill this gap, we employ the Lagrangian method to cast a
constrained MDP into a constrained saddle-point problem in which max/min
players correspond to primal/dual variables, respectively, and develop two
single-time-scale policy-based primal-dual algorithms with non-asymptotic
convergence of their policy iterates to an optimal constrained policy.
Specifically, we first propose a regularized policy gradient primal-dual
(RPG-PD) method that updates the policy using an entropy-regularized policy
gradient, and the dual via a quadratic-regularized gradient ascent,
simultaneously. We prove that the policy primal-dual iterates of RPG-PD
converge to a regularized saddle point with a sublinear rate, while the policy
iterates converge sublinearly to an optimal constrained policy. We further
instantiate RPG-PD in large state or action spaces by including function
approximation in policy parametrization, and establish similar sublinear
last-iterate policy convergence. Second, we propose an optimistic policy
gradient primal-dual (OPG-PD) method that employs the optimistic gradient
method to update primal/dual variables, simultaneously. We prove that the
policy primal-dual iterates of OPG-PD converge to a saddle point that contains
an optimal constrained policy, with a linear rate. To the best of our
knowledge, this work appears to be the first non-asymptotic policy last-iterate
convergence result for single-time-scale algorithms in constrained MDPs.


------------------------------------------------------------------------------

Title:
Decomposition and Interleaving for Variance Reduction of Post-click  Metrics

Abstract: In this study, we propose an efficient method for comparing the post-click
metric (e.g., dwell time and conversion rate) of multiple rankings in online
experiments. The proposed method involves (1) the decomposition of the
post-click metric measurement of a ranking into a click model estimation and a
post-click metric measurement of each item in the ranking, and (2) interleaving
of multiple rankings to produce a single ranking that preferentially exposes
items possessing a high population variance. The decomposition of the
post-click metric measurement enables the free layout of items in a ranking and
focuses on the measurement of the post-click metric of each item in the
multiple rankings. The interleaving of multiple rankings reduces the sample
variance of the items possessing a high population variance by optimizing a
ranking to be presented to the users so that those items received more samples
of the post-click metric. In addition, we provide a proof that the proposed
method leads to the minimization of the evaluation error in the ranking
comparison and propose two practical techniques to stabilize the online
experiment. We performed a comprehensive simulation experiment and a real
service setting experiment. The experimental results revealed that (1) the
proposed method outperformed existing methods in terms of efficiency and
accuracy, and the performance was especially remarkable when the input rankings
shared many items, and (2) the two stabilization techniques successfully
improved the evaluation accuracy and efficiency.


------------------------------------------------------------------------------

Title:
A Bayesian Take on Gaussian Process Networks

Abstract: Gaussian Process Networks (GPNs) are a class of directed graphical models
which employ Gaussian processes as priors for the conditional expectation of
each variable given its parents in the network. The model allows describing
continuous joint distributions in a compact but flexible manner with minimal
parametric assumptions on the dependencies between variables. Bayesian
structure learning of GPNs requires computing the posterior over graphs of the
network and is computationally infeasible even in low dimensions. This work
implements Monte Carlo and Markov Chain Monte Carlo methods to sample from the
posterior distribution of network structures. As such, the approach follows the
Bayesian paradigm, comparing models via their marginal likelihood and computing
the posterior probability of the GPN features. Simulation studies show that our
method outperforms state-of-the-art algorithms in recovering the graphical
structure of the network and provides an accurate approximation of its
posterior distribution.


------------------------------------------------------------------------------

Title:
Computing large deviation prefactors of stochastic dynamical systems  based on machine learning

Abstract: In this paper, we present large deviation theory that characterizes the
exponential estimate for rare events of stochastic dynamical systems in the
limit of weak noise. We aim to consider next-to-leading-order approximation for
more accurate calculation of mean exit time via computing large deviation
prefactors with the research efforts of machine learning. More specifically, we
design a neural network framework to compute quasipotential, most probable
paths and prefactors based on the orthogonal decomposition of vector field. We
corroborate the higher effectiveness and accuracy of our algorithm with a
practical example. Numerical experiments demonstrate its powerful function in
exploring internal mechanism of rare events triggered by weak random
fluctuations.


------------------------------------------------------------------------------

Title:
Spatio-temporal DeepKriging for Interpolation and Probabilistic  Forecasting

Abstract: Gaussian processes (GP) and Kriging are widely used in traditional
spatio-temporal mod-elling and prediction. These techniques typically
presuppose that the data are observed from a stationary GP with parametric
covariance structure. However, processes in real-world applications often
exhibit non-Gaussianity and nonstationarity. Moreover, likelihood-based
inference for GPs is computationally expensive and thus prohibitive for large
datasets. In this paper we propose a deep neural network (DNN) based two-stage
model for spatio-temporal interpolation and forecasting. Interpolation is
performed in the first step, which utilizes a dependent DNN with the embedding
layer constructed with spatio-temporal basis functions. For the second stage,
we use Long-Short Term Memory (LSTM) and convolutional LSTM to forecast future
observations at a given location. We adopt the quantile-based loss function in
the DNN to provide probabilistic forecasting. Compared to Kriging, the proposed
method does not require specifying covariance functions or making stationarity
assumption, and is computationally efficient. Therefore, it is suitable for
large-scale prediction of complex spatio-temporal processes. We apply our
method to monthly $PM_{2.5}$ data at more than $200,000$ space-time locations
from January 1999 to December 2022 for fast imputation of missing values and
forecasts with uncertainties.


------------------------------------------------------------------------------

Title:
Visually grounded few-shot word learning in low-resource settings

Abstract: We propose a visually grounded speech model that learns new words and their
visual depictions from just a few word-image example pairs. Given a set of test
images and a spoken query, we ask the model which image depicts the query word.
Previous work has simplified this few-shot learning problem by either using an
artificial setting with digit word-image pairs or by using a large number of
examples per class. Moreover, all previous studies were performed using English
speech-image data. We propose an approach that can work on natural word-image
pairs but with less examples, i.e. fewer shots, and then illustrate how this
approach can be applied for multimodal few-shot learning in a real low-resource
language, Yoruba. Our approach involves using the given word-image example
pairs to mine new unsupervised word-image training pairs from large collections
of unlabelledspeech and images. Additionally, we use a word-to-image attention
mechanism to determine word-image similarity. With this new model, we achieve
better performance with fewer shots than previous approaches on an existing
English benchmark. Many of the model's mistakes are due to confusion between
visual concepts co-occurring in similar contexts. The experiments on Yoruba
show the benefit of transferring knowledge from a multimodal model trained on a
larger set of English speech-image data.


------------------------------------------------------------------------------

Title:
Towards mutual synchronization of serially connected Spin Torque  Oscillators based on magnetic tunnel junctions

Abstract: Multiple neuromorphic applications require the tuning of two or more devices
to a common signal. Various types of neuromorphic computation can be realized
using spintronic oscillators, where the DC current induces magnetization
precession, which turns into an AC voltage generator. However, in spintronics,
synchronization of two oscillators using a DC signal is still a challenging
problem because it requires a certain degree of similarity between devices that
are to be synchronized, which may be difficult to achieve due to device
parameter distribution during the fabrication process. In this work, we present
experimental results on the mechanisms of synchronization of spin-torque
oscillators. Devices are based on magnetic tunnel junction with a
perpendicularly magnetized free layer and take advantage of a uniform
magnetization precision in the presence of the magnetic field and a DC bias. By
using an external microwave source, we show the optimal condition for the
synchronization of the magnetic tunnel junctions. Finally, we present results
on the in-series connection of two junctions and discuss the possible path
towards improving oscillation power and linewidth. In addition, using numerical
simulations of the coupled oscillators model, we aim to reproduce the
conditions of the experiments and determine the tolerance for achieving
synchronization.


------------------------------------------------------------------------------

Title:
Deep Learning Methods for Retinal Blood Vessel Segmentation: Evaluation  on Images with Retinopathy of Prematurity

Abstract: Automatic blood vessel segmentation from retinal images plays an important
role in the diagnosis of many systemic and eye diseases, including retinopathy
of prematurity. Current state-of-the-art research in blood vessel segmentation
from retinal images is based on convolutional neural networks. The solutions
proposed so far are trained and tested on images from a few available retinal
blood vessel segmentation datasets, which might limit their performance when
given an image with retinopathy of prematurity signs. In this paper, we
evaluate the performance of three high-performing convolutional neural networks
for retinal blood vessel segmentation in the context of blood vessel
segmentation on retinopathy of prematurity retinal images. The main motive
behind the study is to test if existing public datasets suffice to develop a
high-performing predictor that could assist an ophthalmologist in retinopathy
of prematurity diagnosis. To do so, we create a dataset consisting solely of
retinopathy of prematurity images with retinal blood vessel annotations
manually labeled by two observers, where one is the ophthalmologist experienced
in retinopathy of prematurity treatment. Experimental results show that all
three solutions have difficulties in detecting the retinal blood vessels of
infants due to a lower contrast compared to images from public datasets as
demonstrated by a significant drop in classification sensitivity. All three
solutions segment alongside retinal also choroidal blood vessels which are not
used to diagnose retinopathy of prematurity, but instead represent noise and
are confused with retinal blood vessels. By visual and numerical observations,
we observe that existing solutions for retinal blood vessel segmentation need
improvement toward more detailed datasets or deeper models in order to assist
the ophthalmologist in retinopathy of prematurity diagnosis.


------------------------------------------------------------------------------

Title:
Principles for Initialization and Architecture Selection in Graph Neural  Networks with ReLU Activations

Abstract: This article derives and validates three principles for initialization and
architecture selection in finite width graph neural networks (GNNs) with ReLU
activations. First, we theoretically derive what is essentially the unique
generalization to ReLU GNNs of the well-known He-initialization. Our
initialization scheme guarantees that the average scale of network outputs and
gradients remains order one at initialization. Second, we prove in finite width
vanilla ReLU GNNs that oversmoothing is unavoidable at large depth when using
fixed aggregation operator, regardless of initialization. We then prove that
using residual aggregation operators, obtained by interpolating a fixed
aggregation operator with the identity, provably alleviates oversmoothing at
initialization. Finally, we show that the common practice of using residual
connections with a fixup-type initialization provably avoids correlation
collapse in final layer features at initialization. Through ablation studies we
find that using the correct initialization, residual aggregation operators, and
residual connections in the forward pass significantly and reliably speeds up
early training dynamics in deep ReLU GNNs on a variety of tasks.


------------------------------------------------------------------------------

Title:
Graph Based Long-Term And Short-Term Interest Model for Click-Through  Rate Prediction

Abstract: Click-through rate (CTR) prediction aims to predict the probability that the
user will click an item, which has been one of the key tasks in online
recommender and advertising systems. In such systems, rich user behavior (viz.
long- and short-term) has been proved to be of great value in capturing user
interests. Both industry and academy have paid much attention to this topic and
propose different approaches to modeling with long-term and short-term user
behavior data. But there are still some unresolved issues. More specially, (1)
rule and truncation based methods to extract information from long-term
behavior are easy to cause information loss, and (2) single feedback behavior
regardless of scenario to extract information from short-term behavior lead to
information confusion and noise. To fill this gap, we propose a Graph based
Long-term and Short-term interest Model, termed GLSM. It consists of a
multi-interest graph structure for capturing long-term user behavior, a
multi-scenario heterogeneous sequence model for modeling short-term
information, then an adaptive fusion mechanism to fused information from
long-term and short-term behaviors. Comprehensive experiments on real-world
datasets, GLSM achieved SOTA score on offline metrics. At the same time, the
GLSM algorithm has been deployed in our industrial application, bringing 4.9%
CTR and 4.3% GMV lift, which is significant to the business.


------------------------------------------------------------------------------

Title:
Segment Anything Model (SAM) for Radiation Oncology

Abstract: In this study, we evaluate the performance of the Segment Anything Model
(SAM) model in clinical radiotherapy. We collected real clinical cases from
four regions at the Mayo Clinic: prostate, lung, gastrointestinal, and head \&
neck, which are typical treatment sites in radiation oncology. For each case,
we selected the OARs of concern in radiotherapy planning and compared the Dice
and Jaccard outcomes between clinical manual delineation, automatic
segmentation using SAM's "segment anything" mode, and automatic segmentation
using SAM with box prompt. Our results indicate that SAM performs better in
automatic segmentation for the prostate and lung regions, while its performance
in the gastrointestinal and head \& neck regions was relatively inferior. When
considering the size of the organ and the clarity of its boundary, SAM displays
better performance for larger organs with clear boundaries, such as the lung
and liver, and worse for smaller organs with unclear boundaries, like the
parotid and cochlea. These findings align with the generally accepted
variations in difficulty level associated with manual delineation of different
organs at different sites in clinical radiotherapy. Given that SAM, a single
trained model, could handle the delineation of OARs in four regions, these
results also demonstrate SAM's robust generalization capabilities in automatic
segmentation for radiotherapy, i.e., achieving delineation of different
radiotherapy OARs using a generic automatic segmentation model. SAM's
generalization capabilities across different regions make it technically
feasible to develop a generic model for automatic segmentation in radiotherapy.


------------------------------------------------------------------------------

Title:
Statistical Tests for Replacing Human Decision Makers with Algorithms

Abstract: This paper proposes a statistical framework with which artificial
intelligence can improve human decision making. The performance of each human
decision maker is first benchmarked against machine predictions; we then
replace the decisions made by a subset of the decision makers with the
recommendation from the proposed artificial intelligence algorithm. Using a
large nationwide dataset of pregnancy outcomes and doctor diagnoses from
prepregnancy checkups of reproductive age couples, we experimented with both a
heuristic frequentist approach and a Bayesian posterior loss function approach
with an application to abnormal birth detection. We find that our algorithm on
a test dataset results in a higher overall true positive rate and a lower false
positive rate than the diagnoses made by doctors only. We also find that the
diagnoses of doctors from rural areas are more frequently replaceable,
suggesting that artificial intelligence assisted decision making tends to
improve precision more in less developed regions.


------------------------------------------------------------------------------

Title:
Efficient Large-scale Nonstationary Spatial Covariance Function  Estimation Using Convolutional Neural Networks

Abstract: Spatial processes observed in various fields, such as climate and
environmental science, often occur on a large scale and demonstrate spatial
nonstationarity. Fitting a Gaussian process with a nonstationary Mat\'ern
covariance is challenging. Previous studies in the literature have tackled this
challenge by employing spatial partitioning techniques to estimate the
parameters that vary spatially in the covariance function. The selection of
partitions is an important consideration, but it is often subjective and lacks
a data-driven approach. To address this issue, in this study, we utilize the
power of Convolutional Neural Networks (ConvNets) to derive subregions from the
nonstationary data. We employ a selection mechanism to identify subregions that
exhibit similar behavior to stationary fields. In order to distinguish between
stationary and nonstationary random fields, we conducted training on ConvNet
using various simulated data. These simulations are generated from Gaussian
processes with Mat\'ern covariance models under a wide range of parameter
settings, ensuring adequate representation of both stationary and nonstationary
spatial data. We assess the performance of the proposed method with synthetic
and real datasets at a large scale. The results revealed enhanced accuracy in
parameter estimations when relying on ConvNet-based partition compared to
traditional user-defined approaches.


------------------------------------------------------------------------------

Title:
Beyond Normal: On the Evaluation of Mutual Information Estimators

Abstract: Mutual information is a general statistical dependency measure which has
found applications in representation learning, causality, domain generalization
and computational biology. However, mutual information estimators are typically
evaluated on simple families of probability distributions, namely multivariate
normal distribution and selected distributions with one-dimensional random
variables. In this paper, we show how to construct a diverse family of
distributions with known ground-truth mutual information and propose a
language-independent benchmarking platform for mutual information estimators.
We discuss the general applicability and limitations of classical and neural
estimators in settings involving high dimensions, sparse interactions,
long-tailed distributions, and high mutual information. Finally, we provide
guidelines for practitioners on how to select appropriate estimator adapted to
the difficulty of problem considered and issues one needs to consider when
applying an estimator to a new data set.


------------------------------------------------------------------------------

Title:
Stabilization and Spill-Free Transfer of Viscous Liquid in a Tank

Abstract: Flow control occupies a special place in the fields of partial differential
equations (PDEs) and control theory, where the complex behavior of solutions of
nonlinear dynamics in very high dimension is not just to be understood but also
to be assigned specific desired properties, by feedback control. Among several
benchmark problems in flow control, the liquid-tank problem is particularly
attractive as a research topic. In the liquid-tank problem the objective is to
move a tank filled with liquid, suppress the nonlinear oscillations of the
liquid in the process, bring the tank and liquid to rest, and avoid liquid
spillage in the process. In other words, this is a problem of nonlinear PDE
stabilization subject to state constraints.
This review article focuses only on recent results on liquid-tank
stabilization for viscous liquids. All possible cases are studied: with and
without friction from the tank walls, with and without surface tension.
Moreover, novel results are provided for the linearization of the tank-liquid
system. The linearization of the tank-liquid system gives a high-order PDE
which is a combination of a wave equation with Kelvin-Voigt damping and an
Euler-Bernoulli beam equation.
The feedback design methodology presented in the article is based on Control
Lyapunov Functionals (CLFs), suitably extended from the CLF methodology for
ODEs to the infinite-dimensional case. The CLFs proposed are modifications and
augmentations of the total energy functionals for the tank-liquid system, so
that the dissipative effects of viscosity, friction, and surface tension are
captured and additional dissipation by feedback is made relatively easy.
The article closes with an extensive list of open problems.


------------------------------------------------------------------------------

Title:
Hypergraph Classification via Persistent Homology

Abstract: Persistent homology is a mathematical tool used for studying the shape of
data by extracting its topological features. It has gained popularity in
network science due to its applicability in various network mining problems,
including clustering, graph classification, and graph neural networks. The
definition of persistent homology for graphs is relatively straightforward, as
graphs possess distinct intrinsic distances and a simplicial complex structure.
However, hypergraphs present a challenge in preserving topological information
since they may not have a simplicial complex structure. In this paper, we
define several topological characterizations of hypergraphs in defining
hypergraph persistent homology to prioritize different higher-order structures
within hypergraphs. We further use these persistent homology filtrations in
classifying four different real-world hypergraphs and compare their performance
to the state-of-the-art graph neural network models. Experimental results
demonstrate that persistent homology filtrations are effective in classifying
hypergraphs and outperform the baseline models. To the best of our knowledge,
this study represents the first systematic attempt to tackle the hypergraph
classification problem using persistent homology.


------------------------------------------------------------------------------

Title:
PINQI: An End-to-End Physics-Informed Approach to Learned Quantitative  MRI Reconstruction

Abstract: Quantitative Magnetic Resonance Imaging (qMRI) enables the reproducible
measurement of biophysical parameters in tissue. The challenge lies in solving
a nonlinear, ill-posed inverse problem to obtain the desired tissue parameter
maps from acquired raw data. While various learned and non-learned approaches
have been proposed, the existing learned methods fail to fully exploit the
prior knowledge about the underlying MR physics, i.e. the signal model and the
acquisition model. In this paper, we propose PINQI, a novel qMRI reconstruction
method that integrates the knowledge about the signal, acquisition model, and
learned regularization into a single end-to-end trainable neural network. Our
approach is based on unrolled alternating optimization, utilizing
differentiable optimization blocks to solve inner linear and non-linear
optimization tasks, as well as convolutional layers for regularization of the
intermediate qualitative images and parameter maps. This design enables PINQI
to leverage the advantages of both the signal model and learned regularization.
We evaluate the performance of our proposed network by comparing it with
recently published approaches in the context of highly undersampled
$T_1$-mapping, using both a simulated brain dataset, as well as real scanner
data acquired from a physical phantom and in-vivo data from healthy volunteers.
The results demonstrate the superiority of our proposed solution over existing
methods and highlight the effectiveness of our method in real-world scenarios.


------------------------------------------------------------------------------

Title:
Prior-knowledge-informed deep learning for lacune detection and  quantification using multi-site brain MRI

Abstract: Lacunes of presumed vascular origin, also referred to as lacunar infarcts,
are important to assess cerebral small vessel disease and cognitive diseases
such as dementia. However, visual rating of lacunes from imaging data is
challenging, time-consuming, and rater-dependent, owing to their small size,
sparsity, and mimics. Whereas recent developments in automatic algorithms have
shown to make the detection of lacunes faster while preserving sensitivity,
they also showed a large number of false positives, which makes them
impractical for use in clinical practice or large-scale studies. Here, we
develop a novel framework that, in addition to lacune detection, outputs a
categorical burden score. This score could provide a more practical estimate of
lacune presence that simplifies and effectively accelerates the imaging
assessment of lacunes. We hypothesize that the combination of detection and the
categorical score makes the procedure less sensitive to noisy labels.


------------------------------------------------------------------------------

Title:
Polynomial approximation on disjoint segments and amplification of  approximation

Abstract: We construct explicit easily implementable polynomial approximations of
sufficiently high accuracy for locally constant functions on the union of
disjoint segments. This problem has important applications in several areas of
numerical analysis, complexity theory, quantum algorithms, etc. The one, most
relevant for us, is the amplification of approximation method: it allows to
construct approximations of higher degree $M$ and better accuracy from the
approximations of degree $m$.


------------------------------------------------------------------------------

Title:
Harnessing the Power of Adversarial Prompting and Large Language Models  for Robust Hypothesis Generation in Astronomy

Abstract: This study investigates the application of Large Language Models (LLMs),
specifically GPT-4, within Astronomy. We employ in-context prompting, supplying
the model with up to 1000 papers from the NASA Astrophysics Data System, to
explore the extent to which performance can be improved by immersing the model
in domain-specific literature. Our findings point towards a substantial boost
in hypothesis generation when using in-context prompting, a benefit that is
further accentuated by adversarial prompting. We illustrate how adversarial
prompting empowers GPT-4 to extract essential details from a vast knowledge
base to produce meaningful hypotheses, signaling an innovative step towards
employing LLMs for scientific research in Astronomy.


------------------------------------------------------------------------------

Title:
Graph-Based Conditions for Feedback Stabilization of Switched and LPV  Systems

Abstract: This paper presents novel stabilizability conditions for switched linear
systems with arbitrary and uncontrollable underlying switching signals. We
distinguish and study two particular settings: i) the \emph{robust} case, in
which the active mode is completely unknown and unobservable, and ii) the
\emph{mode-dependent} case, in which the controller depends on the current
active switching mode. The technical developments are based on graph-theory
tools, relying in particular on the path-complete Lyapunov functions framework.
The main idea is to use directed and labeled graphs to encode Lyapunov
inequalities to design robust and mode-dependent piecewise linear
state-feedback controllers. This results in novel and flexible conditions, with
the particular feature of being in the form of linear matrix inequalities
(LMIs). Our technique thus provides a first controller-design strategy allowing
piecewise linear feedback maps and piecewise quadratic (control) Lyapunov
functions by means of semidefinite programming. Numerical examples illustrate
the application of the proposed techniques, the relations between the graph
order, the robustness, and the performance of the closed loop.


------------------------------------------------------------------------------

Title:
Coevolution of cognition and cooperation in structured populations under  reinforcement learning

Abstract: We study the evolution of behavior under reinforcement learning in a
Prisoner's Dilemma where agents interact in a regular network and can learn
about whether they play one-shot or repeatedly by incurring a cost of
deliberation. With respect to other behavioral rules used in the literature,
(i) we confirm the existence of a threshold value of the probability of
repeated interaction, switching the emergent behavior from intuitive defector
to dual-process cooperator; (ii) we find a different role of the node degree,
with smaller degrees reducing the evolutionary success of dual-process
cooperators; (iii) we observe a higher frequency of deliberation.


------------------------------------------------------------------------------

Title:
Integrated photonics modular arithmetic processor

Abstract: Integrated photonics computing has emerged as a promising approach to
overcome the limitations in computing power of electronic processors in the
post-Moore era by leveraging the superiority of photonic systems in the field
of information processing, just as how optical communication disrupted
traditional electrical communication. However, current integrated photonics
computing systems struggle to achieve reliable and high-precision calculations,
failing to fulfil the general requirements of computational tasks. Moreover,
they heavily rely on the electronics AD/DA conversion interfaces, which
significantly compromises their performance. Here, we propose a novel photonic
computing architecture based on optical phase and modular arithmetic with
scalable precision. And we experimentally demonstrated the computation accuracy
that was previously unattainable with existing photonics computing methods.
Furthermore, this architecture seamlessly integrates our proposed optical DAC
and optical quantizer, marking the first breakthrough in freeing photonic
computing from the constraints of electronic AD/DA converters.


------------------------------------------------------------------------------

Title:
Hearing Lips in Noise: Universal Viseme-Phoneme Mapping and Transfer for  Robust Audio-Visual Speech Recognition

Abstract: Audio-visual speech recognition (AVSR) provides a promising solution to
ameliorate the noise-robustness of audio-only speech recognition with visual
information. However, most existing efforts still focus on audio modality to
improve robustness considering its dominance in AVSR task, with noise
adaptation techniques such as front-end denoise processing. Though effective,
these methods are usually faced with two practical challenges: 1) lack of
sufficient labeled noisy audio-visual training data in some real-world
scenarios and 2) less optimal model generality to unseen testing noises. In
this work, we investigate the noise-invariant visual modality to strengthen
robustness of AVSR, which can adapt to any testing noises while without
dependence on noisy training data, a.k.a., unsupervised noise adaptation.
Inspired by human perception mechanism, we propose a universal viseme-phoneme
mapping (UniVPM) approach to implement modality transfer, which can restore
clean audio from visual signals to enable speech recognition under any noisy
conditions. Extensive experiments on public benchmarks LRS3 and LRS2 show that
our approach achieves the state-of-the-art under various noisy as well as clean
conditions. In addition, we also outperform previous state-of-the-arts on
visual speech recognition task.


------------------------------------------------------------------------------

Title:
Beyond Geometry: Comparing the Temporal Structure of Computation in  Neural Circuits with Dynamical Similarity Analysis

Abstract: How can we tell whether two neural networks are utilizing the same internal
processes for a particular computation? This question is pertinent for multiple
subfields of both neuroscience and machine learning, including neuroAI,
mechanistic interpretability, and brain-machine interfaces. Standard approaches
for comparing neural networks focus on the spatial geometry of latent states.
Yet in recurrent networks, computations are implemented at the level of neural
dynamics, which do not have a simple one-to-one mapping with geometry. To
bridge this gap, we introduce a novel similarity metric that compares two
systems at the level of their dynamics. Our method incorporates two components:
Using recent advances in data-driven dynamical systems theory, we learn a
high-dimensional linear system that accurately captures core features of the
original nonlinear dynamics. Next, we compare these linear approximations via a
novel extension of Procrustes Analysis that accounts for how vector fields
change under orthogonal transformation. Via four case studies, we demonstrate
that our method effectively identifies and distinguishes dynamic structure in
recurrent neural networks (RNNs), whereas geometric methods fall short. We
additionally show that our method can distinguish learning rules in an
unsupervised manner. Our method therefore opens the door to novel data-driven
analyses of the temporal structure of neural computation, and to more rigorous
testing of RNNs as models of the brain.


------------------------------------------------------------------------------

Title:
Convergence and concentration properties of constant step-size SGD  through Markov chains

Abstract: We consider the optimization of a smooth and strongly convex objective using
constant step-size stochastic gradient descent (SGD) and study its properties
through the prism of Markov chains. We show that, for unbiased gradient
estimates with mildly controlled variance, the iteration converges to an
invariant distribution in total variation distance. We also establish this
convergence in Wasserstein-2 distance under a relaxed assumption on the
gradient noise distribution compared to previous work. Thanks to the invariance
property of the limit distribution, our analysis shows that the latter inherits
sub-Gaussian or sub-exponential concentration properties when these hold true
for the gradient. This allows the derivation of high-confidence bounds for the
final estimate. Finally, under such conditions in the linear case, we obtain a
dimension-free deviation bound for the Polyak-Ruppert average of a tail
sequence. All our results are non-asymptotic and their consequences are
discussed through a few applications.


------------------------------------------------------------------------------

Title:
Generalized FDTD Scheme for Moving Electromagnetic Structures with  Arbitrary Space-Time Configurations

Abstract: We present a generalized FDTD scheme to simulate moving electromagnetic
structures with arbitrary space-time configurations. This scheme is a local
adaptation and 2+1-dimensional extension of the uniform and 1+1-dimensional
scheme recently reported in [1]. The local adaptation, which is allowed by the
inherently matched nature of the generalized Yee cell to the conventional Yee
cell, extends the range of applicability of the scheme in [1] to moving
structures that involve multiple and arbitrary velocity profiles while being
fully compatible with conventional absorbing boundary conditions and standard
treatments of medium dispersion. We show that a direct application of the
conventional FDTD scheme predicts qualitatively correct spectral transitions
but quantitatively erroneous scattering amplitudes, we infer from this
observation generalized, hybrid - physical and auxiliary (non-physical) -
fields that automatically satisfy moving boundary conditions in the laboratory
frame, and accordingly establish local update equations based on the related
Maxwell's equations and constitutive relations. We finally validate and
illustrate the proposed method by three canonical examples - a space-time
interface, a space-time wedge and a space-time accelerated interface - whose
combination represent arbitrary space-time configurations. The proposed scheme
fills an important gap in the open literature on computational electromagnetics
and offers an unprecedented, direct solution for moving structures in
commercial software platforms.


------------------------------------------------------------------------------

Title:
Online Dynamic Submodular Optimization

Abstract: We propose new algorithms with provable performance for online binary
optimization subject to general constraints and in dynamic settings. We
consider the subset of problems in which the objective function is submodular.
We propose the online submodular greedy algorithm (OSGA) which solves to
optimality an approximation of the previous round's loss function to avoid the
NP-hardness of the original problem. We extend OSGA to a generic approximation
function. We show that OSGA has a dynamic regret bound similar to the tightest
bounds in online convex optimization. For instances where no approximation
exists or a computationally simpler implementation is desired, we design the
online submodular projected gradient descent (OSPGD) by leveraging the Lov\'asz
extension. We obtain a regret bound that is akin to the conventional online
gradient descent (OGD). Finally, we numerically test our algorithms in two
power system applications: fast-timescale demand response and real-time
distribution network reconfiguration.


------------------------------------------------------------------------------

Title:
To Fold or Not to Fold: Graph Regularized Tensor Train for Visual Data  Completion

Abstract: Tensor train (TT) representation has achieved tremendous success in visual
data completion tasks, especially when it is combined with tensor folding.
However, folding an image or video tensor breaks the original data structure,
leading to local information loss as nearby pixels may be assigned into
different dimensions and become far away from each other. In this paper, to
fully preserve the local information of the original visual data, we explore
not folding the data tensor, and at the same time adopt graph information to
regularize local similarity between nearby entries. To overcome the high
computational complexity introduced by the graph-based regularization in the TT
completion problem, we propose to break the original problem into multiple
sub-problems with respect to each TT core fiber, instead of each TT core as in
traditional methods. Furthermore, to avoid heavy parameter tuning, a sparsity
promoting probabilistic model is built based on the generalized inverse
Gaussian (GIG) prior, and an inference algorithm is derived under the
mean-field approximation. Experiments on both synthetic data and real-world
visual data show the superiority of the proposed methods.


------------------------------------------------------------------------------

Title:
The News Delivery Channel Recommendation Based on Granular Neural  Network

Abstract: With the continuous maturation and expansion of neural network technology,
deep neural networks have been widely utilized as the fundamental building
blocks of deep learning in a variety of applications, including speech
recognition, machine translation, image processing, and the creation of
recommendation systems. Therefore, many real-world complex problems can be
solved by the deep learning techniques. As is known, traditional news
recommendation systems mostly employ techniques based on collaborative
filtering and deep learning, but the performance of these algorithms is
constrained by the sparsity of the data and the scalability of the approaches.
In this paper, we propose a recommendation model using granular neural network
model to recommend news to appropriate channels by analyzing the properties of
news. Specifically, a specified neural network serves as the foundation for the
granular neural network that the model is considered to be build. Different
information granularities are attributed to various types of news material, and
different information granularities are released between networks in various
ways. When processing data, granular output is created, which is compared to
the interval values pre-set on various platforms and used to quantify the
analysis's effectiveness. The analysis results could help the media to match
the proper news in depth, maximize the public attention of the news and the
utilization of media resources.


------------------------------------------------------------------------------

Title:
Learning-based sound speed reconstruction and aberration correction in  linear-array photoacoustic/ultrasound imaging

Abstract: Photoacoustic (PA) image reconstruction involves acoustic inversion that
necessitates the specification of the speed of sound (SoS) within the medium of
propagation. Due to the lack of information on the spatial distribution of the
SoS within heterogeneous soft tissue, a homogeneous SoS distribution (such as
1540 m/s) is typically assumed in PA image reconstruction, similar to that of
ultrasound (US) imaging. Failure to compensate the SoS variations leads to
aberration artefacts, deteriorating the image quality. In this work, we
developed a deep learning framework for SoS reconstruction and subsequent
aberration correction in a dual-modal PA/US imaging system sharing a clinical
US probe. As the PA and US data were inherently co-registered, the
reconstructed SoS distribution from US channel data using deep neural networks
was utilised for accurate PA image reconstruction. On a numerical and a
tissue-mimicking phantom, this framework was able to significantly suppress US
aberration artefacts, with the structural similarity index measure (SSIM) of up
to 0.8109 and 0.8128 as compared to the conventional approach (0.6096 and
0.5985, respectively). The networks, trained only on simulated US data, also
demonstrated a good generalisation ability on data from ex vivo tissues and the
wrist and fingers of healthy human volunteers, and thus could be valuable in
various in vivo applications to enhance PA image reconstruction.


------------------------------------------------------------------------------

Title:
Implicit neural representation with physics-informed neural networks for  the reconstruction of the early part of room impulse responses

Abstract: Recently deep learning and machine learning approaches have been widely
employed for various applications in acoustics. Nonetheless, in the area of
sound field processing and reconstruction classic methods based on the
solutions of wave equation are still widespread. Recently, physics-informed
neural networks have been proposed as a deep learning paradigm for solving
partial differential equations which govern physical phenomena, bridging the
gap between purely data-driven and model based methods. Here, we exploit
physics-informed neural networks to reconstruct the early part of missing room
impulse responses in an uniform linear array. This methodology allows us to
exploit the underlying law of acoustics, i.e., the wave equation, forcing the
neural network to generate physically meaningful solutions given only a limited
number of data points. The results on real measurements show that the proposed
model achieves accurate reconstruction and performance in line with respect to
state-of-the-art deep-learning and compress sensing techniques while
maintaining a lightweight architecture.


------------------------------------------------------------------------------

Title:
Top-down machine learning of coarse-grained protein force-fields

Abstract: Developing accurate and efficient coarse-grained representations of proteins
is crucial for understanding their folding, function, and interactions over
extended timescales. Our methodology involves simulating proteins with
molecular dynamics and utilizing the resulting trajectories to train a neural
network potential through differentiable trajectory reweighting. Remarkably,
this method requires only the native conformation of proteins, eliminating the
need for labeled data derived from extensive simulations or memory-intensive
end-to-end differentiable simulations. Once trained, the model can be employed
to run parallel molecular dynamics simulations and sample folding events for
proteins both within and beyond the training distribution, showcasing its
extrapolation capabilities. By applying Markov State Models, native-like
conformations of the simulated proteins can be predicted from the
coarse-grained simulations. Owing to its theoretical transferability and
ability to use solely experimental static structures as training data, we
anticipate that this approach will prove advantageous for developing new
protein force fields and further advancing the study of protein dynamics,
folding, and interactions.


------------------------------------------------------------------------------

Title:
Human Limits in Machine Learning: Prediction of Plant Phenotypes Using  Soil Microbiome Data

Abstract: The preservation of soil health has been identified as one of the main
challenges of the XXI century given its vast (and potentially threatening)
ramifications in agriculture, human health and biodiversity. Here, we provide
the first deep investigation of the predictive potential of machine-learning
models to understand the connections between soil and biological phenotypes.
Indeed, we investigate an integrative framework performing accurate
machine-learning-based prediction of plant phenotypes from biological, chemical
and physical properties of the soil via two models: random forest and Bayesian
neural network. We show that prediction is improved, as evidenced by higher
weighted F1 scores, when incorporating into the models environmental features
like soil physicochemical properties and microbial population density in
addition to the microbiome information. Furthermore, by exploring multiple data
preprocessing strategies such as normalization, zero replacement, and data
augmentation, we confirm that human decisions have a huge impact on the
predictive performance. In particular, we show that the naive total sum scaling
normalization that is commonly used in microbiome research is not the optimal
strategy to maximize predictive power. In addition, we find that accurately
defined labels are more important than normalization, taxonomic level or model
characteristics. That is, if humans are unable to classify the samples and
provide accurate labels, the performance of machine-learning models will be
limited. Lastly, we present strategies for domain scientists via a full model
selection decision tree to identify the human choices that maximize the
prediction power of the models. Our work is accompanied by open source
reproducible scripts (this https URL) for
maximum outreach among the microbiome research community.


------------------------------------------------------------------------------

Title:
Globally optimal solutions to a class of fractional optimization  problems based on proximity gradient algorithm

Abstract: We establish globally optimal solutions to a class of fractional optimization
problems on a class of constraint sets, whose key characteristics are as
follows: 1) The numerator and the denominator of the objective function are
both convex, semi-algebraic, Lipschitz continuous and differentiable with
Lipschitz continuous gradients on the constraint set. 2) The constraint set is
closed, convex and semi-algebraic. Compared with Dinkelbach's approach, our
novelty falls into the following aspects: 1) Dinkelbach's has to solve a
concave maximization problem in each iteration, which is nontrivial to obtain a
solution, while ours only needs to conduct one proximity gradient operation in
each iteration. 2) Dinkelbach's requires at least one nonnegative point for the
numerator to proceed the algorithm, but ours does not, which is available to a
much wider class of situations. 3) Dinkelbach's requires a closed and bounded
constraint set, while ours only needs the closedness but not necessarily the
boundedness. Therefore, our approach is viable for many more practical models,
like optimizing the Sharpe ratio (SR) or the Information ratio in mathematical
finance. Numerical experiments show that our approach achieves the ground-truth
solutions in two simple examples. For real-world financial data, it outperforms
several existing approaches for SR maximization.


------------------------------------------------------------------------------

Title:
CAMP-Net: Context-Aware Multi-Prior Network for Accelerated MRI  Reconstruction

Abstract: Despite promising advances in deep learning-based MRI reconstruction methods,
restoring high-frequency image details and textures remains a challenging
problem for accelerated MRI. To tackle this challenge, we propose a novel
context-aware multi-prior network (CAMP-Net) for MRI reconstruction. CAMP-Net
leverages the complementary nature of multiple prior knowledge and explores
data redundancy between adjacent slices in the hybrid domain to improve image
quality. It incorporates three interleaved modules respectively for image
enhancement, k-space restoration, and calibration consistency to jointly learn
context-aware multiple priors in an end-to-end fashion. The image enhancement
module learns a coil-combined image prior to suppress noise-like artifacts,
while the k-space restoration module explores multi-coil k-space correlations
to recover high-frequency details. The calibration consistency module embeds
the known physical properties of MRI acquisition to ensure consistency of
k-space correlations extracted from measurements and the artifact-free image
intermediate. The resulting low- and high-frequency reconstructions are
hierarchically aggregated in a frequency fusion module and iteratively refined
to progressively reconstruct the final image. We evaluated the generalizability
and robustness of our method on three large public datasets with various
accelerations and sampling patterns. Comprehensive experiments demonstrate that
CAMP-Net outperforms state-of-the-art methods in terms of reconstruction
quality and quantitative $T_2$ mapping.


------------------------------------------------------------------------------

Title:
Compression and Reduced Representation Techniques for Patch-Based  Relaxation

Abstract: Patch-based relaxation refers to a family of methods for solving linear
systems which partitions the matrix into smaller pieces often corresponding to
groups of adjacent degrees of freedom residing within patches of the
computational domain. The two most common families of patch-based methods are
block-Jacobi and Schwarz methods, where the former typically corresponds to
non-overlapping domains and the later implies some overlap. We focus on cases
where each patch consists of the degrees of freedom within a finite element
method mesh cell. Patch methods often capture complex local physics much more
effectively than simpler point-smoothers such as Jacobi; however, forming,
inverting, and applying each patch can be prohibitively expensive in terms of
both storage and computation time. To this end, we propose several approaches
for performing analysis on these patches and constructing a reduced
representation. The compression techniques rely on either matrix norm
comparisons or unsupervised learning via a clustering approach. We illustrate
how it is frequently possible to retain/factor less than 5% of all patches and
still develop a method that converges with the same number of iterations or
slightly more than when all patches are stored/factored.


------------------------------------------------------------------------------

Title:
Regularized Robust MDPs and Risk-Sensitive MDPs: Equivalence, Policy  Gradient, and Sample Complexity

Abstract: This paper focuses on reinforcement learning for the regularized robust
Markov decision process (MDP) problem, an extension of the robust MDP
framework. We first introduce the risk-sensitive MDP and establish the
equivalence between risk-sensitive MDP and regularized robust MDP. This
equivalence offers an alternative perspective for addressing the regularized
RMDP and enables the design of efficient learning algorithms. Given this
equivalence, we further derive the policy gradient theorem for the regularized
robust MDP problem and prove the global convergence of the exact policy
gradient method under the tabular setting with direct parameterization. We also
propose a sample-based offline learning algorithm, namely the robust fitted-Z
iteration (RFZI), for a specific regularized robust MDP problem with a
KL-divergence regularization term and analyze the sample complexity of the
algorithm. Our results are also supported by numerical simulations.


------------------------------------------------------------------------------

Title:
Deep Learning Framework with Multi-Head Dilated Encoders for Enhanced  Segmentation of Cervical Cancer on Multiparametric Magnetic Resonance Imaging

Abstract: T2-weighted magnetic resonance imaging (MRI) and diffusion-weighted imaging
(DWI) are essential components for cervical cancer diagnosis. However,
combining these channels for training deep learning models are challenging due
to misalignment of images. Here, we propose a novel multi-head framework that
uses dilated convolutions and shared residual connections for separate encoding
of multiparametric MRI images. We employ a residual U-Net model as a baseline,
and perform a series of architectural experiments to evaluate the tumor
segmentation performance based on multiparametric input channels and feature
encoding configurations. All experiments were performed using a cohort
including 207 patients with locally advanced cervical cancer. Our proposed
multi-head model using separate dilated encoding for T2W MRI, and combined
b1000 DWI and apparent diffusion coefficient (ADC) images achieved the best
median Dice coefficient similarity (DSC) score, 0.823 (95% confidence interval
(CI), 0.595-0.797), outperforming the conventional multi-channel model, DSC
0.788 (95% CI, 0.568-0.776), although the difference was not statistically
significant (p>0.05). We investigated channel sensitivity using 3D GRAD-CAM and
channel dropout, and highlighted the critical importance of T2W and ADC
channels for accurate tumor segmentations. However, our results showed that
b1000 DWI had a minor impact on overall segmentation performance. We
demonstrated that the use of separate dilated feature extractors and
independent contextual learning improved the model's ability to reduce the
boundary effects and distortion of DWI, leading to improved segmentation
performance. Our findings can have significant implications for the development
of robust and generalizable models that can extend to other multi-modal
segmentation applications.


------------------------------------------------------------------------------

Title:
MIR-GAN: Refining Frame-Level Modality-Invariant Representations with  Adversarial Network for Audio-Visual Speech Recognition

Abstract: Audio-visual speech recognition (AVSR) attracts a surge of research interest
recently by leveraging multimodal signals to understand human speech.
Mainstream approaches addressing this task have developed sophisticated
architectures and techniques for multi-modality fusion and representation
learning. However, the natural heterogeneity of different modalities causes
distribution gap between their representations, making it challenging to fuse
them. In this paper, we aim to learn the shared representations across
modalities to bridge their gap. Different from existing similar methods on
other multimodal tasks like sentiment analysis, we focus on the temporal
contextual dependencies considering the sequence-to-sequence task setting of
AVSR. In particular, we propose an adversarial network to refine frame-level
modality-invariant representations (MIR-GAN), which captures the commonality
across modalities to ease the subsequent multimodal fusion process. Extensive
experiments on public benchmarks LRS3 and LRS2 show that our approach
outperforms the state-of-the-arts.


------------------------------------------------------------------------------

Title:
Application of Deep Learning for Predictive Maintenance of Oilfield  Equipment

Abstract: This thesis explored applications of the new emerging techniques of
artificial intelligence and deep learning (neural networks in particular) for
predictive maintenance, diagnostics and prognostics. Many neural architectures
such as fully-connected, convolutional and recurrent neural networks were
developed and tested on public datasets such as NASA C-MAPSS, Case Western
Reserve University Bearings and FEMTO Bearings datasets to diagnose equipment
health state and/or predict the remaining useful life (RUL) before breakdown.
Many data processing and feature extraction procedures were used in combination
with deep learning techniques such as dimensionality reduction (Principal
Component Analysis) and signal processing (Fourier and Wavelet analyses) in
order to create more meaningful and robust features to use as an input for
neural networks architectures. This thesis also explored the potential use of
these techniques in predictive maintenance within oil rigs for monitoring
oilfield critical equipment in order to reduce unpredicted downtime and
maintenance costs.


------------------------------------------------------------------------------

Title:
Deep graph kernel point processes

Abstract: Point process models are widely used to analyze asynchronous events occurring
within a graph that reflect how different types of events influence one
another. Predicting future events' times and types is a crucial task, and the
size and topology of the graph add to the challenge of the problem. Recent
neural point process models unveil the possibility of capturing intricate
inter-event-category dependencies. However, such methods utilize an unfiltered
history of events, including all event categories in the intensity computation
for each target event type. In this work, we propose a graph point process
method where event interactions occur based on a latent graph topology. The
corresponding undirected graph has nodes representing event categories and
edges indicating potential contribution relationships. We then develop a novel
deep graph kernel to characterize the triggering and inhibiting effects between
events. The intrinsic influence structures are incorporated via the graph
neural network (GNN) model used to represent the learnable kernel. The
computational efficiency of the GNN approach allows our model to scale to large
graphs. Comprehensive experiments on synthetic and real-world data show the
superior performance of our approach against the state-of-the-art methods in
predicting future events and uncovering the relational structure among data.


------------------------------------------------------------------------------

Title:
Effect-Invariant Mechanisms for Policy Generalization

Abstract: Policy learning is an important component of many real-world learning
systems. A major challenge in policy learning is how to adapt efficiently to
unseen environments or tasks. Recently, it has been suggested to exploit
invariant conditional distributions to learn models that generalize better to
unseen environments. However, assuming invariance of entire conditional
distributions (which we call full invariance) may be too strong of an
assumption in practice. In this paper, we introduce a relaxation of full
invariance called effect-invariance (e-invariance for short) and prove that it
is sufficient, under suitable assumptions, for zero-shot policy generalization.
We also discuss an extension that exploits e-invariance when we have a small
sample from the test environment, enabling few-shot policy generalization. Our
work does not assume an underlying causal graph or that the data are generated
by a structural causal model; instead, we develop testing procedures to test
e-invariance directly from data. We present empirical results using simulated
data and a mobile health intervention dataset to demonstrate the effectiveness
of our approach.


------------------------------------------------------------------------------

Title:
Convergent spectral inclusion sets for banded matrices

Abstract: We obtain sequences of inclusion sets for the spectrum, essential spectrum,
and pseudospectrum of banded, in general non-normal, matrices of finite or
infinite size. Each inclusion set is the union of the pseudospectra of certain
submatrices of a chosen size $n$. Via the choice of $n$, one can balance
accuracy of approximation against computational cost, and we show, in the case
of infinite matrices, convergence as $n\to\infty$ of the respective inclusion
set to the corresponding spectral set.


------------------------------------------------------------------------------

Title:
Mean-field Analysis of Generalization Errors

Abstract: We propose a novel framework for exploring weak and $L_2$ generalization
errors of algorithms through the lens of differential calculus on the space of
probability measures. Specifically, we consider the KL-regularized empirical
risk minimization problem and establish generic conditions under which the
generalization error convergence rate, when training on a sample of size $n$,
is $\mathcal{O}(1/n)$. In the context of supervised learning with a one-hidden
layer neural network in the mean-field regime, these conditions are reflected
in suitable integrability and regularity assumptions on the loss and activation
functions.


------------------------------------------------------------------------------

Title:
ProMIL: Probabilistic Multiple Instance Learning for Medical Imaging

Abstract: Multiple Instance Learning (MIL) is a weakly-supervised problem in which one
label is assigned to the whole bag of instances. An important class of MIL
models is instance-based, where we first classify instances and then aggregate
those predictions to obtain a bag label. The most common MIL model is when we
consider a bag as positive if at least one of its instances has a positive
label. However, this reasoning does not hold in many real-life scenarios, where
the positive bag label is often a consequence of a certain percentage of
positive instances. To address this issue, we introduce a dedicated
instance-based method called ProMIL, based on deep neural networks and
Bernstein polynomial estimation. An important advantage of ProMIL is that it
can automatically detect the optimal percentage level for decision-making. We
show that ProMIL outperforms standard instance-based MIL in real-world medical
applications. We make the code available.


------------------------------------------------------------------------------

Title:
Meta-Analysis of Transfer Learning for Segmentation of Brain Lesions

Abstract: A major challenge in stroke research and stroke recovery predictions is the
determination of a stroke lesion's extent and its impact on relevant brain
systems. Manual segmentation of stroke lesions from 3D magnetic resonance (MR)
imaging volumes, the current gold standard, is not only very time-consuming,
but its accuracy highly depends on the operator's experience. As a result,
there is a need for a fully automated segmentation method that can efficiently
and objectively measure lesion extent and the impact of each lesion to predict
impairment and recovery potential which might be beneficial for clinical,
translational, and research settings. We have implemented and tested a fully
automatic method for stroke lesion segmentation which was developed using eight
different 2D-model architectures trained via transfer learning (TL) and mixed
data approaches. Additionally, the final prediction was made using a novel
ensemble method involving stacking and agreement window. Our novel method was
evaluated in a novel in-house dataset containing 22 T1w brain MR images, which
were challenging in various perspectives, but mostly because they included T1w
MR images from the subacute (which typically less well defined T1 lesions) and
chronic stroke phase (which typically means well defined T1-lesions).
Cross-validation results indicate that our new method can efficiently and
automatically segment lesions fast and with high accuracy compared to ground
truth. In addition to segmentation, we provide lesion volume and weighted
lesion load of relevant brain systems based on the lesions' overlap with a
canonical structural motor system that stretches from the cortical motor region
to the lowest end of the brain stem.


------------------------------------------------------------------------------

Title:
LM-VC: Zero-shot Voice Conversion via Speech Generation based on  Language Models

Abstract: Language model (LM) based audio generation frameworks, e.g., AudioLM, have
recently achieved new state-of-the-art performance in zero-shot audio
generation. In this paper, we explore the feasibility of LMs for zero-shot
voice conversion. An intuitive approach is to follow AudioLM - Tokenizing
speech into semantic and acoustic tokens respectively by HuBERT and
SoundStream, and converting source semantic tokens to target acoustic tokens
conditioned on acoustic tokens of the target speaker. However, such an approach
encounters several issues: 1) the linguistic content contained in semantic
tokens may get dispersed during multi-layer modeling while the lengthy speech
input in the voice conversion task makes contextual learning even harder; 2)
the semantic tokens still contain speaker-related information, which may be
leaked to the target speech, lowering the target speaker similarity; 3) the
generation diversity in the sampling of the LM can lead to unexpected outcomes
during inference, leading to unnatural pronunciation and speech quality
degradation. To mitigate these problems, we propose LM-VC, a two-stage language
modeling approach that generates coarse acoustic tokens for recovering the
source linguistic content and target speaker's timbre, and then reconstructs
the fine for acoustic details as converted speech. Specifically, to enhance
content preservation and facilitates better disentanglement, a masked prefix LM
with a mask prediction strategy is used for coarse acoustic modeling. This
model is encouraged to recover the masked content from the surrounding context
and generate target speech based on the target speaker's utterance and
corrupted semantic tokens. Besides, to further alleviate the sampling error in
the generation, an external LM, which employs window attention to capture the
local acoustic relations, is introduced to participate in the coarse acoustic
modeling.


------------------------------------------------------------------------------

Title:
Structural Gender Imbalances in Ballet Collaboration Networks

Abstract: Ballet, a mainstream performing art predominantly associated with women,
exhibits significant gender imbalances in leading positions. However, the
collaboration's structural composition on gender representation in the field
remains unexplored. Our study investigates the gendered labor force composition
and collaboration patterns in ballet creations. Our findings reveal gender
disparities in ballet creations aligned with gendered collaboration patterns
and women occupying more peripheral network positions respect to men.
Productivity disparities show women accessing 20-25\% of ballet creations
compared to men. Mathematically derived perception errors show the
underestimation of women artists' representation within ballet collaboration
networks, potentially impacting women's careers in the field. Our study
highlights the structural disadvantages that women face in ballet and
emphasizes the need for a more inclusive and equal professional environment to
improve the career development of women in the ballet industry. These insights
contribute to a broader understanding of structural gender imbalances in
artistic domains and can inform cultural organizations about potential
affirmative actions towards a better representation of women leaders in ballet.


------------------------------------------------------------------------------

Title:
Deep Learning-based Auto-encoder for Time-offset Faster-than-Nyquist  Downlink NOMA with Timing Errors and Imperfect CSI

Abstract: In this paper, we examine the encoding and decoding of transmitted sequences
for downlink time-offset with faster than Nyquist signaling NOMA (T-NOMA). As a
baseline, we use the singular value decomposition (SVD)-based scheme proposed
in previous studies for encoding and decoding. Even though this SVD-based
scheme provides reliable communication, its time complexity increases
quadratically with the sequence length. We propose a convolutional neural
network (CNN) auto-encoder (AE) for encoding and decoding with linear time
complexity. We explain the design of the encoder and decoder architectures and
the training criteria. By examining several variants of the CNN AE, we show
that it can achieve an excellent trade-off between performance and complexity.
A proposed CNN AE outperforms the SVD method using a lower implementation
complexity by approximately 2 dB in a T-NOMA system with two users assuming no
timing offset errors or channel state information estimation errors. In the
presence of channel state information (CSI) error variance of 1$\%$ and uniform
timing error at $\pm$4\% of the symbol interval, the proposed CNN AE provides
up to 10 dB SNR gain over the SVD method. We also propose a novel modified
training objective function consisting of a linear combination of the
traditionally used cross-entropy (CE) loss function and a closed-form
expression for the bit error rate (BER) called the Q-loss function. Simulations
show that the modified loss function achieves SNR gains of up to 1 dB over the
CE loss function alone. Finally, we investigate several novel CNN architectures
for both the encoder and decoder components of the AE that employ additional
linear feed-forward connections between the CNN stages; experiments show that
these architectural innovations achieve additional SNR gains of up to 2.2 dB
over the standard serial CNN AE architecture.


------------------------------------------------------------------------------

Title:
A Passivity-Based Method for Accelerated Convex Optimisation

Abstract: This study presents a constructive methodology for designing accelerated
convex optimisation algorithms in continuous-time domain. The two key enablers
are the classical concept of passivity in control theory and the time-dependent
change of variables that maps the output of the internal dynamic system to the
optimisation variables. The Lyapunov function associated with the optimisation
dynamics is obtained as a natural consequence of specifying the internal
dynamics that drives the state evolution as a passive linear time-invariant
system. The passivity-based methodology provides a general framework that has
the flexibility to generate convex optimisation algorithms with the guarantee
of different convergence rate bounds on the objective function value. The same
principle applies to the design of online parameter update algorithms for
adaptive control by re-defining the output of internal dynamics to allow for
the feedback interconnection with tracking error dynamics.


------------------------------------------------------------------------------

Title:
eCat: An End-to-End Model for Multi-Speaker TTS & Many-to-Many  Fine-Grained Prosody Transfer

Abstract: We present eCat, a novel end-to-end multispeaker model capable of: a)
generating long-context speech with expressive and contextually appropriate
prosody, and b) performing fine-grained prosody transfer between any pair of
seen speakers. eCat is trained using a two-stage training approach. In Stage I,
the model learns speaker-independent word-level prosody representations in an
end-to-end fashion from speech. In Stage II, we learn to predict the prosody
representations using the contextual information available in text. We compare
eCat to CopyCat2, a model capable of both fine-grained prosody transfer (FPT)
and multi-speaker TTS. We show that eCat statistically significantly reduces
the gap in naturalness between CopyCat2 and human recordings by an average of
46.7% across 2 languages, 3 locales, and 7 speakers, along with better
target-speaker similarity in FPT. We also compare eCat to VITS, and show a
statistically significant preference.


------------------------------------------------------------------------------

Title:
Unraveling the Interconnected Axes of Heterogeneity in Machine Learning  for Democratic and Inclusive Advancements

Abstract: The growing utilization of machine learning (ML) in decision-making processes
raises questions about its benefits to society. In this study, we identify and
analyze three axes of heterogeneity that significantly influence the trajectory
of ML products. These axes are i) values, culture and regulations, ii) data
composition, and iii) resource and infrastructure capacity. We demonstrate how
these axes are interdependent and mutually influence one another, emphasizing
the need to consider and address them jointly. Unfortunately, the current
research landscape falls short in this regard, often failing to adopt a
holistic approach. We examine the prevalent practices and methodologies that
skew these axes in favor of a selected few, resulting in power concentration,
homogenized control, and increased dependency. We discuss how this fragmented
study of the three axes poses a significant challenge, leading to an
impractical solution space that lacks reflection of real-world scenarios.
Addressing these issues is crucial to ensure a more comprehensive understanding
of the interconnected nature of society and to foster the democratic and
inclusive development of ML systems that are more aligned with real-world
complexities and its diverse requirements.


------------------------------------------------------------------------------

Title:
Probabilistic matching of real and generated data statistics in  generative adversarial networks

Abstract: Generative adversarial networks constitute a powerful approach to generative
modeling. While generated samples often are indistinguishable from real data,
there is no guarantee that they will follow the true data distribution. In this
work, we propose a method to ensure that the distributions of certain generated
data statistics coincide with the respective distributions of the real data. In
order to achieve this, we add a Kullback-Leibler term to the generator loss
function: the KL divergence is taken between the true distributions as
represented by a conditional energy-based model, and the corresponding
generated distributions obtained from minibatch values at each iteration. We
evaluate the method on a synthetic dataset and two real-world datasets and
demonstrate improved performance of our method.


------------------------------------------------------------------------------

Title:
Opinion formation on evolving network. The DPA method applied to a  nonlocal cross-diffusion PDE-ODE system

Abstract: We study a system of nonlocal aggregation cross-diffusion PDEs that describe
the evolution of opinion densities on a network. The PDEs are coupled with a
system of ODEs that describe the time evolution of the agents on the network.
Firstly, we apply the Deterministic Particle Approximation (DPA) method to the
aforementioned system in order to prove the existence of solutions under
suitable assumptions on the interactions between agents. Later on, we present
an explicit model for opinion formation on an evolving network. The opinions
evolve based on both the distance between the agents on the network and the
'attitude areas,' which depend on the distance between the agents' opinions.
The position of the agents on the network evolves based on the distance between
the agents' opinions. The goal is to study radicalization, polarization, and
fragmentation of the population while changing its open-mindedness and the
radius of interaction.


------------------------------------------------------------------------------

Title:
A labeled dataset of cloud types using data from GOES-16 and CloudSat

Abstract: In this paper we present the development of a dataset consisting of 91
Multi-band Cloud and Moisture Product Full-Disk (MCMIPF) from the Advanced
Baseline Imager (ABI) on board GOES-16 geostationary satellite with 91
temporally and spatially corresponding CLDCLASS products from the CloudSat
polar satellite. The products are diurnal, corresponding to the months of
January and February 2019 and were chosen such that the products from both
satellites can be co-located over South America. The CLDCLASS product provides
the cloud type observed for each of the orbit's steps and the GOES-16 multiband
images contain pixels that can be co-located with these data. We develop an
algorithm that returns a product in the form of a table that provides pixels
from multiband images labelled with the type of cloud observed in them. These
labelled data conformed in this particular structure are very useful to perform
supervised learning. This was corroborated by training a simple linear
artificial neural network based on the work of Gorooh et al. (2020), which gave
good results, especially for the classification of deep convective clouds.


------------------------------------------------------------------------------

Title:
SpreadDetect: Detection of spreading change in a network over time

Abstract: Change-point analysis has been successfully applied to the detect changes in
multivariate data streams over time. In many applications, when data are
observed over a graph/network, change does not occur simultaneously but instead
spread from an initial source coordinate to the neighbouring coordinates over
time. We propose a new method, SpreadDetect, that estimates both the source
coordinate and the initial timepoint of change in such a setting. We prove that
under appropriate conditions, the SpreadDetect algorithm consistently estimates
both the source coordinate and the timepoint of change and that the minimal
signal size detectable by the algorithm is minimax optimal. The practical
utility of the algorithm is demonstrated through numerical experiments and a
COVID-19 real dataset.


------------------------------------------------------------------------------

Title:
SURT 2.0: Advances in Transducer-based Multi-talker Speech Recognition

Abstract: The Streaming Unmixing and Recognition Transducer (SURT) model was proposed
recently as an end-to-end approach for continuous, streaming, multi-talker
speech recognition (ASR). Despite impressive results on multi-turn meetings,
SURT has notable limitations: (i) it suffers from leakage and omission related
errors; (ii) it is computationally expensive, due to which it has not seen
adoption in academia; and (iii) it has only been evaluated on synthetic
mixtures. In this work, we propose several modifications to the original SURT
which are carefully designed to fix the above limitations. In particular, we
(i) change the unmixing module to a mask estimator that uses dual-path
modeling, (ii) use a streaming zipformer encoder and a stateless decoder for
the transducer, (iii) perform mixture simulation using force-aligned
subsegments, (iv) pre-train the transducer on single-speaker data, (v) use
auxiliary objectives in the form of masking loss and encoder CTC loss, and (vi)
perform domain adaptation for far-field recognition. We show that our
modifications allow SURT 2.0 to outperform its predecessor in terms of
multi-talker ASR results, while being efficient enough to train with academic
resources. We conduct our evaluations on 3 publicly available meeting
benchmarks -- LibriCSS, AMI, and ICSI, where our best model achieves WERs of
16.9%, 44.6% and 32.2%, respectively, on far-field unsegmented recordings. We
release training recipes and pre-trained models:
this https URL


------------------------------------------------------------------------------

Title:
Projection-Free Online Convex Optimization via Efficient Newton  Iterations

Abstract: This paper presents new projection-free algorithms for Online Convex
Optimization (OCO) over a convex domain $\mathcal{K} \subset \mathbb{R}^d$.
Classical OCO algorithms (such as Online Gradient Descent) typically need to
perform Euclidean projections onto the convex set $\cK$ to ensure feasibility
of their iterates. Alternative algorithms, such as those based on the
Frank-Wolfe method, swap potentially-expensive Euclidean projections onto
$\mathcal{K}$ for linear optimization over $\mathcal{K}$. However, such
algorithms have a sub-optimal regret in OCO compared to projection-based
algorithms. In this paper, we look at a third type of algorithms that output
approximate Newton iterates using a self-concordant barrier for the set of
interest. The use of a self-concordant barrier automatically ensures
feasibility without the need for projections. However, the computation of the
Newton iterates requires a matrix inverse, which can still be expensive. As our
main contribution, we show how the stability of the Newton iterates can be
leveraged to compute the inverse Hessian only a vanishing fraction of the
rounds, leading to a new efficient projection-free OCO algorithm with a
state-of-the-art regret bound.


------------------------------------------------------------------------------

Title:
Can predictive models be used for causal inference?

Abstract: Supervised machine learning (ML) and deep learning (DL) algorithms excel at
predictive tasks, but it is commonly assumed that they often do so by
exploiting non-causal correlations, which may limit both interpretability and
generalizability. Here, we show that this trade-off between explanation and
prediction is not as deep and fundamental as expected. Whereas ML and DL
algorithms will indeed tend to use non-causal features for prediction when fed
indiscriminately with all data, it is possible to constrain the learning
process of any ML and DL algorithm by selecting features according to Pearl's
backdoor adjustment criterion. In such a situation, some algorithms, in
particular deep neural networks, can provide near unbiased effect estimates
under feature collinearity. Remaining biases are explained by the specific
algorithmic structures as well as hyperparameter choice. Consequently, optimal
hyperparameter settings are different when tuned for prediction or inference,
confirming the general expectation of a trade-off between prediction and
explanation. However, the effect of this trade-off is small compared to the
effect of a causally constrained feature selection. Thus, once the causal
relationship between the features is accounted for, the difference between
prediction and explanation may be much smaller than commonly assumed. We also
show that such causally constrained models generalize better to new data with
altered collinearity structures, suggesting generalization failure may often be
due to a lack of causal learning. Our results not only provide a perspective
for using ML for inference of (causal) effects but also help to improve the
generalizability of fitted ML and DL models to new data.


------------------------------------------------------------------------------

Title:
Non-contact Sensing for Anomaly Detection in Wind Turbine Blades: A  focus-SVDD with Complex-Valued Auto-Encoder Approach

Abstract: The occurrence of manufacturing defects in wind turbine blade (WTB)
production can result in significant increases in operation and maintenance
costs and lead to severe and disastrous consequences. Therefore, inspection
during the manufacturing process is crucial to ensure consistent fabrication of
composite materials. Non-contact sensing techniques, such as Frequency
Modulated Continuous Wave (FMCW) radar, are becoming increasingly popular as
they offer a full view of these complex structures during curing. In this
paper, we enhance the quality assurance of manufacturing utilizing FMCW radar
as a non-destructive sensing modality. Additionally, a novel anomaly detection
pipeline is developed that offers the following advantages: (1) We use the
analytic representation of the Intermediate Frequency signal of the FMCW radar
as a feature to disentangle material-specific and round-trip delay information
from the received wave. (2) We propose a novel anomaly detection methodology
called focus Support Vector Data Description (focus-SVDD). This methodology
involves defining the limit boundaries of the dataset after removing healthy
data features, thereby focusing on the attributes of anomalies. (3) The
proposed method employs a complex-valued autoencoder to remove healthy features
and we introduces a new activation function called Exponential Amplitude Decay
(EAD). EAD takes advantage of the Rayleigh distribution, which characterizes an
instantaneous amplitude signal. The effectiveness of the proposed method is
demonstrated through its application to collected data, where it shows superior
performance compared to other state-of-the-art unsupervised anomaly detection
methods. This method is expected to make a significant contribution not only to
structural health monitoring but also to the field of deep complex-valued data
processing and SVDD application.


------------------------------------------------------------------------------

Title:
Bounds on the genus for 2-cell embeddings of prefix-reversal graphs

Abstract: In this paper, we provide bounds for the genus of the pancake graph
$\mathbb{P}_n$, burnt pancake graph $\mathbb{BP}_n$, and undirected generalized
pancake graph $\mathbb{P}_m(n)$. Our upper bound for $\mathbb{P}_n$ is sharper
than the previously-known bound, and the other bounds presented are the first
of their kind. Our proofs are constructive and rely on finding an appropriate
rotation system (also referred to in the literature as Edmonds' permutation
technique) where certain cycles in the graphs we consider become boundaries of
regions of a 2-cell embedding. A key ingredient in the proof of our bounds for
the genus $\mathbb{P}_n$ and $\mathbb{BP}_n$ is a labeling algorithm of their
vertices that allows us to implement rotation systems to bound the number of
regions of a 2-cell embedding of said graphs.


------------------------------------------------------------------------------

Title:
Taming Diffusion Models for Music-driven Conducting Motion Generation

Abstract: Generating the motion of orchestral conductors from a given piece of symphony
music is a challenging task since it requires a model to learn semantic music
features and capture the underlying distribution of real conducting motion.
Prior works have applied Generative Adversarial Networks (GAN) to this task,
but the promising diffusion model, which recently showed its advantages in
terms of both training stability and output quality, has not been exploited in
this context. This paper presents Diffusion-Conductor, a novel DDIM-based
approach for music-driven conducting motion generation, which integrates the
diffusion model to a two-stage learning framework. We further propose a random
masking strategy to improve the feature robustness, and use a pair of geometric
loss functions to impose additional regularizations and increase motion
diversity. We also design several novel metrics, including Frechet Gesture
Distance (FGD) and Beat Consistency Score (BC) for a more comprehensive
evaluation of the generated motion. Experimental results demonstrate the
advantages of our model.


------------------------------------------------------------------------------

Title:
Minimax optimal testing by classification

Abstract: This paper considers an ML inspired approach to hypothesis testing known as
classifier/classification-accuracy testing ($\mathsf{CAT}$). In $\mathsf{CAT}$,
one first trains a classifier by feeding it labeled synthetic samples generated
by the null and alternative distributions, which is then used to predict labels
of the actual data samples. This method is widely used in practice when the
null and alternative are only specified via simulators (as in many scientific
experiments).
We study goodness-of-fit, two-sample ($\mathsf{TS}$) and likelihood-free
hypothesis testing ($\mathsf{LFHT}$), and show that $\mathsf{CAT}$ achieves
(near-)minimax optimal sample complexity in both the dependence on the
total-variation ($\mathsf{TV}$) separation $\epsilon$ and the probability of
error $\delta$ in a variety of non-parametric settings, including discrete
distributions, $d$-dimensional distributions with a smooth density, and the
Gaussian sequence model. In particular, we close the high probability sample
complexity of $\mathsf{LFHT}$ for each class. As another highlight, we recover
the minimax optimal complexity of $\mathsf{TS}$ over discrete distributions,
which was recently established by Diakonikolas et al. (2021). The corresponding
$\mathsf{CAT}$ simply compares empirical frequencies in the first half of the
data, and rejects the null when the classification accuracy on the second half
is better than random.


------------------------------------------------------------------------------

Title:
Conditional expectation via compact kernels

Abstract: The separate tasks of denoising, conditional expectation and manifold
learning can often be posed in a common setting of finding the conditional
expectations arising from a product of two random variables. This paper focuses
on this more general problem and describes an operator theoretic approach to
estimating the conditional expectation. Kernel integral operators are used as a
compactification tool, to set up the estimation problem as a linear inverse
problem in a reproducing kernel Hilbert space. This equation is shown to have
solutions that are stable to numerical approximation, thus guaranteeing the
convergence of data-driven implementations. The overall technique is easy to
implement, and their successful application to some real-world problems are
also shown.


------------------------------------------------------------------------------

Title:
Super-resolving sparse observations in partial differential equations: A  physics-constrained convolutional neural network approach

Abstract: We propose the physics-constrained convolutional neural network (PC-CNN) to
infer the high-resolution solution from sparse observations of spatiotemporal
and nonlinear partial differential equations. Results are shown for a chaotic
and turbulent fluid motion, whose solution is high-dimensional, and has fine
spatiotemporal scales. We show that, by constraining prior physical knowledge
in the CNN, we can infer the unresolved physical dynamics without using the
high-resolution dataset in the training. This opens opportunities for
super-resolution of experimental data and low-resolution simulations.


------------------------------------------------------------------------------

Title:
Hybrid Dealiased Convolutions

Abstract: This paper proposes a practical and efficient solution for computing
convolutions using hybrid dealiasing. It offers an alternative to explicit or
implicit dealiasing and includes an optimized hyperparameter tuning algorithm
that uses experience to find the optimal parameters. Machine learning
algorithms and efficient heuristics are also developed to estimate optimal
parameters for larger convolution problems using only small squares/rectangles.


------------------------------------------------------------------------------

Title:
Human-In-the-Loop for Bayesian Autonomous Materials Phase Mapping

Abstract: Autonomous experimentation (AE) combines machine learning and research
hardware automation in a closed loop, guiding subsequent experiments toward
user goals. As applied to materials research, AE can accelerate materials
exploration, reducing time and cost compared to traditional Edisonian studies.
Additionally, integrating knowledge from diverse sources including theory,
simulations, literature, and domain experts can boost AE performance. Domain
experts may provide unique knowledge addressing tasks that are difficult to
automate. Here, we present a set of methods for integrating human input into an
autonomous materials exploration campaign for composition-structure phase
mapping. The methods are demonstrated on x-ray diffraction data collected from
a thin film ternary combinatorial library. At any point during the campaign,
the user can choose to provide input by indicating regions-of-interest, likely
phase regions, and likely phase boundaries based on their prior knowledge
(e.g., knowledge of the phase map of a similar material system), along with
quantifying their certainty. The human input is integrated by defining a set of
probabilistic priors over the phase map. Algorithm output is a probabilistic
distribution over potential phase maps, given the data, model, and human input.
We demonstrate a significant improvement in phase mapping performance given
appropriate human input.


------------------------------------------------------------------------------

Title:
Deep learning-based group-wise registration for longitudinal MRI  analysis in glioma

Abstract: Glioma growth may be quantified with longitudinal image registration.
However, the large mass-effects and tissue changes across images pose an added
challenge. Here, we propose a longitudinal, learning-based, and groupwise
registration method for the accurate and unbiased registration of glioma MRI.
We evaluate on a dataset from the Glioma Longitudinal AnalySiS consortium and
compare it to classical registration methods. We achieve comparable Dice
coefficients, with more detailed registrations, while significantly reducing
the runtime to under a minute. The proposed methods may serve as an alternative
to classical toolboxes, to provide further insight into glioma growth.


------------------------------------------------------------------------------

Title:
Eight challenges in developing theory of intelligence

Abstract: A good theory of mathematical beauty is more practical than any current
observation, as new predictions of physical reality can be verified
self-consistently. This belief applies to the current status of understanding
deep neural networks including large language models and even the biological
intelligence. Toy models provide a metaphor of physical reality, allowing
mathematically formulating that reality (i.e., the so-called theory), which can
be updated as more conjectures are justified or refuted. One does not need to
pack all details into a model, but rather, more abstract models are
constructed, as complex systems like brains or deep networks have many sloppy
dimensions but much less stiff dimensions that strongly impact macroscopic
observables. This kind of bottom-up mechanistic modeling is still promising in
the modern era of understanding the natural or artificial intelligence. Here,
we shed light on eight challenges in developing theory of intelligence
following this theoretical paradigm.


------------------------------------------------------------------------------

Title:
RetinexFlow for CT metal artifact reduction

Abstract: Metal artifacts is a major challenge in computed tomography (CT) imaging,
significantly degrading image quality and making accurate diagnosis difficult.
However, previous methods either require prior knowledge of the location of
metal implants, or have modeling deviations with the mechanism of artifact
formation, which limits the ability to obtain high-quality CT images. In this
work, we formulate metal artifacts reduction problem as a combination of
decomposition and completion tasks. And we propose RetinexFlow, which is a
novel end-to-end image domain model based on Retinex theory and conditional
normalizing flow, to solve it. Specifically, we first design a feature
decomposition encoder for decomposing the metal implant component and inherent
component, and extracting the inherent feature. Then, it uses a
feature-to-image flow module to complete the metal artifact-free CT image step
by step through a series of invertible transformations. These designs are
incorporated in our model with a coarse-to-fine strategy, enabling it to
achieve superior performance. The experimental results on on simulation and
clinical datasets show our method achieves better quantitative and qualitative
results, exhibiting better visual performance in artifact removal and image
fidelity


------------------------------------------------------------------------------

Title:
Reliability and repeatability of ISO 3382-3 metrics based on repeated  acoustic measurements in open-plan offices

Abstract: This paper investigates variability in the key ISO 3382-3:2012 metrics, based
primarily on the repeatability and reliability of these metrics, using repeated
measurements in open-plan offices. Two types of repeated measurements were
performed in offices, Type1 (n=36), where the same path over workstations was
measured from opposite ends, and Type2 (n=7), where two different measurement
paths were measured. Overall, most of the Type1 results seem reasonable
considering repeats were conducted in complicated room acoustic environments,
while Type2 repeats would benefit from larger sample sizes in future studies.
Some recommendations are outlined for the ISO 3382-3 methodology vis-a-vis
Type1 and Type2 repeats, including future research directions that go beyond
increased sample sizes. (This is an abridged version of the abstract. Please
see the paper for the full abstract)


------------------------------------------------------------------------------

Title:
Machine learning of hidden variables in multiscale fluid simulation

Abstract: Solving fluid dynamics equations often requires the use of closure relations
that account for missing microphysics. For example, when solving equations
related to fluid dynamics for systems with a large Reynolds number, sub-grid
effects become important and a turbulence closure is required, and in systems
with a large Knudsen number, kinetic effects become important and a kinetic
closure is required. By adding an equation governing the growth and transport
of the quantity requiring the closure relation, it becomes possible to capture
microphysics through the introduction of ``hidden variables'' that are
non-local in space and time. The behavior of the ``hidden variables'' in
response to the fluid conditions can be learned from a higher fidelity or
ab-initio model that contains all the microphysics. In our study, a partial
differential equation simulator that is end-to-end differentiable is used to
train judiciously placed neural networks against ground-truth simulations. We
show that this method enables an Euler equation based approach to reproduce
non-linear, large Knudsen number plasma physics that can otherwise only be
modeled using Boltzmann-like equation simulators such as Vlasov or
Particle-In-Cell modeling.


------------------------------------------------------------------------------

Title:
Diffusion model based data generation for partial differential equations

Abstract: In a preliminary attempt to address the problem of data scarcity in
physics-based machine learning, we introduce a novel methodology for data
generation in physics-based simulations. Our motivation is to overcome the
limitations posed by the limited availability of numerical data. To achieve
this, we leverage a diffusion model that allows us to generate synthetic data
samples and test them for two canonical cases: (a) the steady 2-D Poisson
equation, and (b) the forced unsteady 2-D Navier-Stokes (NS)
{vorticity-transport} equation in a confined box. By comparing the generated
data samples against outputs from classical solvers, we assess their accuracy
and examine their adherence to the underlying physics laws. In this way, we
emphasize the importance of not only satisfying visual and statistical
comparisons with solver data but also ensuring the generated data's conformity
to physics laws, thus enabling their effective utilization in downstream tasks.


------------------------------------------------------------------------------

Title:
Realistic Restorer: artifact-free flow restorer(AF2R) for MRI motion  artifact removal

Abstract: Motion artifact is a major challenge in magnetic resonance imaging (MRI) that
severely degrades image quality, reduces examination efficiency, and makes
accurate diagnosis difficult. However, previous methods often relied on
implicit models for artifact correction, resulting in biases in modeling the
artifact formation mechanism and characterizing the relationship between
artifact information and anatomical details. These limitations have hindered
the ability to obtain high-quality MR images. In this work, we incorporate the
artifact generation mechanism to reestablish the relationship between artifacts
and anatomical content in the image domain, highlighting the superiority of
explicit models over implicit models in medical problems. Based on this, we
propose a novel end-to-end image domain model called AF2R, which addresses this
problem using conditional normalization flow. Specifically, we first design a
feature encoder to extract anatomical features from images with motion
artifacts. Then, through a series of reversible transformations using the
feature-to-image flow module, we progressively obtain MR images unaffected by
motion artifacts. Experimental results on simulated and real datasets
demonstrate that our method achieves better performance in both quantitative
and qualitative results, preserving better anatomical details.


------------------------------------------------------------------------------

Title:
Quantum Algorithms for the Shortest Common Superstring and Text  Assembling Problems

Abstract: In this paper, we consider two versions of the Text Assembling problem. We
are given a sequence of strings $s^1,\dots,s^n$ of total length $L$ that is a
dictionary, and a string $t$ of length $m$ that is texts. The first version of
the problem is assembling $t$ from the dictionary. The second version is the
``Shortest Superstring Problem''(SSP) or the ``Shortest Common Superstring
Problem''(SCS). In this case, $t$ is not given, and we should construct the
shortest string (we call it superstring) that contains each string from the
given sequence as a substring. These problems are connected with the sequence
assembly method for reconstructing a long DNA sequence from small fragments.
For both problems, we suggest new quantum algorithms that work better than
their classical counterparts. In the first case, we present a quantum algorithm
with $O(m+\log m\sqrt{nL})$ running time. In the case of SSP, we present a
quantum algorithm with running time $O(n^3 1.728^n +L
+\sqrt{L}n^{1.5}+\sqrt{L}n\log^2L\log^2n)$.


------------------------------------------------------------------------------

Title:
P-tensors: a General Formalism for Constructing Higher Order Message  Passing Networks

Abstract: Several recent papers have recently shown that higher order graph neural
networks can achieve better accuracy than their standard message passing
counterparts, especially on highly structured graphs such as molecules. These
models typically work by considering higher order representations of subgraphs
contained within a given graph and then perform some linear maps between them.
We formalize these structures as permutation equivariant tensors, or P-tensors,
and derive a basis for all linear maps between arbitrary order equivariant
P-tensors. Experimentally, we demonstrate this paradigm achieves state of the
art performance on several benchmark datasets.


------------------------------------------------------------------------------

Title:
Mutual Interference Mitigation in PMCW Automotive Radar

Abstract: This paper addresses the challenge of mutual interference in phase-modulated
continuous wave (PMCW) millimeter-wave (mmWave) automotive radar systems. The
increasing demand for advanced driver assistance systems (ADAS) has led to a
proliferation of vehicles equipped with mmWave radar systems that operate in
the same frequency band, resulting in mutual interference that can degrade
radar performance creating safety hazards. We consider scenarios involving two
similar PMCW radar systems and propose an effective technique for a cooperative
design of transmit waveforms such that the mutual interference between them is
minimized. The proposed approach is numerically evaluated via simulations of a
mmWave automotive radar system. The results demonstrate that the proposed
technique notably reduces mutual interference and enhances radar detection
performance while imposing very little computational cost and a negligible
impact on existing infrastructure in practical automotive radar systems


------------------------------------------------------------------------------

Title:
On stability and state-norm estimation of switched systems under  restricted switching

Abstract: This paper deals with the analysis of input/output-to-state stability (IOSS)
and construction of state-norm estimators for continuous-time switched
nonlinear systems under restricted switching. Our contributions are twofold.
First, given a family of systems, possibly containing unstable dynamics, a set
of admissible switches between the subsystems and admissible minimum and
maximum dwell times on the subsystems, we identify a class of switching signals
that obeys the given restrictions and preserves IOSS of the resulting switched
system. Second, we design two classes of state-norm estimators for switched
systems under our class of stabilizing switching signals. These estimators are
switched systems themselves with two subsystems -- one stable and one unstable.
The key apparatus for our analysis is multiple Lyapunov-like functions. A
numerical example is presented to demonstrate our results.


------------------------------------------------------------------------------

Title:
Ternary extremal four-negacirculant self-dual codes

Abstract: In this note, we give basic properties of ternary four-negacirculant
self-dual codes. By exhaustive computer search based on the properties, we
complete a classification of ternary extremal four-negacirculant self-dual
codes of lengths 40, 44, 48, 52 and 60.


------------------------------------------------------------------------------

Title:
Convolutional GRU Network for Seasonal Prediction of the El  Niño-Southern Oscillation

Abstract: Predicting sea surface temperature (SST) within the El Ni\~no-Southern
Oscillation (ENSO) region has been extensively studied due to its significant
influence on global temperature and precipitation patterns. Statistical models
such as linear inverse model (LIM), analog forecasting (AF), and recurrent
neural network (RNN) have been widely used for ENSO prediction, offering
flexibility and relatively low computational expense compared to large dynamic
models. However, these models have limitations in capturing spatial patterns in
SST variability or relying on linear dynamics. Here we present a modified
Convolutional Gated Recurrent Unit (ConvGRU) network for the ENSO region
spatio-temporal sequence prediction problem, along with the Ni\~no 3.4 index
prediction as a down stream task. The proposed ConvGRU network, with an
encoder-decoder sequence-to-sequence structure, takes historical SST maps of
the Pacific region as input and generates future SST maps for subsequent months
within the ENSO region. To evaluate the performance of the ConvGRU network, we
trained and tested it using data from multiple large climate models. The
results demonstrate that the ConvGRU network significantly improves the
predictability of the Ni\~no 3.4 index compared to LIM, AF, and RNN. This
improvement is evidenced by extended useful prediction range, higher Pearson
correlation, and lower root-mean-square error. The proposed model holds promise
for improving our understanding and predicting capabilities of the ENSO
phenomenon and can be broadly applicable to other weather and climate
prediction scenarios with spatial patterns and teleconnections.


------------------------------------------------------------------------------

Title:
Two simultaneous talkers distract more than one in simulated  multi-talker environments, regardless of overall sound levels typical of  open-plan offices

Abstract: The irrelevant speech effect (ISE) characterizes detriment to cognitive task
performance in the presence of irrelevant speech. This paper examines whether
the ISE varies due to the number of simultaneously active nearby talkers (for
up to two talkers), or the overall sound level, within the context of a
simulated open-plan office. Two experiments were conducted within a
climate-controlled chamber that was set-up as a medium-sized open-plan office.
The cognitive tasks performed by the participants included the digit recall
task, and a writing task, within a room acoustic simulation of realistic
multi-talker speech from spatially separated talkers. Within Experiment 1
(n=60), an increase in the number of talkers from none (T0) to one (T1), and
from one to two (T2) simultaneous talkers resulted in statistically significant
decline in the digit recall task performances, with effect sizes of 24% (i.e.,
T1 vs. T0), and 12% (i.e., T2 vs. T1), respectively. The pauses between words
during the writing task were similar for T0 and T1, but showed a statistically
significant increase within T2 vs. T1, with an effect size of 12%. The findings
of Experiment 1 are inconsistent with the maximally distracting status
attributed to T1 in some studies, but is consistent with findings in other
studies. Within Experiment 2 (n = 62), the cognitive performance in T2 remained
largely invariant between 45 and 57 dB (A-weighted sound pressure levels),
which represents a typical range of levels within open-plan offices. In
general, these findings have relevance for characterizing auditory distraction
within complex multi-talker environments; both in laboratory studies and actual
open-plan offices. (Abridged version; please see the paper for the full
abstract)


------------------------------------------------------------------------------

Title:
Deep Huber quantile regression networks

Abstract: Typical machine learning regression applications aim to report the mean or
the median of the predictive probability distribution, via training with a
squared or an absolute error scoring function. The importance of issuing
predictions of more functionals of the predictive probability distribution
(quantiles and expectiles) has been recognized as a means to quantify the
uncertainty of the prediction. In deep learning (DL) applications, that is
possible through quantile and expectile regression neural networks (QRNN and
ERNN respectively). Here we introduce deep Huber quantile regression networks
(DHQRN) that nest QRNNs and ERNNs as edge cases. DHQRN can predict Huber
quantiles, which are more general functionals in the sense that they nest
quantiles and expectiles as limiting cases. The main idea is to train a deep
learning algorithm with the Huber quantile regression function, which is
consistent for the Huber quantile functional. As a proof of concept, DHQRN are
applied to predict house prices in Australia. In this context, predictive
performances of three DL architectures are discussed along with evidential
interpretation of results from an economic case study.


------------------------------------------------------------------------------

Title:
$\texttt{causalAssembly}$: Generating Realistic Production Data for  Benchmarking Causal Discovery

Abstract: Algorithms for causal discovery have recently undergone rapid advances and
increasingly draw on flexible nonparametric methods to process complex data.
With these advances comes a need for adequate empirical validation of the
causal relationships learned by different algorithms. However, for most real
data sources true causal relations remain unknown. This issue is further
compounded by privacy concerns surrounding the release of suitable high-quality
data. To help address these challenges, we gather a complex dataset comprising
measurements from an assembly line in a manufacturing context. This line
consists of numerous physical processes for which we are able to provide ground
truth causal relationships on the basis of a detailed study of the underlying
physics. We use the assembly line data and associated ground truth information
to build a system for generation of semisynthetic manufacturing data that
supports benchmarking of causal discovery methods. To accomplish this, we
employ distributional random forests in order to flexibly estimate and
represent conditional distributions that may be combined into joint
distributions that strictly adhere to a causal model over the observed
variables. The estimated conditionals and tools for data generation are made
available in our Python library $\texttt{causalAssembly}$. Using the library,
we showcase how to benchmark several well-known causal discovery algorithms.


------------------------------------------------------------------------------

Title:
Improving Audio Caption Fluency with Automatic Error Correction

Abstract: Automated audio captioning (AAC) is an important cross-modality translation
task, aiming at generating descriptions for audio clips. However, captions
generated by previous AAC models have faced ``false-repetition'' errors due to
the training objective. In such scenarios, we propose a new task of AAC error
correction and hope to reduce such errors by post-processing AAC outputs. To
tackle this problem, we use observation-based rules to corrupt captions without
errors, for pseudo grammatically-erroneous sentence generation. One pair of
corrupted and clean sentences can thus be used for training. We train a neural
network-based model on the synthetic error dataset and apply the model to
correct real errors in AAC outputs. Results on two benchmark datasets indicate
that our approach significantly improves fluency while maintaining semantic
information.


------------------------------------------------------------------------------

Title:
GAN-based Image Compression with Improved RDO Process

Abstract: GAN-based image compression schemes have shown remarkable progress lately due
to their high perceptual quality at low bit rates. However, there are two main
issues, including 1) the reconstructed image perceptual degeneration in color,
texture, and structure as well as 2) the inaccurate entropy model. In this
paper, we present a novel GAN-based image compression approach with improved
rate-distortion optimization (RDO) process. To achieve this, we utilize the
DISTS and MS-SSIM metrics to measure perceptual degeneration in color, texture,
and structure. Besides, we absorb the discretized gaussian-laplacian-logistic
mixture model (GLLMM) for entropy modeling to improve the accuracy in
estimating the probability distributions of the latent representation. During
the evaluation process, instead of evaluating the perceptual quality of the
reconstructed image via IQA metrics, we directly conduct the Mean Opinion Score
(MOS) experiment among different codecs, which fully reflects the actual
perceptual results of humans. Experimental results demonstrate that the
proposed method outperforms the existing GAN-based methods and the
state-of-the-art hybrid codec (i.e., VVC).


------------------------------------------------------------------------------

Title:
Experts' cognition-driven ensemble deep learning for external validation  of predicting pathological complete response to neoadjuvant chemotherapy from  histological images in breast cancer

Abstract: In breast cancer imaging, there has been a trend to directly predict
pathological complete response (pCR) to neoadjuvant chemotherapy (NAC) from
histological images based on deep learning (DL). However, it has been a
commonly known problem that the constructed DL-based models numerically have
better performances in internal validation than in external validation. The
primary reason for this situation lies in that the distribution of the external
data for validation is different from the distribution of the training data for
the construction of the predictive model. In this paper, we aim to alleviate
this situation with a more intrinsic approach. We propose an experts'
cognition-driven ensemble deep learning (ECDEDL) approach for external
validation of predicting pCR to NAC from histological images in breast cancer.
The proposed ECDEDL, which takes the cognition of both pathology and artificial
intelligence experts into consideration to improve the generalization of the
predictive model to the external validation, more intrinsically approximates
the working paradigm of a human being which will refer to his various working
experiences to make decisions. The proposed ECDEDL approach was validated with
695 WSIs collected from the same center as the primary dataset to develop the
predictive model and perform the internal validation, and 340 WSIs collected
from other three centers as the external dataset to perform the external
validation. In external validation, the proposed ECDEDL approach improves the
AUCs of pCR prediction from 61.52(59.80-63.26) to 67.75(66.74-68.80) and the
Accuracies of pCR prediction from 56.09(49.39-62.79) to 71.01(69.44-72.58). The
proposed ECDEDL was quite effective for external validation, numerically more
approximating the internal validation.


------------------------------------------------------------------------------

Title:
Learning operators for identifying weak solutions to the Navier-Stokes  equations

Abstract: This paper focuses on investigating the learning operators for identifying
weak solutions to the Navier-Stokes equations. Our objective is to establish a
connection between the initial data as input and the weak solution as output.
To achieve this, we employ a combination of deep learning methods and
compactness argument to derive learning operators for weak solutions for any
large initial data in 2D, and for low-dimensional initial data in 3D.
Additionally, we utilize the universal approximation theorem to derive a lower
bound on the number of sensors required to achieve accurate identification of
weak solutions to the Navier-Stokes equations. Our results demonstrate the
potential of using deep learning techniques to address challenges in the study
of fluid mechanics, particularly in identifying weak solutions to the
Navier-Stokes equations.


------------------------------------------------------------------------------

Title:
House-Swapping with Objective Indifferences

Abstract: We study the classic house-swapping problem of Shapley and Scarf (1974) in a
setting where agents may have "objective" indifferences, i.e., indifferences
that are shared by all agents. In other words, if any one agent is indifferent
between two houses, then all agents are indifferent between those two houses.
The most direct interpretation is the presence of multiple copies of the same
object. Our setting is a special case of the house-swapping problem with
general indifferences. We derive a simple, easily interpretable algorithm that
produces the unique strict core allocation of the house-swapping market, if it
exists. Our algorithm runs in square polynomial time, a substantial improvement
over the cubed time methods for the more general problem.


------------------------------------------------------------------------------

Title:
Wireless Data Link at 1Gbps using 256 QAM

Abstract: This report describes the design and proposal of a wireless link capable of
broadcasting at 1 Gbps. For this application, isotropic antennas, 256 QAM
modulation, and BER level less than 1e-5, without using error correction
coding, were implemented. A frequency of 5GHz was employed to achieve such high
data rates. For unlicensed operations in this frequency range, the FCC
allocates a 5.15 - 5.35 GHz frequency range with maximum acceptable power
levels no greater than 250mW(~24dBm)[2]. Due to its inexpensiveness and
simplicity, the transceiver architecture and all its subsystems used the
homodyne system. The complete system architecture is described with some of
their most significant performance characteristics, including modulation,
fundamental and 3rd harmonics, power spectra, and constellation diagrams. To
conclude, a Bill of Materials (BOM), costs, and associated specifications were
included.


------------------------------------------------------------------------------

Title:
Accelerated, physics-inspired inference of skeletal muscle  microstructure from diffusion-weighted MRI

Abstract: Muscle health is a critical component of overall health and quality of life.
However, current measures of skeletal muscle health take limited account of
microstructural variations within muscle, which play a crucial role in
mediating muscle function. To address this, we present a physics-inspired,
machine learning-based framework for the non-invasive and in vivo estimation of
microstructural organization in skeletal muscle from diffusion-weighted MRI
(dMRI). To reduce the computational expense associated with direct numerical
simulations of dMRI physics, a polynomial meta-model is developed that
accurately represents the input/output relationships of a high-fidelity
numerical model. This meta-model is used to develop a Gaussian process (GP)
model to provide voxel-wise estimates and confidence intervals of
microstructure organization in skeletal muscle. Given noise-free data, the GP
model accurately estimates microstructural parameters. In the presence of
noise, the diameter, intracellular diffusion coefficient, and membrane
permeability are accurately estimated with narrow confidence intervals, while
volume fraction and extracellular diffusion coefficient are poorly estimated
and exhibit wide confidence intervals. A reduced-acquisition GP model,
consisting of one-third the diffusion-encoding measurements, is shown to
predict parameters with similar accuracy to the original model. The fiber
diameter and volume fraction estimated by the reduced GP model is validated via
histology, with both parameters within their associated confidence intervals,
demonstrating the capability of the proposed framework as a promising
non-invasive tool for assessing skeletal muscle health and function.


------------------------------------------------------------------------------

Title:
Weighted structure tensor total variation for image denoising

Abstract: Based on the variational framework of the image denoising problem, we
introduce a novel image denoising regularizer that combines anisotropic total
variation model (ATV) and structure tensor total variation model (STV) in this
paper. The model can effectively capture the first-order information of the
image and maintain local features during the denoising process by applying the
matrix weighting operator proposed in the ATV model to the patch-based Jacobian
matrix in the STV model. Denoising experiments on grayscale and RGB color
images demonstrate that the suggested model can produce better restoration
quality in comparison to other well-known methods based on
total-variation-based models and the STV model.


------------------------------------------------------------------------------

Title:
Practical Equivariances via Relational Conditional Neural Processes

Abstract: Conditional Neural Processes (CNPs) are a class of metalearning models
popular for combining the runtime efficiency of amortized inference with
reliable uncertainty quantification. Many relevant machine learning tasks, such
as spatio-temporal modeling, Bayesian Optimization and continuous control,
contain equivariances -- for example to translation -- which the model can
exploit for maximal performance. However, prior attempts to include
equivariances in CNPs do not scale effectively beyond two input dimensions. In
this work, we propose Relational Conditional Neural Processes (RCNPs), an
effective approach to incorporate equivariances into any neural process model.
Our proposed method extends the applicability and impact of equivariant neural
processes to higher dimensions. We empirically demonstrate the competitive
performance of RCNPs on a large array of tasks naturally containing
equivariances.


------------------------------------------------------------------------------

Title:
Physics Constrained Unsupervised Deep Learning for Rapid, High  Resolution Scanning Coherent Diffraction Reconstruction

Abstract: By circumventing the resolution limitations of optics, coherent diffractive
imaging (CDI) and ptychography are making their way into scientific fields
ranging from X-ray imaging to astronomy. Yet, the need for time consuming
iterative phase recovery hampers real-time imaging. While supervised deep
learning strategies have increased reconstruction speed, they sacrifice image
quality. Furthermore, these methods' demand for extensive labeled training data
is experimentally burdensome. Here, we propose an unsupervised physics-informed
neural network reconstruction method, PtychoPINN, that retains the intrinsic
speed of deep learning-based reconstruction while improving reconstruction
quality by combining the diffraction forward map with real-space constraints
from overlapping measurements. In particular, PtychoPINN significantly advances
generalizability, accuracy (evidenced by a 10 dB PSNR increase), and linear
resolution (with a 3- to 6-fold gain). This blend of performance and speed
offers exciting prospects for high-resolution real-time imaging in
high-throughput environments such as X-ray free electron lasers (XFELs) and
diffraction-limited light sources.


------------------------------------------------------------------------------

Title:
Development of a Deep Learning System for Intra-Operative Identification  of Cancer Metastases

Abstract: For several cancer patients, operative resection with curative intent can end
up in early recurrence of the cancer. Current limitations in peri-operative
cancer staging and especially intra-operative misidentification of visible
metastases is likely the main reason leading to unnecessary operative
interventions in the affected individuals. Here, we evaluate whether an
artificial intelligence (AI) system can improve recognition of peritoneal
surface metastases on routine staging laparoscopy images from patients with
gastrointestinal malignancies. In a simulated setting evaluating biopsied
peritoneal lesions, a prototype deep learning surgical guidance system
outperformed oncologic surgeons in identifying peritoneal surface metastases.
In this environment the developed AI model would have improved the
identification of metastases by 5% while reducing the number of unnecessary
biopsies by 28% compared to current standard practice. Evaluating non-biopsied
peritoneal lesions, the findings support the possibility that the AI system
could identify peritoneal surface metastases that were falsely deemed benign in
clinical practice. Our findings demonstrate the technical feasibility of an AI
system for intra-operative identification of peritoneal surface metastases, but
require future assessment in a multi-institutional clinical setting.


------------------------------------------------------------------------------

Title:
Vision Guided MIMO Radar Beamforming for Enhanced Vital Signs Detection  in Crowds

Abstract: Radar as a remote sensing technology has been used to analyze human activity
for decades. Despite all the great features such as motion sensitivity, privacy
preservation, penetrability, and more, radar has limited spatial degrees of
freedom compared to optical sensors and thus makes it challenging to sense
crowded environments without prior information. In this paper, we develop a
novel dual-sensing system, in which a vision sensor is leveraged to guide
digital beamforming in a multiple-input multiple-output (MIMO) radar. Also, we
develop a calibration algorithm to align the two types of sensors and show that
the calibrated dual system achieves about two centimeters precision in
three-dimensional space within a field of view of $75^\circ$ by $65^\circ$ and
for a range of two meters. Finally, we show that the proposed approach is
capable of detecting the vital signs simultaneously for a group of closely
spaced subjects, sitting and standing, in a cluttered environment, which
highlights a promising direction for vital signs detection in realistic
environments.


------------------------------------------------------------------------------

Title:
Rehearsal-Free Online Continual Learning for Automatic Speech  Recognition

Abstract: Fine-tuning an Automatic Speech Recognition (ASR) model to new domains
results in degradation on original domains, referred to as Catastrophic
Forgetting (CF). Continual Learning (CL) attempts to train ASR models without
suffering from CF. While in ASR, offline CL is usually considered, online CL is
a more realistic but also more challenging scenario where the model, unlike in
offline CL, does not know when a task boundary occurs. Rehearsal-based methods,
which store previously seen utterances in a memory, are often considered for
online CL, in ASR and other research domains. However, recent research has
shown that weight averaging is an effective method for offline CL in ASR. Based
on this result, we propose, in this paper, a rehearsal-free method applicable
for online CL. Our method outperforms all baselines, including rehearsal-based
methods, in two experiments. Our method is a next step towards general CL for
ASR, which should enable CL in all scenarios with few if any constraints.


------------------------------------------------------------------------------

Title:
LVVC: A Learned Versatile Video Coding Framework for Efficient  Human-Machine Vision

Abstract: Almost all digital videos are coded into compact representations before being
transmitted. Such compact representations need to be decoded back to pixels
before being displayed to human and - as usual - before being
processed/analyzed by machine vision algorithms. For machine vision, it is more
efficient at least conceptually, to process/analyze the coded representations
directly without decoding them into pixels. Motivated by this concept, we
propose a learned versatile video coding (LVVC) framework, which targets on
learning compact representations to support both decoding and direct
processing/analysis, thereby being versatile for both human and machine vision.
Our LVVC framework has a feature-based compression loop, where one frame is
encoded (resp. decoded) to intermediate features, and the intermediate features
are referenced for encoding (resp. decoding) the following frames. Our proposed
feature-based compression loop has two key technologies, one is feature-based
temporal context mining, and the other is cross-domain motion encoder/decoder.
With the LVVC framework, the intermediate features may be used to reconstruct
videos, or be fed into different task networks. The LVVC framework is
implemented and evaluated with video reconstruction, video processing, and
video analysis tasks on the well-established benchmark datasets. The evaluation
results demonstrate the compression efficiency of the proposed LVVC framework.


------------------------------------------------------------------------------

Title:
Low-Resource Text-to-Speech Using Specific Data and Noise Augmentation

Abstract: Many neural text-to-speech architectures can synthesize nearly natural speech
from text inputs. These architectures must be trained with tens of hours of
annotated and high-quality speech data. Compiling such large databases for
every new voice requires a lot of time and effort. In this paper, we describe a
method to extend the popular Tacotron-2 architecture and its training with data
augmentation to enable single-speaker synthesis using a limited amount of
specific training data. In contrast to elaborate augmentation methods proposed
in the literature, we use simple stationary noises for data augmentation. Our
extension is easy to implement and adds almost no computational overhead during
training and inference. Using only two hours of training data, our approach was
rated by human listeners to be on par with the baseline Tacotron-2 trained with
23.5 hours of LJSpeech data. In addition, we tested our model with a
semantically unpredictable sentences test, which showed that both models
exhibit similar intelligibility levels.


------------------------------------------------------------------------------

Title:
Interpreting Deep Neural Networks with the Package innsight

Abstract: The R package innsight offers a general toolbox for revealing variable-wise
interpretations of deep neural networks' predictions with so-called feature
attribution methods. Aside from the unified and user-friendly framework, the
package stands out in three ways: It is generally the first R package
implementing feature attribution methods for neural networks. Secondly, it
operates independently of the deep learning library allowing the interpretation
of models from any R package, including keras, torch, neuralnet, and even
custom models. Despite its flexibility, innsight benefits internally from the
torch package's fast and efficient array calculations, which builds on LibTorch
$-$ PyTorch's C++ backend $-$ without a Python dependency. Finally, it offers a
variety of visualization tools for tabular, signal, image data or a combination
of these. Additionally, the plots can be rendered interactively using the
plotly package.


------------------------------------------------------------------------------

Title:
Channel Spatial Based Few-Shot Bird Sounds Event Detection

Abstract: This paper proposed a model for bird sound detection, which belongs to a
small sample of categories in the every day long tail distribution. Therefore,
we study bird sounds detection using the few-shot learning paradigm. By
combining channel and spatial attention mechanisms, better feature
representations can be learned from few-shot training datasets. We construct a
Metric Channel-Spatial Network model by merging a Channel Spatial SE block into
the prototype network to combine it with these attention mechanisms. We then
run the Metric Channel Spatial Network model on the benchmark of DCASE 2022
Take5 dataset and obtain an F-measure of $66.84\%$ and PSDS of $58.98\%$. The
experiment demonstrates the combination of channel and spatial attention
mechanisms can effectively improve the performance of bird sound classification
and detection.


------------------------------------------------------------------------------

Title:
Learning High-Dimensional Nonparametric Differential Equations via  Multivariate Occupation Kernel Functions

Abstract: Learning a nonparametric system of ordinary differential equations (ODEs)
from $n$ trajectory snapshots in a $d$-dimensional state space requires
learning $d$ functions of $d$ variables. Explicit formulations scale
quadratically in $d$ unless additional knowledge about system properties, such
as sparsity and symmetries, is available. In this work, we propose a linear
approach to learning using the implicit formulation provided by vector-valued
Reproducing Kernel Hilbert Spaces. By rewriting the ODEs in a weaker integral
form, which we subsequently minimize, we derive our learning algorithm. The
minimization problem's solution for the vector field relies on multivariate
occupation kernel functions associated with the solution trajectories. We
validate our approach through experiments on highly nonlinear simulated and
real data, where $d$ may exceed 100. We further demonstrate the versatility of
the proposed method by learning a nonparametric first order quasilinear partial
differential equation.


------------------------------------------------------------------------------

Title:
Semi-Supervised Learning for Multi-Label Cardiovascular Diseases  Prediction:A Multi-Dataset Study

Abstract: Electrocardiography (ECG) is a non-invasive tool for predicting
cardiovascular diseases (CVDs). Current ECG-based diagnosis systems show
promising performance owing to the rapid development of deep learning
techniques. However, the label scarcity problem, the co-occurrence of multiple
CVDs and the poor performance on unseen datasets greatly hinder the widespread
application of deep learning-based models. Addressing them in a unified
framework remains a significant challenge. To this end, we propose a
multi-label semi-supervised model (ECGMatch) to recognize multiple CVDs
simultaneously with limited supervision. In the ECGMatch, an ECGAugment module
is developed for weak and strong ECG data augmentation, which generates diverse
samples for model training. Subsequently, a hyperparameter-efficient framework
with neighbor agreement modeling and knowledge distillation is designed for
pseudo-label generation and refinement, which mitigates the label scarcity
problem. Finally, a label correlation alignment module is proposed to capture
the co-occurrence information of different CVDs within labeled samples and
propagate this information to unlabeled samples. Extensive experiments on four
datasets and three protocols demonstrate the effectiveness and stability of the
proposed model, especially on unseen datasets. As such, this model can pave the
way for diagnostic systems that achieve robust performance on multi-label CVDs
prediction with limited supervision.


------------------------------------------------------------------------------

Title:
Metasurface-based Spectral Convolutional Neural Network for Matter  Meta-imaging

Abstract: Convolutional neural networks (CNNs) are representative models of artificial
neural networks (ANNs), that form the backbone of modern computer vision.
However, the considerable power consumption and limited computing speed of
electrical computing platforms restrict further development of CNNs. Optical
neural networks are considered the next-generation physical implementations of
ANNs to break the bottleneck. This study proposes a spectral convolutional
neural network (SCNN) with the function of matter meta-imaging, namely
identifying the composition of matter and mapping its distribution in space.
This SCNN includes an optical convolutional layer (OCL) and a reconfigurable
electrical backend. The OCL is implemented by integrating very large-scale,
pixel-aligned metasurfaces on a CMOS image sensor, which accepts 3D raw
datacubes of natural images, containing two-spatial and one-spectral
dimensions, at megapixels directly as input to realize the matter meta-imaging.
This unique optoelectronic framework empowers in-sensor optical analog
computing at extremely high energy efficiency eliminating the need for coherent
light sources and greatly reducing the computing load of the electrical
backend. We employed the SCNN framework on several real-world complex tasks. It
achieved accuracies of 96.4% and 100% for pathological diagnosis and real-time
face anti-spoofing at video rate, respectively. The SCNN framework, with an
unprecedented new function of substance identification, provides a feasible
optoelectronic and integrated optical CNN implementation for edge devices or
cellphones with limited computing capabilities, facilitating diverse
applications, such as intelligent robotics, industrial automation, medical
diagnosis, and astronomy.


------------------------------------------------------------------------------

Title:
Detailed retinal vessel segmentation without human annotations using  simulated optical coherence tomography angiographs

Abstract: Optical coherence tomography angiography (OCTA) is a non-invasive imaging
modality that can acquire high-resolution volumes of the retinal vasculature
and aid the diagnosis of ocular, neurological and cardiac diseases.
Segmentation of the visible blood vessels is a common first step when
extracting quantitative biomarkers from these images. Classical segmentation
algorithms based on thresholding are strongly affected by image artifacts and
limited signal-to-noise ratio. The use of modern, deep learning-based
segmentation methods has been inhibited by a lack of large datasets with
detailed annotations of the blood vessels. To address this issue, recent work
has employed transfer learning, where a segmentation network is trained on
synthetic OCTA images and is then applied to real data. However, the previously
proposed simulation models are incapable of faithfully modeling the retinal
vasculature and do not provide effective domain adaptation. Because of this,
current methods are not able to fully segment the retinal vasculature, in
particular the smallest capillaries. In this work, we present a lightweight
simulation of the retinal vascular network based on space colonization for
faster and more realistic OCTA synthesis. Moreover, we introduce three contrast
adaptation pipelines to decrease the domain gap between real and artificial
images. We demonstrate the superior performance of our approach in extensive
quantitative and qualitative experiments on three public datasets that compare
our method to traditional computer vision algorithms and supervised training
using human annotations. Finally, we make our entire pipeline publicly
available, including the source code, pretrained models, and a large dataset of
synthetic OCTA images.


------------------------------------------------------------------------------

Title:
Distributed Semi-Supervised Sparse Statistical Inference

Abstract: This paper is devoted to studying the semi-supervised sparse statistical
inference in a distributed setup. An efficient multi-round distributed debiased
estimator, which integrates both labeled and unlabelled data, is developed. We
will show that the additional unlabeled data helps to improve the statistical
rate of each round of iteration. Our approach offers tailored debiasing methods
for $M$-estimation and generalized linear model according to the specific form
of the loss function. Our method also applies to a non-smooth loss like
absolute deviation loss. Furthermore, our algorithm is computationally
efficient since it requires only one estimation of a high-dimensional inverse
covariance matrix. We demonstrate the effectiveness of our method by presenting
simulation studies and real data applications that highlight the benefits of
incorporating unlabeled data.


------------------------------------------------------------------------------

Title:
Substitutional Alloying Using Crystal Graph Neural Networks

Abstract: Materials discovery, especially for applications that require extreme
operating conditions, requires extensive testing that naturally limits the
ability to inquire the wealth of possible compositions. Machine Learning (ML)
has nowadays a well established role in facilitating this effort in systematic
ways. The increasing amount of available accurate DFT data represents a solid
basis upon which new ML models can be trained and tested. While conventional
models rely on static descriptors, generally suitable for a limited class of
systems, the flexibility of Graph Neural Networks (GNNs) allows for direct
learning representations on graphs, such as the ones formed by crystals. We
utilize crystal graph neural networks (CGNN) to predict crystal properties with
DFT level accuracy, through graphs with encoding of the atomic (node/vertex),
bond (edge), and global state attributes. In this work, we aim at testing the
ability of the CGNN MegNet framework in predicting a number of properties of
systems previously unseen from the model, obtained by adding a substitutional
defect in bulk crystals that are included in the training set. We perform DFT
validation to assess the accuracy in the prediction of formation energies and
structural features (such as elastic moduli). Using CGNNs, one may identify
promising paths in alloy discovery.


------------------------------------------------------------------------------

Title:
Stretched sinograms for limited-angle tomographic reconstruction with  neural networks

Abstract: We present a direct method for limited angle tomographic reconstruction using
convolutional networks. The key to our method is to first stretch every tilt
view in the direction perpendicular to the tilt axis by the secant of the tilt
angle. These stretched views are then fed into a 2-D U-Net which directly
outputs the 3-D reconstruction. We train our networks by minimizing the mean
squared error between the network's generated reconstruction and a ground truth
3-D volume. To demonstrate and evaluate our method, we synthesize tilt views
from a 3-D image of fly brain tissue acquired with Focused Ion Beam Scanning
Electron Microscopy. We compare our method to using a U-Net to directly
reconstruct the unstretched tilt views and show that this simple stretching
procedure leads to significantly better reconstructions. We also compare to
using a network to clean up reconstructions generated by backprojection and
filtered backprojection, and find that this simple stretching procedure also
gives lower mean squared error on previously unseen images.


------------------------------------------------------------------------------

Title:
A VAE Approach to Sample Multivariate Extremes

Abstract: Generating accurate extremes from an observational data set is crucial when
seeking to estimate risks associated with the occurrence of future extremes
which could be larger than those already observed. Applications range from the
occurrence of natural disasters to financial crashes. Generative approaches
from the machine learning community do not apply to extreme samples without
careful adaptation. Besides, asymptotic results from extreme value theory (EVT)
give a theoretical framework to model multivariate extreme events, especially
through the notion of multivariate regular variation. Bridging these two
fields, this paper details a variational autoencoder (VAE) approach for
sampling multivariate heavy-tailed distributions, i.e., distributions likely to
have extremes of particularly large intensities. We illustrate the relevance of
our approach on a synthetic data set and on a real data set of discharge
measurements along the Danube river network. The latter shows the potential of
our approach for flood risks' assessment. In addition to outperforming the
standard VAE for the tested data sets, we also provide a comparison with a
competing EVT-based generative approach. On the tested cases, our approach
improves the learning of the dependency structure between extremes.


------------------------------------------------------------------------------

Title:
High-dimensional Contextual Bandit Problem without Sparsity

Abstract: In this research, we investigate the high-dimensional linear contextual
bandit problem where the number of features $p$ is greater than the budget $T$,
or it may even be infinite. Differing from the majority of previous works in
this field, we do not impose sparsity on the regression coefficients. Instead,
we rely on recent findings on overparameterized models, which enables us to
analyze the performance the minimum-norm interpolating estimator when data
distributions have small effective ranks. We propose an explore-then-commit
(EtC) algorithm to address this problem and examine its performance. Through
our analysis, we derive the optimal rate of the ETC algorithm in terms of $T$
and show that this rate can be achieved by balancing exploration and
exploitation. Moreover, we introduce an adaptive explore-then-commit (AEtC)
algorithm that adaptively finds the optimal balance. We assess the performance
of the proposed algorithms through a series of simulations.


------------------------------------------------------------------------------

Title:
A Survey of Contextual Optimization Methods for Decision Making under  Uncertainty

Abstract: Recently there has been a surge of interest in operations research (OR) and
the machine learning (ML) community in combining prediction algorithms and
optimization techniques to solve decision-making problems in the face of
uncertainty. This gave rise to the field of contextual optimization, under
which data-driven procedures are developed to prescribe actions to the
decision-maker that make the best use of the most recently updated information.
A large variety of models and methods have been presented in both OR and ML
literature under a variety of names, including data-driven optimization,
prescriptive optimization, predictive stochastic programming, policy
optimization, (smart) predict/estimate-then-optimize, decision-focused
learning, (task-based) end-to-end learning/forecasting/optimization, etc.
Focusing on single and two-stage stochastic programming problems, this review
article identifies three main frameworks for learning policies from data and
discusses their strengths and limitations. We present the existing models and
methods under a uniform notation and terminology and classify them according to
the three main frameworks identified. Our objective with this survey is to both
strengthen the general understanding of this active field of research and
stimulate further theoretical and algorithmic advancements in integrating ML
and stochastic programming.


------------------------------------------------------------------------------

Title:
CLIP2Protect: Protecting Facial Privacy using Text-Guided Makeup via  Adversarial Latent Search

Abstract: The success of deep learning based face recognition systems has given rise to
serious privacy concerns due to their ability to enable unauthorized tracking
of users in the digital world. Existing methods for enhancing privacy fail to
generate naturalistic images that can protect facial privacy without
compromising user experience. We propose a novel two-step approach for facial
privacy protection that relies on finding adversarial latent codes in the
low-dimensional manifold of a pretrained generative model. The first step
inverts the given face image into the latent space and finetunes the generative
model to achieve an accurate reconstruction of the given image from its latent
code. This step produces a good initialization, aiding the generation of
high-quality faces that resemble the given identity. Subsequently, user-defined
makeup text prompts and identity-preserving regularization are used to guide
the search for adversarial codes in the latent space. Extensive experiments
demonstrate that faces generated by our approach have stronger black-box
transferability with an absolute gain of 12.06% over the state-of-the-art
facial privacy protection approach under the face verification task. Finally,
we demonstrate the effectiveness of the proposed approach for commercial face
recognition systems. Our code is available at
this https URL


------------------------------------------------------------------------------

Title:
Enhancing variational quantum state diagonalization using reinforcement  learning techniques

Abstract: The development of variational quantum algorithms is crucial for the
application of NISQ computers. Such algorithms require short quantum circuits,
which are more amenable to implementation on near-term hardware, and many such
methods have been developed. One of particular interest is the so-called the
variational diagonalization method, which constitutes an important algorithmic
subroutine, and it can be used directly for working with data encoded in
quantum states. In particular, it can be applied to discern the features of
quantum states, such as entanglement properties of a system, or in quantum
machine learning algorithms. In this work, we tackle the problem of designing a
very shallow quantum circuit, required in the quantum state diagonalization
task, by utilizing reinforcement learning. To achieve this, we utilize a novel
encoding method that can be used to tackle the problem of circuit depth
optimization using a reinforcement learning approach. We demonstrate that our
approach provides a solid approximation to the diagonalization task while using
a small number of gates. The circuits proposed by the reinforcement learning
methods are shallower than the standard variational quantum state
diagonalization algorithm, and thus can be used in situations where the depth
of quantum circuits is limited by the hardware capabilities.


------------------------------------------------------------------------------

Title:
Memory-Constrained Algorithms for Convex Optimization via Recursive  Cutting-Planes

Abstract: We propose a family of recursive cutting-plane algorithms to solve
feasibility problems with constrained memory, which can also be used for
first-order convex optimization. Precisely, in order to find a point within a
ball of radius $\epsilon$ with a separation oracle in dimension $d$ -- or to
minimize $1$-Lipschitz convex functions to accuracy $\epsilon$ over the unit
ball -- our algorithms use $\mathcal O(\frac{d^2}{p}\ln \frac{1}{\epsilon})$
bits of memory, and make $\mathcal O((C\frac{d}{p}\ln \frac{1}{\epsilon})^p)$
oracle calls, for some universal constant $C \geq 1$. The family is
parametrized by $p\in[d]$ and provides an oracle-complexity/memory trade-off in
the sub-polynomial regime $\ln\frac{1}{\epsilon}\gg\ln d$. While several works
gave lower-bound trade-offs (impossibility results) -- we explicit here their
dependence with $\ln\frac{1}{\epsilon}$, showing that these also hold in any
sub-polynomial regime -- to the best of our knowledge this is the first class
of algorithms that provides a positive trade-off between gradient descent and
cutting-plane methods in any regime with $\epsilon\leq 1/\sqrt d$. The
algorithms divide the $d$ variables into $p$ blocks and optimize over blocks
sequentially, with approximate separation vectors constructed using a variant
of Vaidya's method. In the regime $\epsilon \leq d^{-\Omega(d)}$, our algorithm
with $p=d$ achieves the information-theoretic optimal memory usage and improves
the oracle-complexity of gradient descent.


------------------------------------------------------------------------------

Title:
Non-asymptotic System Identification for Linear Systems with Nonlinear  Policies

Abstract: This paper considers a single-trajectory system identification problem for
linear systems under general nonlinear and/or time-varying policies with i.i.d.
random excitation noises. The problem is motivated by safe learning-based
control for constrained linear systems, where the safe policies during the
learning process are usually nonlinear and time-varying for satisfying the
state and input constraints. In this paper, we provide a non-asymptotic error
bound for least square estimation when the data trajectory is generated by any
nonlinear and/or time-varying policies as long as the generated state and
action trajectories are bounded. This significantly generalizes the existing
non-asymptotic guarantees for linear system identification, which usually
consider i.i.d. random inputs or linear policies. Interestingly, our error
bound is consistent with that for linear policies with respect to the
dependence on the trajectory length, system dimensions, and excitation levels.
Lastly, we demonstrate the applications of our results by safe learning with
robust model predictive control and provide numerical analysis.


------------------------------------------------------------------------------

Title:
Evaluation of Speech Representations for MOS prediction

Abstract: In this paper, we evaluate feature extraction models for predicting speech
quality. We also propose a model architecture to compare embeddings of
supervised learning and self-supervised learning models with embeddings of
speaker verification models to predict the metric MOS. Our experiments were
performed on the VCC2018 dataset and a Brazilian-Portuguese dataset called
BRSpeechMOS, which was created for this work. The results show that the Whisper
model is appropriate in all scenarios: with both the VCC2018 and BRSpeech- MOS
datasets. Among the supervised and self-supervised learning models using
BRSpeechMOS, Whisper-Small achieved the best linear correlation of 0.6980, and
the speaker verification model, SpeakerNet, had linear correlation of 0.6963.
Using VCC2018, the best supervised and self-supervised learning model,
Whisper-Large, achieved linear correlation of 0.7274, and the best model
speaker verification, TitaNet, achieved a linear correlation of 0.6933.
Although the results of the speaker verification models are slightly lower, the
SpeakerNet model has only 5M parameters, making it suitable for real-time
applications, and the TitaNet model produces an embedding of size 192, the
smallest among all the evaluated models. The experiment results are
reproducible with publicly available source-code1 .


------------------------------------------------------------------------------

Title:
On the minimum number of arcs in $4$-dicritical oriented graphs

Abstract: The dichromatic number $\vec{\chi}(D)$ of a digraph $D$ is the minimum number
of colours needed to colour the vertices of a digraph such that each colour
class induces an acyclic subdigraph. A digraph $D$ is $k$-dicritical if
$\vec{\chi}(D) = k$ and each proper subdigraph $H$ of $D$ satisfies
$\vec{\chi}(H) < k$.
For integers $k$ and $n$, we define $d_k(n)$ (respectively $o_k(n)$) as the
minimum number of arcs possible in a $k$-dicritical digraph (respectively
oriented graph). Kostochka and Stiebitz have shown that $d_4(n) \geq
\frac{10}{3}n -\frac{4}{3}$. They also conjectured that there is a constant $c$
such that $o_k(n) \geq cd_k(n)$ for $k\geq 3$ and $n$ large enough. This
conjecture is known to be true for $k=3$ (Aboulker et al.).
In this work, we prove that every $4$-dicritical oriented graph on $n$
vertices has at least $(\frac{10}{3}+\frac{1}{51})n-1$ arcs, showing the
conjecture for $k=4$. We also characterise exactly the $k$-dicritical digraphs
on $n$ vertices with exactly $\frac{10}{3}n -\frac{4}{3}$ arcs.


------------------------------------------------------------------------------

Title:
Non-Contact Monitoring of Dehydration using RF Data Collected off the  Chest and the Hand

Abstract: We report a novel non-contact method for dehydration monitoring. We utilize a
transmit software defined radio (SDR) that impinges a wideband radio frequency
(RF) signal (of frequency 5.23 GHz) onto either the chest or the hand of a
subject who sits nearby. Further, another SDR in the closed vicinity collects
the RF signals reflected off the chest (or passed through the hand) of the
subject. Note that the two SDRs exchange orthogonal frequency division
multiplexing (OFDM) signal, whose individual subcarriers get modulated once it
reflects off (passes through) the chest (the hand) of the subject. This way,
the signal collected by the receive SDR consists of channel frequency response
(CFR) that captures the variation in the blood osmolality due to dehydration.
The received raw CFR data is then passed through a handful of machine learning
(ML) classifiers which once trained, output the classification result (i.e.,
whether a subject is hydrated or dehydrated). For the purpose of training our
ML classifiers, we have constructed our custom HCDDM-RF-5 dataset by collecting
data from 5 Muslim subjects (before and after sunset) who were fasting during
the month of Ramadan. Specifically, we have implemented and tested the
following ML classifiers (and their variants): K-nearest neighbour (KNN),
support vector machine (SVM), decision tree (DT), ensemble classifier, and
neural network classifier. Among all the classifiers, the neural network
classifier acheived the best classification accuracy, i.e., an accuracy of
93.8% for the proposed CBDM method, and an accuracy of 96.15% for the proposed
HBDM method. Compared to prior work where the reported accuracy is 97.83%, our
proposed non-contact method is slightly inferior (as we report a maximum
accuracy of 96.15%); nevertheless, the advantages of our non-contact
dehydration method speak for themselves.


------------------------------------------------------------------------------

Title:
Prediction model for rare events in longitudinal follow-up and  resampling methods

Abstract: We consider the problem of model building for rare events prediction in
longitudinal follow-up studies. In this paper, we compare several resampling
methods to improve standard regression models on a real life example. We
evaluate the effect of the sampling rate on the predictive performances of the
models. To evaluate the predictive performance of a longitudinal model, we
consider a validation technique that takes into account time and corresponds to
the actual use in real life.


------------------------------------------------------------------------------

Title:
MUBen: Benchmarking the Uncertainty of Pre-Trained Models for Molecular  Property Prediction

Abstract: Large Transformer models pre-trained on massive unlabeled molecular data have
shown great success in predicting molecular properties. However, these models
can be prone to overfitting during fine-tuning, resulting in over-confident
predictions on test data that fall outside of the training distribution. To
address this issue, uncertainty quantification (UQ) methods can be used to
improve the models' calibration of predictions. Although many UQ approaches
exist, not all of them lead to improved performance. While some studies have
used UQ to improve molecular pre-trained models, the process of selecting
suitable backbone and UQ methods for reliable molecular uncertainty estimation
remains underexplored. To address this gap, we present MUBen, which evaluates
different combinations of backbone and UQ models to quantify their performance
for both property prediction and uncertainty estimation. By fine-tuning various
backbone molecular representation models using different molecular descriptors
as inputs with UQ methods from different categories, we critically assess the
influence of architectural decisions and training strategies. Our study offers
insights for selecting UQ and backbone models, which can facilitate research on
uncertainty-critical applications in fields such as materials science and drug
discovery.


------------------------------------------------------------------------------

Title:
Creating Multi-Level Skill Hierarchies in Reinforcement Learning

Abstract: What is a useful skill hierarchy for an autonomous agent? We propose an
answer based on the graphical structure of an agent's interaction with its
environment. Our approach uses hierarchical graph partitioning to expose the
structure of the graph at varying timescales, producing a skill hierarchy with
multiple levels of abstraction. At each level of the hierarchy, skills move the
agent between regions of the state space that are well connected within
themselves but weakly connected to each other. We illustrate the utility of the
proposed skill hierarchy in a wide variety of domains in the context of
reinforcement learning.


------------------------------------------------------------------------------

Title:
PanoOcc: Unified Occupancy Representation for Camera-based 3D Panoptic  Segmentation

Abstract: Comprehensive modeling of the surrounding 3D world is key to the success of
autonomous driving. However, existing perception tasks like object detection,
road structure segmentation, depth & elevation estimation, and open-set object
localization each only focus on a small facet of the holistic 3D scene
understanding task. This divide-and-conquer strategy simplifies the algorithm
development procedure at the cost of losing an end-to-end unified solution to
the problem. In this work, we address this limitation by studying camera-based
3D panoptic segmentation, aiming to achieve a unified occupancy representation
for camera-only 3D scene understanding. To achieve this, we introduce a novel
method called PanoOcc, which utilizes voxel queries to aggregate spatiotemporal
information from multi-frame and multi-view images in a coarse-to-fine scheme,
integrating feature learning and scene representation into a unified occupancy
representation. We have conducted extensive ablation studies to verify the
effectiveness and efficiency of the proposed method. Our approach achieves new
state-of-the-art results for camera-based semantic segmentation and panoptic
segmentation on the nuScenes dataset. Furthermore, our method can be easily
extended to dense occupancy prediction and has shown promising performance on
the Occ3D benchmark. The code will be released at
this https URL


------------------------------------------------------------------------------

Title:
Shor's Algorithm Does Not Factor Large Integers in the Presence of Noise

Abstract: We consider Shor's quantum factoring algorithm in the setting of noisy
quantum gates. Under a generic model of random noise for (controlled) rotation
gates, we prove that the algorithm does not factor integers of the form $pq$
when the noise exceeds a vanishingly small level in terms of $n$ -- the number
of bits of the integer to be factored, where $p$ and $q$ are from a
well-defined set of primes of positive density. We further prove that with
probability $1 - o(1)$ over random prime pairs $(p,q)$, Shor's factoring
algorithm does not factor numbers of the form $pq$, with the same level of
random noise present.


------------------------------------------------------------------------------

Title:
High-order finite-volume integration schemes for subsonic  magnetohydrodynamics

Abstract: We present an efficient dimension-by-dimension finite-volume method which
solves the adiabatic magnetohydrodynamics equations at high discretization
order, using the constrained-transport approach on Cartesian grids. Results are
presented up to tenth order of accuracy. This method requires only one
reconstructed value per face for each computational cell. A passage through
high-order point values leads to a modest growth of computational cost with
increasing discretization order. At a given resolution, these high-order
schemes present significantly less numerical dissipation than commonly employed
lower-order approaches. Thus, results of comparable accuracy are achievable at
a substantially coarser resolution, yielding overall performance gains. We also
present a way to include physical dissipative terms: viscosity, magnetic
diffusivity and cooling functions, respecting the finite-volume and
constrained-transport frameworks.


------------------------------------------------------------------------------

Title:
Efficient Approximations of Complete Interatomic Potentials for Crystal  Property Prediction

Abstract: We study property prediction for crystal materials. A crystal structure
consists of a minimal unit cell that is repeated infinitely in 3D space. How to
accurately represent such repetitive structures in machine learning models
remains unresolved. Current methods construct graphs by establishing edges only
between nearby nodes, thereby failing to faithfully capture infinite repeating
patterns and distant interatomic interactions. In this work, we propose several
innovations to overcome these limitations. First, we propose to model
physics-principled interatomic potentials directly instead of only using
distances as in many existing methods. These potentials include the Coulomb
potential, London dispersion potential, and Pauli repulsion potential. Second,
we model the complete set of potentials among all atoms, instead of only
between nearby atoms as in existing methods. This is enabled by our
approximations of infinite potential summations with provable error bounds. We
further develop efficient algorithms to compute the approximations. Finally, we
propose to incorporate our computations of complete interatomic potentials into
message passing neural networks for representation learning. We perform
experiments on the JARVIS and Materials Project benchmarks for evaluation.
Results show that the use of interatomic potentials and complete interatomic
potentials leads to consistent performance improvements with reasonable
computational costs. Our code is publicly available as part of the AIRS library
(this https URL).


------------------------------------------------------------------------------

Title:
The STOIC2021 COVID-19 AI challenge: applying reusable training  methodologies to private data

Abstract: Challenges drive the state-of-the-art of automated medical image analysis.
The quantity of public training data that they provide can limit the
performance of their solutions. Public access to the training methodology for
these solutions remains absent. This study implements the Type Three (T3)
challenge format, which allows for training solutions on private data and
guarantees reusable training methodologies. With T3, challenge organizers train
a codebase provided by the participants on sequestered training data. T3 was
implemented in the STOIC2021 challenge, with the goal of predicting from a
computed tomography (CT) scan whether subjects had a severe COVID-19 infection,
defined as intubation or death within one month. STOIC2021 consisted of a
Qualification phase, where participants developed challenge solutions using
2000 publicly available CT scans, and a Final phase, where participants
submitted their training methodologies with which solutions were trained on CT
scans of 9724 subjects. The organizers successfully trained six of the eight
Final phase submissions. The submitted codebases for training and running
inference were released publicly. The winning solution obtained an area under
the receiver operating characteristic curve for discerning between severe and
non-severe COVID-19 of 0.815. The Final phase solutions of all finalists
improved upon their Qualification phase solutions.


------------------------------------------------------------------------------

Title:
Training shallow ReLU networks on noisy data using hinge loss: when do  we overfit and is it benign?

Abstract: We study benign overfitting in two-layer ReLU networks trained using gradient
descent and hinge loss on noisy data for binary classification. In particular,
we consider linearly separable data for which a relatively small proportion of
labels are corrupted or flipped. We identify conditions on the margin of the
clean data that give rise to three distinct training outcomes: benign
overfitting, in which zero loss is achieved and with high probability test data
is classified correctly; overfitting, in which zero loss is achieved but test
data is misclassified with probability lower bounded by a constant; and
non-overfitting, in which clean points, but not corrupt points, achieve zero
loss and again with high probability test data is classified correctly. Our
analysis provides a fine-grained description of the dynamics of neurons
throughout training and reveals two distinct phases: in the first phase clean
points achieve close to zero loss, in the second phase clean points oscillate
on the boundary of zero loss while corrupt points either converge towards zero
loss or are eventually zeroed by the network. We prove these results using a
combinatorial approach that involves bounding the number of clean versus
corrupt updates across these phases of training.


------------------------------------------------------------------------------

Title:
Ensemble Framework for Cardiovascular Disease Prediction

Abstract: Heart disease is the major cause of non-communicable and silent death
worldwide. Heart diseases or cardiovascular diseases are classified into four
types: coronary heart disease, heart failure, congenital heart disease, and
cardiomyopathy. It is vital to diagnose heart disease early and accurately in
order to avoid further injury and save patients' lives. As a result, we need a
system that can predict cardiovascular disease before it becomes a critical
situation. Machine learning has piqued the interest of researchers in the field
of medical sciences. For heart disease prediction, researchers implement a
variety of machine learning methods and approaches. In this work, to the best
of our knowledge, we have used the dataset from IEEE Data Port which is one of
the online available largest datasets for cardiovascular diseases individuals.
The dataset isa combination of Hungarian, Cleveland, Long Beach VA, Switzerland
& Statlog datasets with important features such as Maximum Heart Rate Achieved,
Serum Cholesterol, Chest Pain Type, Fasting blood sugar, and so on. To assess
the efficacy and strength of the developed model, several performance measures
are used, such as ROC, AUC curve, specificity, F1-score, sensitivity, MCC, and
accuracy. In this study, we have proposed a framework with a stacked ensemble
classifier using several machine learning algorithms including ExtraTrees
Classifier, Random Forest, XGBoost, and so on. Our proposed framework attained
an accuracy of 92.34% which is higher than the existing literature.


------------------------------------------------------------------------------

Title:
Studying Generalization on Memory-Based Methods in Continual Learning

Abstract: One of the objectives of Continual Learning is to learn new concepts
continually over a stream of experiences and at the same time avoid
catastrophic forgetting. To mitigate complete knowledge overwriting,
memory-based methods store a percentage of previous data distributions to be
used during training. Although these methods produce good results, few studies
have tested their out-of-distribution generalization properties, as well as
whether these methods overfit the replay memory. In this work, we show that
although these methods can help in traditional in-distribution generalization,
they can strongly impair out-of-distribution generalization by learning
spurious features and correlations. Using a controlled environment, the Synbol
benchmark generator (Lacoste et al., 2020), we demonstrate that this lack of
out-of-distribution generalization mainly occurs in the linear classifier.


------------------------------------------------------------------------------

Title:
Fairness in Multi-Task Learning via Wasserstein Barycenters

Abstract: Algorithmic Fairness is an established field in machine learning that aims to
reduce biases in data. Recent advances have proposed various methods to ensure
fairness in a univariate environment, where the goal is to de-bias a single
task. However, extending fairness to a multi-task setting, where more than one
objective is optimised using a shared representation, remains underexplored. To
bridge this gap, we develop a method that extends the definition of
\textit{Strong Demographic Parity} to multi-task learning using multi-marginal
Wasserstein barycenters. Our approach provides a closed form solution for the
optimal fair multi-task predictor including both regression and binary
classification tasks. We develop a data-driven estimation procedure for the
solution and run numerical experiments on both synthetic and real datasets. The
empirical results highlight the practical value of our post-processing
methodology in promoting fair decision-making.


------------------------------------------------------------------------------

Title:
Transferability of Winning Lottery Tickets in Neural Network  Differential Equation Solvers

Abstract: Recent work has shown that renormalisation group theory is a useful framework
with which to describe the process of pruning neural networks via iterative
magnitude pruning. This report formally describes the link between RG theory
and IMP and extends previous results around the Lottery Ticket Hypothesis and
Elastic Lottery Hypothesis to Hamiltonian Neural Networks for solving
differential equations. We find lottery tickets for two Hamiltonian Neural
Networks and demonstrate transferability between the two systems, with accuracy
being dependent on integration times. The universality of the two systems is
then analysed using tools from an RG perspective.


------------------------------------------------------------------------------

Title:
Investigating Prompting Techniques for Zero- and Few-Shot Visual  Question Answering

Abstract: Visual question answering (VQA) is a challenging task that requires the
ability to comprehend and reason with visual information. While recent
vision-language models have made strides, they continue to struggle with
zero-shot VQA, particularly in handling complex compositional questions and
adapting to new domains i.e. knowledge-based reasoning. This paper explores the
use of various prompting strategies, focusing on the BLIP2 model, to enhance
zero-shot VQA performance. We conduct a comprehensive investigation across
several VQA datasets, examining the effectiveness of different question
templates, the role of few-shot exemplars, the impact of chain-of-thought (CoT)
reasoning, and the benefits of incorporating image captions as additional
visual cues. Despite the varied outcomes, our findings demonstrate that
carefully designed question templates and the integration of additional visual
cues, like image captions, can contribute to improved VQA performance,
especially when used in conjunction with few-shot examples. However, we also
identify a limitation in the use of chain-of-thought rationalization, which
negatively affects VQA accuracy. Our study thus provides critical insights into
the potential of prompting for improving zero-shot VQA performance.


------------------------------------------------------------------------------

Title:
Evaluating Superhuman Models with Consistency Checks

Abstract: If machine learning models were to achieve superhuman abilities at various
reasoning or decision-making tasks, how would we go about evaluating such
models, given that humans would necessarily be poor proxies for ground truth?
In this paper, we propose a framework for evaluating superhuman models via
consistency checks. Our premise is that while the correctness of superhuman
decisions may be impossible to evaluate, we can still surface mistakes if the
model's decisions fail to satisfy certain logical, human-interpretable rules.
We instantiate our framework on three tasks where correctness of decisions is
hard to evaluate due to either superhuman model abilities, or to otherwise
missing ground truth: evaluating chess positions, forecasting future events,
and making legal judgments. We show that regardless of a model's (possibly
superhuman) performance on these tasks, we can discover logical inconsistencies
in decision making. For example: a chess engine assigning opposing valuations
to semantically identical boards; GPT-4 forecasting that sports records will
evolve non-monotonically over time; or an AI judge assigning bail to a
defendant only after we add a felony to their criminal record.


------------------------------------------------------------------------------

Title:
Lightweight Attribute Localizing Models for Pedestrian Attribute  Recognition

Abstract: Pedestrian Attribute Recognition (PAR) deals with the problem of identifying
features in a pedestrian image. It has found interesting applications in person
retrieval, suspect re-identification and soft biometrics. In the past few
years, several Deep Neural Networks (DNNs) have been designed to solve the
task; however, the developed DNNs predominantly suffer from
over-parameterization and high computational complexity. These problems hinder
them from being exploited in resource-constrained embedded devices with limited
memory and computational capacity. By reducing a network's layers using
effective compression techniques, such as tensor decomposition, neural network
compression is an effective method to tackle these problems. We propose novel
Lightweight Attribute Localizing Models (LWALM) for Pedestrian Attribute
Recognition (PAR). LWALM is a compressed neural network obtained after
effective layer-wise compression of the Attribute Localization Model (ALM)
using the Canonical Polyadic Decomposition with Error Preserving Correction
(CPD-EPC) algorithm.


------------------------------------------------------------------------------

Title:
Towards Quantum Federated Learning

Abstract: Quantum Federated Learning (QFL) is an emerging interdisciplinary field that
merges the principles of Quantum Computing (QC) and Federated Learning (FL),
with the goal of leveraging quantum technologies to enhance privacy, security,
and efficiency in the learning process. Currently, there is no comprehensive
survey for this interdisciplinary field. This review offers a thorough,
holistic examination of QFL. We aim to provide a comprehensive understanding of
the principles, techniques, and emerging applications of QFL. We discuss the
current state of research in this rapidly evolving field, identify challenges
and opportunities associated with integrating these technologies, and outline
future directions and open research questions. We propose a unique taxonomy of
QFL techniques, categorized according to their characteristics and the quantum
techniques employed. As the field of QFL continues to progress, we can
anticipate further breakthroughs and applications across various industries,
driving innovation and addressing challenges related to data privacy, security,
and resource optimization. This review serves as a first-of-its-kind
comprehensive guide for researchers and practitioners interested in
understanding and advancing the field of QFL.


------------------------------------------------------------------------------

Title:
A Metaheuristic-based Machine Learning Approach for Energy Prediction in  Mobile App Development

Abstract: Energy consumption plays a vital role in mobile App development for
developers and end-users, and it is considered one of the most crucial factors
for purchasing a smartphone. In addition, in terms of sustainability, it is
essential to find methods to reduce the energy consumption of mobile devices
since the extensive use of billions of smartphones worldwide significantly
impacts the environment. Despite the existence of several energy-efficient
programming practices in Android, the leading mobile ecosystem, machine
learning-based energy prediction algorithms for mobile App development have yet
to be reported. Therefore, this paper proposes a histogram-based gradient
boosting classification machine (HGBC), boosted by a metaheuristic approach,
for energy prediction in mobile App development. Our metaheuristic approach is
responsible for two issues. First, it finds redundant and irrelevant features
without any noticeable change in performance. Second, it performs a
hyper-parameter tuning for the HGBC algorithm. Since our proposed metaheuristic
approach is algorithm-independent, we selected 12 algorithms for the search
strategy to find the optimal search algorithm. Our finding shows that a
success-history-based parameter adaption for differential evolution with linear
population size (L-SHADE) offers the best performance. It can improve
performance and decrease the number of features effectively. Our extensive set
of experiments clearly shows that our proposed approach can provide significant
results for energy consumption prediction.


------------------------------------------------------------------------------

Title:
Query-Free Evasion Attacks Against Machine Learning-Based Malware  Detectors with Generative Adversarial Networks

Abstract: Malware detectors based on machine learning (ML) have been shown to be
susceptible to adversarial malware examples. However, current methods to
generate adversarial malware examples still have their limits. They either rely
on detailed model information (gradient-based attacks), or on detailed outputs
of the model - such as class probabilities (score-based attacks), neither of
which are available in real-world scenarios. Alternatively, adversarial
examples might be crafted using only the label assigned by the detector
(label-based attack) to train a substitute network or an agent using
reinforcement learning. Nonetheless, label-based attacks might require querying
a black-box system from a small number to thousands of times, depending on the
approach, which might not be feasible against malware detectors. This work
presents a novel query-free approach to craft adversarial malware examples to
evade ML-based malware detectors. To this end, we have devised a GAN-based
framework to generate adversarial malware examples that look similar to benign
executables in the feature space. To demonstrate the suitability of our
approach we have applied the GAN-based attack to three common types of features
usually employed by static ML-based malware detectors: (1) Byte histogram
features, (2) API-based features, and (3) String-based features. Results show
that our model-agnostic approach performs on par with MalGAN, while generating
more realistic adversarial malware examples without requiring any query to the
malware detectors. Furthermore, we have tested the generated adversarial
examples against state-of-the-art multimodal and deep learning malware
detectors, showing a decrease in detection performance, as well as a decrease
in the average number of detections by the anti-malware engines in VirusTotal.


------------------------------------------------------------------------------

Title:
Amortized Inference for Gaussian Process Hyperparameters of Structured  Kernels

Abstract: Learning the kernel parameters for Gaussian processes is often the
computational bottleneck in applications such as online learning, Bayesian
optimization, or active learning. Amortizing parameter inference over different
datasets is a promising approach to dramatically speed up training time.
However, existing methods restrict the amortized inference procedure to a fixed
kernel structure. The amortization network must be redesigned manually and
trained again in case a different kernel is employed, which leads to a large
overhead in design time and training time. We propose amortizing kernel
parameter inference over a complete kernel-structure-family rather than a fixed
kernel structure. We do that via defining an amortization network over pairs of
datasets and kernel structures. This enables fast kernel inference for each
element in the kernel family without retraining the amortization network. As a
by-product, our amortization network is able to do fast ensembling over kernel
structures. In our experiments, we show drastically reduced inference time
combined with competitive test performance for a large set of kernels and
datasets.


------------------------------------------------------------------------------

Title:
Practical Sharpness-Aware Minimization Cannot Converge All the Way to  Optima

Abstract: Sharpness-Aware Minimization (SAM) is an optimizer that takes a descent step
based on the gradient at a perturbation $y_t = x_t + \rho \frac{\nabla
f(x_t)}{\lVert \nabla f(x_t) \rVert}$ of the current point $x_t$. Existing
studies prove convergence of SAM for smooth functions, but they do so by
assuming decaying perturbation size $\rho$ and/or no gradient normalization in
$y_t$, which is detached from practice. To address this gap, we study
deterministic/stochastic versions of SAM with practical configurations (i.e.,
constant $\rho$ and gradient normalization in $y_t$) and explore their
convergence properties on smooth functions with (non)convexity assumptions.
Perhaps surprisingly, in many scenarios, we find out that SAM has limited
capability to converge to global minima or stationary points. For smooth
strongly convex functions, we show that while deterministic SAM enjoys tight
global convergence rates of $\tilde \Theta(\frac{1}{T^2})$, the convergence
bound of stochastic SAM suffers an inevitable additive term $O(\rho^2)$,
indicating convergence only up to neighborhoods of optima. In fact, such
$O(\rho^2)$ factors arise for stochastic SAM in all the settings we consider,
and also for deterministic SAM in nonconvex cases; importantly, we prove by
examples that such terms are unavoidable. Our results highlight vastly
different characteristics of SAM with vs. without decaying perturbation size or
gradient normalization, and suggest that the intuitions gained from one version
may not apply to the other.


------------------------------------------------------------------------------

Title:
Friend or Foe? Exploring the Implications of Large Language Models on  the Science System

Abstract: The advent of ChatGPT by OpenAI has prompted extensive discourse on its
potential implications for science and higher education. While the impact on
education has been a primary focus, there is limited empirical research on the
effects of large language models (LLMs) and LLM-based chatbots on science and
scientific practice. To investigate this further, we conducted a Delphi study
involving 72 experts specialising in research and AI. The study focused on
applications and limitations of LLMs, their effects on the science system,
ethical and legal considerations, and the required competencies for their
effective use. Our findings highlight the transformative potential of LLMs in
science, particularly in administrative, creative, and analytical tasks.
However, risks related to bias, misinformation, and quality assurance need to
be addressed through proactive regulation and science education. This research
contributes to informed discussions on the impact of generative AI in science
and helps identify areas for future action.


------------------------------------------------------------------------------

Title:
The Use of Web Archives in Disinformation Research

Abstract: In recent years, journalists and other researchers have used web archives as
an important resource for their study of disinformation. This paper provides
several examples of this use and also brings together some of the work that the
Old Dominion University Web Science and Digital Libraries (WS-DL) research
group has done in this area. We will show how web archives have been used to
investigate changes to webpages, study archived social media including deleted
content, and study known disinformation that has been archived.


------------------------------------------------------------------------------

Title:
Data Protection for Data Privacy-A South African Problem?

Abstract: This study proposes a comprehensive framework for enhancing data security and
privacy within organizations through data protection awareness. It employs a
quantitative method and survey research strategy to assess the level of data
protection awareness among employees of a public organization.


------------------------------------------------------------------------------

Title:
Evolutionary Algorithms in the Light of SGD: Limit Equivalence, Minima  Flatness, and Transfer Learning

Abstract: Whenever applicable, the Stochastic Gradient Descent (SGD) has shown itself
to be unreasonably effective. Instead of underperforming and getting trapped in
local minima due to the batch noise, SGD leverages it to learn to generalize
better and find minima that are good enough for the entire dataset. This led to
numerous theoretical and experimental investigations, especially in the context
of Artificial Neural Networks (ANNs), leading to better machine learning
algorithms. However, SGD is not applicable in a non-differentiable setting,
leaving all that prior research off the table.
In this paper, we show that a class of evolutionary algorithms (EAs) inspired
by the Gillespie-Orr Mutational Landscapes model for natural evolution is
formally equivalent to SGD in certain settings and, in practice, is well
adapted to large ANNs. We refer to such EAs as Gillespie-Orr EA class (GO-EAs)
and empirically show how an insight transfer from SGD can work for them. We
then show that for ANNs trained to near-optimality or in the transfer learning
setting, the equivalence also allows transferring the insights from the
Mutational Landscapes model to SGD.
We then leverage this equivalence to experimentally show how SGD and GO-EAs
can provide mutual insight through examples of minima flatness, transfer
learning, and mixing of individuals in EAs applied to large models.


------------------------------------------------------------------------------

Title:
Sheffield's Submission to the AmericasNLP Shared Task on Machine  Translation into Indigenous Languages

Abstract: In this paper we describe the University of Sheffield's submission to the
AmericasNLP 2023 Shared Task on Machine Translation into Indigenous Languages
which comprises the translation from Spanish to eleven indigenous languages.
Our approach consists of extending, training, and ensembling different
variations of NLLB-200. We use data provided by the organizers and data from
various other sources such as constitutions, handbooks, news articles, and
backtranslations generated from monolingual data. On the dev set, our best
submission outperforms the baseline by 11% average chrF across all languages,
with substantial improvements particularly for Aymara, Guarani and Quechua. On
the test set, we achieve the highest average chrF of all the submissions, we
rank first in four of the eleven languages, and at least one of our submissions
ranks in the top 3 for all languages.


------------------------------------------------------------------------------

Title:
Joint multi-modal Self-Supervised pre-training in Remote Sensing:  Application to Methane Source Classification

Abstract: With the current ubiquity of deep learning methods to solve computer vision
and remote sensing specific tasks, the need for labelled data is growing
constantly. However, in many cases, the annotation process can be long and
tedious depending on the expertise needed to perform reliable annotations. In
order to alleviate this need for annotations, several self-supervised methods
have recently been proposed in the literature. The core principle behind these
methods is to learn an image encoder using solely unlabelled data samples. In
earth observation, there are opportunities to exploit domain-specific remote
sensing image data in order to improve these methods. Specifically, by
leveraging the geographical position associated with each image, it is possible
to cross reference a location captured from multiple sensors, leading to
multiple views of the same locations. In this paper, we briefly review the core
principles behind so-called joint-embeddings methods and investigate the usage
of multiple remote sensing modalities in self-supervised pre-training. We
evaluate the final performance of the resulting encoders on the task of methane
source classification.


------------------------------------------------------------------------------

Title:
You Don't Need Robust Machine Learning to Manage Adversarial Attack  Risks

Abstract: The robustness of modern machine learning (ML) models has become an
increasing concern within the community. The ability to subvert a model into
making errant predictions using seemingly inconsequential changes to input is
startling, as is our lack of success in building models robust to this concern.
Existing research shows progress, but current mitigations come with a high cost
and simultaneously reduce the model's accuracy. However, such trade-offs may
not be necessary when other design choices could subvert the risk. In this
survey we review the current literature on attacks and their real-world
occurrences, or limited evidence thereof, to critically evaluate the real-world
risks of adversarial machine learning (AML) for the average entity. This is
done with an eye toward how one would then mitigate these attacks in practice,
the risks for production deployment, and how those risks could be managed. In
doing so we elucidate that many AML threats do not warrant the cost and
trade-offs of robustness due to a low likelihood of attack or availability of
superior non-ML mitigations. Our analysis also recommends cases where an actor
should be concerned about AML to the degree where robust ML models are
necessary for a complete deployment.


------------------------------------------------------------------------------

Title:
Towards a theory of natural directed paths

Abstract: We introduce the abstract setting of presheaf category on a thick category of
cubes. Precubical sets, symmetric transverse sets, symmetric precubical sets
and the new category of (non-symmetric) transverse sets are examples of this
structure. All these presheaf categories share the same metric and homotopical
properties from a directed homotopy point of view. This enables us to extend
Raussen's notion of natural $d$-path for each of them. Finally, we adapt
Ziemia\'{n}ski's notion of cube chain to this abstract setting and we prove
that it has the expected behavior on precubical sets. As an application, we
verify that the formalization of the parallel composition with synchronization
of process algebra using the coskeleton functor of the category of symmetric
transverse sets has a category of cube chains with the correct homotopy type.


------------------------------------------------------------------------------

Title:
The Evolution theory of Learning: From Natural Selection to  Reinforcement Learning

Abstract: Evolution is a fundamental process that shapes the biological world we
inhabit, and reinforcement learning is a powerful tool used in artificial
intelligence to develop intelligent agents that learn from their environment.
In recent years, researchers have explored the connections between these two
seemingly distinct fields, and have found compelling evidence that they are
more closely related than previously thought. This paper examines these
connections and their implications, highlighting the potential for
reinforcement learning principles to enhance our understanding of evolution and
the role of feedback in evolutionary systems.


------------------------------------------------------------------------------

Title:
Autophonic Loudness of Singers in Simulated Room Acoustic Environments

Abstract: This paper aims to study the effect of room acoustics and phonemes on the
perception of loudness of one's own voice (autophonic loudness) for a group of
trained singers. For a set of five phonemes, 20 singers vocalized over several
autophonic loudness ratios, while maintaining pitch constancy over extreme
voice levels, within five simulated rooms. There were statistically significant
differences in the slope of the autophonic loudness function (logarithm of
autophonic loudness as a function of voice sound pressure level) for the five
phonemes, with slopes ranging from 1.3 (/a:/) to 2.0 (/z:/). There was no
significant variation in the autophonic loudness function slopes with
variations in room acoustics. The autophonic room response, which represents a
systematic decrease in voice levels with increasing levels of room reflections,
was also studied, with some evidence found in support. Overall, the average
slope of the autophonic room response for the three corner vowels (/a:/, /i:/,
and /u:/) was -1.4 for medium autophonic loudness. The findings relating to the
slope of the autophonic loudness function are in agreement with the findings of
previous studies where the sensorimotor mechanisms in regulating voice were
shown to be more important in the perception of autophonic loudness than
hearing of room acoustics. However, the role of room acoustics, in terms of the
autophonic room response, is shown to be more complicated, requiring further
inquiry. Overall, it is shown that autophonic loudness grows at more than twice
the rate of loudness growth for sounds created outside the human body.


------------------------------------------------------------------------------

Title:
CML-TTS A Multilingual Dataset for Speech Synthesis in Low-Resource  Languages

Abstract: In this paper, we present CML-TTS, a recursive acronym for
CML-Multi-Lingual-TTS, a new Text-to-Speech (TTS) dataset developed at the
Center of Excellence in Artificial Intelligence (CEIA) of the Federal
University of Goias (UFG). CML-TTS is based on Multilingual LibriSpeech (MLS)
and adapted for training TTS models, consisting of audiobooks in seven
languages: Dutch, French, German, Italian, Portuguese, Polish, and Spanish.
Additionally, we provide the YourTTS model, a multi-lingual TTS model, trained
using 3,176.13 hours from CML-TTS and also with 245.07 hours from LibriTTS, in
English. Our purpose in creating this dataset is to open up new research
possibilities in the TTS area for multi-lingual models. The dataset is publicly
available under the CC-BY 4.0 license1.


------------------------------------------------------------------------------

Title:
Flow-Bench: A Dataset for Computational Workflow Anomaly Detection

Abstract: A computational workflow, also known as workflow, consists of tasks that must
be executed in a specific order to attain a specific goal. Often, in fields
such as biology, chemistry, physics, and data science, among others, these
workflows are complex and are executed in large-scale, distributed, and
heterogeneous computing environments that are prone to failures and performance
degradations. Therefore, anomaly detection for workflows is an important
paradigm that aims to identify unexpected behavior or errors in workflow
execution. This crucial task to improve the reliability of workflow executions
must be assisted by machine learning-based techniques. However, such
application is limited, in large part, due to the lack of open datasets and
benchmarking. To address this gap, we make the following contributions in this
paper: (1) we systematically inject anomalies and collect raw execution logs
from workflows executing on distributed infrastructures; (2) we summarize the
statistics of new datasets, as well as a set of open datasets, and provide
insightful analyses; (3) we benchmark unsupervised anomaly detection techniques
by converting workflows into both tabular and graph-structured data. Our
findings allow us to examine the effectiveness and efficiencies of the
benchmark methods and identify potential research opportunities for improvement
and generalization. The dataset and benchmark code are available online with
MIT License for public usage.


------------------------------------------------------------------------------

Title:
ClinicalGPT: Large Language Models Finetuned with Diverse Medical Data  and Comprehensive Evaluation

Abstract: Large language models have exhibited exceptional performance on various
Natural Language Processing (NLP) tasks, leveraging techniques such as the
pre-training, and instruction fine-tuning. Despite these advances, their
effectiveness in medical applications is limited, due to challenges such as
factual inaccuracies, reasoning abilities, and lack grounding in real-world
experience. In this study, we present ClinicalGPT, a language model explicitly
designed and optimized for clinical scenarios. By incorporating extensive and
diverse real-world data, such as medical records, domain-specific knowledge,
and multi-round dialogue consultations in the training process, ClinicalGPT is
better prepared to handle multiple clinical task. Furthermore, we introduce a
comprehensive evaluation framework that includes medical knowledge
question-answering, medical exams, patient consultations, and diagnostic
analysis of medical records. Our results demonstrate that ClinicalGPT
significantly outperforms other models in these tasks, highlighting the
effectiveness of our approach in adapting large language models to the critical
domain of healthcare.


------------------------------------------------------------------------------

Title:
Coaching a Teachable Student

Abstract: We propose a novel knowledge distillation framework for effectively teaching
a sensorimotor student agent to drive from the supervision of a privileged
teacher agent. Current distillation for sensorimotor agents methods tend to
result in suboptimal learned driving behavior by the student, which we
hypothesize is due to inherent differences between the input, modeling
capacity, and optimization processes of the two agents. We develop a novel
distillation scheme that can address these limitations and close the gap
between the sensorimotor agent and its privileged teacher. Our key insight is
to design a student which learns to align their input features with the
teacher's privileged Bird's Eye View (BEV) space. The student then can benefit
from direct supervision by the teacher over the internal representation
learning. To scaffold the difficult sensorimotor learning task, the student
model is optimized via a student-paced coaching mechanism with various
auxiliary supervision. We further propose a high-capacity imitation learned
privileged agent that surpasses prior privileged agents in CARLA and ensures
the student learns safe driving behavior. Our proposed sensorimotor agent
results in a robust image-based behavior cloning agent in CARLA, improving over
current models by over 20.6% in driving score without requiring LiDAR,
historical observations, ensemble of models, on-policy data aggregation or
reinforcement learning.


------------------------------------------------------------------------------

Title:
Calculation of the transient response of lossless transmission lines

Abstract: We present an analytical calculation of the transient response of ideal (i.e.
lossless) transmission lines. The calculation presented considers a length of
transmission line connected to a signal generator with output impedance
$Z_\mathrm{g}$ and terminated with a load impedance $Z_\mathrm{L}$. The
approach taken is to analyze a circuit model of the system in the
complex-frequency or $s$-domain and then apply an inverse Laplace transform to
recover the time-domain response. We consider both rectangular pulses and
voltage steps (i.e. the Heaviside function) applied to the input of the
transmission line. Initially, we assume that $Z_\mathrm{g}$ and $Z_\mathrm{L}$
are purely real/resistive. At the end of the paper, we demonstrate how the
calculations can be generalized to consider reactive impedances.


------------------------------------------------------------------------------

Title:
Going public: the role of public participation approaches in commercial  AI labs

Abstract: In recent years, discussions of responsible AI practices have seen growing
support for "participatory AI" approaches, intended to involve members of the
public in the design and development of AI systems. Prior research has
identified a lack of standardised methods or approaches for how to use
participatory approaches in the AI development process. At present, there is a
dearth of evidence on attitudes to and approaches for participation in the
sites driving major AI developments: commercial AI labs. Through 12
semi-structured interviews with industry practitioners and subject-matter
experts, this paper explores how commercial AI labs understand participatory AI
approaches and the obstacles they have faced implementing these practices in
the development of AI systems and research. We find that while interviewees
view participation as a normative project that helps achieve "societally
beneficial" AI systems, practitioners face numerous barriers to embedding
participatory approaches in their companies: participation is expensive and
resource intensive, it is "atomised" within companies, there is concern about
exploitation, there is no incentive to be transparent about its adoption, and
it is complicated by a lack of clear context. These barriers result in a
piecemeal approach to participation that confers no decision-making power to
participants and has little ongoing impact for AI labs. This papers
contribution is to provide novel empirical research on the implementation of
public participation in commercial AI labs, and shed light on the current
challenges of using participatory approaches in this context.


------------------------------------------------------------------------------

Title:
Just One Byte (per gradient): A Note on Low-Bandwidth Decentralized  Language Model Finetuning Using Shared Randomness

Abstract: Language model training in distributed settings is limited by the
communication cost of gradient exchanges. In this short note, we extend recent
work from Malladi et al. (2023), using shared randomness to perform distributed
fine-tuning with low bandwidth. The method is a natural decentralized extension
of memory-efficient Simultaneous Perturbation Stochastic Approximation (SPSA).
Each iteration, each machine seeds a Random Number Generator (RNG) to perform
local reproducible perturbations on model weights and calculate and exchange
scalar projected gradients, which are then used to update each model. By using
a (machine, sample) identifier as the random seed, each model can regenerate
one another's perturbations. As machines only exchange single-byte projected
gradients, this is highly communication efficient. There are also potential
privacy benefits, as projected gradients may be calculated on different
training data, and models never access the other's data. Our approach not only
drastically reduces communication bandwidth requirements but also accommodates
dynamic addition or removal of machines during the training process and retains
the memory-efficient and inference-only advantages of recent work. We perform
proof-of-concept experiments to demonstrate the potential usefulness of this
method, building off of rich literature on distributed optimization and
memory-efficient training.


------------------------------------------------------------------------------

Title:
On the Interplay of Subset Selection and Informed Graph Neural Networks

Abstract: Machine learning techniques paired with the availability of massive datasets
dramatically enhance our ability to explore the chemical compound space by
providing fast and accurate predictions of molecular properties. However,
learning on large datasets is strongly limited by the availability of
computational resources and can be infeasible in some scenarios. Moreover, the
instances in the datasets may not yet be labelled and generating the labels can
be costly, as in the case of quantum chemistry computations. Thus, there is a
need to select small training subsets from large pools of unlabelled data
points and to develop reliable ML methods that can effectively learn from small
training sets. This work focuses on predicting the molecules atomization energy
in the QM9 dataset. We investigate the advantages of employing domain
knowledge-based data sampling methods for an efficient training set selection
combined with informed ML techniques. In particular, we show how maximizing
molecular diversity in the training set selection process increases the
robustness of linear and nonlinear regression techniques such as kernel methods
and graph neural networks. We also check the reliability of the predictions
made by the graph neural network with a model-agnostic explainer based on the
rate distortion explanation framework.


------------------------------------------------------------------------------

Title:
Unsupervised Learning of Style-Aware Facial Animation from Real Acting  Performances

Abstract: This paper presents a novel approach for text/speech-driven animation of a
photo-realistic head model based on blend-shape geometry, dynamic textures, and
neural rendering. Training a VAE for geometry and texture yields a parametric
model for accurate capturing and realistic synthesis of facial expressions from
a latent feature vector. Our animation method is based on a conditional CNN
that transforms text or speech into a sequence of animation parameters. In
contrast to previous approaches, our animation model learns
disentangling/synthesizing different acting-styles in an unsupervised manner,
requiring only phonetic labels that describe the content of training sequences.
For realistic real-time rendering, we train a U-Net that refines
rasterization-based renderings by computing improved pixel colors and a
foreground matte. We compare our framework qualitatively/quantitatively against
recent methods for head modeling as well as facial animation and evaluate the
perceived rendering/animation quality in a user-study, which indicates large
improvements compared to state-of-the-art approaches


------------------------------------------------------------------------------

Title:
Boundary Blending: Reconsidering the Design of Multi-View Visualizations

Abstract: Multiple-view visualizations (MVs) have been widely used for visual analysis.
Each view shows some part of the data in a usable way, and together multiple
views enable a holistic understanding of the data under investigation. For
example, an analyst may check a social network graph, a map of sensitive
locations, a table of transaction records, and a collection of reports to
identify suspicious activities. While each view is designed to preserve its own
visual context with visible borders or perceivable spatial distance from
others, the key to solving real-world analysis problems often requires
"breaking" such boundaries, and further integrating and synthesizing the data
scattered across multiple views. This calls for blending the boundaries in MVs,
instead of simply breaking them, which brings key questions: what are possible
boundaries in MVs, and what are design options that can support the boundary
blending in MVs? To answer these questions, we present three boundaries in MVs:
1) data boundary, 2) representation boundary, and 3) semantic boundary,
corresponding to three major aspects regarding the usage of MVs: encoded
information, visual representation, and interpretation. Then, we discuss four
design strategies (highlighting, linking, embedding, and extending) and their
pros and cons for supporting boundary blending in MVs. We conclude our
discussion with future research opportunities.


------------------------------------------------------------------------------

Title:
Stable nodal projection method on octree grids

Abstract: We propose a novel collocated projection method for solving the
incompressible Navier-Stokes equations with arbitrary boundaries. Our approach
employs non-graded octree grids, where all variables are stored at the nodes.
To discretize the viscosity and projection steps, we utilize supra-convergent
finite difference approximations with sharp boundary treatments. We demonstrate
the stability of our projection on uniform grids, identify a sufficient
stability condition on adaptive grids, and validate these findings numerically.
We further demonstrate the accuracy and capabilities of our solver with several
canonical two- and three-dimensional simulations of incompressible fluid flows.
Overall, our method is second-order accurate, allows for dynamic grid
adaptivity with arbitrary geometries, and reduces the overhead in code
development through data collocation.


------------------------------------------------------------------------------

Title:
Building Blocks for a Complex-Valued Transformer Architecture

Abstract: Most deep learning pipelines are built on real-valued operations to deal with
real-valued inputs such as images, speech or music signals. However, a lot of
applications naturally make use of complex-valued signals or images, such as
MRI or remote sensing. Additionally the Fourier transform of signals is
complex-valued and has numerous applications. We aim to make deep learning
directly applicable to these complex-valued signals without using projections
into $\mathbb{R}^2$. Thus we add to the recent developments of complex-valued
neural networks by presenting building blocks to transfer the transformer
architecture to the complex domain. We present multiple versions of a
complex-valued Scaled Dot-Product Attention mechanism as well as a
complex-valued layer normalization. We test on a classification and a sequence
generation task on the MusicNet dataset and show improved robustness to
overfitting while maintaining on-par performance when compared to the
real-valued transformer architecture.


------------------------------------------------------------------------------

Title:
Efficient HDR Reconstruction from Real-World Raw Images

Abstract: High dynamic range (HDR) imaging is still a significant yet challenging
problem due to the limited dynamic range of generic image sensors. Most
existing learning-based HDR reconstruction methods take a set of
bracketed-exposure sRGB images to extend the dynamic range, and thus are
computational- and memory-inefficient by requiring the Image Signal Processor
(ISP) to produce multiple sRGB images from the raw ones. In this paper, we
propose to broaden the dynamic range from the raw inputs and perform only one
ISP processing for the reconstructed HDR raw image. Our key insights are
threefold: (1) we design a new computational raw HDR data formation pipeline
and construct the first real-world raw HDR dataset, RealRaw-HDR; (2) we develop
a lightweight-efficient HDR model, RepUNet, using the structural
re-parameterization technique; (3) we propose a plug-and-play motion alignment
loss to mitigate motion misalignment between short- and long-exposure images.
Extensive experiments demonstrate that our approach achieves state-of-the-art
performance in both visual quality and quantitative metrics.


------------------------------------------------------------------------------

Title:
Improving Spectrum-Based Localization of Multiple Faults by Iterative  Test Suite Reduction

Abstract: Spectrum-based fault localization (SBFL) works well for single-fault programs
but its accuracy decays for increasing fault numbers. We present FLITSR (Fault
Localization by Iterative Test Suite Reduction), a novel SBFL extension that
improves the localization of a given base metric specifically in the presence
of multiple faults. FLITSR iteratively selects reduced versions of the test
suite that better localize the individual faults in the system. This allows it
to identify and re-rank faults ranked too low by the base metric because they
were masked by other program elements. We evaluated FLITSR over method-level
spectra from an existing large synthetic dataset comprising 75000 variants of
15 open-source projects with up to 32 injected faults, as well as method-level
and statement-level spectra from a new dataset with 326 true multi-fault
versions from the Defects4J benchmark set containing up to 14 real faults. For
all three spectrum types we consistently see substantial reductions of the
average wasted efforts at different fault levels, of 30%-90% over the best base
metric, and generally similarly large increases in precision and recall, albeit
with larger variance across the underlying projects. For the method-level real
faults, FLITSR also substantially outperforms GRACE, a state-of-the-art
learning-based fault localizer.


------------------------------------------------------------------------------

Title:
DisasterNets: Embedding Machine Learning in Disaster Mapping

Abstract: Disaster mapping is a critical task that often requires on-site experts and
is time-consuming. To address this, a comprehensive framework is presented for
fast and accurate recognition of disasters using machine learning, termed
DisasterNets. It consists of two stages, space granulation and attribute
granulation. The space granulation stage leverages supervised/semi-supervised
learning, unsupervised change detection, and domain adaptation with/without
source data techniques to handle different disaster mapping scenarios.
Furthermore, the disaster database with the corresponding geographic
information field properties is built by using the attribute granulation stage.
The framework is applied to earthquake-triggered landslide mapping and
large-scale flood mapping. The results demonstrate a competitive performance
for high-precision, high-efficiency, and cross-scene recognition of disasters.
To bridge the gap between disaster mapping and machine learning communities, we
will provide an openly accessible tool based on DisasterNets. The framework and
tool will be available at this https URL


------------------------------------------------------------------------------

Title:
Direct parametrisation of invariant manifolds for generic non-autonomous  systems including superharmonic resonances

Abstract: The direct parametrisation method for invariant manifold is a model-order
reduction technique that can be directly applied to finite element problems in
order to derive efficient and converged reduced-order models (ROMs) for
non-linear structures. In the field of nonlinear vibrations, it has already
been applied to autonomous and non-autonomous problems in order to propose ROMs
that can compute backbones and frequency-response curves of structures with
geometric nonlinearity. While previous developments used a first-order
development in order to cope with the non-autonomous term, this assumption is
here relaxed by proposing a completely different treatment. The key idea is to
enlarge the dimension of the dynamical system to make it autonomous and treat
the added coordinates related to the forcing as already being written with
normal coordinates. The parametrisation method is derived with this starting
assumption and, as a key consequence, the resonance relationships appearing
through the homological equations involve multiple occurrences of the forcing
frequency, showing that with this new development, one is able to compute ROMs
for superharmonic resonance. The method is implemented and validated on
academic test cases involving beams and arches. It is numerically demonstrated
that the method generates efficient ROMs for 3:1 and 2:1 superharmonic
resonances, as well as converged results for problems where the first-order
truncation on the non-autonomous terms used in previous developments showed a
clear limitation.


------------------------------------------------------------------------------

Title:
RealImpact: A Dataset of Impact Sound Fields for Real Objects

Abstract: Objects make unique sounds under different perturbations, environment
conditions, and poses relative to the listener. While prior works have modeled
impact sounds and sound propagation in simulation, we lack a standard dataset
of impact sound fields of real objects for audio-visual learning and
calibration of the sim-to-real gap. We present RealImpact, a large-scale
dataset of real object impact sounds recorded under controlled conditions.
RealImpact contains 150,000 recordings of impact sounds of 50 everyday objects
with detailed annotations, including their impact locations, microphone
locations, contact force profiles, material labels, and RGBD images. We make
preliminary attempts to use our dataset as a reference to current simulation
methods for estimating object impact sounds that match the real world.
Moreover, we demonstrate the usefulness of our dataset as a testbed for
acoustic and audio-visual learning via the evaluation of two benchmark tasks,
including listener location classification and visual acoustic matching.


------------------------------------------------------------------------------

Title:
Uncited articles and their effect on the concentration of citations

Abstract: Empirical evidence demonstrates that citations received by scholarly
publications follow a pattern of preferential attachment, resulting in a
power-law distribution. Such asymmetry has sparked significant debate regarding
the use of citations for research evaluation. However, a consensus has yet to
be established concerning the historical trends in citation concentration. Are
citations becoming more concentrated in a small number of articles? Or have
recent geopolitical and technical changes in science led to more decentralized
distributions? This ongoing debate stems from a lack of technical clarity in
measuring inequality. Given the variations in citation practices across
disciplines and over time, it is crucial to account for multiple factors that
can influence the findings. This article explores how reference-based and
citation-based approaches, uncited articles, citation inflation, the expansion
of bibliometric databases, disciplinary differences, and self-citations affect
the evolution of citation concentration. Our results indicate a decreasing
trend in citation concentration, primarily driven by a decline in uncited
articles, which, in turn, can be attributed to the growing significance of Asia
and Europe. On the whole, our findings clarify current debates on citation
concentration and show that, contrary to a widely-held belief, citations are
increasingly scattered.


------------------------------------------------------------------------------

Title:
A Vortex Damping Outflow Forcing for Multiphase Flows with Sharp  Interfacial Jumps

Abstract: Outflow boundaries play an important role in multiphase fluid dynamics
simulations involving low Weber numbers and large jump in physical variables.
Inadequate treatment of these jumps at outflow generates undesirable fluid
disturbances within the computational domain. We introduce a forcing term for
incompressible Navier-Stokes equations that is coupled with a fixed pressure
outflow boundary condition to enable stable exit of these disturbances from the
domain boundary. The forcing term acts as a damping mechanism to control
vortices that are generated by droplets/bubbles in multiphase flows, and is
designed to be a general formulation that can be applied to a variety of
fluid-flow simulations involving phase transition and sharp interfacial jumps.
Validation cases are provided to demonstrate applicability of this formulation
to pool and flow boiling problems, where bubble induced vortices during
evaporation and condensation can lead to instabilities at outflow boundaries
that eventually propagate downstream to corrupt numerical solution.
Computational experiments are performed using \flashx, which is a composable
open-source software instrument designed for multiscale fluid dynamics
simulations on heterogenous architectures.


------------------------------------------------------------------------------

Title:
Drag-guided diffusion models for vehicle image generation

Abstract: Denoising diffusion models trained at web-scale have revolutionized image
generation. The application of these tools to engineering design is an
intriguing possibility, but is currently limited by their inability to parse
and enforce concrete engineering constraints. In this paper, we take a step
towards this goal by proposing physics-based guidance, which enables
optimization of a performance metric (as predicted by a surrogate model) during
the generation process. As a proof-of-concept, we add drag guidance to Stable
Diffusion, which allows this tool to generate images of novel vehicles while
simultaneously minimizing their predicted drag coefficients.


------------------------------------------------------------------------------

Title:
Fast Approximations of Quantifier Elimination

Abstract: Quantifier elimination (qelim) is used in many automated reasoning tasks
including program synthesis, exist-forall solving, quantified SMT, Model
Checking, and solving Constrained Horn Clauses (CHCs). Exact qelim is
computationally expensive. Hence, it is often approximated. For example, Z3
uses "light" pre-processing to reduce the number of quantified variables.
CHC-solver Spacer uses model-based projection (MBP) to under-approximate qelim
relative to a given model, and over-approximations of qelim can be used as
abstractions.
In this paper, we present the QEL framework for fast approximations of qelim.
QEL provides a uniform interface for both quantifier reduction and model-based
projection. QEL builds on the egraph data structure -- the core of the EUF
decision procedure in SMT -- by casting quantifier reduction as a problem of
choosing ground (i.e., variable-free) representatives for equivalence classes.
We have used QEL to implement MBP for the theories of Arrays and Algebraic Data
Types (ADTs). We integrated QEL and our new MBP in Z3 and evaluated it within
several tasks that rely on quantifier approximations, outperforming
state-of-the-art.


------------------------------------------------------------------------------

Title:
Squeezing nnU-Nets with Knowledge Distillation for On-Board Cloud  Detection

Abstract: Cloud detection is a pivotal satellite image pre-processing step that can be
performed both on the ground and on board a satellite to tag useful images. In
the latter case, it can reduce the amount of data to downlink by pruning the
cloudy areas, or to make a satellite more autonomous through data-driven
acquisition re-scheduling. We approach this task with nnU-Nets, a
self-reconfigurable framework able to perform meta-learning of a segmentation
network over various datasets. Unfortunately, such models are commonly
memory-inefficient due to their (very) large architectures. To benefit from
them in on-board processing, we compress nnU-Nets with knowledge distillation
into much smaller and compact U-Nets. Our experiments, performed over
Sentinel-2 and Landsat-8 images revealed that nnU-Nets deliver state-of-the-art
performance without any manual design. Our approach was ranked within the top
7% best solutions (across 847 teams) in the On Cloud N: Cloud Cover Detection
Challenge, where we reached the Jaccard index of 0.882 over more than 10k
unseen Sentinel-2 images (the winners obtained 0.897, the baseline U-Net with
the ResNet-34 backbone: 0.817, and the classic Sentinel-2 image thresholding:
0.652). Finally, we showed that knowledge distillation enables to elaborate
dramatically smaller (almost 280x) U-Nets when compared to nnU-Nets while still
maintaining their segmentation capabilities.


------------------------------------------------------------------------------

Title:
Feeding control and water quality monitoring in aquaculture systems:  Opportunities and challenges

Abstract: Aquaculture systems can benefit from the recent development of advanced
control strategies to reduce operating costs and fish loss and increase growth
production efficiency, resulting in fish welfare and health. Monitoring the
water quality and controlling feeding are fundamental elements of balancing
fish productivity and shaping the fish growth process. Currently, most
fish-feeding processes are conducted manually in different phases and rely on
time-consuming and challenging artificial discrimination. The feeding control
approach influences fish growth and breeding through the feed conversion rate;
hence, controlling these feeding parameters is crucial for enhancing fish
welfare and minimizing general fishery costs. The high concentration of
environmental factors, such as a high ammonia concentration and pH, affect the
water quality and fish survival. Therefore, there is a critical need to develop
control strategies to determine optimal, efficient, and reliable feeding
processes and monitor water quality. This paper reviews the main control design
techniques for fish growth in aquaculture systems, namely algorithms that
optimize the feeding and water quality of a dynamic fish growth process.
Specifically, we review model-based control approaches and model-free
reinforcement learning strategies to optimize the growth and survival of the
fish or track a desired reference live-weight growth trajectory. The model-free
framework uses an approximate fish growth dynamic model and does not satisfy
constraints. We discuss how model-based approaches can support a reinforcement
learning framework to efficiently handle constraint satisfaction and find
better trajectories and policies from value-based reinforcement learning.


------------------------------------------------------------------------------

Title:
Pose Graph Optimization for a MAV Indoor Localization Fusing 5GNR TOA  with an IMU

Abstract: This paper explores the potential of 5G new radio (NR) Time-of-Arrival (TOA)
data for indoor drone localization under different scenarios and conditions
when fused with inertial measurement unit (IMU) data. Our approach involves
performing graph-based optimization to estimate the drone's position and
orientation from the multiple sensor measurements. Due to the lack of
real-world data, we use Matlab 5G toolbox and QuaDRiGa (quasi-deterministic
radio channel generator) channel simulator to generate TOA measurements for the
EuRoC MAV indoor dataset that provides IMU readings and ground truths 6DoF
poses of a flying drone. Hence, we create twelve sequences combining three
predefined indoor scenarios setups of QuaDRiGa with 2 to 5 base station
antennas. Therefore, experimental results demonstrate that, for a sufficient
number of base stations and a high bandwidth 5G configuration, the pose graph
optimization approach achieves accurate drone localization, with an average
error of less than 15 cm on the overall trajectory. Furthermore, the adopted
graph-based optimization algorithm is fast and can be easily implemented for
onboard real-time pose tracking on a micro aerial vehicle (MAV).


------------------------------------------------------------------------------

Title:
LabelBench: A Comprehensive Framework for Benchmarking Label-Efficient  Learning

Abstract: Labeled data are critical to modern machine learning applications, but
obtaining labels can be expensive. To mitigate this cost, machine learning
methods, such as transfer learning, semi-supervised learning and active
learning, aim to be label-efficient: achieving high predictive performance from
relatively few labeled examples. While obtaining the best label-efficiency in
practice often requires combinations of these techniques, existing benchmark
and evaluation frameworks do not capture a concerted combination of all such
techniques. This paper addresses this deficiency by introducing LabelBench, a
new computationally-efficient framework for joint evaluation of multiple
label-efficient learning techniques. As an application of LabelBench, we
introduce a novel benchmark of state-of-the-art active learning methods in
combination with semi-supervised learning for fine-tuning pretrained vision
transformers. Our benchmark demonstrates better label-efficiencies than
previously reported in active learning. LabelBench's modular codebase is
open-sourced for the broader community to contribute label-efficient learning
methods and benchmarks. The repository can be found at:
this https URL


------------------------------------------------------------------------------

Title:
CANDID: Correspondence AligNment for Deep-burst Image Denoising

Abstract: With the advent of mobile phone photography and point-and-shoot cameras,
deep-burst imaging is widely used for a number of photographic effects such as
depth of field, super-resolution, motion deblurring, and image denoising. In
this work, we propose to solve the problem of deep-burst image denoising by
including an optical flow-based correspondence estimation module which aligns
all the input burst images with respect to a reference frame. In order to deal
with varying noise levels the individual burst images are pre-filtered with
different settings. Exploiting the established correspondences one network
block predicts a pixel-wise spatially-varying filter kernel to smooth each
image in the original and prefiltered bursts before fusing all images to
generate the final denoised output. The resulting pipeline achieves
state-of-the-art results by combining all available information provided by the
burst.


------------------------------------------------------------------------------

Title:
An Efficient Algorithm for Power Dominating Set

Abstract: The problem Power Dominating Set (PDS) is motivated by the placement of
phasor measurement units to monitor electrical networks. It asks for a minimum
set of vertices in a graph that observes all remaining vertices by exhaustively
applying two observation rules. Our contribution is twofold. First, we
determine the parameterized complexity of PDS by proving it is $W[P]$-complete
when parameterized with respect to the solution size. We note that it was only
known to be $W[2]$-hard before. Our second and main contribution is a new
algorithm for PDS that efficiently solves practical instances.
Our algorithm consists of two complementary parts. The first is a set of
reduction rules for PDS that can also be used in conjunction with previously
existing algorithms. The second is an algorithm for solving the remaining
kernel based on the implicit hitting set approach. Our evaluation on a set of
power grid instances from the literature shows that our solver outperforms
previous state-of-the-art solvers for PDS by more than one order of magnitude
on average. Furthermore, our algorithm can solve previously unsolved instances
of continental scale within a few minutes.


------------------------------------------------------------------------------

Title:
Robot Learning with Sensorimotor Pre-training

Abstract: We present a self-supervised sensorimotor pre-training approach for robotics.
Our model, called RPT, is a Transformer that operates on sequences of
sensorimotor tokens. Given a sequence of camera images, proprioceptive robot
states, and past actions, we encode the interleaved sequence into tokens, mask
out a random subset, and train a model to predict the masked-out content. We
hypothesize that if the robot can predict the missing content it has acquired a
good model of the physical world that can enable it to act. RPT is designed to
operate on latent visual representations which makes prediction tractable,
enables scaling to 10x larger models, and 10 Hz inference on a real robot. To
evaluate our approach, we collect a dataset of 20,000 real-world trajectories
over 9 months using a combination of motion planning and model-based grasping
algorithms. We find that pre-training on this data consistently outperforms
training from scratch, leads to 2x improvements in the block stacking task, and
has favorable scaling properties.


------------------------------------------------------------------------------

Title:
Sample-Efficient On-Policy Imitation Learning from Observations

Abstract: Imitation learning from demonstrations (ILD) aims to alleviate numerous
shortcomings of reinforcement learning through the use of demonstrations.
However, in most real-world applications, expert action guidance is absent,
making the use of ILD impossible. Instead, we consider imitation learning from
observations (ILO), where no expert actions are provided, making it a
significantly more challenging problem to address. Existing methods often
employ on-policy learning, which is known to be sample-costly. This paper
presents SEILO, a novel sample-efficient on-policy algorithm for ILO, that
combines standard adversarial imitation learning with inverse dynamics
modeling. This approach enables the agent to receive feedback from both the
adversarial procedure and a behavior cloning loss. We empirically demonstrate
that our proposed algorithm requires fewer interactions with the environment to
achieve expert performance compared to other state-of-the-art on-policy ILO and
ILD methods.


------------------------------------------------------------------------------

Title:
Can robots mold soft plastic materials by shaping depth images?

Abstract: Can robots mold soft plastic materials by shaping depth images? The short
answer is no: current day robots can't. In this article, we address the problem
of shaping plastic material with an anthropomorphic arm/hand robot, which
observes the material with a fixed depth camera. Robots capable of molding
could assist humans in many tasks, such as cooking, scooping or gardening. Yet,
the problem is complex, due to its high-dimensionality at both perception and
control levels. To address it, we design three alternative data-based methods
for predicting the effect of robot actions on the material. Then, the robot can
plan the sequence of actions and their positions, to mold the material into a
desired shape. To make the prediction problem tractable, we rely on two
original ideas. First, we prove that under reasonable assumptions, the shaping
problem can be mapped from point cloud to depth image space, with many benefits
(simpler processing, no need for registration, lower computation time and
memory requirements). Second, we design a novel, simple metric for quickly
measuring the distance between two depth images. The metric is based on the
inherent point cloud representation of depth images, which enables direct and
consistent comparison of image pairs through a non-uniform scaling approach,
and therefore opens promising perspectives for designing \textit{depth image --
based} robot controllers. We assess our approach in a series of unprecedented
experiments, where a robotic arm/hand molds flour from initial to final shapes,
either with its own dataset, or by transfer learning from a human dataset. We
conclude the article by discussing the limitations of our framework and those
of current day hardware, which make human-like robot molding a challenging open
research problem.


------------------------------------------------------------------------------

Title:
Correlation Clustering of Bird Sounds

Abstract: Bird sound classification is the task of relating any sound recording to
those species of bird that can be heard in the recording. Here, we study bird
sound clustering, the task of deciding for any pair of sound recordings whether
the same species of bird can be heard in both. We address this problem by first
learning, from a training set, probabilities of pairs of recordings being
related in this way, and then inferring a maximally probable partition of a
test set by correlation clustering. We address the following questions: How
accurate is this clustering, compared to a classification of the test set? How
do the clusters thus inferred relate to the clusters obtained by
classification? How accurate is this clustering when applied to recordings of
bird species not heard during training? How effective is this clustering in
separating, from bird sounds, environmental noise not heard during training?


------------------------------------------------------------------------------

Title:
Samplet basis pursuit

Abstract: We consider kernel-based learning in samplet coordinates with
l1-regularization. The application of an l1-regularization term enforces
sparsity of the coefficients with respect to the samplet basis. Therefore, we
call this approach samplet basis pursuit. Samplets are wavelet-type signed
measures, which are tailored to scattered data. They provide similar properties
as wavelets in terms of localization, multiresolution analysis, and data
compression. The class of signals that can sparsely be represented in a samplet
basis is considerably larger than the class of signals which exhibit a sparse
representation in the single-scale basis. In particular, every signal that can
be represented by the superposition of only a few features of the canonical
feature map is also sparse in samplet coordinates. We propose the efficient
solution of the problem under consideration by combining soft-shrinkage with
the semi-smooth Newton method and compare the approach to the fast iterative
shrinkage thresholding algorithm. We present numerical benchmarks as well as
applications to surface reconstruction from noisy data and to the
reconstruction of temperature data using a dictionary of multiple kernels.


------------------------------------------------------------------------------

Title:
Towards Better Certified Segmentation via Diffusion Models

Abstract: The robustness of image segmentation has been an important research topic in
the past few years as segmentation models have reached production-level
accuracy. However, like classification models, segmentation models can be
vulnerable to adversarial perturbations, which hinders their use in
critical-decision systems like healthcare or autonomous driving. Recently,
randomized smoothing has been proposed to certify segmentation predictions by
adding Gaussian noise to the input to obtain theoretical guarantees. However,
this method exhibits a trade-off between the amount of added noise and the
level of certification achieved. In this paper, we address the problem of
certifying segmentation prediction using a combination of randomized smoothing
and diffusion models. Our experiments show that combining randomized smoothing
and diffusion models significantly improves certified robustness, with results
indicating a mean improvement of 21 points in accuracy compared to previous
state-of-the-art methods on Pascal-Context and Cityscapes public datasets. Our
method is independent of the selected segmentation model and does not need any
additional specialized training procedure.


------------------------------------------------------------------------------

Title:
Learning to Summarize and Answer Questions about a Virtual Robot's Past  Actions

Abstract: When robots perform long action sequences, users will want to easily and
reliably find out what they have done. We therefore demonstrate the task of
learning to summarize and answer questions about a robot agent's past actions
using natural language alone. A single system with a large language model at
its core is trained to both summarize and answer questions about action
sequences given ego-centric video frames of a virtual robot and a question
prompt. To enable training of question answering, we develop a method to
automatically generate English-language questions and answers about objects,
actions, and the temporal order in which actions occurred during episodes of
robot action in the virtual environment. Training one model to both summarize
and answer questions enables zero-shot transfer of representations of objects
learned through question answering to improved action summarization. %
involving objects not seen in training to summarize.


------------------------------------------------------------------------------

Title:
Group Orthogonalization Regularization For Vision Models Adaptation and  Robustness

Abstract: As neural networks become deeper, the redundancy within their parameters
increases. This phenomenon has led to several methods that attempt to reduce
the correlation between convolutional filters. We propose a computationally
efficient regularization technique that encourages orthonormality between
groups of filters within the same layer. Our experiments show that when
incorporated into recent adaptation methods for diffusion models and vision
transformers (ViTs), this regularization improves performance on downstream
tasks. We further show improved robustness when group orthogonality is enforced
during adversarial training. Our code is available at
this https URL


------------------------------------------------------------------------------

Title:
Rewriting the Script: Adapting Text Instructions for Voice Interaction

Abstract: Voice assistants have sharply risen in popularity in recent years, but their
use has been limited mostly to simple applications like music, hands-free
search, or control of internet-of-things devices. What would it take for voice
assistants to guide people through more complex tasks? In our work, we study
the limitations of the dominant approach voice assistants take to complex task
guidance: reading aloud written instructions. Using recipes as an example, we
observe twelve participants cook at home with a state-of-the-art voice
assistant. We learn that the current approach leads to nine challenges,
including obscuring the bigger picture, overwhelming users with too much
information, and failing to communicate affordances. Instructions delivered by
a voice assistant are especially difficult because they cannot be skimmed as
easily as written instructions. Alexa in particular did not surface crucial
details to the user or answer questions well. We draw on our observations to
propose eight ways in which voice assistants can ``rewrite the script'' --
summarizing, signposting, splitting, elaborating, volunteering, reordering,
redistributing, and visualizing -- to transform written sources into forms that
are readily communicated through spoken conversation. We conclude with a vision
of how modern advancements in natural language processing can be leveraged for
intelligent agents to guide users effectively through complex tasks.


------------------------------------------------------------------------------

Title:
Numerical Approximations of a Class of Nonlinear Second-Order Boundary  Value Problems using Galerkin-Compact Finite Difference Method

Abstract: In this study, we examine numerical approximations for 2nd-order
linear-nonlinear differential equations with diverse boundary conditions,
followed by the residual corrections of the first approximations. We first
obtain numerical results using the Galerkin weighted residual approach with
Bernstein polynomials. The generation of residuals is brought on by the fact
that our first approximation is computed using numerical methods. To minimize
these residuals, we use the compact finite difference scheme of 4th-order
convergence to solve the error differential equations in accordance with the
error boundary conditions. We also introduce the formulation of the compact
finite difference method of fourth-order convergence for the nonlinear BVPs.
The improved approximations are produced by adding the error values derived
from the approximations of the error differential equation to the weighted
residual values. Numerical results are compared to the exact solutions and to
the solutions available in the published literature to validate the proposed
scheme, and high accuracy is achieved in all cases


------------------------------------------------------------------------------

Title:
MixedTeacher : Knowledge Distillation for fast inference textural  anomaly detection

Abstract: For a very long time, unsupervised learning for anomaly detection has been at
the heart of image processing research and a stepping stone for high
performance industrial automation process. With the emergence of CNN, several
methods have been proposed such as Autoencoders, GAN, deep feature extraction,
etc. In this paper, we propose a new method based on the promising concept of
knowledge distillation which consists of training a network (the student) on
normal samples while considering the output of a larger pretrained network (the
teacher). The main contributions of this paper are twofold: First, a reduced
student architecture with optimal layer selection is proposed, then a new
Student-Teacher architecture with network bias reduction combining two teachers
is proposed in order to jointly enhance the performance of anomaly detection
and its localization accuracy. The proposed texture anomaly detector has an
outstanding capability to detect defects in any texture and a fast inference
time compared to the SOTA methods.


------------------------------------------------------------------------------

Title:
Prototype Learning for Explainable Regression

Abstract: The lack of explainability limits the adoption of deep learning models in
clinical practice. While methods exist to improve the understanding of such
models, these are mainly saliency-based and developed for classification,
despite many important tasks in medical imaging being continuous regression
problems. Therefore, in this work, we present ExPeRT: an explainable
prototype-based model specifically designed for regression tasks. Our proposed
model makes a sample prediction from the distances to a set of learned
prototypes in latent space, using a weighted mean of prototype labels. The
distances in latent space are regularized to be relative to label differences,
and each of the prototypes can be visualized as a sample from the training set.
The image-level distances are further constructed from patch-level distances,
in which the patches of both images are structurally matched using optimal
transport. We demonstrate our proposed model on the task of brain age
prediction on two image datasets: adult MR and fetal ultrasound. Our approach
achieved state-of-the-art prediction performance while providing insight in the
model's reasoning process.


------------------------------------------------------------------------------

Title:
Towards Better Orthogonality Regularization with Disentangled Norm in  Training Deep CNNs

Abstract: Orthogonality regularization has been developed to prevent deep CNNs from
training instability and feature redundancy. Among existing proposals, kernel
orthogonality regularization enforces orthogonality by minimizing the residual
between the Gram matrix formed by convolutional filters and the orthogonality
matrix.
We propose a novel measure for achieving better orthogonality among filters,
which disentangles diagonal and correlation information from the residual. The
model equipped with the measure under the principle of imposing strict
orthogonality between filters surpasses previous regularization methods in
near-orthogonality. Moreover, we observe the benefits of improved strict filter
orthogonality in relatively shallow models, but as model depth increases, the
performance gains in models employing strict kernel orthogonality decrease
sharply.
Furthermore, based on the observation of the potential conflict between
strict kernel orthogonality and growing model capacity, we propose a relaxation
theory on kernel orthogonality regularization. The relaxed kernel orthogonality
achieves enhanced performance on models with increased capacity, shedding light
on the burden of strict kernel orthogonality on deep model performance.
We conduct extensive experiments with our kernel orthogonality regularization
toolkit on ResNet and WideResNet in CIFAR-10 and CIFAR-100. We observe
state-of-the-art gains in model performance from the toolkit, which includes
both strict orthogonality and relaxed orthogonality regularization, and obtain
more robust models with expressive features. These experiments demonstrate the
efficacy of our toolkit and subtly provide insights into the often overlooked
challenges posed by strict orthogonality, addressing the burden of strict
orthogonality on capacity-rich models.


------------------------------------------------------------------------------

Title:
Tactile-Reactive Roller Grasper

Abstract: Manipulation of objects within a robot's hand is one of the most important
challenges in achieving robot dexterity. The "Roller Graspers" refers to a
family of non-anthropomorphic hands utilizing motorized, rolling fingertips to
achieve in-hand manipulation. These graspers manipulate grasped objects by
commanding the rollers to exert forces that propel the object in the desired
motion directions. In this paper, we explore the possibility of robot in-hand
manipulation through tactile-guided rolling. We do so by developing the
Tactile-Reactive Roller Grasper (TRRG), which incorporates camera-based tactile
sensing with compliant, steerable cylindrical fingertips, with accompanying
sensor information processing and control strategies. We demonstrated that the
combination of tactile feedback and the actively rolling surfaces enables a
variety of robust in-hand manipulation applications. In addition, we also
demonstrated object reconstruction techniques using tactile-guided rolling. A
controlled experiment was conducted to provide insights on the benefits of
tactile-reactive rollers for manipulation. We considered two manipulation
cases: when the fingers are manipulating purely through rolling and when they
are periodically breaking and reestablishing contact as in regrasping. We found
that tactile-guided rolling can improve the manipulation robustness by allowing
the grasper to perform necessary fine grip adjustments in both manipulation
cases, indicating that hybrid rolling fingertip and finger-gaiting designs may
be a promising research direction.


------------------------------------------------------------------------------

Title:
Data-Driven Model Discrimination of Switched Nonlinear Systems with  Temporal Logic Inference

Abstract: This paper addresses the problem of data-driven model discrimination for
unknown switched systems with unknown linear temporal logic (LTL)
specifications, representing tasks, that govern their mode sequences, where
only sampled data of the unknown dynamics and tasks are available. To tackle
this problem, we propose data-driven methods to over-approximate the unknown
dynamics and to infer the unknown specifications such that both set-membership
models of the unknown dynamics and LTL formulas are guaranteed to include the
ground truth model and specification/task. Moreover, we present an
optimization-based algorithm for analyzing the distinguishability of a set of
learned/inferred model-task pairs as well as a model discrimination algorithm
for ruling out model-task pairs from this set that are inconsistent with new
observations at run time. Further, we present an approach for reducing the size
of inferred specifications to increase the computational efficiency of the
model discrimination algorithms.


------------------------------------------------------------------------------

Title:
Model-based versus model-free feeding control and water quality  monitoring for fish growth tracking in aquaculture systems

Abstract: The high concentration level of the environmental factors, such as a high
ammonia concentration and pH level, affect the water quality, affecting fish's
survival and mass death. Therefore, there is a critical need to develop control
strategies to determine optimal, efficient, and reliable feeding and water
quality monitoring processes. In this paper, we revisit the representative fish
growth model describing the total biomass change by incorporating the fish
population density and mortality. Since the measurement data of the total
biomass and population from the aquaculture systems are limited and difficult
to obtain, we validate the new dynamic population model with the individual
fish growth data for tracking control purposes. We specifically focus on
relative feeding as a manipulated variable to design traditional and optimal
control to track the desired weight reference within the sub-optimal
temperature and dissolved oxygen profiles under different levels of unionized
ammonia exposure. Then, we propose a Q-learning approach that learns an optimal
feeding control policy from the simulated data of the fish growth weight
trajectories while managing the ammonia effects. The proposed Q-learning
feeding control prevents fish mortality and achieves good tracking errors of
the fish weight under the different levels of unionized ammonia. However, it
maintains a relative food consumption that potentially underfeeds the fish.
Finally, we propose an optimal algorithm that optimizes the feeding and water
quality of the dynamic fish population growth process. We also show that the
model predictive control decreases fish mortality and reduces food consumption
in all different cases of unionized ammonia exposure.


------------------------------------------------------------------------------

Title:
Nearly-Optimal Hierarchical Clustering for Well-Clustered Graphs

Abstract: This paper presents two efficient hierarchical clustering (HC) algorithms
with respect to Dasgupta's cost function. For any input graph $G$ with a clear
cluster-structure, our designed algorithms run in nearly-linear time in the
input size of $G$, and return an $O(1)$-approximate HC tree with respect to
Dasgupta's cost function. We compare the performance of our algorithm against
the previous state-of-the-art on synthetic and real-world datasets and show
that our designed algorithm produces comparable or better HC trees with much
lower running time.


------------------------------------------------------------------------------

Title:
MagicBrush: A Manually Annotated Dataset for Instruction-Guided Image  Editing

Abstract: Text-guided image editing is widely needed in daily life, ranging from
personal use to professional applications such as Photoshop. However, existing
methods are either zero-shot or trained on an automatically synthesized
dataset, which contains a high volume of noise. Thus, they still require lots
of manual tuning to produce desirable outcomes in practice. To address this
issue, we introduce MagicBrush (this https URL),
the first large-scale, manually annotated dataset for instruction-guided real
image editing that covers diverse scenarios: single-turn, multi-turn,
mask-provided, and mask-free editing. MagicBrush comprises over 10K manually
annotated triples (source image, instruction, target image), which supports
trainining large-scale text-guided image editing models. We fine-tune
InstructPix2Pix on MagicBrush and show that the new model can produce much
better images according to human evaluation. We further conduct extensive
experiments to evaluate current image editing baselines from multiple
dimensions including quantitative, qualitative, and human evaluations. The
results reveal the challenging nature of our dataset and the gap between
current baselines and real-world editing needs.


------------------------------------------------------------------------------

Title:
SLACK: Stable Learning of Augmentations with Cold-start and KL  regularization

Abstract: Data augmentation is known to improve the generalization capabilities of
neural networks, provided that the set of transformations is chosen with care,
a selection often performed manually. Automatic data augmentation aims at
automating this process. However, most recent approaches still rely on some
prior information; they start from a small pool of manually-selected default
transformations that are either used to pretrain the network or forced to be
part of the policy learned by the automatic data augmentation algorithm. In
this paper, we propose to directly learn the augmentation policy without
leveraging such prior knowledge. The resulting bilevel optimization problem
becomes more challenging due to the larger search space and the inherent
instability of bilevel optimization algorithms. To mitigate these issues (i) we
follow a successive cold-start strategy with a Kullback-Leibler regularization,
and (ii) we parameterize magnitudes as continuous distributions. Our approach
leads to competitive results on standard benchmarks despite a more challenging
setting, and generalizes beyond natural images.


------------------------------------------------------------------------------

Title:
Co-Certificate Learning with SAT Modulo Symmetries

Abstract: We present a new SAT-based method for generating all graphs up to isomorphism
that satisfy a given co-NP property. Our method extends the SAT Modulo Symmetry
(SMS) framework with a technique that we call co-certificate learning. If SMS
generates a candidate graph that violates the given co-NP property, we obtain a
certificate for this violation, i.e., `co-certificate' for the co-NP property.
The co-certificate gives rise to a clause that the SAT solver, serving as SMS's
backend, learns as part of its CDCL procedure. We demonstrate that SMS plus
co-certificate learning is a powerful method that allows us to improve the
best-known lower bound on the size of Kochen-Specker vector systems, a problem
that is central to the foundations of quantum mechanics and has been studied
for over half a century. Our approach is orders of magnitude faster and scales
significantly better than a recently proposed SAT-based method.


------------------------------------------------------------------------------

Title:
GenORM: Generalizable One-shot Rope Manipulation with Parameter-Aware  Policy

Abstract: Due to the inherent uncertainty in their deformability during motion,
previous methods in rope manipulation often require hundreds of real-world
demonstrations to train a manipulation policy for each rope, even for simple
tasks such as rope goal reaching, which hinder their applications in our
ever-changing world. To address this issue, we introduce GenORM, a framework
that allows the manipulation policy to handle different deformable ropes with a
single real-world demonstration. To achieve this, we augment the policy by
conditioning it on deformable rope parameters and training it with a diverse
range of simulated deformable ropes so that the policy can adjust actions based
on different rope parameters. At the time of inference, given a new rope,
GenORM estimates the deformable rope parameters by minimizing the disparity
between the grid density of point clouds of real-world demonstrations and
simulations. With the help of a differentiable physics simulator, we require
only a single real-world demonstration. Empirical validations on both simulated
and real-world rope manipulation setups clearly show that our method can
manipulate different ropes with a single demonstration and significantly
outperforms the baseline in both environments (62% improvement in in-domain
ropes, and 15% improvement in out-of-distribution ropes in simulation, 26%
improvement in real-world), demonstrating the effectiveness of our approach in
one-shot rope manipulation.


------------------------------------------------------------------------------

Title:
Energy-Based Cross Attention for Bayesian Context Update in  Text-to-Image Diffusion Models

Abstract: Despite the remarkable performance of text-to-image diffusion models in image
generation tasks, recent studies have raised the issue that generated images
sometimes cannot capture the intended semantic contents of the text prompts,
which phenomenon is often called semantic misalignment. To address this, here
we present a novel energy-based model (EBM) framework. Specifically, we first
formulate EBMs of latent image representations and text embeddings in each
cross-attention layer of the denoising autoencoder. Then, we obtain the
gradient of the log posterior of context vectors, which can be updated and
transferred to the subsequent cross-attention layer, thereby implicitly
minimizing a nested hierarchy of energy functions. Our latent EBMs further
allow zero-shot compositional generation as a linear combination of
cross-attention outputs from different contexts. Using extensive experiments,
we demonstrate that the proposed method is highly effective in handling various
image generation tasks, including multi-concept generation, text-guided image
inpainting, and real and synthetic image editing.


------------------------------------------------------------------------------

Title:
A note on the convergence of a class of adaptive optimal identifiers

Abstract: This paper proposes a unifying framework for the convergence analysis of a
class of adaptive optimal identifiers. The considered class of identifiers is
constructed from the sequence of minimizing sets of a family of objective
functions. For the purpose of the analysis we introduce a generalized version
of the classical persistence of excitation condition. Based on this an
Input-to-State-Stability (ISS)-type of property is derived for the studied
class of adaptive estimators.


------------------------------------------------------------------------------

Title:
No Strong Feelings One Way or Another: Re-operationalizing Neutrality in  Natural Language Inference

Abstract: Natural Language Inference (NLI) has been a cornerstone task in evaluating
language models' inferential reasoning capabilities. However, the standard
three-way classification scheme used in NLI has well-known shortcomings in
evaluating models' ability to capture the nuances of natural human reasoning.
In this paper, we argue that the operationalization of the neutral label in
current NLI datasets has low validity, is interpreted inconsistently, and that
at least one important sense of neutrality is often ignored. We uncover the
detrimental impact of these shortcomings, which in some cases leads to
annotation datasets that actually decrease performance on downstream tasks. We
compare approaches of handling annotator disagreement and identify flaws in a
recent NLI dataset that designs an annotator study based on a problematic
operationalization. Our findings highlight the need for a more refined
evaluation framework for NLI, and we hope to spark further discussion and
action in the NLP community.


------------------------------------------------------------------------------

Title:
Enhancing Fault Resilience of QNNs by Selective Neuron Splitting

Abstract: The superior performance of Deep Neural Networks (DNNs) has led to their
application in various aspects of human life. Safety-critical applications are
no exception and impose rigorous reliability requirements on DNNs. Quantized
Neural Networks (QNNs) have emerged to tackle the complexity of DNN
accelerators, however, they are more prone to reliability issues.
In this paper, a recent analytical resilience assessment method is adapted
for QNNs to identify critical neurons based on a Neuron Vulnerability Factor
(NVF). Thereafter, a novel method for splitting the critical neurons is
proposed that enables the design of a Lightweight Correction Unit (LCU) in the
accelerator without redesigning its computational part.
The method is validated by experiments on different QNNs and datasets. The
results demonstrate that the proposed method for correcting the faults has a
twice smaller overhead than a selective Triple Modular Redundancy (TMR) while
achieving a similar level of fault resiliency.


------------------------------------------------------------------------------

Title:
Uncertainty Quantification via Spatial-Temporal Tweedie Model for  Zero-inflated and Long-tail Travel Demand Prediction

Abstract: crucial for transportation management. However, traditional spatial-temporal
deep learning models grapple with addressing the sparse and long-tail
characteristics in high-resolution O-D matrices and quantifying prediction
uncertainty. This dilemma arises from the numerous zeros and over-dispersed
demand patterns within these matrices, which challenge the Gaussian assumption
inherent to deterministic deep learning models. To address these challenges, we
propose a novel approach: the Spatial-Temporal Tweedie Graph Neural Network
(STTD). The STTD introduces the Tweedie distribution as a compelling
alternative to the traditional 'zero-inflated' model and leverages spatial and
temporal embeddings to parameterize travel demand distributions. Our
evaluations using real-world datasets highlight STTD's superiority in providing
accurate predictions and precise confidence intervals, particularly in
high-resolution scenarios.


------------------------------------------------------------------------------

Title:
Variational Sequential Optimal Experimental Design using Reinforcement  Learning

Abstract: We introduce variational sequential Optimal Experimental Design (vsOED), a
new method for optimally designing a finite sequence of experiments under a
Bayesian framework and with information-gain utilities. Specifically, we adopt
a lower bound estimator for the expected utility through variational
approximation to the Bayesian posteriors. The optimal design policy is solved
numerically by simultaneously maximizing the variational lower bound and
performing policy gradient updates. We demonstrate this general methodology for
a range of OED problems targeting parameter inference, model discrimination,
and goal-oriented prediction. These cases encompass explicit and implicit
likelihoods, nuisance parameters, and physics-based partial differential
equation models. Our vsOED results indicate substantially improved sample
efficiency and reduced number of forward model simulations compared to previous
sequential design algorithms.


------------------------------------------------------------------------------

Title:
Jumanji: a Diverse Suite of Scalable Reinforcement Learning Environments  in JAX

Abstract: Open-source reinforcement learning (RL) environments have played a crucial
role in driving progress in the development of AI algorithms. In modern RL
research, there is a need for simulated environments that are performant,
scalable, and modular to enable their utilization in a wider range of potential
real-world applications. Therefore, we present Jumanji, a suite of diverse RL
environments specifically designed to be fast, flexible, and scalable. Jumanji
provides a suite of environments focusing on combinatorial problems frequently
encountered in industry, as well as challenging general decision-making tasks.
By leveraging the efficiency of JAX and hardware accelerators like GPUs and
TPUs, Jumanji enables rapid iteration of research ideas and large-scale
experimentation, ultimately empowering more capable agents. Unlike existing RL
environment suites, Jumanji is highly customizable, allowing users to tailor
the initial state distribution and problem complexity to their needs.
Furthermore, we provide actor-critic baselines for each environment,
accompanied by preliminary findings on scaling and generalization scenarios.
Jumanji aims to set a new standard for speed, adaptability, and scalability of
RL environments.


------------------------------------------------------------------------------

Title:
Are Large Language Models Really Good Logical Reasoners? A Comprehensive  Evaluation From Deductive, Inductive and Abductive Views

Abstract: Large Language Models (LLMs) have achieved great success in various natural
language tasks. It has aroused much interest in evaluating the specific
reasoning capability of LLMs, such as multilingual reasoning and mathematical
reasoning. However, as one of the key reasoning perspectives, logical reasoning
capability has not yet been thoroughly evaluated. In this work, we aim to
bridge those gaps and provide comprehensive evaluations. Firstly, to offer
systematic evaluations, this paper selects fifteen typical logical reasoning
datasets and organizes them into deductive, inductive, abductive and mixed-form
reasoning settings. Considering the comprehensiveness of evaluations, we
include three representative LLMs (i.e., text-davinci-003, ChatGPT and BARD)
and evaluate them on all selected datasets under zero-shot, one-shot and
three-shot settings. Secondly, different from previous evaluations relying only
on simple metrics (e.g., accuracy), we propose fine-level evaluations from
objective and subjective manners, covering both answers and explanations. Also,
to uncover the logical flaws of LLMs, bad cases will be attributed to five
error types from two dimensions. Thirdly, to avoid the influences of knowledge
bias and purely focus on benchmarking the logical reasoning capability of LLMs,
we propose a new dataset with neutral content. It contains 3K samples and
covers deductive, inductive and abductive reasoning settings. Based on the
in-depth evaluations, this paper finally concludes the ability maps of logical
reasoning capability from six dimensions (i.e., correct, rigorous, self-aware,
active, oriented and no hallucination). It reflects the pros and cons of LLMs
and gives guiding directions for future works.


------------------------------------------------------------------------------

Title:
Fairness in Preference-based Reinforcement Learning

Abstract: In this paper, we address the issue of fairness in preference-based
reinforcement learning (PbRL) in the presence of multiple objectives. The main
objective is to design control policies that can optimize multiple objectives
while treating each objective fairly. Toward this objective, we design a new
fairness-induced preference-based reinforcement learning or FPbRL. The main
idea of FPbRL is to learn vector reward functions associated with multiple
objectives via new welfare-based preferences rather than reward-based
preference in PbRL, coupled with policy learning via maximizing a generalized
Gini welfare function. Finally, we provide experiment studies on three
different environments to show that the proposed FPbRL approach can achieve
both efficiency and equity for learning effective and fair policies.


------------------------------------------------------------------------------

Title:
Demystifying GPT Self-Repair for Code Generation

Abstract: Large Language Models (LLMs) have shown remarkable aptitude in code
generation but still struggle on challenging programming tasks. Self-repair --
in which the model debugs and fixes mistakes in its own code -- has recently
become a popular way to boost performance in these settings. However, only very
limited studies on how and when self-repair works effectively exist in the
literature, and one might wonder to what extent a model is really capable of
providing accurate feedback on why the code is wrong when that code was
generated by the same model. In this paper, we analyze GPT-3.5 and GPT-4's
ability to perform self-repair on APPS, a challenging dataset consisting of
diverse coding challenges. To do so, we first establish a new evaluation
strategy dubbed pass@t that measures the pass rate of the tasks against the
total number of tokens sampled from the model, enabling a fair comparison to
purely sampling-based approaches. With this evaluation strategy, we find that
the effectiveness of self-repair is only seen in GPT-4. We also observe that
self-repair is bottlenecked by the feedback stage; using GPT-4 to give feedback
on the programs generated by GPT-3.5 and using expert human programmers to give
feedback on the programs generated by GPT-4, we unlock significant performance
gains.


------------------------------------------------------------------------------

Title:
Unlocking the Potential of User Feedback: Leveraging Large Language  Model as User Simulator to Enhance Dialogue System

Abstract: Dialogue systems and large language models (LLMs) have gained considerable
attention. However, the direct utilization of LLMs as task-oriented dialogue
(TOD) models has been found to underperform compared to smaller task-specific
models. Nonetheless, it is crucial to acknowledge the significant potential of
LLMs and explore improved approaches for leveraging their impressive abilities.
Motivated by the goal of leveraging LLMs, we propose an alternative approach
called User-Guided Response Optimization (UGRO) to combine it with a smaller
TOD model. This approach uses LLM as annotation-free user simulator to assess
dialogue responses, combining them with smaller fine-tuned end-to-end TOD
models. By utilizing the satisfaction feedback generated by LLMs, UGRO further
optimizes the supervised fine-tuned TOD model. Specifically, the TOD model
takes the dialogue history as input and, with the assistance of the user
simulator's feedback, generates high-satisfaction responses that meet the
user's requirements. Through empirical experiments on two TOD benchmarks, we
validate the effectiveness of our method. The results demonstrate that our
approach outperforms previous state-of-the-art (SOTA) results.


------------------------------------------------------------------------------

Title:
Advanced discretization techniques for hyperelastic physics-augmented  neural networks

Abstract: In the present work, advanced spatial and temporal discretization techniques
are tailored to hyperelastic physics-augmented neural networks, i.e., neural
network based constitutive models which fulfill all relevant mechanical
conditions of hyperelasticity by construction. The framework takes into account
the structure of neural network-based constitutive models, in particular, that
their derivatives are more complex compared to analytical models. The proposed
framework allows for convenient mixed Hu-Washizu like finite element
formulations applicable to nearly incompressible material behavior. The key
feature of this work is a tailored energy-momentum scheme for time
discretization, which allows for energy and momentum preserving dynamical
simulations. Both the mixed formulation and the energy-momentum discretization
are applied in finite element analysis. For this, a hyperelastic
physics-augmented neural network model is calibrated to data generated with an
analytical potential. In all finite element simulations, the proposed
discretization techniques show excellent performance. All of this demonstrates
that, from a formal point of view, neural networks are essentially mathematical
functions. As such, they can be applied in numerical methods as
straightforwardly as analytical constitutive models. Nevertheless, their
special structure suggests to tailor advanced discretization methods, to arrive
at compact mathematical formulations and convenient implementations.


------------------------------------------------------------------------------

Title:
Evaluating hardware differences for crowdsourcing and traditional  recruiting methods

Abstract: The most frequently used method to collect research data online is
crowdsouring and its use continues to grow rapidly. This report investigates
for the first time whether researchers also have to expect significantly
different hardware performance when deploying to Amazon Mechanical Turk
(MTurk). This is assessed by collecting basic hardware parameters (Operating
System, GPU, and used browser) from Amazon Mechanical Turk (MTurk) and a
traditional recruitment method (i.e., snowballing). The significant hardware
differences between crowdsourcing participants (MTurk) and snowball recruiting
are reported including relevant descriptive statistics for assessing hardware
performance of 3D web applications. The report suggests that hardware
differences need to be considered to obtain valid results if the designed
experiment application requires graphical intense computations and relies on a
coherent user experience of MTurk and more established recruitment strategies
(i.e. snowballing).


------------------------------------------------------------------------------

Title:
AvatarBooth: High-Quality and Customizable 3D Human Avatar Generation

Abstract: We introduce AvatarBooth, a novel method for generating high-quality 3D
avatars using text prompts or specific images. Unlike previous approaches that
can only synthesize avatars based on simple text descriptions, our method
enables the creation of personalized avatars from casually captured face or
body images, while still supporting text-based model generation and editing.
Our key contribution is the precise avatar generation control by using dual
fine-tuned diffusion models separately for the human face and body. This
enables us to capture intricate details of facial appearance, clothing, and
accessories, resulting in highly realistic avatar generations. Furthermore, we
introduce pose-consistent constraint to the optimization process to enhance the
multi-view consistency of synthesized head images from the diffusion model and
thus eliminate interference from uncontrolled human poses. In addition, we
present a multi-resolution rendering strategy that facilitates coarse-to-fine
supervision of 3D avatar generation, thereby enhancing the performance of the
proposed system. The resulting avatar model can be further edited using
additional text descriptions and driven by motion sequences. Experiments show
that AvatarBooth outperforms previous text-to-3D methods in terms of rendering
and geometric quality from either text prompts or specific images. Please check
our project website at this https URL


------------------------------------------------------------------------------

Title:
C2F2NeUS: Cascade Cost Frustum Fusion for High Fidelity and  Generalizable Neural Surface Reconstruction

Abstract: There is an emerging effort to combine the two popular technical paths, i.e.,
the multi-view stereo (MVS) and neural implicit surface (NIS), in scene
reconstruction from sparse views. In this paper, we introduce a novel
integration scheme that combines the multi-view stereo with neural signed
distance function representations, which potentially overcomes the limitations
of both methods. MVS uses per-view depth estimation and cross-view fusion to
generate accurate surface, while NIS relies on a common coordinate volume.
Based on this, we propose to construct per-view cost frustum for finer geometry
estimation, and then fuse cross-view frustums and estimate the implicit signed
distance functions to tackle noise and hole issues. We further apply a cascade
frustum fusion strategy to effectively captures global-local information and
structural consistency. Finally, we apply cascade sampling and a
pseudo-geometric loss to foster stronger integration between the two
architectures. Extensive experiments demonstrate that our method reconstructs
robust surfaces and outperforms existing state-of-the-art methods.


------------------------------------------------------------------------------

Title:
Subset Selection Based On Multiple Rankings in the Presence of Bias:  Effectiveness of Fairness Constraints for Multiwinner Voting Score Functions

Abstract: We consider the problem of subset selection where one is given multiple
rankings of items and the goal is to select the highest ``quality'' subset.
Score functions from the multiwinner voting literature have been used to
aggregate rankings into quality scores for subsets. We study this setting of
subset selection problems when, in addition, rankings may contain systemic or
unconscious biases toward a group of items. For a general model of input
rankings and biases, we show that requiring the selected subset to satisfy
group fairness constraints can improve the quality of the selection with
respect to unbiased rankings. Importantly, we show that for fairness
constraints to be effective, different multiwinner score functions may require
a drastically different number of rankings: While for some functions, fairness
constraints need an exponential number of rankings to recover a
close-to-optimal solution, for others, this dependency is only polynomial. This
result relies on a novel notion of ``smoothness'' of submodular functions in
this setting that quantifies how well a function can ``correctly'' assess the
quality of items in the presence of bias. The results in this paper can be used
to guide the choice of multiwinner score functions for the subset selection
setting considered here; we additionally provide a tool to empirically enable
this.


------------------------------------------------------------------------------

Title:
Vehicle Occurrence-based Parking Space Detection

Abstract: Smart-parking solutions use sensors, cameras, and data analysis to improve
parking efficiency and reduce traffic congestion. Computer vision-based methods
have been used extensively in recent years to tackle the problem of parking lot
management, but most of the works assume that the parking spots are manually
labeled, impacting the cost and feasibility of deployment. To fill this gap,
this work presents an automatic parking space detection method, which receives
a sequence of images of a parking lot and returns a list of coordinates
identifying the detected parking spaces. The proposed method employs instance
segmentation to identify cars and, using vehicle occurrence, generate a heat
map of parking spaces. The results using twelve different subsets from the
PKLot and CNRPark-EXT parking lot datasets show that the method achieved an
AP25 score up to 95.60\% and AP50 score up to 79.90\%.


------------------------------------------------------------------------------

Title:
Verification and Validation of the Stakeholder Tool for Assessing  Radioactive Transportation (START)

Abstract: The U.S. Department of Energy (DOE) Office of Integrated Waste Management is
planning for the eventual transportation, storage, and disposal of spent
nuclear fuel (SNF) and high-level radioactive waste (HLW) from nuclear power
plant and DOE sites. The Stakeholder Tool for Assessing Radioactive
Transportation (START) is a web-based, geospatial decision-support tool
developed for evaluating routing options and other aspects of transporting SNF
and HLW, covering rail, truck, barge, and intermodal infrastructure and
operations in the continental United States. The verification and validation
(V&V) process is intended to independently assess START to provide confidence
in the ability of START to accurately provide intended results. The V&V process
checks the START tool using a variety of methods, ranging from independent hand
calculations to comparison of START performance and results to those of other
codes. The V&V activity was conducted independently from the START development
team with opportunities to provide feedback and collaborate throughout the
process. The V&V analyzed attributes of transportation routes produced by
START, including route distance and both population and population density
captured within buffer zones around routes. Population in the buffer zone,
population density in the buffer zone, and route distance were all identified
as crucial outputs of the START code and were subject to V&V tasks. Some of the
improvements identified through the V&V process were standardizing the
underlying population data in START, changing the projection of the population
raster data, and changes to the methodology used for population density to
improve its applicability for expected users. This collaboration also led to
suggested improvements to some of the underlying shape file segments within
START.


------------------------------------------------------------------------------

Title:
Sparq: A Custom RISC-V Vector Processor for Efficient Sub-Byte Quantized  Inference

Abstract: Convolutional Neural Networks (CNNs) are used in a wide range of
applications, with full-precision CNNs achieving high accuracy at the expense
of portability. Recent progress in quantization techniques has demonstrated
that sub-byte Quantized Neural Networks (QNNs) achieve comparable or superior
accuracy while significantly reducing the computational cost and memory
footprint. However, sub-byte computation on commodity hardware is sub-optimal
due to the lack of support for such precision. In this paper, we introduce
Sparq, a Sub-byte vector Processor designed for the AcceleRation of QNN
inference. This processor is based on a modified version of Ara, an open-source
64-bit RISC-V ``V'' compliant processor. Sparq is implemented in GLOBAL
FOUNDRIES 22FDX FD-SOI technology and extends the Instruction Set Architecture
(ISA) by adding a new multiply-shift-accumulate instruction to improve sub-byte
computation effciency. The floating-point unit is also removed to minimize area
and power usage. To demonstrate Sparq performance, we implement an
ultra-low-precision (1-bit to 4-bit) vectorized conv2d operation taking
advantage of the dedicated hardware. We show that Sparq can significantly
accelerate sub-byte computations with respectively 3.2 times, and 1.7 times
acceleration over an optimized 16-bit 2D convolution for 2-bit and 4-bit
quantization.


------------------------------------------------------------------------------

Title:
A Numerically Robust and Stable Time-Space Pseudospectral Approach for  Generalized Burgers-Fisher Equation

Abstract: In this article, we present the time-space Chebyshev pseudospectral method
(TS-CPsM) to approximate a solution to the generalised Burgers-Fisher (gBF)
equation. The Chebyshev-Gauss-Lobatto (CGL) points serve as the foundation for
the recommended method, which makes use of collocations in both the time and
space directions. Further, using a mapping, the non-homogeneous
initial-boundary value problem is transformed into a homogeneous problem, and a
system of algebraic equations is obtained. The numerical approach known as
Newton-Raphson is implemented in order to get the desired results for the
system. The proposed method's stability analysis has been performed. Different
researchers' considerations on test problems have been explored to illustrate
the robustness and practicality of the approach presented. The approximate
solutions we found using the proposed method are highly accurate and
significantly better than the existing results.


------------------------------------------------------------------------------

Title:
INDCOR white paper 4: Evaluation of Interactive Narrative Design For  Complexity Representations

Abstract: While a strength of Interactive Digital Narratives (IDN) is its support for
multiperspectivity, this also poses a substantial challenge to its evaluation.
Moreover, evaluation has to assess the system's ability to represent a complex
reality as well as the user's understanding of that complex reality as a result
of the experience of interacting with the system. This is needed to measure an
IDN's efficiency and effectiveness in representing the chosen complex
phenomenon. We here present some empirical methods employed by INDCOR members
in their research including UX toolkits and scales. Particularly, we consider
the impact of IDN on transformative learning and its evaluation through
self-reporting and other alternatives.


------------------------------------------------------------------------------

Title:
Addressing Societal Issues in Interactive Digital Narratives

Abstract: This white paper introduces Interactive Digital Narratives (IDN) as a
powerful tool for tackling the complex challenges we face in today's society.
In the scope of the COST Action 18230 - Interactive Narrative Design for
Complexity Representation, a group of researchers dedicated to studying media,
systematically selected six case studies of IDNs, including educational games,
news media, and social media content, that confront and challenge the existing
traditional media landscape. These case studies cover a wide range of important
societal issues, such as racism, coloniality, feminist social movements,
cultural heritage, war, and disinformation. By exploring this broad range of
examples, we aim to demonstrate how IDN can effectively address social
complexity in an interactive, participatory, and engaging manner. We encourage
you to examine these case studies and discover for yourself how IDN can be used
as a creative tool to address complex societal issues. This white paper might
be inspiring for journalists, digital content creators, game designers,
developers, educators using information and communication technologies in the
classroom, or anyone interested in learning how to use IDN tools to tackle
complex societal issues. In this sense, along with key scientific references,
we offer key takeaways at the end of this paper that might be helpful for media
practitioners at large, in two main ways: 1) Designing IDNs to address complex
societal issues and 2) Using IDNs to engage audiences with complex societal
issues.


------------------------------------------------------------------------------

Title:
Evaluating the Efficacy of ChatGPT-4 in Providing Scientific References  across Diverse Disciplines

Abstract: This work conducts a comprehensive exploration into the proficiency of
OpenAI's ChatGPT-4 in sourcing scientific references within an array of
research disciplines. Our in-depth analysis encompasses a wide scope of fields
including Computer Science (CS), Mechanical Engineering (ME), Electrical
Engineering (EE), Biomedical Engineering (BME), and Medicine, as well as their
more specialized sub-domains. Our empirical findings indicate a significant
variance in ChatGPT-4's performance across these disciplines. Notably, the
validity rate of suggested articles in CS, BME, and Medicine surpasses 65%,
whereas in the realms of ME and EE, the model fails to verify any article as
valid. Further, in the context of retrieving articles pertinent to niche
research topics, ChatGPT-4 tends to yield references that align with the
broader thematic areas as opposed to the narrowly defined topics of interest.
This observed disparity underscores the pronounced variability in accuracy
across diverse research fields, indicating the potential requirement for model
refinement to enhance its functionality in academic research. Our investigation
offers valuable insights into the current capacities and limitations of
AI-powered tools in scholarly research, thereby emphasizing the indispensable
role of human oversight and rigorous validation in leveraging such models for
academic pursuits.


------------------------------------------------------------------------------

Title:
Wasserstein distributional robustness of neural networks

Abstract: Deep neural networks are known to be vulnerable to adversarial attacks (AA).
For an image recognition task, this means that a small perturbation of the
original can result in the image being misclassified. Design of such attacks as
well as methods of adversarial training against them are subject of intense
research. We re-cast the problem using techniques of Wasserstein
distributionally robust optimization (DRO) and obtain novel contributions
leveraging recent insights from DRO sensitivity analysis. We consider a set of
distributional threat models. Unlike the traditional pointwise attacks, which
assume a uniform bound on perturbation of each input data point, distributional
threat models allow attackers to perturb inputs in a non-uniform way. We link
these more general attacks with questions of out-of-sample performance and
Knightian uncertainty. To evaluate the distributional robustness of neural
networks, we propose a first-order AA algorithm and its multi-step version. Our
attack algorithms include Fast Gradient Sign Method (FGSM) and Projected
Gradient Descent (PGD) as special cases. Furthermore, we provide a new
asymptotic estimate of the adversarial accuracy against distributional threat
models. The bound is fast to compute and first-order accurate, offering new
insights even for the pointwise AA. It also naturally yields out-of-sample
performance guarantees. We conduct numerical experiments on the CIFAR-10
dataset using DNNs on RobustBench to illustrate our theoretical results. Our
code is available at this https URL


------------------------------------------------------------------------------

Title:
Actor-Critic Model Predictive Control

Abstract: Despite its success, Model Predictive Control (MPC) often requires intensive
task-specific engineering and tuning. On the other hand, Reinforcement Learning
(RL) architectures minimize this effort, but need extensive data collection and
lack interpretability and safety. An open research question is how to combine
the advantages of RL and MPC to exploit the best of both worlds. This paper
introduces a novel modular RL architecture that bridges these two approaches.
By placing a differentiable MPC in the heart of an actor-critic RL agent, the
proposed system enables short-term predictions and optimization of actions
based on system dynamics, while retaining the end-to-end training benefits and
exploratory behavior of an RL agent. The proposed approach effectively handles
two different time-horizon scales: short-term decisions managed by the actor
MPC and long term ones managed by the critic network. This provides a promising
direction for RL, which combines the advantages of model-based and end-to-end
learning methods. We validate the approach in simulated and real-world
experiments on a quadcopter platform performing different high-level tasks, and
show that the proposed method can learn complex behaviours end-to-end while
retaining the properties of an MPC.


------------------------------------------------------------------------------

Title:
HePCo: Data-Free Heterogeneous Prompt Consolidation for Continual  Federated Learning

Abstract: In this paper, we focus on the important yet understudied problem of
Continual Federated Learning (CFL), where a server communicates with a set of
clients to incrementally learn new concepts over time without sharing or
storing any data. The complexity of this problem is compounded by challenges
from both the Continual and Federated Learning perspectives. Specifically,
models trained in a CFL setup suffer from catastrophic forgetting which is
exacerbated by data heterogeneity across clients. Existing attempts at this
problem tend to impose large overheads on clients and communication channels or
require access to stored data which renders them unsuitable for real-world use
due to privacy. In this paper, we attempt to tackle forgetting and
heterogeneity while minimizing overhead costs and without requiring access to
any stored data. We achieve this by leveraging a prompting based approach (such
that only prompts and classifier heads have to be communicated) and proposing a
novel and lightweight generation and distillation scheme to consolidate client
models at the server. We formulate this problem for image classification and
establish strong baselines for comparison, conduct experiments on CIFAR-100 as
well as challenging, large-scale datasets like ImageNet-R and DomainNet. Our
approach outperforms both existing methods and our own baselines by as much as
7% while significantly reducing communication and client-level computation
costs.


------------------------------------------------------------------------------

Title:
Approaching Unanticipated Consequences

Abstract: In an ever-changing world, even software that fulfils its requirements may
have un-envisioned aftereffects with significant impacts. We explored how such
impacts can be better understood at the pre-design phase in support of
organisational preparedness. We considered three real-world case studies and
engaged with literature from several disciplines to develop a conceptual
framework. Across three workshops with industry practitioners and academics
creative strategies from speculative design practices were used to prompt
engagement with the framework. We found participant groups navigated the model
with either a convergent or divergent intent. The academics, operating in an
exploratory mode, came to a broad understanding of a class of technologies
through its impacts. Operating in an anticipatory mode the industry
practitioners came to a specific understanding of a technology's potential in
their workplace. The study demonstrated potential for the conceptual framework
to be used as a tool with implications for research and practice.


------------------------------------------------------------------------------

Title:
On Evolvability and Behavior Landscapes in Neuroevolutionary Divergent  Search

Abstract: Evolvability refers to the ability of an individual genotype (solution) to
produce offspring with mutually diverse phenotypes. Recent research has
demonstrated that divergent search methods, particularly novelty search,
promote evolvability by implicitly creating selective pressure for it. The main
objective of this paper is to provide a novel perspective on the relationship
between neuroevolutionary divergent search and evolvability. In order to
achieve this, several types of walks from the literature on fitness landscape
analysis are first adapted to this context. Subsequently, the interplay between
neuroevolutionary divergent search and evolvability under varying amounts of
evolutionary pressure and under different diversity metrics is investigated. To
this end, experiments are performed on Fetch Pick and Place, a robotic arm
task. Moreover, the performed study in particular sheds light on the structure
of the genotype-phenotype mapping (the behavior landscape). Finally, a novel
definition of evolvability that takes into account the evolvability of
offspring and is appropriate for use with discretized behavior spaces is
proposed, together with a Markov-chain-based estimation method for it.


------------------------------------------------------------------------------

Title:
Revealing the impact of social circumstances on the selection of cancer  therapy through natural language processing of social work notes

Abstract: We aimed to investigate the impact of social circumstances on cancer therapy
selection using natural language processing to derive insights from social
worker documentation. We developed and employed a Bidirectional Encoder
Representations from Transformers (BERT) based approach, using a hierarchical
multi-step BERT model (BERT-MS) to predict the prescription of targeted cancer
therapy to patients based solely on documentation by clinical social workers.
Our corpus included free-text clinical social work notes, combined with
medication prescription information, for all patients treated for breast
cancer. We conducted a feature importance analysis to pinpoint the specific
social circumstances that impact cancer therapy selection. Using only social
work notes, we consistently predicted the administration of targeted therapies,
suggesting systematic differences in treatment selection exist due to
non-clinical factors. The UCSF-BERT model, pretrained on clinical text at UCSF,
outperformed other publicly available language models with an AUROC of 0.675
and a Macro F1 score of 0.599. The UCSF BERT-MS model, capable of leveraging
multiple pieces of notes, surpassed the UCSF-BERT model in both AUROC and
Macro-F1. Our feature importance analysis identified several clinically
intuitive social determinants of health (SDOH) that potentially contribute to
disparities in treatment. Our findings indicate that significant disparities
exist among breast cancer patients receiving different types of therapies based
on social determinants of health. Social work reports play a crucial role in
understanding these disparities in clinical decision-making.


------------------------------------------------------------------------------

Title:
Process Knowledge-infused Learning for Clinician-friendly Explanations

Abstract: Language models have the potential to assess mental health using social media
data. By analyzing online posts and conversations, these models can detect
patterns indicating mental health conditions like depression, anxiety, or
suicidal thoughts. They examine keywords, language markers, and sentiment to
gain insights into an individual's mental well-being. This information is
crucial for early detection, intervention, and support, improving mental health
care and prevention strategies. However, using language models for mental
health assessments from social media has two limitations: (1) They do not
compare posts against clinicians' diagnostic processes, and (2) It's
challenging to explain language model outputs using concepts that the clinician
can understand, i.e., clinician-friendly explanations. In this study, we
introduce Process Knowledge-infused Learning (PK-iL), a new learning paradigm
that layers clinical process knowledge structures on language model outputs,
enabling clinician-friendly explanations of the underlying language model
predictions. We rigorously test our methods on existing benchmark datasets,
augmented with such clinical process knowledge, and release a new dataset for
assessing suicidality. PK-iL performs competitively, achieving a 70% agreement
with users, while other XAI methods only achieve 47% agreement (average
inter-rater agreement of 0.72). Our evaluations demonstrate that PK-iL
effectively explains model predictions to clinicians.


------------------------------------------------------------------------------

Title:
GRM: Generative Relevance Modeling Using Relevance-Aware Sample  Estimation for Document Retrieval

Abstract: Recent studies show that Generative Relevance Feedback (GRF), using text
generated by Large Language Models (LLMs), can enhance the effectiveness of
query expansion. However, LLMs can generate irrelevant information that harms
retrieval effectiveness. To address this, we propose Generative Relevance
Modeling (GRM) that uses Relevance-Aware Sample Estimation (RASE) for more
accurate weighting of expansion terms. Specifically, we identify similar real
documents for each generated document and use a neural re-ranker to estimate
their relevance. Experiments on three standard document ranking benchmarks show
that GRM improves MAP by 6-9% and R@1k by 2-4%, surpassing previous methods.


------------------------------------------------------------------------------

Title:
Efficient Search and Detection of Relevant Plant Parts using  Semantics-Aware Active Vision

Abstract: To automate harvesting and de-leafing of tomato plants using robots, it is
important to search and detect the relevant plant parts, namely tomatoes,
peduncles, and petioles. This is challenging due to high levels of occlusion in
tomato greenhouses. Active vision is a promising approach which helps robots to
deliberately plan camera viewpoints to overcome occlusion and improve
perception accuracy. However, current active-vision algorithms cannot
differentiate between relevant and irrelevant plant parts, making them
inefficient for targeted perception of specific plant parts. We propose a
semantic active-vision strategy that uses semantic information to identify the
relevant plant parts and prioritises them during view planning using an
attention mechanism. We evaluated our strategy using 3D models of tomato plants
with varying structural complexity, which closely represented occlusions in the
real world. We used a simulated environment to gain insights into our strategy,
while ensuring repeatability and statistical significance. At the end of ten
viewpoints, our strategy was able to correctly detect 85.5% of the plant parts,
about 4 parts more on average per plant compared to a volumetric active-vision
strategy. Also, it detected 5 and 9 parts more compared to two predefined
strategies and 11 parts more compared to a random strategy. It also performed
reliably with a median of 88.9% correctly-detected objects per plant in 96
experiments. Our strategy was also robust to uncertainty in plant and
plant-part position, plant complexity, and different viewpoint sampling
strategies. We believe that our work could significantly improve the speed and
robustness of automated harvesting and de-leafing in tomato crop production.


------------------------------------------------------------------------------

Title:
Stabilized Neural Differential Equations for Learning Constrained  Dynamics

Abstract: Many successful methods to learn dynamical systems from data have recently
been introduced. However, assuring that the inferred dynamics preserve known
constraints, such as conservation laws or restrictions on the allowed system
states, remains challenging. We propose stabilized neural differential
equations (SNDEs), a method to enforce arbitrary manifold constraints for
neural differential equations. Our approach is based on a stabilization term
that, when added to the original dynamics, renders the constraint manifold
provably asymptotically stable. Due to its simplicity, our method is compatible
with all common neural ordinary differential equation (NODE) models and broadly
applicable. In extensive empirical evaluations, we demonstrate that SNDEs
outperform existing methods while extending the scope of which types of
constraints can be incorporated into NODE training.


------------------------------------------------------------------------------

Title:
Politeness Stereotypes and Attack Vectors: Gender Stereotypes in  Japanese and Korean Language Models

Abstract: In efforts to keep up with the rapid progress and use of large language
models, gender bias research is becoming more prevalent in NLP. Non-English
bias research, however, is still in its infancy with most work focusing on
English. In our work, we study how grammatical gender bias relating to
politeness levels manifests in Japanese and Korean language models. Linguistic
studies in these languages have identified a connection between gender bias and
politeness levels, however it is not yet known if language models reproduce
these biases. We analyze relative prediction probabilities of the male and
female grammatical genders using templates and find that informal polite speech
is most indicative of the female grammatical gender, while rude and formal
speech is most indicative of the male grammatical gender. Further, we find
politeness levels to be an attack vector for allocational gender bias in
cyberbullying detection models. Cyberbullies can evade detection through simple
techniques abusing politeness levels. We introduce an attack dataset to (i)
identify representational gender bias across politeness levels, (ii)
demonstrate how gender biases can be abused to bypass cyberbullying detection
models and (iii) show that allocational biases can be mitigated via training on
our proposed dataset. Through our findings we highlight the importance of bias
research moving beyond its current English-centrism.


------------------------------------------------------------------------------

Title:
Echocardiography Segmentation Using Neural ODE-based Diffeomorphic  Registration Field

Abstract: Convolutional neural networks (CNNs) have recently proven their excellent
ability to segment 2D cardiac ultrasound images. However, the majority of
attempts to perform full-sequence segmentation of cardiac ultrasound videos
either rely on models trained only on keyframe images or fail to maintain the
topology over time. To address these issues, in this work, we consider
segmentation of ultrasound video as a registration estimation problem and
present a novel method for diffeomorphic image registration using neural
ordinary differential equations (Neural ODE). In particular, we consider the
registration field vector field between frames as a continuous trajectory ODE.
The estimated registration field is then applied to the segmentation mask of
the first frame to obtain a segment for the whole cardiac cycle. The proposed
method, Echo-ODE, introduces several key improvements compared to the previous
state-of-the-art. Firstly, by solving a continuous ODE, the proposed method
achieves smoother segmentation, preserving the topology of segmentation maps
over the whole sequence (Hausdorff distance: 3.7-4.4). Secondly, it maintains
temporal consistency between frames without explicitly optimizing for temporal
consistency attributes, achieving temporal consistency in 91% of the videos in
the dataset. Lastly, the proposed method is able to maintain the clinical
accuracy of the segmentation maps (MAE of the LVEF: 2.7-3.1). The results show
that our method surpasses the previous state-of-the-art in multiple aspects,
demonstrating the importance of spatial-temporal data processing for the
implementation of Neural ODEs in medical imaging applications. These findings
open up new research directions for solving echocardiography segmentation
tasks.


------------------------------------------------------------------------------

Title:
Framework and Benchmarks for Combinatorial and Mixed-variable Bayesian  Optimization

Abstract: This paper introduces a modular framework for Mixed-variable and
Combinatorial Bayesian Optimization (MCBO) to address the lack of systematic
benchmarking and standardized evaluation in the field. Current MCBO papers
often introduce non-diverse or non-standard benchmarks to evaluate their
methods, impeding the proper assessment of different MCBO primitives and their
combinations. Additionally, papers introducing a solution for a single MCBO
primitive often omit benchmarking against baselines that utilize the same
methods for the remaining primitives. This omission is primarily due to the
significant implementation overhead involved, resulting in a lack of controlled
assessments and an inability to showcase the merits of a contribution
effectively. To overcome these challenges, our proposed framework enables an
effortless combination of Bayesian Optimization components, and provides a
diverse set of synthetic and real-world benchmarking tasks. Leveraging this
flexibility, we implement 47 novel MCBO algorithms and benchmark them against
seven existing MCBO solvers and five standard black-box optimization algorithms
on ten tasks, conducting over 4000 experiments. Our findings reveal a superior
combination of MCBO primitives outperforming existing approaches and illustrate
the significance of model fit and the use of a trust region. We make our MCBO
library available under the MIT license at
\url{this https URL}.


------------------------------------------------------------------------------

Title:
Smart Sentiment Analysis-based Search Engine Classification Intelligence

Abstract: Search engines are widely used for finding information on the internet.
However, there are limitations in the current search approach, such as
providing popular but not necessarily relevant results. This research addresses
the issue of polysemy in search results by implementing a search function that
determines the sentimentality of the retrieved information. The study utilizes
a web crawler to collect data from the British Broadcasting Corporation (BBC)
news site, and the sentimentality of the news articles is determined using the
Sentistrength program. The results demonstrate that the proposed search
function improves recall value while accurately retrieving nonpolysemous news.
Furthermore, Sentistrength outperforms deep learning and clustering methods in
classifying search results. The methodology presented in this article can be
applied to analyze the sentimentality and reputation of entities on the
internet.


------------------------------------------------------------------------------

Title:
Dynamic Decision Tree Ensembles for Energy-Efficient Inference on IoT  Edge Nodes

Abstract: With the increasing popularity of Internet of Things (IoT) devices, there is
a growing need for energy-efficient Machine Learning (ML) models that can run
on constrained edge nodes. Decision tree ensembles, such as Random Forests
(RFs) and Gradient Boosting (GBTs), are particularly suited for this task,
given their relatively low complexity compared to other alternatives. However,
their inference time and energy costs are still significant for edge hardware.
Given that said costs grow linearly with the ensemble size, this paper proposes
the use of dynamic ensembles, that adjust the number of executed trees based
both on a latency/energy target and on the complexity of the processed input,
to trade-off computational cost and accuracy. We focus on deploying these
algorithms on multi-core low-power IoT devices, designing a tool that
automatically converts a Python ensemble into optimized C code, and exploring
several optimizations that account for the available parallelism and memory
hierarchy. We extensively benchmark both static and dynamic RFs and GBTs on
three state-of-the-art IoT-relevant datasets, using an 8-core ultra-lowpower
System-on-Chip (SoC), GAP8, as the target platform. Thanks to the proposed
early-stopping mechanisms, we achieve an energy reduction of up to 37.9% with
respect to static GBTs (8.82 uJ vs 14.20 uJ per inference) and 41.7% with
respect to static RFs (2.86 uJ vs 4.90 uJ per inference), without losing
accuracy compared to the static model.


------------------------------------------------------------------------------

Title:
MementoHash: A Stateful, Minimal Memory, Best Performing Consistent Hash  Algorithm

Abstract: Consistent hashing is used in distributed systems and networking applications
to spread data evenly and efficiently across a cluster of nodes. In this paper,
we present MementoHash, a novel consistent hashing algorithm that eliminates
known limitations of state-of-the-art algorithms while keeping optimal
performance and minimal memory usage. We describe the algorithm in detail,
provide a pseudo-code implementation, and formally establish its solid
theoretical guarantees. To measure the efficacy of MementoHash, we compare its
performance, in terms of memory usage and lookup time, to that of
state-of-the-art algorithms, namely, AnchorHash, DxHash, and JumpHash. Unlike
JumpHash, MementoHash can handle random failures. Moreover, MementoHash does
not require fixing the overall capacity of the cluster (as AnchorHash and
DxHash do), allowing it to scale indefinitely. The number of removed nodes
affects the performance of all the considered algorithms. Therefore, we conduct
experiments considering three different scenarios: stable (no removed nodes),
one-shot removals (90% of the nodes removed at once), and incremental removals.
We report experimental results that averaged a varying number of nodes from ten
to one million. Results indicate that our algorithm shows optimal lookup
performance and minimal memory usage in its best-case scenario. It behaves
better than AnchorHash and DxHash in its average-case scenario and at least as
well as those two algorithms in its worst-case scenario. However, the
worst-case scenario for MementoHash occurs when more than 70% of the nodes
fail, which describes a unlikely scenario. Therefore, MementoHash shows the
best performance during the regular life cycle of a cluster.


------------------------------------------------------------------------------

Title:
Understanding Deep Generative Models with Generalized Empirical  Likelihoods

Abstract: Understanding how well a deep generative model captures a distribution of
high-dimensional data remains an important open challenge. It is especially
difficult for certain model classes, such as Generative Adversarial Networks
and Diffusion Models, whose models do not admit exact likelihoods. In this
work, we demonstrate that generalized empirical likelihood (GEL) methods offer
a family of diagnostic tools that can identify many deficiencies of deep
generative models (DGMs). We show, with appropriate specification of moment
conditions, that the proposed method can identify which modes have been
dropped, the degree to which DGMs are mode imbalanced, and whether DGMs
sufficiently capture intra-class diversity. We show how to combine techniques
from Maximum Mean Discrepancy and Generalized Empirical Likelihood to create
not only distribution tests that retain per-sample interpretability, but also
metrics that include label information. We find that such tests predict the
degree of mode dropping and mode imbalance up to 60% better than metrics such
as improved precision/recall.


------------------------------------------------------------------------------

Title:
A Hierarchical Bayesian Model for Deep Few-Shot Meta Learning

Abstract: We propose a novel hierarchical Bayesian model for learning with a large
(possibly infinite) number of tasks/episodes, which suits well the few-shot
meta learning problem. We consider episode-wise random variables to model
episode-specific target generative processes, where these local random
variables are governed by a higher-level global random variate. The global
variable helps memorize the important information from historic episodes while
controlling how much the model needs to be adapted to new episodes in a
principled Bayesian manner. Within our model framework, the prediction on a
novel episode/task can be seen as a Bayesian inference problem. However, a main
obstacle in learning with a large/infinite number of local random variables in
online nature, is that one is not allowed to store the posterior distribution
of the current local random variable for frequent future updates, typical in
conventional variational inference. We need to be able to treat each local
variable as a one-time iterate in the optimization. We propose a
Normal-Inverse-Wishart model, for which we show that this one-time iterate
optimization becomes feasible due to the approximate closed-form solutions for
the local posterior distributions. The resulting algorithm is more attractive
than the MAML in that it is not required to maintain computational graphs for
the whole gradient optimization steps per episode. Our approach is also
different from existing Bayesian meta learning methods in that unlike dealing
with a single random variable for the whole episodes, our approach has a
hierarchical structure that allows one-time episodic optimization, desirable
for principled Bayesian learning with many/infinite tasks. The code is
available at \url{this https URL}.


------------------------------------------------------------------------------

Title:
Inspire creativity with ORIBA: Transform Artists' Original Characters  into Chatbots through Large Language Model

Abstract: This research delves into the intersection of illustration art and artificial
intelligence (AI), focusing on how illustrators engage with AI agents that
embody their original characters (OCs). We introduce 'ORIBA', a customizable AI
chatbot that enables illustrators to converse with their OCs. This approach
allows artists to not only receive responses from their OCs but also to observe
their inner monologues and behavior. Despite the existing tension between
artists and AI, our study explores innovative collaboration methods that are
inspiring to illustrators. By examining the impact of AI on the creative
process and the boundaries of authorship, we aim to enhance human-AI
interactions in creative fields, with potential applications extending beyond
illustration to interactive storytelling and more.


------------------------------------------------------------------------------

Title:
Gradient is All You Need?

Abstract: In this paper we provide a novel analytical perspective on the theoretical
understanding of gradient-based learning algorithms by interpreting
consensus-based optimization (CBO), a recently proposed multi-particle
derivative-free optimization method, as a stochastic relaxation of gradient
descent. Remarkably, we observe that through communication of the particles,
CBO exhibits a stochastic gradient descent (SGD)-like behavior despite solely
relying on evaluations of the objective function. The fundamental value of such
link between CBO and SGD lies in the fact that CBO is provably globally
convergent to global minimizers for ample classes of nonsmooth and nonconvex
objective functions, hence, on the one side, offering a novel explanation for
the success of stochastic relaxations of gradient descent. On the other side,
contrary to the conventional wisdom for which zero-order methods ought to be
inefficient or not to possess generalization abilities, our results unveil an
intrinsic gradient descent nature of such heuristics. This viewpoint
furthermore complements previous insights into the working principles of CBO,
which describe the dynamics in the mean-field limit through a nonlinear
nonlocal partial differential equation that allows to alleviate complexities of
the nonconvex function landscape. Our proofs leverage a completely nonsmooth
analysis, which combines a novel quantitative version of the Laplace principle
(log-sum-exp trick) and the minimizing movement scheme (proximal iteration). In
doing so, we furnish useful and precise insights that explain how stochastic
perturbations of gradient descent overcome energy barriers and reach deep
levels of nonconvex functions. Instructive numerical illustrations support the
provided theoretical insights.


------------------------------------------------------------------------------

Title:
The Information Bottleneck's Ordinary Differential Equation: First-Order  Root-Tracking for the IB

Abstract: The Information Bottleneck (IB) is a method of lossy compression. Its
rate-distortion (RD) curve describes the fundamental tradeoff between input
compression and the preservation of relevant information. However, it conceals
the underlying dynamics of optimal input encodings. We argue that these
typically follow a piecewise smooth trajectory as the input information is
being compressed, as recently shown in RD. These smooth dynamics are
interrupted when an optimal encoding changes qualitatively, at a bifurcation.
By leveraging the IB's intimate relations with RD, sub-optimal solutions can be
seen to collide or exchange optimality there.
Despite the acceptance of the IB and its applications, there are surprisingly
few techniques to solve it numerically, even for finite problems whose
distribution is known. We derive anew the IB's first-order Ordinary
Differential Equation, which describes the dynamics underlying its optimal
tradeoff curve. To exploit these dynamics, one needs not only to detect IB
bifurcations but also to identify their type in order to handle them
accordingly. Rather than approaching the optimal IB curve from sub-optimal
directions, the latter allows us to follow a solution's trajectory along the
optimal curve, under mild assumptions. Thereby, translating an understanding of
IB bifurcations into a surprisingly accurate numerical algorithm.


------------------------------------------------------------------------------

Title:
Full Parameter Fine-tuning for Large Language Models with Limited  Resources

Abstract: Large Language Models (LLMs) have revolutionized Natural Language Processing
(NLP) but demand massive GPU resources for training. Lowering the threshold for
LLMs training would encourage greater participation from researchers,
benefiting both academia and society. While existing approaches have focused on
parameter-efficient fine-tuning, which tunes or adds a small number of
parameters, few have addressed the challenge of tuning the full parameters of
LLMs with limited resources. In this work, we propose a new optimizer,
LOw-Memory Optimization (LOMO), which fuses the gradient computation and the
parameter update in one step to reduce memory usage. By integrating LOMO with
existing memory saving techniques, we reduce memory usage to 10.8% compared to
the standard approach (DeepSpeed solution). Consequently, our approach enables
the full parameter fine-tuning of a 65B model on a single machine with 8 RTX
3090, each with 24GB memory.


------------------------------------------------------------------------------

Title:
$\pi2\text{vec}$: Policy Representations with Successor Features

Abstract: This paper describes $\pi2\text{vec}$, a method for representing behaviors of
black box policies as feature vectors. The policy representations capture how
the statistics of foundation model features change in response to the policy
behavior in a task agnostic way, and can be trained from offline data, allowing
them to be used in offline policy selection. This work provides a key piece of
a recipe for fusing together three modern lines of research: Offline policy
evaluation as a counterpart to offline RL, foundation models as generic and
powerful state representations, and efficient policy selection in resource
constrained environments.


------------------------------------------------------------------------------

Title:
An approach to provide serverless scientific pipelines within the  context of SKA

Abstract: Function-as-a-Service (FaaS) is a type of serverless computing that allows
developers to write and deploy code as individual functions, which can be
triggered by specific events or requests. FaaS platforms automatically manage
the underlying infrastructure, scaling it up or down as needed, being highly
scalable, cost-effective and offering a high level of abstraction. Prototypes
being developed within the SKA Regional Center Network (SRCNet) are exploring
models for data distribution, software delivery and distributed computing with
the goal of moving and executing computation to where the data is. Since SKA
will be the largest data producer on the planet, it will be necessary to
distribute this massive volume of data to the SRCNet nodes that will serve as a
hub for computing and analysis operations on the closest data. Within this
context, in this work we want to validate the feasibility of designing and
deploying functions and applications commonly used in radio interferometry
workflows within a FaaS platform to demonstrate the value of this computing
model as an alternative to explore for data processing in the distributed nodes
of the SRCNet. We have analyzed several FaaS platforms and successfully
deployed one of them, where we have imported several functions using two
different methods: microfunctions from the CASA framework, which are written in
Python code, and highly specific native applications like wsclean. Therefore,
we have designed a simple catalogue that can be easily scaled to provide all
the key features of FaaS in highly distributed environments using
orchestrators, as well as having the ability to integrate them with workflows
or APIs. This paper contributes to the ongoing discussion of the potential of
FaaS models for scientific data processing, particularly in the context of
large-scale, distributed projects such as SKA.


------------------------------------------------------------------------------

Title:
Analysis of the Age of Information in Age-Threshold Slotted ALOHA

Abstract: We investigate the performance of a random access network consisting of
source-destination dipoles. The source nodes transmit information packets to
their destinations over a shared spectrum. All the transmitters in this network
adhere to an age threshold slotted ALOHA (TSA) protocol: every source node
remains silent until the age of information (AoI) reaches a threshold, after
which the source accesses the radio channel with a certain probability. We
derive a tight approximation for the signal-to-interference-plus-noise ratio
(SINR) meta distribution and verify its accuracy through simulations. We also
obtain analytical expressions for the average AoI. Our analysis reveals that
when the network is densely deployed, employing TSA significantly decreases the
average AoI. The update rate and age threshold must be jointly optimized to
fully exploit the potential of the TSA protocol.


------------------------------------------------------------------------------

Title:
Scaling Open-Vocabulary Object Detection

Abstract: Open-vocabulary object detection has benefited greatly from pretrained
vision-language models, but is still limited by the amount of available
detection training data. While detection training data can be expanded by using
Web image-text pairs as weak supervision, this has not been done at scales
comparable to image-level pretraining. Here, we scale up detection data with
self-training, which uses an existing detector to generate pseudo-box
annotations on image-text pairs. Major challenges in scaling self-training are
the choice of label space, pseudo-annotation filtering, and training
efficiency. We present the OWLv2 model and OWL-ST self-training recipe, which
address these challenges. OWLv2 surpasses the performance of previous
state-of-the-art open-vocabulary detectors already at comparable training
scales (~10M examples). However, with OWL-ST, we can scale to over 1B examples,
yielding further large improvement: With an L/14 architecture, OWL-ST improves
AP on LVIS rare classes, for which the model has seen no human box annotations,
from 31.2% to 44.6% (43% relative improvement). OWL-ST unlocks Web-scale
training for open-world localization, similar to what has been seen for image
classification and language modelling.


------------------------------------------------------------------------------

Title:
Terahertz Near-Field Communications and Sensing

Abstract: This article focuses on the near-field effect in terahertz (THz)
communications and sensing systems. By equipping with extremely large-scale
antenna arrays (ELAAs), the near-field region in THz systems can be possibly
extended to hundreds of meters in proximity to THz transceivers, which
necessitates the consideration of near-field effect in the THz band both for
the communications and sensing. We first review the main characteristics of the
near-field region in the THz bands. The signal propagation in the near-field
region is characterized by spherical waves rather than planar waves in the
far-field region. This distinction introduces a new distance dimension to the
communication and sensing channels, which brings new opportunities and
challenges for both THz communications and sensing. More particularly, 1) For
THz communications, the near-field effect enables a new mechanism for
beamforming, namely, beamfocusing, in the focusing region. Furthermore, in THz
multiple-input and multiple-output (MIMO) systems, the near-field effect can be
exploited to combat the multiplexing gain degradation caused by the sparse THz
channels. To address the near-field beam split effect caused by the
conventional frequency-independent hybrid beamforming architecture in THz
wideband communications, we propose a pair of wideband beamforming optimization
approaches by a new hybrid beamforming architecture based on true-time-delayers
(TTDs). 2) For THz sensing, joint angle and distance sensing can be achieved in
the near-field region. Additionally, the near-field beam split becomes a
beneficial effect for enhancing the sensing performance by focusing on multiple
possible target locations rather than a drawback encountered in communications.
Finally, several topics for future research are discussed.


------------------------------------------------------------------------------

Title:
RED$^{\rm FM}$: a Filtered and Multilingual Relation Extraction Dataset

Abstract: Relation Extraction (RE) is a task that identifies relationships between
entities in a text, enabling the acquisition of relational facts and bridging
the gap between natural language and structured knowledge. However, current RE
models often rely on small datasets with low coverage of relation types,
particularly when working with languages other than English. In this paper, we
address the above issue and provide two new resources that enable the training
and evaluation of multilingual RE systems. First, we present SRED$^{\rm FM}$,
an automatically annotated dataset covering 18 languages, 400 relation types,
13 entity types, totaling more than 40 million triplet instances. Second, we
propose RED$^{\rm FM}$, a smaller, human-revised dataset for seven languages
that allows for the evaluation of multilingual RE systems. To demonstrate the
utility of these novel datasets, we experiment with the first end-to-end
multilingual RE model, mREBEL, that extracts triplets, including entity types,
in multiple languages. We release our resources and model checkpoints at
this https URL


------------------------------------------------------------------------------

Title:
State-Of-The-Practice in Quality Assurance in Java-Based Open Source  Software Development

Abstract: To ensure the quality of software systems, software engineers can make use of
a variety of quality assurance approaches, such as software testing, modern
code review, automated static analysis, and build automation. Each of these
quality assurance practices has been studied in depth in isolation, but there
is a clear knowledge gap when it comes to our understanding of how these
approaches are being used in conjunction or not. In our study, we broadly
investigate whether and how these quality assurance approaches are being used
in conjunction in the development of 1,454 popular open source software
projects on GitHub. Our study indicates that typically projects do not follow
all quality assurance practices together with high intensity. In fact, we only
observe weak correlation among some quality assurance practices. In general,
our study provides a deeper understanding of how existing quality assurance
approaches are currently being used in Java-based open source software
development. Besides, we specifically zoomed in on the more mature projects in
our dataset, and generally, we observe that more mature projects are more
intense in their application of the quality assurance practices, with more
focus on their ASAT usage and code reviewing, but no strong change in their CI
usage.


------------------------------------------------------------------------------

Title:
Learning CO$_2$ plume migration in faulted reservoirs with Graph Neural  Networks

Abstract: Deep-learning-based surrogate models provide an efficient complement to
numerical simulations for subsurface flow problems such as CO$_2$ geological
storage. Accurately capturing the impact of faults on CO$_2$ plume migration
remains a challenge for many existing deep learning surrogate models based on
Convolutional Neural Networks (CNNs) or Neural Operators. We address this
challenge with a graph-based neural model leveraging recent developments in the
field of Graph Neural Networks (GNNs). Our model combines graph-based
convolution Long-Short-Term-Memory (GConvLSTM) with a one-step GNN model,
MeshGraphNet (MGN), to operate on complex unstructured meshes and limit
temporal error accumulation. We demonstrate that our approach can accurately
predict the temporal evolution of gas saturation and pore pressure in a
synthetic reservoir with impermeable faults. Our results exhibit a better
accuracy and a reduced temporal error accumulation compared to the standard MGN
model. We also show the excellent generalizability of our algorithm to mesh
configurations, boundary conditions, and heterogeneous permeability fields not
included in the training set. This work highlights the potential of GNN-based
methods to accurately and rapidly model subsurface flow with complex faults and
fractures.


------------------------------------------------------------------------------

Title:
Two arbitrary-order constraint-preserving schemes for the Yang--Mills  equations on polyhedral meshes

Abstract: Two numerical schemes are proposed and investigated for the Yang--Mills
equations, which can be seen as a nonlinear generalisation of the Maxwell
equations set on Lie algebra-valued functions, with similarities to certain
formulations of General Relativity. Both schemes are built on the Discrete de
Rham (DDR) method, and inherit from its main features: an arbitrary order of
accuracy, and applicability to generic polyhedral meshes. They make use of the
complex property of the DDR, together with a Lagrange-multiplier approach, to
preserve, at the discrete level, a nonlinear constraint associated with the
Yang--Mills equations. We also show that the schemes satisfy a discrete energy
dissipation (the dissipation coming solely from the implicit time stepping).
Issues around the practical implementations of the schemes are discussed; in
particular, the assembly of the local contributions in a way that minimises the
price we pay in dealing with nonlinear terms, in conjunction with the
tensorisation coming from the Lie algebra. Numerical tests are provided using a
manufactured solution, and show that both schemes display a convergence in
$L^2$-norm of the potential and electrical fields in $\mathcal O(h^{k+1})$
(provided that the time step is of that order), where $k$ is the polynomial
degree chosen for the DDR complex. We also numerically demonstrate the
preservation of the constraint.


------------------------------------------------------------------------------

Title:
Synchronizing Machine Learning Algorithms, Realtime Robotic Control and  Simulated Environment with o80

Abstract: Robotic applications require the integration of various modalities,
encompassing perception, control of real robots and possibly the control of
simulated environments. While the state-of-the-art robotic software solutions
such as ROS 2 provide most of the required features, flexible synchronization
between algorithms, data streams and control loops can be tedious. o80 is a
versatile C++ framework for robotics which provides a shared memory model and a
command framework for real-time critical systems. It enables expert users to
set up complex robotic systems and generate Python bindings for scientists.
o80's unique feature is its flexible synchronization between processes,
including the traditional blocking commands and the novel ``bursting mode'',
which allows user code to control the execution of the lower process control
loop. This makes it particularly useful for setups that mix real and simulated
environments.


------------------------------------------------------------------------------

Title:
A Signal Temporal Logic Planner for Ergonomic Human-Robot Collaboration

Abstract: This paper proposes a method for designing human-robot collaboration tasks
and generating corresponding trajectories. The method uses high-level
specifications, expressed as a Signal Temporal Logic (STL) formula, to
automatically synthesize task assignments and trajectories. To illustrate the
approach, we focus on a specific task: a multi-rotor aerial vehicle performing
object handovers in a power line setting. The motion planner considers
limitations, such as payload capacity and recharging constraints, while
ensuring that the trajectories are feasible. Additionally, the method enables
users to specify robot behaviors that take into account human comfort (e.g.,
ergonomics, preferences) while using high-level goals and constraints. The
approach is validated through numerical analyzes in MATLAB and realistic Gazebo
simulations using a mock-up scenario.


------------------------------------------------------------------------------

Title:
Efficient Coflow Scheduling in Hybrid-Switched Data Center Networks

Abstract: To improve the application-level communication performance, scheduling of
coflows, a collection of parallel flows sharing the same objective, is
prevalent in modern data center networks (DCNs). Meanwhile, a hybrid-switched
DCN design combining optical circuit switches (OPS) and electrical packet
switches (EPS) for transmitting high-volume traffic and low-volume traffic
separately has received considerable research attention recently. Efficient
scheduling of coflows on hybrid network links is crucial for reducing the
overall communication time. However, because of the reconfiguration delay in
the circuit switch due to the ultra-high transmission rate and the limitation
of bandwidth in the packet switch, coflow scheduling becomes increasingly
challenging. The existing coflow scheduling algorithms in hybrid-switched DCNs
are all heuristic and provide no performance guarantees. In this work, we
propose an approximation algorithm with the worst-case performance guarantee of
2+ \lambda?, where \lambda? is a factor related to system parameters and demand
characteristics, for single coflow scheduling in hybridswitched DCN to minimize
the coflow completion time (CCT). Extensive simulations based on Facebook data
traces show that our algorithm outperforms the state-of-the-art schemes
Solstice by 1.14? and Reco-Sin by 1.42? in terms of minimizing CCT.


------------------------------------------------------------------------------

Title:
CroCoDai: A Stablecoin for Cross-Chain Commerce

Abstract: Decentralized Finance (DeFi), in which digital assets are exchanged without
trusted intermediaries, has grown rapidly in value in recent years. The global
DeFi ecosystem is fragmented into multiple blockchains, fueling the demand for
cross-chain commerce. Existing approaches for cross-chain transactions, e.g.,
bridges and cross-chain deals, achieve atomicity by locking assets in escrow.
However, locking up assets increases the financial risks for the participants,
especially due to price fluctuations and the long latency of cross-chain
transactions. Stablecoins, which are pegged to a non-volatile asset such as the
US dollar, help mitigate the risk associated with price fluctuations. However,
existing stablecoin designs are tied to individual blockchain platforms, and
trusted parties or complex protocols are needed to exchange stablecoin tokens
between blockchains.
Our goal is to design a practical stablecoin for cross-chain commerce.
Realizing this goal requires addressing two challenges. The first challenge is
to support a large and growing number of blockchains efficiently. The second
challenge is to be resilient to price fluctuations and blockchain platform
failures. We present CroCoDai to address these challenges. We also present
three prototype implementations of our stablecoin system, and show that it
incurs small execution overhead.


------------------------------------------------------------------------------

Title:
Representation and decomposition of functions in DAG-DNNs and structural  network pruning

Abstract: The conclusions provided by deep neural networks (DNNs) must be carefully
scrutinized to determine whether they are universal or architecture dependent.
The term DAG-DNN refers to a graphical representation of a DNN in which the
architecture is expressed as a direct-acyclic graph (DAG), on which arcs are
associated with functions. The level of a node denotes the maximum number of
hops between the input node and the node of interest. In the current study, we
demonstrate that DAG-DNNs can be used to derive all functions defined on
various sub-architectures of the DNN. We also demonstrate that the functions
defined in a DAG-DNN can be derived via a sequence of lower-triangular
matrices, each of which provides the transition of functions defined in
sub-graphs up to nodes at a specified level. The lifting structure associated
with lower-triangular matrices makes it possible to perform the structural
pruning of a network in a systematic manner. The fact that decomposition is
universally applicable to all DNNs means that network pruning could
theoretically be applied to any DNN, regardless of the underlying architecture.
We demonstrate that it is possible to obtain the winning ticket (sub-network
and initialization) for a weak version of the lottery ticket hypothesis, based
on the fact that the sub-network with initialization can achieve training
performance on par with that of the original network using the same number of
iterations or fewer.


------------------------------------------------------------------------------

Title:
Semi-Offline Reinforcement Learning for Optimized Text Generation

Abstract: In reinforcement learning (RL), there are two major settings for interacting
with the environment: online and offline. Online methods explore the
environment at significant time cost, and offline methods efficiently obtain
reward signals by sacrificing exploration capability. We propose semi-offline
RL, a novel paradigm that smoothly transits from offline to online settings,
balances exploration capability and training cost, and provides a theoretical
foundation for comparing different RL settings. Based on the semi-offline
formulation, we present the RL setting that is optimal in terms of optimization
cost, asymptotic error, and overfitting error bound. Extensive experiments show
that our semi-offline approach is efficient and yields comparable or often
better performance compared with state-of-the-art methods.


------------------------------------------------------------------------------

Title:
Using Natural Language Processing and Networks to Automate Structured  Literature Reviews: An Application to Farmers Climate Change Adaptation

Abstract: The fast-growing number of research articles makes it problematic for
scholars to keep track of the new findings related to their areas of expertise.
Furthermore, linking knowledge across disciplines in rapidly developing fields
becomes challenging for complex topics like climate change that demand
interdisciplinary solutions. At the same time, the rise of Black Box types of
text summarization makes it difficult to understand how text relationships are
built, let alone relate to existing theories conceptualizing cause-effect
relationships and permitting hypothesizing. This work aims to sensibly use
Natural Language Processing by extracting variables relations and synthesizing
their findings using networks while relating to key concepts dominant in
relevant disciplines. As an example, we apply our methodology to the analysis
of farmers' adaptation to climate change. For this, we perform a Natural
Language Processing analysis of publications returned by Scopus in August 2022.
Results show that the use of Natural Language Processing together with networks
in a descriptive manner offers a fast and interpretable way to synthesize
literature review findings as long as researchers back up results with theory.


------------------------------------------------------------------------------

Title:
GPINN: Physics-informed Neural Network with Graph Embedding

Abstract: This work proposes a Physics-informed Neural Network framework with Graph
Embedding (GPINN) to perform PINN in graph, i.e. topological space instead of
traditional Euclidean space, for improved problem-solving efficiency. The
method integrates topological data into the neural network's computations,
which significantly boosts the performance of the Physics-Informed Neural
Network (PINN). The graph embedding technique infuses extra dimensions into the
input space to encapsulate the spatial characteristics of a graph while
preserving the properties of the original space. The selection of these extra
dimensions is guided by the Fiedler vector, offering an optimised pathologic
notation of the graph. Two case studies are conducted, which demonstrate
significant improvement in the performance of GPINN in comparison to
traditional PINN, particularly in its superior ability to capture physical
features of the solution.


------------------------------------------------------------------------------

Title:
Automatic Trade-off Adaptation in Offline RL

Abstract: Recently, offline RL algorithms have been proposed that remain adaptive at
runtime. For example, the LION algorithm \cite{lion} provides the user with an
interface to set the trade-off between behavior cloning and optimality w.r.t.
the estimated return at runtime. Experts can then use this interface to adapt
the policy behavior according to their preferences and find a good trade-off
between conservatism and performance optimization. Since expert time is
precious, we extend the methodology with an autopilot that automatically finds
the correct parameterization of the trade-off, yielding a new algorithm which
we term AutoLION.


------------------------------------------------------------------------------

Title:
Collapsed Inference for Bayesian Deep Learning

Abstract: Bayesian neural networks (BNNs) provide a formalism to quantify and calibrate
uncertainty in deep learning. Current inference approaches for BNNs often
resort to few-sample estimation for scalability, which can harm predictive
performance, while its alternatives tend to be computationally prohibitively
expensive. We tackle this challenge by revealing a previously unseen connection
between inference on BNNs and volume computation problems. With this
observation, we introduce a novel collapsed inference scheme that performs
Bayesian model averaging using collapsed samples. It improves over a
Monte-Carlo sample by limiting sampling to a subset of the network weights
while pairing it with some closed-form conditional distribution over the rest.
A collapsed sample represents uncountably many models drawn from the
approximate posterior and thus yields higher sample efficiency. Further, we
show that the marginalization of a collapsed sample can be solved analytically
and efficiently despite the non-linearity of neural networks by leveraging
existing volume computation solvers. Our proposed use of collapsed samples
achieves a balance between scalability and accuracy. On various regression and
classification tasks, our collapsed Bayesian deep learning approach
demonstrates significant improvements over existing methods and sets a new
state of the art in terms of uncertainty estimation as well as predictive
performance.


------------------------------------------------------------------------------

Title:
Temporal Difference Learning with Experience Replay

Abstract: Temporal-difference (TD) learning is widely regarded as one of the most
popular algorithms in reinforcement learning (RL). Despite its widespread use,
it has only been recently that researchers have begun to actively study its
finite time behavior, including the finite time bound on mean squared error and
sample complexity. On the empirical side, experience replay has been a key
ingredient in the success of deep RL algorithms, but its theoretical effects on
RL have yet to be fully understood. In this paper, we present a simple
decomposition of the Markovian noise terms and provide finite-time error bounds
for TD-learning with experience replay. Specifically, under the Markovian
observation model, we demonstrate that for both the averaged iterate and final
iterate cases, the error term induced by a constant step-size can be
effectively controlled by the size of the replay buffer and the mini-batch
sampled from the experience replay buffer.


------------------------------------------------------------------------------

Title:
Discourse Representation Structure Parsing for Chinese

Abstract: Previous work has predominantly focused on monolingual English semantic
parsing. We, instead, explore the feasibility of Chinese semantic parsing in
the absence of labeled data for Chinese meaning representations. We describe
the pipeline of automatically collecting the linearized Chinese meaning
representation data for sequential-to sequential neural networks. We further
propose a test suite designed explicitly for Chinese semantic parsing, which
provides fine-grained evaluation for parsing performance, where we aim to study
Chinese parsing difficulties. Our experimental results show that the difficulty
of Chinese semantic parsing is mainly caused by adverbs. Realizing Chinese
parsing through machine translation and an English parser yields slightly lower
performance than training a model directly on Chinese data.


------------------------------------------------------------------------------

Title:
Lost and not Found: An Investigation of Recovery Methods for  Multi-Factor Authentication

Abstract: Multi-Factor Authentication is intended to strengthen the security of
password-based authentication by adding another factor, such as hardware tokens
or one-time passwords using mobile apps. However, this increased authentication
security comes with potential drawbacks that can lead to account and asset
loss. If users lose access to their additional authentication factors for any
reason, they will be locked out of their accounts. Consequently, services that
provide Multi-Factor Authentication should deploy procedures to allow their
users to recover from losing access to their additional factor that are both
secure and easy-to-use. To the best of our knowledge, we are the first to
first-hand investigate the security and user experience of deployed
Multi-Factor Authentication recovery procedures. We first evaluate the official
help and support pages of 1,303 websites that provide Multi-Factor
Authentication and collect documented information about their recovery
procedures. Second, we select a subset of 71 websites, create accounts, set up
Multi-Factor Authentication, and perform an in-depth investigation of their
recovery procedure security and user experience. We find that many websites
deploy insecure Multi-Factor Authentication recovery procedures and allowed us
to circumvent and disable Multi-Factor Authentication when having access to the
accounts' associated email addresses. Furthermore, we commonly observed
discrepancies between our in-depth analysis and the official help and support
pages, implying that information meant to aid users is often either incorrect
or outdated.


------------------------------------------------------------------------------

Title:
Building blocks for complex tasks: Robust generative event extraction  for radiology reports under domain shifts

Abstract: This paper explores methods for extracting information from radiology reports
that generalize across exam modalities to reduce requirements for annotated
data. We demonstrate that multi-pass T5-based text-to-text generative models
exhibit better generalization across exam modalities compared to approaches
that employ BERT-based task-specific classification layers. We then develop
methods that reduce the inference cost of the model, making large-scale corpus
processing more feasible for clinical applications. Specifically, we introduce
a generative technique that decomposes complex tasks into smaller subtask
blocks, which improves a single-pass model when combined with multitask
training. In addition, we leverage target-domain contexts during inference to
enhance domain adaptation, enabling use of smaller models. Analyses offer
insights into the benefits of different cost reduction strategies.


------------------------------------------------------------------------------

Title:
The False Dawn: Reevaluating Google's Reinforcement Learning for Chip  Macro Placement

Abstract: Reinforcement learning (RL) for physical design of silicon chips in a Google
2021 Nature paper stirred controversy due to poorly documented claims that
raised eyebrows and attracted critical media coverage. The Nature paper
withheld most inputs needed to produce reported results and some critical steps
in the methodology. But two separate evaluations filled in the gaps and
demonstrated that Google RL lags behind human designers, behind a well-known
algorithm (Simulated Annealing), and also behind generally-available commercial
software. Crosschecked data indicate that the integrity of the Nature paper is
substantially undermined owing to errors in the conduct, analysis and
reporting.


------------------------------------------------------------------------------

Title:
A New Low-Rank Learning Robust Quaternion Tensor Completion Method for  Color Video Inpainting Problem and Fast Algorithms

Abstract: The color video inpainting problem is one of the most challenging problem in
the modern imaging science. It aims to recover a color video from a small part
of pixels that may contain noise. However, there are less of robust models that
can simultaneously preserve the coupling of color channels and the evolution of
color video frames. In this paper, we present a new robust quaternion tensor
completion (RQTC) model to solve this challenging problem and derive the exact
recovery theory. The main idea is to build a quaternion tensor optimization
model to recover a low-rank quaternion tensor that represents the targeted
color video and a sparse quaternion tensor that represents noise. This new
model is very efficient to recover high dimensional data that satisfies the
prior low-rank assumption. To solve the case without low-rank property, we
introduce a new low-rank learning RQTC model, which rearranges similar patches
classified by a quaternion learning method into smaller tensors satisfying the
prior low-rank assumption. We also propose fast algorithms with global
convergence guarantees. In numerical experiments, the proposed methods
successfully recover color videos with eliminating color contamination and
keeping the continuity of video scenery, and their solutions are of higher
quality in terms of PSNR and SSIM values than the state-of-the-art algorithms.


------------------------------------------------------------------------------

Title:
Label-noise-tolerant medical image classification via self-attention and  self-supervised learning

Abstract: Deep neural networks (DNNs) have been widely applied in medical image
classification and achieve remarkable classification performance. These
achievements heavily depend on large-scale accurately annotated training data.
However, label noise is inevitably introduced in the medical image annotation,
as the labeling process heavily relies on the expertise and experience of
annotators. Meanwhile, DNNs suffer from overfitting noisy labels, degrading the
performance of models. Therefore, in this work, we innovatively devise
noise-robust training approach to mitigate the adverse effects of noisy labels
in medical image classification. Specifically, we incorporate contrastive
learning and intra-group attention mixup strategies into the vanilla supervised
learning. The contrastive learning for feature extractor helps to enhance
visual representation of DNNs. The intra-group attention mixup module
constructs groups and assigns self-attention weights for group-wise samples,
and subsequently interpolates massive noisy-suppressed samples through weighted
mixup operation. We conduct comparative experiments on both synthetic and
real-world noisy medical datasets under various noise levels. Rigorous
experiments validate that our noise-robust method with contrastive learning and
attention mixup can effectively handle with label noise, and is superior to
state-of-the-art methods. An ablation study also shows that both components
contribute to boost model performance. The proposed method demonstrates its
capability of curb label noise and has certain potential toward real-world
clinic applications.


------------------------------------------------------------------------------

Title:
The Big Data Myth: Using Diffusion Models for Dataset Generation to  Train Deep Detection Models

Abstract: Despite the notable accomplishments of deep object detection models, a major
challenge that persists is the requirement for extensive amounts of training
data. The process of procuring such real-world data is a laborious undertaking,
which has prompted researchers to explore new avenues of research, such as
synthetic data generation techniques. This study presents a framework for the
generation of synthetic datasets by fine-tuning pretrained stable diffusion
models. The synthetic datasets are then manually annotated and employed for
training various object detection models. These detectors are evaluated on a
real-world test set of 331 images and compared against a baseline model that
was trained on real-world images. The results of this study reveal that the
object detection models trained on synthetic data perform similarly to the
baseline model. In the context of apple detection in orchards, the average
precision deviation with the baseline ranges from 0.09 to 0.12. This study
illustrates the potential of synthetic data generation techniques as a viable
alternative to the collection of extensive training data for the training of
deep models.


------------------------------------------------------------------------------

Title:
Cross-corpus Readability Compatibility Assessment for English Texts

Abstract: Text readability assessment has gained significant attention from researchers
in various domains. However, the lack of exploration into corpus compatibility
poses a challenge as different research groups utilize different corpora. In
this study, we propose a novel evaluation framework, Cross-corpus text
Readability Compatibility Assessment (CRCA), to address this issue. The
framework encompasses three key components: (1) Corpus: CEFR, CLEC, CLOTH, NES,
OSP, and RACE. Linguistic features, GloVe word vector representations, and
their fusion features were extracted. (2) Classification models: Machine
learning methods (XGBoost, SVM) and deep learning methods (BiLSTM,
Attention-BiLSTM) were employed. (3) Compatibility metrics: RJSD, RRNSS, and
NDCG metrics. Our findings revealed: (1) Validated corpus compatibility, with
OSP standing out as significantly different from other datasets. (2) An
adaptation effect among corpora, feature representations, and classification
methods. (3) Consistent outcomes across the three metrics, validating the
robustness of the compatibility assessment framework. The outcomes of this
study offer valuable insights into corpus selection, feature representation,
and classification methods, and it can also serve as a beginning effort for
cross-corpus transfer learning.


------------------------------------------------------------------------------

Title:
Cybersecurity Career Requirements: A Literature Review

Abstract: This study employs a systematic literature review approach to identify the
requirements of a career as a cybersecurity professional. It aims to raise
public awareness regarding opportunities in the Information Security (IS)
profession. A total of 1,520 articles were identified from four academic
databases by searching using the terms "cybersecurity" and "skills". After
rigorous screening according to various criteria, 31 papers remained. The
findings of these studies were thematically analyzed to describe the knowledge
and skills an IS professional should possess. The research found that a
considerable investment in time is necessary for cybersecurity professionals to
reach the required technical proficiency. It also identified female gender
barriers to cybersecurity careers due to the unique requirements of the field
and suggests that females may successfully enter at lower levels and progress
up the tiers as circumstances dictate.


------------------------------------------------------------------------------

Title:
Can Orbital Servers Provide Mars-Wide Edge Computing?

Abstract: Human landing, exploration and settlement on Mars will require local compute
resources at the Mars edge. Landing such resources on Mars is an expensive
endeavor. Instead, in this paper we lay out how concepts from low-Earth orbit
edge computing may be applied to Mars edge computing. This could lower
launching costs of compute resources for Mars while also providing Mars-wide
networking and compute coverage. We propose a possible Mars compute
constellation, discuss applications, analyze feasibility, and raise research
questions for future work.


------------------------------------------------------------------------------

Title:
Reducing Computational Costs in Sentiment Analysis: Tensorized Recurrent  Networks vs. Recurrent Networks

Abstract: Anticipating audience reaction towards a certain text is integral to several
facets of society ranging from politics, research, and commercial industries.
Sentiment analysis (SA) is a useful natural language processing (NLP) technique
that utilizes lexical/statistical and deep learning methods to determine
whether different-sized texts exhibit positive, negative, or neutral emotions.
Recurrent networks are widely used in machine-learning communities for problems
with sequential data. However, a drawback of models based on Long-Short Term
Memory networks and Gated Recurrent Units is the significantly high number of
parameters, and thus, such models are computationally expensive. This drawback
is even more significant when the available data are limited. Also, such models
require significant over-parameterization and regularization to achieve optimal
performance. Tensorized models represent a potential solution. In this paper,
we classify the sentiment of some social media posts. We compare traditional
recurrent models with their tensorized version, and we show that with the
tensorized models, we reach comparable performances with respect to the
traditional models while using fewer resources for the training.


------------------------------------------------------------------------------

Title:
Pushing the Limits of ChatGPT on NLP Tasks

Abstract: Despite the success of ChatGPT, its performances on most NLP tasks are still
well below the supervised baselines. In this work, we looked into the causes,
and discovered that its subpar performance was caused by the following factors:
(1) token limit in the prompt does not allow for the full utilization of the
supervised datasets; (2) mismatch between the generation nature of ChatGPT and
NLP tasks; (3) intrinsic pitfalls of LLMs models, e.g., hallucination, overly
focus on certain keywords, etc.
In this work, we propose a collection of general modules to address these
issues, in an attempt to push the limits of ChatGPT on NLP tasks. Our proposed
modules include (1) a one-input-multiple-prompts strategy that employs multiple
prompts for one input to accommodate more demonstrations; (2) using fine-tuned
models for better demonstration retrieval; (3) transforming tasks to formats
that are more tailored to the generation nature; (4) employing reasoning
strategies that are tailored to addressing the task-specific complexity; (5)
the self-verification strategy to address the hallucination issue of LLMs; (6)
the paraphrase strategy to improve the robustness of model predictions.
We conduct experiments on 21 datasets of 10 representative NLP tasks,
including question answering, commonsense reasoning, natural language
inference, sentiment analysis, named entity recognition, entity-relation
extraction, event extraction, dependency parsing, semantic role labeling, and
part-of-speech tagging. Using the proposed assemble of techniques, we are able
to significantly boost the performance of ChatGPT on the selected NLP tasks,
achieving performances comparable to or better than supervised baselines, or
even existing SOTA performances.


------------------------------------------------------------------------------

Title:
An Analysis of Physiological and Psychological Responses in Virtual  Reality and Flat Screen Gaming

Abstract: Recent research has focused on the effectiveness of Virtual Reality (VR) in
games as a more immersive method of interaction. However, there is a lack of
robust analysis of the physiological effects between VR and flatscreen (FS)
gaming. This paper introduces the first systematic comparison and analysis of
emotional and physiological responses to commercially available games in VR and
FS environments. To elicit these responses, we first selected four games
through a pilot study of 6 participants to cover all four quadrants of the
valence-arousal space. Using these games, we recorded the physiological
activity, including Blood Volume Pulse and Electrodermal Activity, and
self-reported emotions of 33 participants in a user study. Our data analysis
revealed that VR gaming elicited more pronounced emotions, higher arousal,
increased cognitive load and stress, and lower dominance than FS gaming. The
Virtual Reality and Flat Screen (VRFS) dataset, containing over 15 hours of
multimodal data comparing FS and VR gaming across different games, is also made
publicly available for research purposes. Our analysis provides valuable
insights for further investigations into the physiological and emotional
effects of VR and FS gaming.


------------------------------------------------------------------------------

Title:
A context model for collecting diversity-aware data

Abstract: Diversity-aware data are essential for a robust modeling of human behavior in
context. In addition, being the human behavior of interest for numerous
applications, data must also be reusable across domain, to ensure diversity of
interpretations. Current data collection techniques allow only a partial
representation of the diversity of people and often generate data that is
difficult to reuse. To fill this gap, we propose a data collection methodology,
within a hybrid machine-artificial intelligence approach, and its related
dataset, based on a comprehensive ontological notion of context which enables
data reusability. The dataset has a sample of 158 participants and is collected
via the iLog smartphone application. It contains more than 170 GB of subjective
and objective data, which comes from 27 smartphone sensors that are associated
with 168,095 self-reported annotations on the participants context. The dataset
is highly reusable, as demonstrated by its diverse applications.


------------------------------------------------------------------------------

Title:
Meta Generative Flow Networks with Personalization for Task-Specific  Adaptation

Abstract: Multi-task reinforcement learning and meta-reinforcement learning have been
developed to quickly adapt to new tasks, but they tend to focus on tasks with
higher rewards and more frequent occurrences, leading to poor performance on
tasks with sparse rewards. To address this issue, GFlowNets can be integrated
into meta-learning algorithms (GFlowMeta) by leveraging the advantages of
GFlowNets on tasks with sparse rewards. However, GFlowMeta suffers from
performance degradation when encountering heterogeneous transitions from
distinct tasks. To overcome this challenge, this paper proposes a personalized
approach named pGFlowMeta, which combines task-specific personalized policies
with a meta policy. Each personalized policy balances the loss on its
personalized task and the difference from the meta policy, while the meta
policy aims to minimize the average loss of all tasks. The theoretical analysis
shows that the algorithm converges at a sublinear rate. Extensive experiments
demonstrate that the proposed algorithm outperforms state-of-the-art
reinforcement learning algorithms in discrete environments.


------------------------------------------------------------------------------

Title:
A Testbed for Carbon-Aware Applications and Systems

Abstract: To mitigate the growing carbon footprint of computing systems, there has been
an increasing focus on carbon-aware approaches that seek to align the power
usage of IT infrastructure with the availability of clean energy.
Unfortunately, research on carbon-aware applications and the required
interfaces between computing and energy systems remain complex, due to the
scarcity of available testing environments. To this day, almost all new
approaches are evaluated on self-implemented simulation testbeds, which leads
to repeated development efforts by researchers and low comparability of
approaches.
In this paper, we present our vision of a co-simulation testbed for
carbon-aware applications and systems. We envision a versatile testbed which
lets users connect domain-specific simulators for components like renewable
power generation, energy storage, and power flow analysis with real software
and hardware. By providing extensibility on the one hand and access to
state-of-the-art implementations, datasets, and best practices on the other, we
hope to accelerate research in carbon-aware computing. In addition, a
co-simulation testbed can be useful for development and operations, like in
continuous testing. We implemented a first prototype of our idea and welcome
the community to contribute to this vision.


------------------------------------------------------------------------------

Title:
Overtaking-enabled Eco-approach Control at Signalized Intersections for  Connected and Automated Vehicles

Abstract: Preceding vehicles typically dominate the movement of following vehicles in
traffic systems, thereby significantly influencing the efficacy of eco-driving
control that concentrates on vehicle speed optimization. To potentially
mitigate the negative effect of preceding vehicles on eco-driving control at
the signalized intersection, this paper proposes an overtakingenabled
eco-approach control (OEAC) strategy. It combines driving lane planning and
speed optimization for connected and automated vehicles to relax the
first-in-first-out queuing policy at the signalized intersection, minimizing
the target vehicle's energy consumption and travel delay. The OEAC adopts a
receding horizon two-stage control framework to derive optimal driving
trajectories for adapting to dynamic traffic conditions. In the first stage,
the driving lane optimization problem is formulated as a Markov decision
process and solved using dynamic programming, which takes into account the
uncertain disturbance from preceding vehicles. In the second stage, the
vehicle's speed trajectory with the minimal driving cost is optimized rapidly
using Pontryagin's minimum principle to obtain the closed-form analytical
optimal solution. Extensive simulations are conducted to evaluate the
effectiveness of the OEAC. The results show that the OEAC is excellent in
driving cost reduction over constant speed and regular eco-approach and
departure strategies in various traffic scenarios, with an average improvement
of 20.91% and 5.62%, respectively.


------------------------------------------------------------------------------

Title:
Tell Me Where to Go: A Composable Framework for Context-Aware Embodied  Robot Navigation

Abstract: Humans have the remarkable ability to navigate through unfamiliar
environments by solely relying on our prior knowledge and descriptions of the
environment. For robots to perform the same type of navigation, they need to be
able to associate natural language descriptions with their associated physical
environment with a limited amount of prior knowledge. Recently, Large Language
Models (LLMs) have been able to reason over billions of parameters and utilize
them in multi-modal chat-based natural language responses. However, LLMs lack
real-world awareness and their outputs are not always predictable. In this
work, we develop NavCon, a low-bandwidth framework that solves this lack of
real-world generalization by creating an intermediate layer between an LLM and
a robot navigation framework in the form of Python code. Our intermediate
shoehorns the vast prior knowledge inherent in an LLM model into a series of
input and output API instructions that a mobile robot can understand. We
evaluate our method across four different environments and command classes on a
mobile robot and highlight our NavCon's ability to interpret contextual
commands.


------------------------------------------------------------------------------

Title:
Krasovskii Passivity for Sampled-data Stabilization and Output Consensus

Abstract: In this paper, we establish the novel concept of Krasovskii passivity for
sampled discrete-time nonlinear systems, enabling Krasovskii-passivity-based
control design under sampling. We consider two separate control objectives:
stabilization and output consensus, where the latter is studied under the
presence of an unknown constant disturbance. Inspired by methodologies in the
continuous-time case, we develop sampled-data control schemes for each control
objective based on Krasovskii passivity. The proposed sampled discrete-time
controllers are respectively validated through simulations on a DC microgrid of
boost converters and a DC microgrid of buck converters whose continuous-time
models and their implicit midpoint discretizations are Krasovskii passive in
each time scale.


------------------------------------------------------------------------------

Title:
EVOPOSE: A Recursive Transformer For 3D Human Pose Estimation With  Kinematic Structure Priors

Abstract: Transformer is popular in recent 3D human pose estimation, which utilizes
long-term modeling to lift 2D keypoints into the 3D space. However, current
transformer-based methods do not fully exploit the prior knowledge of the human
skeleton provided by the kinematic structure. In this paper, we propose a novel
transformer-based model EvoPose to introduce the human body prior knowledge for
3D human pose estimation effectively. Specifically, a Structural Priors
Representation (SPR) module represents human priors as structural features
carrying rich body patterns, e.g. joint relationships. The structural features
are interacted with 2D pose sequences and help the model to achieve more
informative spatiotemporal features. Moreover, a Recursive Refinement (RR)
module is applied to refine the 3D pose outputs by utilizing estimated results
and further injects human priors simultaneously. Extensive experiments
demonstrate the effectiveness of EvoPose which achieves a new state of the art
on two most popular benchmarks, Human3.6M and MPI-INF-3DHP.


------------------------------------------------------------------------------

Title:
A research infrastructure for generating and sharing diversity-aware  data

Abstract: The intensive flow of personal data associated with the trend of
computerizing aspects of people's diversity in their daily lives is associated
with issues concerning not only people protection and their trust in new
technologies, but also bias in the analysis of data and problems in their
management and reuse. Faced with a complex problem, the strategies adopted,
including technologies and services, often focus on individual aspects, which
are difficult to integrate into a broader framework, which can be of effective
support for researchers and developers. Therefore, we argue for the development
of an end-to-end research infrastructure (RI) that enables trustworthy
diversity-aware data within a citizen science community.


------------------------------------------------------------------------------

Title:
Regression-based Physics Informed Neural Networks (Reg-PINNs) for  Magnetopause Tracking

Abstract: The ultimate goal of studying the magnetopause position is to accurately
determine its location. Both traditional empirical computation methods and the
currently popular machine learning approaches have shown promising results. In
this study, we propose a Regression-based Physics-Informed Neural Networks
(Reg-PINNs) that combines physics-based numerical computation with vanilla
machine learning. This new generation of Physics Informed Neural Networks
overcomes the limitations of previous methods restricted to solving ordinary
and partial differential equations by incorporating conventional empirical
models to aid the convergence and enhance the generalization capability of the
neural network. Compared to Shue et al. [1998], our model achieves a reduction
of approximately 30% in root mean square error. The methodology presented in
this study is not only applicable to space research but can also be referenced
in studies across various fields, particularly those involving empirical
models.


------------------------------------------------------------------------------

Title:
BISCUIT: Causal Representation Learning from Binary Interactions

Abstract: Identifying the causal variables of an environment and how to intervene on
them is of core value in applications such as robotics and embodied AI. While
an agent can commonly interact with the environment and may implicitly perturb
the behavior of some of these causal variables, often the targets it affects
remain unknown. In this paper, we show that causal variables can still be
identified for many common setups, e.g., additive Gaussian noise models, if the
agent's interactions with a causal variable can be described by an unknown
binary variable. This happens when each causal variable has two different
mechanisms, e.g., an observational and an interventional one. Using this
identifiability result, we propose BISCUIT, a method for simultaneously
learning causal variables and their corresponding binary interaction variables.
On three robotic-inspired datasets, BISCUIT accurately identifies causal
variables and can even be scaled to complex, realistic environments for
embodied AI.


------------------------------------------------------------------------------

Title:
The Optimality of AIFV Codes in the Class of $2$-bit Delay Decodable  Codes

Abstract: AIFV (almost instantaneous fixed-to-variable length) codes are noiseless
source codes that can attain a shorter average codeword length than Huffman
codes by allowing a time-variant encoder with two code tables and a decoding
delay of at most 2 bits. First, we consider a general class of noiseless source
codes, called k-bit delay decodable codes, in which one allows a finite number
of code tables and a decoding delay of at most k bits for k >= 0. Then we prove
that AIFV codes achieve the optimal average codeword length in the 2-bit delay
decodable codes class.


------------------------------------------------------------------------------

Title:
Parameter-efficient is not sufficient: Exploring Parameter, Memory, and  Time Efficient Adapter Tuning for Dense Predictions

Abstract: Pre-training & fine-tuning is a prevalent paradigm in computer vision (CV).
Recently, parameter-efficient transfer learning (PETL) methods have shown
promising performance in transferring knowledge from pre-trained models with
only a few trainable parameters. Despite their success, the existing PETL
methods in CV can be computationally expensive and require large amounts of
memory and time cost during training, which limits low-resource users from
conducting research and applications on large models. In this work, we propose
Parameter, Memory, and Time Efficient Visual Adapter ($\mathrm{E^3VA}$) tuning
to address this issue. We provide a gradient backpropagation highway for
low-rank adapters which removes large gradient computations for the frozen
pre-trained parameters, resulting in substantial savings of training memory and
training time. Furthermore, we optimise the $\mathrm{E^3VA}$ structure for
dense predictions tasks to promote model performance. Extensive experiments on
COCO, ADE20K, and Pascal VOC benchmarks show that $\mathrm{E^3VA}$ can save up
to 62.2% training memory and 26.2% training time on average, while achieving
comparable performance to full fine-tuning and better performance than most
PETL methods. Note that we can even train the Swin-Large-based Cascade Mask
RCNN on GTX 1080Ti GPUs with less than 1.5% trainable parameters.


------------------------------------------------------------------------------

Title:
End-to-End Vectorized HD-map Construction with Piecewise Bezier Curve

Abstract: Vectorized high-definition map (HD-map) construction, which focuses on the
perception of centimeter-level environmental information, has attracted
significant research interest in the autonomous driving community. Most
existing approaches first obtain rasterized map with the segmentation-based
pipeline and then conduct heavy post-processing for downstream-friendly
vectorization. In this paper, by delving into parameterization-based methods,
we pioneer a concise and elegant scheme that adopts unified piecewise Bezier
curve. In order to vectorize changeful map elements end-to-end, we elaborate a
simple yet effective architecture, named Piecewise Bezier HD-map Network
(BeMapNet), which is formulated as a direct set prediction paradigm and
postprocessing-free. Concretely, we first introduce a novel IPM-PE Align module
to inject 3D geometry prior into BEV features through common position encoding
in Transformer. Then a well-designed Piecewise Bezier Head is proposed to
output the details of each map element, including the coordinate of control
points and the segment number of curves. In addition, based on the
progressively restoration of Bezier curve, we also present an efficient
Point-Curve-Region Loss for supervising more robust and precise HD-map
modeling. Extensive comparisons show that our method is remarkably superior to
other existing SOTAs by 18.0 mAP at least.


------------------------------------------------------------------------------

Title:
I Want This, Not That: Personalized Summarization of Scientific  Scholarly Texts

Abstract: In this paper, we present a proposal for an unsupervised algorithm, P-Summ,
that generates an extractive summary of scientific scholarly text to meet the
personal knowledge needs of the user. The method delves into the latent
semantic space of the document exposed by Weighted Non-negative Matrix
Factorization, and scores sentences in consonance with the knowledge needs of
the user. The novelty of the algorithm lies in its ability to include desired
knowledge and eliminate unwanted knowledge in the personal summary.
We also propose a multi-granular evaluation framework, which assesses the
quality of generated personal summaries at three levels of granularity -
sentence, terms and semantic. The framework uses system generated generic
summary instead of human generated summary as gold standard for evaluating the
quality of personal summary generated by the algorithm. The effectiveness of
the algorithm at the semantic level is evaluated by taking into account the
reference summary and the knowledge signals. We evaluate the performance of
P-Summ algorithm over four data-sets consisting of scientific articles. Our
empirical investigations reveal that the proposed method has the capability to
meet negative (or positive) knowledge preferences of the user.


------------------------------------------------------------------------------

Title:
ReactGenie: An Object-Oriented State Abstraction for Complex Multimodal  Interactions Using Large Language Models

Abstract: Multimodal interactions have been shown to be more flexible, efficient, and
adaptable for diverse users and tasks than traditional graphical interfaces.
However, existing multimodal development frameworks either do not handle the
complexity and compositionality of multimodal commands well or require
developers to write a substantial amount of code to support these multimodal
interactions. In this paper, we present ReactGenie, a programming framework
that uses a shared object-oriented state abstraction to support building
complex multimodal mobile applications. Having different modalities share the
same state abstraction allows developers using ReactGenie to seamlessly
integrate and compose these modalities to deliver multimodal interaction.
ReactGenie is a natural extension to the existing workflow of building a
graphical app, like the workflow with React-Redux. Developers only have to add
a few annotations and examples to indicate how natural language is mapped to
the user-accessible functions in the program. ReactGenie automatically handles
the complex problem of understanding natural language by generating a parser
that leverages large language models.
We evaluated the ReactGenie framework by using it to build three demo apps.
We evaluated the accuracy of the language parser using elicited commands from
crowd workers and evaluated the usability of the generated multimodal app with
16 participants. Our results show that ReactGenie can be used to build
versatile multimodal applications with highly accurate language parsers, and
the multimodal app can lower users' cognitive load and task completion time.


------------------------------------------------------------------------------

Title:
A Design Approach and Prototype Implementation for Factory Monitoring  Based on Virtual and Augmented Reality at the Edge of Industry 4.0

Abstract: Virtual and augmented reality are currently enjoying a great deal of
attention from the research community and the industry towards their adoption
within industrial spaces and processes. However, the current design and
implementation landscape is still very fluid, while the community as a whole
has not yet consolidated into concrete design directions, other than basic
patterns. Other open issues include the choice over a cloud or edge-based
architecture when designing such systems. Within this work, we present our
approach for a monitoring intervention inside a factory space utilizing both VR
and AR, based primarily on edge computing, while also utilizing the cloud. We
discuss its main design directions, as well as a basic ontology to aid in
simple description of factory assets. In order to highlight the design aspects
of our approach, we present a prototype implementation, based on a use case
scenario in a factory site, within the context of the ENERMAN H2020 project.


------------------------------------------------------------------------------

Title:
A flexible algorithm to offload DAG applications for edge computing

Abstract: Multi-access Edge Computing (MEC) is an enabling technology to leverage new
network applications, such as virtual/augmented reality, by providing faster
task processing at the network edge. This is done by deploying servers closer
to the end users to run the network applications. These applications are often
intensive in terms of task processing, memory usage, and communication; thus
mobile devices may take a long time or even not be able to run them
efficiently. By transferring (offloading) the execution of these applications
to the servers at the network edge, it is possible to achieve a lower
completion time (makespan) and meet application requirements. However,
offloading multiple entire applications to the edge server can overwhelm its
hardware and communication channel, as well as underutilize the mobile devices'
hardware. In this paper, network applications are modeled as Directed Acyclic
Graphs (DAGs) and partitioned into tasks, and only part of these tasks are
offloaded to the edge server. This is the DAG application partitioning and
offloading problem, which is known to be NP-hard. To approximate its solution,
this paper proposes the FlexDO algorithm. FlexDO combines a greedy phase with a
permutation phase to find a set of offloading decisions, and then chooses the
one that achieves the shortest makespan. FlexDO is compared with a proposal
from the literature and two baseline decisions, considering realistic DAG
applications extracted from the Alibaba Cluster Trace Program. Results show
that FlexDO is consistently only 3.9% to 8.9% above the optimal makespan in all
test scenarios, which include different levels of CPU availability, a
multi-user case, and different communication channel transmission rates. FlexDO
outperforms both baseline solutions by a wide margin, and is three times closer
to the optimal makespan than its competitor.


------------------------------------------------------------------------------

Title:
Fedstellar: A Platform for Decentralized Federated Learning

Abstract: In 2016, Google proposed Federated Learning (FL) as a novel paradigm to train
Machine Learning (ML) models across the participants of a federation while
preserving data privacy. Since its birth, Centralized FL (CFL) has been the
most used approach, where a central entity aggregates participants' models to
create a global one. However, CFL presents limitations such as communication
bottlenecks, single point of failure, and reliance on a central server.
Decentralized Federated Learning (DFL) addresses these issues by enabling
decentralized model aggregation and minimizing dependency on a central entity.
Despite these advances, current platforms training DFL models struggle with key
issues such as managing heterogeneous federation network topologies. To
overcome these challenges, this paper presents Fedstellar, a novel platform
designed to train FL models in a decentralized, semi-decentralized, and
centralized fashion across diverse federations of physical or virtualized
devices. The Fedstellar implementation encompasses a web application with an
interactive graphical interface, a controller for deploying federations of
nodes using physical or virtual devices, and a core deployed on each device
which provides the logic needed to train, aggregate, and communicate in the
network. The effectiveness of the platform has been demonstrated in two
scenarios: a physical deployment involving single-board devices such as
Raspberry Pis for detecting cyberattacks, and a virtualized deployment
comparing various FL approaches in a controlled environment using MNIST and
CIFAR-10 datasets. In both scenarios, Fedstellar demonstrated consistent
performance and adaptability, achieving F1 scores of 91%, 98%, and 91.2% using
DFL for detecting cyberattacks and classifying MNIST and CIFAR-10,
respectively, reducing training time by 32% compared to centralized approaches.


------------------------------------------------------------------------------

Title:
Towards Long Form Audio-visual Video Understanding

Abstract: We live in a world filled with never-ending streams of multimodal
information. As a more natural recording of the real scenario, long form
audio-visual videos are expected as an important bridge for better exploring
and understanding the world. In this paper, we propose the multisensory
temporal event localization task in long form videos and strive to tackle the
associated challenges. To facilitate this study, we first collect a large-scale
Long Form Audio-visual Video (LFAV) dataset with 5,175 videos and an average
video length of 210 seconds. Each of the collected videos is elaborately
annotated with diversified modality-aware events, in a long-range temporal
sequence. We then propose an event-centric framework for localizing
multisensory events as well as understanding their relations in long form
videos. It includes three phases in different levels: snippet prediction phase
to learn snippet features, event extraction phase to extract event-level
features, and event interaction phase to study event relations. Experiments
demonstrate that the proposed method, utilizing the new LFAV dataset, exhibits
considerable effectiveness in localizing multiple modality-aware events within
long form videos. Project website: this http URL


------------------------------------------------------------------------------

Title:
Fuzzy Feature Selection with Key-based Cryptographic Transformations

Abstract: In the field of cryptography, the selection of relevant features plays a
crucial role in enhancing the security and efficiency of cryptographic
algorithms. This paper presents a novel approach of applying fuzzy feature
selection to key-based cryptographic transformations. The proposed fuzzy
feature selection leverages the power of fuzzy logic to identify and select
optimal subsets of features that contribute most effectively to the
cryptographic transformation process. By incorporating fuzzy feature selection
into key-based cryptographic transformations, this research aims to improve the
resistance against attacks and enhance the overall performance of cryptographic
systems. Experimental evaluations may demonstrate the effectiveness of the
proposed approach in selecting secure key features with minimal computational
overhead. This paper highlights the potential of fuzzy feature selection as a
valuable tool in the design and optimization of key-based cryptographic
algorithms, contributing to the advancement of secure information exchange and
communication in various domains.


------------------------------------------------------------------------------

Title:
Modelling, identification and geometric control of autonomous  quadcopters for agile maneuvering

Abstract: This paper presents a multi-step procedure to construct the dynamic motion
model of an autonomous quadcopter, identify the model parameters, and design a
model-based nonlinear trajectory tracking controller. The aim of the proposed
method is to speed up the commissioning of a new quadcopter design, i.e., to
enable the drone to perform agile maneuvers with high precision in the shortest
time possible. After a brief introduction to the theoretical background of the
modelling and control design, the steps of the proposed method are presented
using the example of a self-developed quadcopter platform. The performance of
the method is tested and evaluated by real flight experiments.


------------------------------------------------------------------------------

Title:
Online Distillation for Pseudo-Relevance Feedback

Abstract: Model distillation has emerged as a prominent technique to improve neural
search models. To date, distillation taken an offline approach, wherein a new
neural model is trained to predict relevance scores between arbitrary queries
and documents. In this paper, we explore a departure from this offline
distillation strategy by investigating whether a model for a specific query can
be effectively distilled from neural re-ranking results (i.e., distilling in an
online setting). Indeed, we find that a lexical model distilled online can
reasonably replicate the re-ranking of a neural model. More importantly, these
models can be used as queries that execute efficiently on indexes. This second
retrieval stage can enrich the pool of documents for re-ranking by identifying
documents that were missed in the first retrieval stage. Empirically, we show
that this approach performs favourably when compared with established pseudo
relevance feedback techniques, dense retrieval methods, and sparse-dense
ensemble "hybrid" approaches.


------------------------------------------------------------------------------

Title:
Sample-Efficient Learning of Novel Visual Concepts

Abstract: Despite the advances made in visual object recognition, state-of-the-art deep
learning models struggle to effectively recognize novel objects in a few-shot
setting where only a limited number of examples are provided. Unlike humans who
excel at such tasks, these models often fail to leverage known relationships
between entities in order to draw conclusions about such objects. In this work,
we show that incorporating a symbolic knowledge graph into a state-of-the-art
recognition model enables a new approach for effective few-shot classification.
In our proposed neuro-symbolic architecture and training methodology, the
knowledge graph is augmented with additional relationships extracted from a
small set of examples, improving its ability to recognize novel objects by
considering the presence of interconnected entities. Unlike existing few-shot
classifiers, we show that this enables our model to incorporate not only
objects but also abstract concepts and affordances. The existence of the
knowledge graph also makes this approach amenable to interpretability through
analysis of the relationships contained within it. We empirically show that our
approach outperforms current state-of-the-art few-shot multi-label
classification methods on the COCO dataset and evaluate the addition of
abstract concepts and affordances on the Visual Genome dataset.


------------------------------------------------------------------------------

Title:
Spatiotemporal-Augmented Graph Neural Networks for Human Mobility  Simulation

Abstract: Human mobility patterns have shown significant applications in
policy-decision scenarios and economic behavior researches. The human mobility
simulation task aims to generate human mobility trajectories given a small set
of trajectory data, which have aroused much concern due to the scarcity and
sparsity of human mobility data. Existing methods mostly rely on the static
relationships of locations, while largely neglect the dynamic spatiotemporal
effects of locations. On the one hand, spatiotemporal correspondences of visit
distributions reveal the spatial proximity and the functionality similarity of
locations. On the other hand, the varying durations in different locations
hinder the iterative generation process of the mobility trajectory. Therefore,
we propose a novel framework to model the dynamic spatiotemporal effects of
locations, namely SpatioTemporal-Augmented gRaph neural networks (STAR). The
STAR framework designs various spatiotemporal graphs to capture the
spatiotemporal correspondences and builds a novel dwell branch to simulate the
varying durations in locations, which is finally optimized in an adversarial
manner. The comprehensive experiments over four real datasets for the human
mobility simulation have verified the superiority of STAR to state-of-the-art
methods. Our code will be made publicly available.


------------------------------------------------------------------------------

Title:
Listener Model for the PhotoBook Referential Game with CLIPScores as  Implicit Reference Chain

Abstract: PhotoBook is a collaborative dialogue game where two players receive private,
partially-overlapping sets of images and resolve which images they have in
common. It presents machines with a great challenge to learn how people build
common ground around multimodal context to communicate effectively. Methods
developed in the literature, however, cannot be deployed to real gameplay since
they only tackle some subtasks of the game, and they require additional
reference chains inputs, whose extraction process is imperfect. Therefore, we
propose a reference chain-free listener model that directly addresses the
game's predictive task, i.e., deciding whether an image is shared with partner.
Our DeBERTa-based listener model reads the full dialogue, and utilizes
CLIPScore features to assess utterance-image relevance. We achieve >77%
accuracy on unseen sets of images/game themes, outperforming baseline by >17
points.


------------------------------------------------------------------------------

Title:
Low-Switching Policy Gradient with Exploration via Online Sensitivity  Sampling

Abstract: Policy optimization methods are powerful algorithms in Reinforcement Learning
(RL) for their flexibility to deal with policy parameterization and ability to
handle model misspecification. However, these methods usually suffer from slow
convergence rates and poor sample complexity. Hence it is important to design
provably sample efficient algorithms for policy optimization. Yet, recent
advances for this problems have only been successful in tabular and linear
setting, whose benign structures cannot be generalized to non-linearly
parameterized policies. In this paper, we address this problem by leveraging
recent advances in value-based algorithms, including bounded eluder-dimension
and online sensitivity sampling, to design a low-switching sample-efficient
policy optimization algorithm, LPO, with general non-linear function
approximation. We show that, our algorithm obtains an $\varepsilon$-optimal
policy with only $\widetilde{O}(\frac{\text{poly}(d)}{\varepsilon^3})$ samples,
where $\varepsilon$ is the suboptimality gap and $d$ is a complexity measure of
the function class approximating the policy. This drastically improves
previously best-known sample bound for policy optimization algorithms,
$\widetilde{O}(\frac{\text{poly}(d)}{\varepsilon^8})$. Moreover, we empirically
test our theory with deep neural nets to show the benefits of the theoretical
inspiration.


------------------------------------------------------------------------------

Title:
Reproducibility in NLP: What Have We Learned from the Checklist?

Abstract: Scientific progress in NLP rests on the reproducibility of researchers'
claims. The *CL conferences created the NLP Reproducibility Checklist in 2020
to be completed by authors at submission to remind them of key information to
include. We provide the first analysis of the Checklist by examining 10,405
anonymous responses to it. First, we find evidence of an increase in reporting
of information on efficiency, validation performance, summary statistics, and
hyperparameters after the Checklist's introduction. Further, we show acceptance
rate grows for submissions with more Yes responses. We find that the 44% of
submissions that gather new data are 5% less likely to be accepted than those
that did not; the average reviewer-rated reproducibility of these submissions
is also 2% lower relative to the rest. We find that only 46% of submissions
claim to open-source their code, though submissions that do have 8% higher
reproducibility score relative to those that do not, the most for any item. We
discuss what can be inferred about the state of reproducibility in NLP, and
provide a set of recommendations for future conferences, including: a) allowing
submitting code and appendices one week after the deadline, and b) measuring
dataset reproducibility by a checklist of data collection practices.


------------------------------------------------------------------------------

Title:
Minimizing an Uncrossed Collection of Drawings

Abstract: In this paper, we introduce the following new concept in graph drawing. Our
task is to find a small collection of drawings such that they all together
satisfy some property that is useful for graph visualization. We propose
investigating a property where each edge is not crossed in at least one drawing
in the collection. We call such collection uncrossed. Such property is
motivated by a quintessential problem of the crossing number, where one asks
for a plane drawing where the number of edge crossings is minimum. Indeed, if
we are allowed to visualize only one drawing, then the one which minimizes the
number of crossings is probably the neatest for the first orientation. However,
a collection of drawings where each highlights a different aspect of a graph
without any crossings could shed even more light on the graph's structure.
We propose two definitions. First, the uncrossed number, minimizes the number
of graph drawings in a collection, satisfying the uncrossed property. Second,
the uncrossed crossing number, minimizes the total number of crossings in the
collection that satisfy the uncrossed property. For both definitions, we
establish initial results. We prove that the uncrossed crossing number is
NP-hard, but there is an FPT algorithm parameterized by the solution size.


------------------------------------------------------------------------------

Title:
Structural Restricted Boltzmann Machine for image denoising and  classification

Abstract: Restricted Boltzmann Machines are generative models that consist of a layer
of hidden variables connected to another layer of visible units, and they are
used to model the distribution over visible variables. In order to gain a
higher representability power, many hidden units are commonly used, which, in
combination with a large number of visible units, leads to a high number of
trainable parameters. In this work we introduce the Structural Restricted
Boltzmann Machine model, which taking advantage of the structure of the data in
hand, constrains connections of hidden units to subsets of visible units in
order to reduce significantly the number of trainable parameters, without
compromising performance. As a possible area of application, we focus on image
modelling. Based on the nature of the images, the structure of the connections
is given in terms of spatial neighbourhoods over the pixels of the image that
constitute the visible variables of the model. We conduct extensive experiments
on various image domains. Image denoising is evaluated with corrupted images
from the MNIST dataset. The generative power of our models is compared to
vanilla RBMs, as well as their classification performance, which is assessed
with five different image domains. Results show that our proposed model has a
faster and more stable training, while also obtaining better results compared
to an RBM with no constrained connections between its visible and hidden units.


------------------------------------------------------------------------------

Title:
PAtt-Lite: Lightweight Patch and Attention MobileNet for Challenging  Facial Expression Recognition

Abstract: Facial Expression Recognition (FER) is a machine learning problem that deals
with recognizing human facial expressions. While existing work has achieved
performance improvements in recent years, FER in the wild and under challenging
conditions remains a challenge. In this paper, a lightweight patch and
attention network based on MobileNetV1, referred to as PAtt-Lite, is proposed
to improve FER performance under challenging conditions. A truncated
ImageNet-pre-trained MobileNetV1 is utilized as the backbone feature extractor
of the proposed method. In place of the truncated layers is a patch extraction
block that is proposed for extracting significant local facial features to
enhance the representation from MobileNetV1, especially under challenging
conditions. An attention classifier is also proposed to improve the learning of
these patched feature maps from the extremely lightweight feature extractor.
The experimental results on public benchmark databases proved the effectiveness
of the proposed method. PAtt-Lite achieved state-of-the-art results on CK+,
RAF-DB, FER2013, FERPlus, and the challenging conditions subsets for RAF-DB and
FERPlus. The source code for the proposed method will be available at
this https URL


------------------------------------------------------------------------------

Title:
Learning to Assist and Communicate with Novice Drone Pilots for Expert  Level Performance

Abstract: Multi-task missions for unmanned aerial vehicles (UAVs) involving inspection
and landing tasks are challenging for novice pilots due to the difficulties
associated with depth perception and the control interface. We propose a shared
autonomy system, alongside supplementary information displays, to assist pilots
to successfully complete multi-task missions without any pilot training. Our
approach comprises of three modules: (1) a perception module that encodes
visual information onto a latent representation, (2) a policy module that
augments pilot's actions, and (3) an information augmentation module that
provides additional information to the pilot. The policy module is trained in
simulation with simulated users and transferred to the real world without
modification in a user study (n=29), alongside supplementary information
schemes including learnt red/green light feedback cues and an augmented reality
display. The pilot's intent is unknown to the policy module and is inferred
from the pilot's input and UAV's states. The assistant increased task success
rate for the landing and inspection tasks from [16.67% & 54.29%] respectively
to [95.59% & 96.22%]. With the assistant, inexperienced pilots achieved similar
performance to experienced pilots. Red/green light feedback cues reduced the
required time by 19.53% and trajectory length by 17.86% for the inspection
task, where participants rated it as their preferred condition due to the
intuitive interface and providing reassurance. This work demonstrates that
simple user models can train shared autonomy systems in simulation, and
transfer to physical tasks to estimate user intent and provide effective
assistance and information to the pilot.


------------------------------------------------------------------------------

Title:
On polynomial Trefftz spaces for the linear time-dependent Schrödinger  equation

Abstract: We study the approximation properties of complex-valued polynomial Trefftz
spaces for the $(d+1)$-dimensional linear time-dependent Schr\"odinger
equation. More precisely, we prove that for the space-time Trefftz
discontinuous Galerkin variational formulation proposed by G\'omez, Moiola
(SIAM. J. Num. Anal. 60(2): 688-714, 2022), the same $h$-convergence rates as
for polynomials of degree $p$ in $(d + 1)$ variables can be obtained in a
mesh-dependent norm by using a space of Trefftz polynomials of anisotropic
degree. For such a space, the dimension is equal to that of the space of
polynomials of degree $2p$ in $d$ variables, and bases are easily constructed.


------------------------------------------------------------------------------

Title:
The 1st-place Solution for CVPR 2023 OpenLane Topology in Autonomous  Driving Challenge

Abstract: We present the 1st-place solution of OpenLane Topology in Autonomous Driving
Challenge. Considering that topology reasoning is based on centerline detection
and traffic element detection, we develop a multi-stage framework for high
performance. Specifically, the centerline is detected by the powerful PETRv2
detector and the popular YOLOv8 is employed to detect the traffic elements.
Further, we design a simple yet effective MLP-based head for topology
prediction. Our method achieves 55\% OLS on the OpenLaneV2 test set, surpassing
the 2nd solution by 8 points.


------------------------------------------------------------------------------

Title:
Hierarchical confusion matrix for classification performance evaluation

Abstract: In this work we propose a novel concept of a hierarchical confusion matrix,
opening the door for popular confusion matrix based (flat) evaluation measures
from binary classification problems, while considering the peculiarities of
hierarchical classification problems. We develop the concept to a generalized
form and prove its applicability to all types of hierarchical classification
problems including directed acyclic graphs, multi path labelling, and non
mandatory leaf node prediction. Finally, we use measures based on the novel
confusion matrix to evaluate models within a benchmark for three real world
hierarchical classification applications and compare the results to established
evaluation measures. The results outline the reasonability of this approach and
its usefulness to evaluate hierarchical classification problems. The
implementation of hierarchical confusion matrix is available on GitHub.


------------------------------------------------------------------------------

Title:
GraphSHA: Synthesizing Harder Samples for Class-Imbalanced Node  Classification

Abstract: Class imbalance is the phenomenon that some classes have much fewer instances
than others, which is ubiquitous in real-world graph-structured scenarios.
Recent studies find that off-the-shelf Graph Neural Networks (GNNs) would
under-represent minor class samples. We investigate this phenomenon and
discover that the subspaces of minor classes being squeezed by those of the
major ones in the latent space is the main cause of this failure. We are
naturally inspired to enlarge the decision boundaries of minor classes and
propose a general framework GraphSHA by Synthesizing HArder minor samples.
Furthermore, to avoid the enlarged minor boundary violating the subspaces of
neighbor classes, we also propose a module called SemiMixup to transmit
enlarged boundary information to the interior of the minor classes while
blocking information propagation from minor classes to neighbor classes.
Empirically, GraphSHA shows its effectiveness in enlarging the decision
boundaries of minor classes, as it outperforms various baseline methods in
class-imbalanced node classification with different GNN backbone encoders over
seven public benchmark datasets. Code is avilable at
this https URL


------------------------------------------------------------------------------

Title:
A Smooth Binary Mechanism for Efficient Private Continual Observation

Abstract: In privacy under continual observation we study how to release differentially
private estimates based on a dataset that evolves over time. The problem of
releasing private prefix sums of $x_1,x_2,x_3,\dots \in\{0,1\}$ (where the
value of each $x_i$ is to be private) is particularly well-studied, and a
generalized form is used in state-of-the-art methods for private stochastic
gradient descent (SGD). The seminal binary mechanism privately releases the
first $t$ prefix sums with noise of variance polylogarithmic in $t$. Recently,
Henzinger et al. and Denisov et al. showed that it is possible to improve on
the binary mechanism in two ways: The variance of the noise can be reduced by a
(large) constant factor, and also made more even across time steps. However,
their algorithms for generating the noise distribution are not as efficient as
one would like in terms of computation time and (in particular) space. We
address the efficiency problem by presenting a simple alternative to the binary
mechanism in which 1) generating the noise takes constant average time per
value, 2) the variance is reduced by a factor about 4 compared to the binary
mechanism, and 3) the noise distribution at each step is identical.
Empirically, a simple Python implementation of our approach outperforms the
running time of the approach of Henzinger et al., as well as an attempt to
improve their algorithm using high-performance algorithms for multiplication
with Toeplitz matrices.


------------------------------------------------------------------------------

Title:
Tighter Prediction Intervals for Causal Outcomes Under Hidden  Confounding

Abstract: Causal inference of exact individual treatment outcomes in the presence of
hidden confounders is rarely possible. Instead, recent work has adapted
conformal prediction to produce outcome intervals. Unfortunately this family of
methods tends to be overly conservative, sometimes giving uninformative
intervals. We introduce an alternative approach termed Caus-Modens, for
characterizing causal outcome intervals by modulated ensembles. Motivated from
Bayesian statistics and ensembled uncertainty quantification, Caus-Modens gives
tighter outcome intervals in practice, measured by the necessary interval size
to achieve sufficient coverage on three separate benchmarks. The last benchmark
is a novel usage of GPT-4 for observational experiments with unknown but
probeable ground truth.


------------------------------------------------------------------------------

Title:
Mixed Fair Division: A Survey

Abstract: The fair allocation of resources to agents is a fundamental problem in
society and has received significant attention and rapid developments from the
game theory and artificial intelligence communities in recent years. The
majority of the fair division literature can be divided along at least two
orthogonal directions: goods versus chores, and divisible versus indivisible
resources. In this survey, besides describing the state of the art, we outline
a number of interesting open questions and directions in three mixed fair
division settings: (i) indivisible goods and chores, (ii) divisible and
indivisible goods (mixed goods), and (iii) fair division of indivisible goods
with subsidy.


------------------------------------------------------------------------------

Title:
UTOPIA: Unconstrained Tracking Objects without Preliminary Examination  via Cross-Domain Adaptation

Abstract: Multiple Object Tracking (MOT) aims to find bounding boxes and identities of
targeted objects in consecutive video frames. While fully-supervised MOT
methods have achieved high accuracy on existing datasets, they cannot
generalize well on a newly obtained dataset or a new unseen domain. In this
work, we first address the MOT problem from the cross-domain point of view,
imitating the process of new data acquisition in practice. Then, a new
cross-domain MOT adaptation from existing datasets is proposed without any
pre-defined human knowledge in understanding and modeling objects. It can also
learn and update itself from the target data feedback. The intensive
experiments are designed on four challenging settings, including MOTSynth to
MOT17, MOT17 to MOT20, MOT17 to VisDrone, and MOT17 to DanceTrack. We then
prove the adaptability of the proposed self-supervised learning strategy. The
experiments also show superior performance on tracking metrics MOTA and IDF1,
compared to fully supervised, unsupervised, and self-supervised
state-of-the-art methods.


------------------------------------------------------------------------------

Title:
DeepMPR: Enhancing Opportunistic Routing in Wireless Networks through  Multi-Agent Deep Reinforcement Learning

Abstract: Opportunistic routing relies on the broadcast capability of wireless
networks. It brings higher reliability and robustness in highly dynamic and/or
severe environments such as mobile or vehicular ad-hoc networks
(MANETs/VANETs). To reduce the cost of broadcast, multicast routing schemes use
the connected dominating set (CDS) or multi-point relaying (MPR) set to
decrease the network overhead and hence, their selection algorithms are
critical. Common MPR selection algorithms are heuristic, rely on coordination
between nodes, need high computational power for large networks, and are
difficult to tune for network uncertainties. In this paper, we use multi-agent
deep reinforcement learning to design a novel MPR multicast routing technique,
DeepMPR, which is outperforming the OLSR MPR selection algorithm while it does
not require MPR announcement messages from the neighbors. Our evaluation
results demonstrate the performance gains of our trained DeepMPR multicast
forwarding policy compared to other popular techniques.


------------------------------------------------------------------------------

Title:
CAJun: Continuous Adaptive Jumping using a Learned Centroidal Controller

Abstract: We present CAJun, a novel hierarchical learning and control framework that
enables legged robots to jump continuously with adaptive jumping distances.
CAJun consists of a high-level centroidal policy and a low-level leg
controller. In particular, we use reinforcement learning (RL) to train the
centroidal policy, which specifies the gait timing, base velocity, and swing
foot position for the leg controller. The leg controller optimizes motor
commands for the swing and stance legs according to the gait timing to track
the swing foot target and base velocity commands using optimal control.
Additionally, we reformulate the stance leg optimizer in the leg controller to
speed up policy training by an order of magnitude. Our system combines the
versatility of learning with the robustness of optimal control. By combining RL
with optimal control methods, our system achieves the versatility of learning
while enjoys the robustness from control methods, making it easily transferable
to real robots. We show that after 20 minutes of training on a single GPU,
CAJun can achieve continuous, long jumps with adaptive distances on a Go1 robot
with small sim-to-real gaps. Moreover, the robot can jump across gaps with a
maximum width of 70cm, which is over 40% wider than existing methods.


------------------------------------------------------------------------------

Title:
Karush-Kuhn-Tucker conditions to build efficient contractors;  Application to TDoA localization

Abstract: This paper proposes an efficient contractor for the TDoA (Time Differential
of Arrival) equation. The contractor is based on a minimal inclusion test which
is built using the Karush-Kuhn-Tucker (KKT) conditions. An application related
to the localization of sound sources using a TDoA technique is proposed.


------------------------------------------------------------------------------

Title:
CMLM-CSE: Based on Conditional MLM Contrastive Learning for Sentence  Embeddings

Abstract: Traditional comparative learning sentence embedding directly uses the encoder
to extract sentence features, and then passes in the comparative loss function
for learning. However, this method pays too much attention to the sentence body
and ignores the influence of some words in the sentence on the sentence
semantics. To this end, we propose CMLM-CSE, an unsupervised contrastive
learning framework based on conditional MLM. On the basis of traditional
contrastive learning, an additional auxiliary network is added to integrate
sentence embedding to perform MLM tasks, forcing sentence embedding to learn
more masked word information. Finally, when Bertbase was used as the
pretraining language model, we exceeded SimCSE by 0.55 percentage points on
average in textual similarity tasks, and when Robertabase was used as the
pretraining language model, we exceeded SimCSE by 0.3 percentage points on
average in textual similarity tasks.


------------------------------------------------------------------------------

Title:
Retrospective: EIE: Efficient Inference Engine on Sparse and Compressed  Neural Network

Abstract: EIE proposed to accelerate pruned and compressed neural networks, exploiting
weight sparsity, activation sparsity, and 4-bit weight-sharing in neural
network accelerators. Since published in ISCA'16, it opened a new design space
to accelerate pruned and sparse neural networks and spawned many
algorithm-hardware co-designs for model compression and acceleration, both in
academia and commercial AI chips. In retrospect, we review the background of
this project, summarize the pros and cons, and discuss new opportunities where
pruning, sparsity, and low precision can accelerate emerging deep learning
workloads.


------------------------------------------------------------------------------

Title:
Combinatorial-restless-bandit-based Transmitter-Receiver Online  Selection for Distributed MIMO Radars With Non-Stationary Channels

Abstract: We track moving targets with a distributed multiple-input multiple-output
(MIMO) radar, for which the transmitters and receivers are appropriately paired
and selected with a limited number of radar stations. We aim to maximize the
sum of the signal-to-interference-plus-noise ratios (SINRs) of all the targets
by sensibly selecting the transmitter-receiver pairs during the tracking
period. A key is to model the optimization problem of selecting the
transmitter-receiver pairs by a restless multi-armed bandit (RMAB) model that
is able to formulate the time-varying signals of the transceiver channels
whenever the channels are being probed or not. We regard the estimated mean
reward (i.e., SINR) as the state of an arm. If an arm is probed, the estimated
mean reward of the arm is the weighted sum of the observed reward and the
predicted mean reward; otherwise, it is the predicted mean reward. We associate
the predicted mean reward with the estimated mean reward at the previous time
slot and the state of the target, which is estimated via the interacting
multiple model-unscented Kalman filter (IMM-UKF). The optimized selection of
transmitter-receiver pairs at each time is accomplished by using Binary
Particle Swarm Optimization (BPSO) based on indexes of arms, each of which is
designed by the upper confidence bound (UCB1) algorithm. Above all, a
multi-group combinatorial-restless-bandit technique taking into account of
different combinations of transmitters and receivers and the closed-loop scheme
between transmitter-receiver pair selection and target state estimation, namely
MG-CRB-CL, is developed to achieve a near-optimal selection strategy and
improve multi-target tracking performance. Simulation results for different
scenarios are provided to verify the effectiveness and superior performance of
our MG-CRB-CL algorithm.


------------------------------------------------------------------------------

Title:
OCTScenes: A Versatile Real-World Dataset of Tabletop Scenes for  Object-Centric Learning

Abstract: Humans possess the cognitive ability to comprehend scenes in a compositional
manner. To empower AI systems with similar abilities, object-centric
representation learning aims to acquire representations of individual objects
from visual scenes without any supervision. Although recent advancements in
object-centric representation learning have achieved remarkable progress on
complex synthesis datasets, there is a huge challenge for application in
complex real-world scenes. One of the essential reasons is the scarcity of
real-world datasets specifically tailored to object-centric representation
learning methods. To solve this problem, we propose a versatile real-world
dataset of tabletop scenes for object-centric learning called OCTScenes, which
is meticulously designed to serve as a benchmark for comparing, evaluating and
analyzing object-centric representation learning methods. OCTScenes contains
5000 tabletop scenes with a total of 15 everyday objects. Each scene is
captured in 60 frames covering a 360-degree perspective. Consequently,
OCTScenes is a versatile benchmark dataset that can simultaneously satisfy the
evaluation of object-centric representation learning methods across static
scenes, dynamic scenes, and multi-view scenes tasks. Extensive experiments of
object-centric representation learning methods for static, dynamic and
multi-view scenes are conducted on OCTScenes. The results demonstrate the
shortcomings of state-of-the-art methods for learning meaningful
representations from real-world data, despite their impressive performance on
complex synthesis datasets. Furthermore, OCTScenes can serves as a catalyst for
advancing existing state-of-the-art methods, inspiring them to adapt to
real-world scenes. Dataset and code are available at
this https URL


------------------------------------------------------------------------------

Title:
From Hypergraph Energy Functions to Hypergraph Neural Networks

Abstract: Hypergraphs are a powerful abstraction for representing higher-order
interactions between entities of interest. To exploit these relationships in
making downstream predictions, a variety of hypergraph neural network
architectures have recently been proposed, in large part building upon
precursors from the more traditional graph neural network (GNN) literature.
Somewhat differently, in this paper we begin by presenting an expressive family
of parameterized, hypergraph-regularized energy functions. We then demonstrate
how minimizers of these energies effectively serve as node embeddings that,
when paired with a parameterized classifier, can be trained end-to-end via a
supervised bilevel optimization process. Later, we draw parallels between the
implicit architecture of the predictive models emerging from the proposed
bilevel hypergraph optimization, and existing GNN architectures in common use.
Empirically, we demonstrate state-of-the-art results on various hypergraph node
classification benchmarks. Code is available at
this https URL


------------------------------------------------------------------------------

Title:
Temporal Causal Mediation through a Point Process: Direct and Indirect  Effects of Healthcare Interventions

Abstract: Deciding on an appropriate intervention requires a causal model of a
treatment, the outcome, and potential mediators. Causal mediation analysis lets
us distinguish between direct and indirect effects of the intervention, but has
mostly been studied in a static setting. In healthcare, data come in the form
of complex, irregularly sampled time-series, with dynamic interdependencies
between a treatment, outcomes, and mediators across time. Existing approaches
to dynamic causal mediation analysis are limited to regular measurement
intervals, simple parametric models, and disregard long-range mediator--outcome
interactions. To address these limitations, we propose a non-parametric
mediator--outcome model where the mediator is assumed to be a temporal point
process that interacts with the outcome process. With this model, we estimate
the direct and indirect effects of an external intervention on the outcome,
showing how each of these affects the whole future trajectory. We demonstrate
on semi-synthetic data that our method can accurately estimate direct and
indirect effects. On real-world healthcare data, our model infers clinically
meaningful direct and indirect effect trajectories for blood glucose after a
surgery.


------------------------------------------------------------------------------

Title:
Cross-Domain Toxic Spans Detection

Abstract: Given the dynamic nature of toxic language use, automated methods for
detecting toxic spans are likely to encounter distributional shift. To explore
this phenomenon, we evaluate three approaches for detecting toxic spans under
cross-domain conditions: lexicon-based, rationale extraction, and fine-tuned
language models. Our findings indicate that a simple method using off-the-shelf
lexicons performs best in the cross-domain setup. The cross-domain error
analysis suggests that (1) rationale extraction methods are prone to false
negatives, while (2) language models, despite performing best for the in-domain
case, recall fewer explicitly toxic words than lexicons and are prone to
certain types of false positives. Our code is publicly available at:
this https URL


------------------------------------------------------------------------------

Title:
Enabling BIM-Driven Robotic Construction Workflows with Closed-Loop  Digital Twins

Abstract: Robots can greatly alleviate physical demands on construction workers while
enhancing both the productivity and safety of construction projects. Leveraging
a Building Information Model (BIM) offers a natural and promising approach to
drive a robotic construction workflow. However, because of uncertainties
inherent on construction sites, such as discrepancies between the designed and
as-built workpieces, robots cannot solely rely on the BIM to guide field
construction work. Human workers are adept at improvising alternative plans
with their creativity and experience and thus can assist robots in overcoming
uncertainties and performing construction work successfully. This research
introduces an interactive closed-loop digital twin system that integrates a BIM
into human-robot collaborative construction workflows. The robot is primarily
driven by the BIM, but it adaptively adjusts its plan based on actual site
conditions while the human co-worker supervises the process. If necessary, the
human co-worker intervenes in the robot's plan by changing the task sequence or
target position, requesting a new motion plan, or modifying the construction
component(s)/material(s) to help the robot navigate uncertainties. To
investigate the physical deployment of the system, a drywall installation case
study is conducted with an industrial robotic arm in a laboratory. In addition,
a block pick-and-place experiment is carried out to evaluate system
performance. Integrating the flexibility of human workers and the autonomy and
accuracy afforded by the BIM, the system significantly increases the robustness
of construction robots in the performance of field construction work.


------------------------------------------------------------------------------

Title:
How do different tokenizers perform on downstream tasks in scriptio  continua languages?: A case study in Japanese

Abstract: This paper investigates the effect of tokenizers on the downstream
performance of pretrained language models (PLMs) in scriptio continua languages
where no explicit spaces exist between words, using Japanese as a case study.
The tokenizer for such languages often consists of a morphological analyzer and
a subword tokenizer, requiring us to conduct a comprehensive study of all
possible pairs. However, previous studies lack this comprehensiveness. We
therefore train extensive sets of tokenizers, build a PLM using each, and
measure the downstream performance on a wide range of tasks. Our results
demonstrate that each downstream task has a different optimal morphological
analyzer, and that it is better to use Byte-Pair-Encoding or Unigram rather
than WordPiece as a subword tokenizer, regardless of the type of task.


------------------------------------------------------------------------------

Title:
A connection method for a defeasible extension of $\mathcal{ALCH}$

Abstract: This paper proposes a connection method \`a la Bibel for an
exception-tolerant family of description logics (DLs). As for the language, we
assume the DL $\mathcal{ALCH}$ extended with two typicality operators: one on
(complex) concepts and one on role names. The language is a variant of
defeasible DLs, as broadly studied in the literature over the past decade, in
which most of these can be embedded. We revisit the definition of the matrix
representation of a knowledge base and establish the conditions for a given
axiom to be provable. We show that the calculus terminates and is sound and
complete w.r.t. a DL version of the preferential semantics widely adopted in
non-monotonic reasoning.


------------------------------------------------------------------------------

Title:
Multi-View Class Incremental Learning

Abstract: Multi-view learning (MVL) has gained great success in integrating information
from multiple perspectives of a dataset to improve downstream task performance.
To make MVL methods more practical in an open-ended environment, this paper
investigates a novel paradigm called multi-view class incremental learning
(MVCIL), where a single model incrementally classifies new classes from a
continual stream of views, requiring no access to earlier views of data.
However, MVCIL is challenged by the catastrophic forgetting of old information
and the interference with learning new concepts. To address this, we first
develop a randomization-based representation learning technique serving for
feature extraction to guarantee their separate view-optimal working states,
during which multiple views belonging to a class are presented sequentially;
Then, we integrate them one by one in the orthogonality fusion subspace spanned
by the extracted features; Finally, we introduce selective weight consolidation
for learning-without-forgetting decision-making while encountering new classes.
Extensive experiments on synthetic and real-world datasets validate the
effectiveness of our approach.


------------------------------------------------------------------------------

Title:
Leveraging Residue Number System for Designing High-Precision Analog  Deep Neural Network Accelerators

Abstract: Achieving high accuracy, while maintaining good energy efficiency, in analog
DNN accelerators is challenging as high-precision data converters are
expensive. In this paper, we overcome this challenge by using the residue
number system (RNS) to compose high-precision operations from multiple
low-precision operations. This enables us to eliminate the information loss
caused by the limited precision of the ADCs. Our study shows that RNS can
achieve 99% FP32 accuracy for state-of-the-art DNN inference using data
converters with only $6$-bit precision. We propose using redundant RNS to
achieve a fault-tolerant analog accelerator. In addition, we show that RNS can
reduce the energy consumption of the data converters within an analog
accelerator by several orders of magnitude compared to a regular fixed-point
approach.


------------------------------------------------------------------------------

Title:
Clickbait Detection via Large Language Models

Abstract: Clickbait, which aims to induce users with some surprising and even thrilling
headlines for increasing click-through rates, permeates almost all online
content publishers, such as news portals and social media. Recently, Large
Language Models (LLMs) have emerged as a powerful instrument and achieved
tremendous success in a serious of NLP downstream tasks. However, it is not yet
known whether LLMs can be served as a high-quality clickbait detection system.
In this paper, we analyze the performance of LLMs in the few-shot scenarios on
a number of English and Chinese benchmark datasets. Experimental results show
that LLMs cannot achieve the best results compared to the state-of-the-art deep
and fine-tuning PLMs methods. Different from the human intuition, the
experiments demonstrated that LLMs cannot make satisfied clickbait detection
just by the headlines.


------------------------------------------------------------------------------

Title:
FewSAR: A Few-shot SAR Image Classification Benchmark

Abstract: Few-shot learning (FSL) is one of the significant and hard problems in the
field of image classification. However, in contrast to the rapid development of
the visible light dataset, the progress in SAR target image classification is
much slower. The lack of unified benchmark is a key reason for this phenomenon,
which may be severely overlooked by the current literature. The researchers of
SAR target image classification always report their new results on their own
datasets and experimental setup. It leads to inefficiency in result comparison
and impedes the further progress of this area. Motivated by this observation,
we propose a novel few-shot SAR image classification benchmark (FewSAR) to
address this issue. FewSAR consists of an open-source Python code library of 15
classic methods in three categories for few-shot SAR image classification. It
provides an accessible and customizable testbed for different few-shot SAR
image classification task. To further understanding the performance of
different few-shot methods, we establish evaluation protocols and conduct
extensive experiments within the benchmark. By analyzing the quantitative
results and runtime under the same setting, we observe that the accuracy of
metric learning methods can achieve the best results. Meta-learning methods and
fine-tuning methods perform poorly on few-shot SAR images, which is primarily
due to the bias of existing datasets. We believe that FewSAR will open up a new
avenue for future research and development, on real-world challenges at the
intersection of SAR image classification and few-shot deep learning. We will
provide our code for the proposed FewSAR at this https URL


------------------------------------------------------------------------------

Title:
Class-Adaptive Self-Training for Relation Extraction with Incompletely  Annotated Training Data

Abstract: Relation extraction (RE) aims to extract relations from sentences and
documents. Existing relation extraction models typically rely on supervised
machine learning. However, recent studies showed that many RE datasets are
incompletely annotated. This is known as the false negative problem in which
valid relations are falsely annotated as 'no_relation'. Models trained with
such data inevitably make similar mistakes during the inference stage.
Self-training has been proven effective in alleviating the false negative
problem. However, traditional self-training is vulnerable to confirmation bias
and exhibits poor performance in minority classes. To overcome this limitation,
we proposed a novel class-adaptive re-sampling self-training framework.
Specifically, we re-sampled the pseudo-labels for each class by precision and
recall scores. Our re-sampling strategy favored the pseudo-labels of classes
with high precision and low recall, which improved the overall recall without
significantly compromising precision. We conducted experiments on
document-level and biomedical relation extraction datasets, and the results
showed that our proposed self-training framework consistently outperforms
existing competitive methods on the Re-DocRED and ChemDisgene datasets when the
training data are incompletely annotated. Our code is released at
this https URL


------------------------------------------------------------------------------

Title:
Multi-Objective and Model-Predictive Tree Search for Spatiotemporal  Informative Planning

Abstract: Adaptive sampling and planning in robotic environmental monitoring are
challenging when the target environmental process varies over space and time.
The underlying environmental dynamics require the planning module to integrate
future environmental changes so that action decisions made earlier do not
quickly become outdated. We propose a Monte Carlo tree search method which not
only well balances the environment exploration and exploitation in space, but
also catches up to the temporal environmental dynamics. This is achieved by
incorporating multi-objective optimization and a look-ahead model-predictive
rewarding mechanism. We show that by allowing the robot to leverage the
simulated and predicted spatiotemporal environmental process, the proposed
informative planning approach achieves a superior performance after comparing
with other baseline methods in terms of the root mean square error of the
environment model and the distance to the ground truth.


------------------------------------------------------------------------------

Title:
Using Machine Learning Methods for Automation of Size Grid Building and  Management

Abstract: Fashion apparel companies require planning for the next season, a year in
advance for supply chain management. This study focuses on size selection
decision making for Levi Strauss. Currently, the region and planning group
level size grids are built and managed manually. The company suffers from the
workload it creates for sizing, merchant and planning teams. This research is
aiming to answer two research questions: "Which sizes should be available to
the planners under each size grid name for the next season(s)?" and "Which
sizes should be adopted for each planning group for the next season(s)?". We
approach to the problem with a classification model, which is one of the
popular models used in machine learning. With this research, a more automated
process was created by using machine learning techniques. A decrease in
workload of the teams in the company is expected after it is put into practice.
Unlike many studies in the state of art for fashion and apparel industry, this
study focuses on sizes where the stock keeping unit represents a product with a
certain size.


------------------------------------------------------------------------------

Title:
CLIPSonic: Text-to-Audio Synthesis with Unlabeled Videos and Pretrained  Language-Vision Models

Abstract: Recent work has studied text-to-audio synthesis using large amounts of paired
text-audio data. However, audio recordings with high-quality text annotations
can be difficult to acquire. In this work, we approach text-to-audio synthesis
using unlabeled videos and pretrained language-vision models. We propose to
learn the desired text-audio correspondence by leveraging the visual modality
as a bridge. We train a conditional diffusion model to generate the audio track
of a video, given a video frame encoded by a pretrained contrastive
language-image pretraining (CLIP) model. At test time, we first explore
performing a zero-shot modality transfer and condition the diffusion model with
a CLIP-encoded text query. However, we observe a noticeable performance drop
with respect to image queries. To close this gap, we further adopt a pretrained
diffusion prior model to generate a CLIP image embedding given a CLIP text
embedding. Our results show the effectiveness of the proposed method, and that
the pretrained diffusion prior can reduce the modality transfer gap. While we
focus on text-to-audio synthesis, the proposed model can also generate audio
from image queries, and it shows competitive performance against a
state-of-the-art image-to-audio synthesis model in a subjective listening test.
This study offers a new direction of approaching text-to-audio synthesis that
leverages the naturally-occurring audio-visual correspondence in videos and the
power of pretrained language-vision models.


------------------------------------------------------------------------------

Title:
Live Exploration of AI-Generated Programs

Abstract: AI-powered programming assistants are increasingly gaining popularity, with
GitHub Copilot alone used by over a million developers worldwide. These tools
are far from perfect, however, producing code suggestions that may be incorrect
or incomplete in subtle ways. As a result, developers face a new set of
challenges when they need to understand, validate, and choose between AI's
suggestions.
This paper explores whether Live Programming, a continuous display of a
program's runtime values, can help address these challenges. We introduce Live
Exploration of AI-Generated Programs, a new interaction model for AI
programming assistants that supports exploring multiple code suggestions
through Live Programming. We implement this interaction model in a prototype
Python environment LEAP and evaluate it through a between-subject study. Our
results motivate several design opportunities for future AI-powered programming
tools.


------------------------------------------------------------------------------

Title:
AUGUST: an Automatic Generation Understudy for Synthesizing  Conversational Recommendation Datasets

Abstract: High-quality data is essential for conversational recommendation systems and
serves as the cornerstone of the network architecture development and training
strategy design. Existing works contribute heavy human efforts to manually
labeling or designing and extending recommender dialogue templates. However,
they suffer from (i) the limited number of human annotators results in that
datasets can hardly capture rich and large-scale cases in the real world, (ii)
the limited experience and knowledge of annotators account for the
uninformative corpus and inappropriate recommendations. In this paper, we
propose a novel automatic dataset synthesis approach that can generate both
large-scale and high-quality recommendation dialogues through a data2text
generation process, where unstructured recommendation conversations are
generated from structured graphs based on user-item information from the real
world. In doing so, we comprehensively exploit: (i) rich personalized user
profiles from traditional recommendation datasets, (ii) rich external knowledge
from knowledge graphs, and (iii) the conversation ability contained in
human-to-human conversational recommendation datasets. Extensive experiments
validate the benefit brought by the automatically synthesized data under
low-resource scenarios and demonstrate the promising potential to facilitate
the development of a more effective conversational recommendation system.


------------------------------------------------------------------------------

Title:
Towards Sustainable Computing: Assessing the Carbon Footprint of  Heterogeneous Systems

Abstract: Decades of progress in energy-efficient and low-power design have
successfully reduced the operational carbon footprint in the semiconductor
industry. However, this has led to an increase in embodied emissions,
encompassing carbon emissions arising from design, manufacturing, packaging,
and other infrastructural activities. While existing research has developed
tools to analyze embodied carbon at the computer architecture level for
traditional monolithic systems, these tools do not apply to near-mainstream
heterogeneous integration (HI) technologies. HI systems offer significant
potential for sustainable computing by minimizing carbon emissions through two
key strategies: ``reducing" computation by reusing pre-designed chiplet IP
blocks and adopting hierarchical approaches to system design. The reuse of
chiplets across multiple designs, even spanning multiple generations of
integrated circuits (ICs), can substantially reduce embodied carbon emissions
throughout the operational lifespan. This paper introduces a carbon analysis
tool specifically designed to assess the potential of HI systems in
facilitating greener VLSI system design and manufacturing approaches. The tool
takes into account scaling, chiplet and packaging yields, design complexity,
and even carbon overheads associated with advanced packaging techniques
employed in heterogeneous systems. Experimental results demonstrate that HI can
achieve a reduction of embodied carbon emissions up to 70\% compared to
traditional large monolithic systems. These findings suggest that HI can pave
the way for sustainable computing practices, contributing to a more
environmentally conscious semiconductor industry.


------------------------------------------------------------------------------

Title:
Emergent Asymmetry of Precision and Recall for Measuring Fidelity and  Diversity of Generative Models in High Dimensions

Abstract: Precision and Recall are two prominent metrics of generative performance,
which were proposed to separately measure the fidelity and diversity of
generative models. Given their central role in comparing and improving
generative models, understanding their limitations are crucially important. To
that end, in this work, we identify a critical flaw in the common approximation
of these metrics using k-nearest-neighbors, namely, that the very
interpretations of fidelity and diversity that are assigned to Precision and
Recall can fail in high dimensions, resulting in very misleading conclusions.
Specifically, we empirically and theoretically show that as the number of
dimensions grows, two model distributions with supports at equal point-wise
distance from the support of the real distribution, can have vastly different
Precision and Recall regardless of their respective distributions, hence an
emergent asymmetry in high dimensions. Based on our theoretical insights, we
then provide simple yet effective modifications to these metrics to construct
symmetric metrics regardless of the number of dimensions. Finally, we provide
experiments on real-world datasets to illustrate that the identified flaw is
not merely a pathological case, and that our proposed metrics are effective in
alleviating its impact.


------------------------------------------------------------------------------

Title:
Is the Volume of a Credal Set a Good Measure for Epistemic Uncertainty?

Abstract: Adequate uncertainty representation and quantification have become imperative
in various scientific disciplines, especially in machine learning and
artificial intelligence. As an alternative to representing uncertainty via one
single probability measure, we consider credal sets (convex sets of probability
measures). The geometric representation of credal sets as $d$-dimensional
polytopes implies a geometric intuition about (epistemic) uncertainty. In this
paper, we show that the volume of the geometric representation of a credal set
is a meaningful measure of epistemic uncertainty in the case of binary
classification, but less so for multi-class classification. Our theoretical
findings highlight the crucial role of specifying and employing uncertainty
measures in machine learning in an appropriate way, and for being aware of
possible pitfalls.


------------------------------------------------------------------------------

Title:
Finite state verifiers with both private and public coins

Abstract: We consider the effects of allowing a finite state verifier in an interactive
proof system to use a bounded number of private coins, in addition to "public"
coins whose outcomes are visible to the prover. Although swapping between
private and public-coin machines does not change the class of verifiable
languages when the verifiers are given reasonably large time and space bounds,
this distinction has well known effects for the capabilities of constant space
verifiers. We show that a constant private-coin "budget" (independent of the
length of the input) increases the power of public-coin interactive proofs with
finite state verifiers considerably, and provide a new characterization of the
complexity class $\rm P$ as the set of languages that are verifiable by such
machines with arbitrarily small error in expected polynomial time.


------------------------------------------------------------------------------

Title:
Optimization of RIS-Aided MIMO -- A Mutually Coupled Loaded Wire Dipole  Model

Abstract: In this letter, we consider a reconfigurable intelligent surface (RIS)
assisted multiple-input multiple-output (MIMO) system in the presence of
scattering objects. The MIMO transmitter and receiver, the RIS, and the
scattering objects are modeled as mutually coupled thin wires connected to load
impedances. We introduce a novel numerical algorithm for optimizing the tunable
loads connected to the RIS. Compared with currently available algorithms, the
proposed approach does not rely on the Neumann series approximation, but it
optimizes the tunable load impedances alternately and one by one. At each
iteration step, a closed-form expression for each impedance is provided by
applying the Gram-Schmidt orthogonalization method. The algorithm is provably
convergent and has a polynomial complexity with the number of RIS elements.
Also, it is shown to outperform, in terms of achievable rate, two benchmark
algorithms, which are based on a similar electromagnetic model, while requiring
fewer iterations and a reduced execution time to reach convergence.


------------------------------------------------------------------------------

Title:
PIEChain -- A Practical Blockchain Interoperability Framework

Abstract: A plethora of different blockchain platforms have emerged in recent years,
but many of them operate in silos. As such, there is a need for reliable
cross-chain communication to enable blockchain interoperability. Blockchain
interoperability is challenging because transactions can typically not be
reverted - as such, if one transaction is committed then the protocol must
ensure that all related transactions are committed as well. Existing
interoperability approaches, e.g., Cosmos and Polkadot, are limited in the
sense that they only support interoperability between their own subchains, or
require intrusive changes to existing blockchains. To overcome this limitation,
we propose PIEChain, a general, Kafka-based cross-chain communication
framework. We utilize PIEChain for a practical case study: a cross-chain
auction in which users who hold tokens on multiple chains bid for a ticket sold
on another chain. PIEChain is the first publicly available, practical
implementation of a general framework for cross-chain communication.


------------------------------------------------------------------------------

Title:
Inverse Scaling: When Bigger Isn't Better

Abstract: Work on scaling laws has found that large language models (LMs) show
predictable improvements to overall loss with increased scale (model size,
training data, and compute). Here, we present evidence for the claim that LMs
may show inverse scaling, or worse task performance with increased scale, e.g.,
due to flaws in the training objective and data. We present empirical evidence
of inverse scaling on 11 datasets collected by running a public contest, the
Inverse Scaling Prize, with a substantial prize pool. Through analysis of the
datasets, along with other examples found in the literature, we identify four
potential causes of inverse scaling: (i) preference to repeat memorized
sequences over following in-context instructions, (ii) imitation of undesirable
patterns in the training data, (iii) tasks containing an easy distractor task
which LMs could focus on, rather than the harder real task, and (iv) correct
but misleading few-shot demonstrations of the task. We release the winning
datasets at this https URL to allow for further investigation
of inverse scaling. Our tasks have helped drive the discovery of U-shaped and
inverted-U scaling trends, where an initial trend reverses, suggesting that
scaling trends are less reliable at predicting the behavior of larger-scale
models than previously understood. Overall, our results suggest that there are
tasks for which increased model scale alone may not lead to progress, and that
more careful thought needs to go into the data and objectives for training
language models.


------------------------------------------------------------------------------

Title:
CHORUS: Foundation Models for Unified Data Discovery and Exploration

Abstract: We explore the application of foundation models to data discovery and
exploration tasks. Foundation models are large language models (LLMs) that show
promising performance on a range of diverse tasks unrelated to their training.
We show that these models are highly applicable to the data discovery and data
exploration domain. When carefully used, they have superior capability on three
representative tasks: table-class detection, column-type annotation and
join-column prediction. On all three tasks, we show that a
foundation-model-based approach outperforms the task-specific models and so the
state of the art. Further, our approach often surpasses human-expert task
performance. This suggests a future direction in which disparate data
management tasks can be unified under foundation models.


------------------------------------------------------------------------------

Title:
Cooperative Multi-Objective Reinforcement Learning for Traffic Signal  Control and Carbon Emission Reduction

Abstract: Existing traffic signal control systems rely on oversimplified rule-based
methods, and even RL-based methods are often suboptimal and unstable. To
address this, we propose a cooperative multi-objective architecture called
Multi-Objective Multi-Agent Deep Deterministic Policy Gradient (MOMA-DDPG),
which estimates multiple reward terms for traffic signal control optimization
using age-decaying weights. Our approach involves two types of agents: one
focuses on optimizing local traffic at each intersection, while the other aims
to optimize global traffic throughput. We evaluate our method using real-world
traffic data collected from an Asian country's traffic cameras. Despite the
inclusion of a global agent, our solution remains decentralized as this agent
is no longer necessary during the inference stage. Our results demonstrate the
effectiveness of MOMA-DDPG, outperforming state-of-the-art methods across all
performance metrics. Additionally, our proposed system minimizes both waiting
time and carbon emissions. Notably, this paper is the first to link carbon
emissions and global agents in traffic signal control.


------------------------------------------------------------------------------

Title:
HomoGCL: Rethinking Homophily in Graph Contrastive Learning

Abstract: Contrastive learning (CL) has become the de-facto learning paradigm in
self-supervised learning on graphs, which generally follows the
"augmenting-contrasting" learning scheme. However, we observe that unlike CL in
computer vision domain, CL in graph domain performs decently even without
augmentation. We conduct a systematic analysis of this phenomenon and argue
that homophily, i.e., the principle that "like attracts like", plays a key role
in the success of graph CL. Inspired to leverage this property explicitly, we
propose HomoGCL, a model-agnostic framework to expand the positive set using
neighbor nodes with neighbor-specific significances. Theoretically, HomoGCL
introduces a stricter lower bound of the mutual information between raw node
features and node embeddings in augmented views. Furthermore, HomoGCL can be
combined with existing graph CL models in a plug-and-play way with light extra
computational overhead. Extensive experiments demonstrate that HomoGCL yields
multiple state-of-the-art results across six public datasets and consistently
brings notable performance improvements when applied to various graph CL
methods. Code is avilable at this https URL


------------------------------------------------------------------------------

Title:
Structured Cooperative Learning with Graphical Model Priors

Abstract: We study how to train personalized models for different tasks on
decentralized devices with limited local data. We propose "Structured
Cooperative Learning (SCooL)", in which a cooperation graph across devices is
generated by a graphical model prior to automatically coordinate mutual
learning between devices. By choosing graphical models enforcing different
structures, we can derive a rich class of existing and novel decentralized
learning algorithms via variational inference. In particular, we show three
instantiations of SCooL that adopt Dirac distribution, stochastic block model
(SBM), and attention as the prior generating cooperation graphs. These EM-type
algorithms alternate between updating the cooperation graph and cooperative
learning of local models. They can automatically capture the cross-task
correlations among devices by only monitoring their model updating in order to
optimize the cooperation graph. We evaluate SCooL and compare it with existing
decentralized learning methods on an extensive set of benchmarks, on which
SCooL always achieves the highest accuracy of personalized models and
significantly outperforms other baselines on communication efficiency. Our code
is available at this https URL


------------------------------------------------------------------------------

Title:
FETNet: Feature Erasing and Transferring Network for Scene Text Removal

Abstract: The scene text removal (STR) task aims to remove text regions and recover the
background smoothly in images for private information protection. Most existing
STR methods adopt encoder-decoder-based CNNs, with direct copies of the
features in the skip connections. However, the encoded features contain both
text texture and structure information. The insufficient utilization of text
features hampers the performance of background reconstruction in text removal
regions. To tackle these problems, we propose a novel Feature Erasing and
Transferring (FET) mechanism to reconfigure the encoded features for STR in
this paper. In FET, a Feature Erasing Module (FEM) is designed to erase text
features. An attention module is responsible for generating the feature
similarity guidance. The Feature Transferring Module (FTM) is introduced to
transfer the corresponding features in different layers based on the attention
guidance. With this mechanism, a one-stage, end-to-end trainable network called
FETNet is constructed for scene text removal. In addition, to facilitate
research on both scene text removal and segmentation tasks, we introduce a
novel dataset, Flickr-ST, with multi-category annotations. A sufficient number
of experiments and ablation studies are conducted on the public datasets and
Flickr-ST. Our proposed method achieves state-of-the-art performance using most
metrics, with remarkably higher quality scene text removal results. The source
code of our work is available at:
\href{https://github.com/GuangtaoLyu/FETNet}{this https URL


------------------------------------------------------------------------------

Title:
AQuA: A Benchmarking Tool for Label Quality Assessment

Abstract: Machine learning (ML) models are only as good as the data they are trained
on. But recent studies have found datasets widely used to train and evaluate ML
models, e.g. ImageNet, to have pervasive labeling errors. Erroneous labels on
the train set hurt ML models' ability to generalize, and they impact evaluation
and model selection using the test set. Consequently, learning in the presence
of labeling errors is an active area of research, yet this field lacks a
comprehensive benchmark to evaluate these methods. Most of these methods are
evaluated on a few computer vision datasets with significant variance in the
experimental protocols. With such a large pool of methods and inconsistent
evaluation, it is also unclear how ML practitioners can choose the right models
to assess label quality in their data. To this end, we propose a benchmarking
environment AQuA to rigorously evaluate methods that enable machine learning in
the presence of label noise. We also introduce a design space to delineate
concrete design choices of label error detection models. We hope that our
proposed design space and benchmark enable practitioners to choose the right
tools to improve their label quality and that our benchmark enables objective
and rigorous evaluation of machine learning tools facing mislabeled data.


------------------------------------------------------------------------------

Title:
Block-State Transformer

Abstract: State space models (SSMs) have shown impressive results on tasks that require
modeling long-range dependencies and efficiently scale to long sequences owing
to their subquadratic runtime complexity. Originally designed for continuous
signals, SSMs have shown superior performance on a plethora of tasks, in vision
and audio; however, SSMs still lag Transformer performance in Language Modeling
tasks. In this work, we propose a hybrid layer named Block-State Transformer
(BST), that internally combines an SSM sublayer for long-range
contextualization, and a Block Transformer sublayer for short-term
representation of sequences. We study three different, and completely
parallelizable, variants that integrate SSMs and block-wise attention. We show
that our model outperforms similar Transformer-based architectures on language
modeling perplexity and generalizes to longer sequences. In addition, the
Block-State Transformer demonstrates more than tenfold increase in speed at the
layer level compared to the Block-Recurrent Transformer when model
parallelization is employed.


------------------------------------------------------------------------------

Title:
Understanding the Application of Utility Theory in Robotics and  Artificial Intelligence: A Survey

Abstract: As a unifying concept in economics, game theory, and operations research,
even in the Robotics and AI field, the utility is used to evaluate the level of
individual needs, preferences, and interests. Especially for decision-making
and learning in multi-agent/robot systems (MAS/MRS), a suitable utility model
can guide agents in choosing reasonable strategies to achieve their current
needs and learning to cooperate and organize their behaviors, optimizing the
system's utility, building stable and reliable relationships, and guaranteeing
each group member's sustainable development, similar to the human society.
Although these systems' complex, large-scale, and long-term behaviors are
strongly determined by the fundamental characteristics of the underlying
relationships, there has been less discussion on the theoretical aspects of
mechanisms and the fields of applications in Robotics and AI. This paper
introduces a utility-orient needs paradigm to describe and evaluate inter and
outer relationships among agents' interactions. Then, we survey existing
literature in relevant fields to support it and propose several promising
research directions along with some open problems deemed necessary for further
investigations.


------------------------------------------------------------------------------

Title:
Multi-Classification using One-versus-One Deep Learning Strategy with  Joint Probability Estimates

Abstract: The One-versus-One (OvO) strategy is an approach of multi-classification
models which focuses on training binary classifiers between each pair of
classes. While the OvO strategy takes advantage of balanced training data, the
classification accuracy is usually hindered by the voting mechanism to combine
all binary classifiers. In this paper, a novel OvO multi-classification model
incorporating a joint probability measure is proposed under the deep learning
framework. In the proposed model, a two-stage algorithm is developed to
estimate the class probability from the pairwise binary classifiers. Given the
binary classifiers, the pairwise probability estimate is calibrated by a
distance measure on the separating feature hyperplane. From that, the class
probability of the subject is estimated by solving a joint probability-based
distance minimization problem. Numerical experiments in different applications
show that the proposed model achieves generally higher classification accuracy
than other state-of-the-art models.


------------------------------------------------------------------------------

Title:
Training generative models from privatized data

Abstract: Local differential privacy (LDP) is a powerful method for privacy-preserving
data collection. In this paper, we develop a framework for training Generative
Adversarial Networks (GAN) on differentially privatized data. We show that
entropic regularization of the Wasserstein distance -- a popular regularization
method in the literature that has been often leveraged for its computational
benefits -- can be used to denoise the data distribution when data is
privatized by common additive noise mechanisms, such as Laplace and Gaussian.
This combination uniquely enables the mitigation of both the regularization
bias and the effects of privatization noise, thereby enhancing the overall
efficacy of the model. We analyse the proposed method, provide sample
complexity results and experimental evidence to support its efficacy.


------------------------------------------------------------------------------

Title:
Average Case Error Estimates of the Strong Lucas Test

Abstract: Reliable probabilistic primality tests are fundamental in public-key
cryptography. In adversarial scenarios, a composite with a high probability of
passing a specific primality test could be chosen. In such cases, we need
worst-case error estimates for the test. However, in many scenarios the numbers
are randomly chosen and thus have significantly smaller error probability.
Therefore, we are interested in average case error estimates. In this paper, we
establish such bounds for the strong Lucas primality test, as only worst-case,
but no average case error bounds, are currently available. This allows us to
use this test with more confidence. We examine an algorithm that draws odd
$k$-bit integers uniformly and independently, runs $t$ independent iterations
of the strong Lucas test with randomly chosen parameters, and outputs the first
number that passes all $t$ consecutive rounds. We attain numerical upper bounds
on the probability on returing a composite. Furthermore, we consider a modified
version of this algorithm that excludes integers divisible by small primes,
resulting in improved bounds. Additionally, we classify the numbers that
contribute most to our estimate.


------------------------------------------------------------------------------

Title:
Kriging Convolutional Networks

Abstract: Spatial interpolation is a class of estimation problems where locations with
known values are used to estimate values at other locations, with an emphasis
on harnessing spatial locality and trends. Traditional Kriging methods have
strong Gaussian assumptions, and as a result, often fail to capture
complexities within the data. Inspired by the recent progress of graph neural
networks, we introduce Kriging Convolutional Networks (KCN), a method of
combining the advantages of Graph Convolutional Networks (GCN) and Kriging.
Compared to standard GCNs, KCNs make direct use of neighboring observations
when generating predictions. KCNs also contain the Kriging method as a specific
configuration. We further improve the model's performance by adding attention.
Empirically, we show that this model outperforms GCNs and Kriging in several
applications. The implementation of KCN using PyTorch is publicized at the
GitHub repository: this https URL


------------------------------------------------------------------------------

Title:
ST-PINN: A Self-Training Physics-Informed Neural Network for Partial  Differential Equations

Abstract: Partial differential equations (PDEs) are an essential computational kernel
in physics and engineering. With the advance of deep learning, physics-informed
neural networks (PINNs), as a mesh-free method, have shown great potential for
fast PDE solving in various applications. To address the issue of low accuracy
and convergence problems of existing PINNs, we propose a self-training
physics-informed neural network, ST-PINN. Specifically, ST-PINN introduces a
pseudo label based self-learning algorithm during training. It employs
governing equation as the pseudo-labeled evaluation index and selects the
highest confidence examples from the sample points to attach the pseudo labels.
To our best knowledge, we are the first to incorporate a self-training
mechanism into physics-informed learning. We conduct experiments on five PDE
problems in different fields and scenarios. The results demonstrate that the
proposed method allows the network to learn more physical information and
benefit convergence. The ST-PINN outperforms existing physics-informed neural
network methods and improves the accuracy by a factor of 1.33x-2.54x. The code
of ST-PINN is available at GitHub: this https URL


------------------------------------------------------------------------------

Title:
Fault Detection in Induction Motors using Functional Dimensionality  Reduction Methods

Abstract: The implementation of strategies for fault detection and diagnosis on
rotating electrical machines is crucial for the reliability and safety of
modern industrial systems. The contribution of this work is a methodology that
combines conventional strategy of Motor Current Signature Analysis with
functional dimensionality reduction methods, namely Functional Principal
Components Analysis and Functional Diffusion Maps, for detecting and
classifying fault conditions in induction motors. The results obtained from the
proposed scheme are very encouraging, revealing a potential use in the future
not only for real-time detection of the presence of a fault in an induction
motor, but also in the identification of a greater number of types of faults
present through an offline analysis.


------------------------------------------------------------------------------

Title:
Relation-Aware Network with Attention-Based Loss for Few-Shot Knowledge  Graph Completion

Abstract: Few-shot knowledge graph completion (FKGC) task aims to predict unseen facts
of a relation with few-shot reference entity pairs. Current approaches randomly
select one negative sample for each reference entity pair to minimize a
margin-based ranking loss, which easily leads to a zero-loss problem if the
negative sample is far away from the positive sample and then out of the
margin. Moreover, the entity should have a different representation under a
different context. To tackle these issues, we propose a novel Relation-Aware
Network with Attention-Based Loss (RANA) framework. Specifically, to better
utilize the plentiful negative samples and alleviate the zero-loss issue, we
strategically select relevant negative samples and design an attention-based
loss function to further differentiate the importance of each negative sample.
The intuition is that negative samples more similar to positive samples will
contribute more to the model. Further, we design a dynamic relation-aware
entity encoder for learning a context-dependent entity representation.
Experiments demonstrate that RANA outperforms the state-of-the-art models on
two benchmark datasets.


------------------------------------------------------------------------------

Title:
Privacy Guarantees for Personal Mobility Data in Humanitarian Response

Abstract: Personal mobility data from mobile phones and other sensors are increasingly
used to inform policymaking during pandemics, natural disasters, and other
humanitarian crises. However, even aggregated mobility traces can reveal
private information about individual movements to potentially malicious actors.
This paper develops and tests an approach for releasing private mobility data,
which provides formal guarantees over the privacy of the underlying subjects.
Specifically, we (1) introduce an algorithm for constructing differentially
private mobility matrices, and derive privacy and accuracy bounds on this
algorithm; (2) use real-world data from mobile phone operators in Afghanistan
and Rwanda to show how this algorithm can enable the use of private mobility
data in two high-stakes policy decisions: pandemic response and the
distribution of humanitarian aid; and (3) discuss practical decisions that need
to be made when implementing this approach, such as how to optimally balance
privacy and accuracy. Taken together, these results can help enable the
responsible use of private mobility data in humanitarian response.


------------------------------------------------------------------------------

Title:
Streamlining Input/Output Logics with Sequent Calculi

Abstract: Input/Output (I/O) logic is a general framework for reasoning about
conditional norms and/or causal relations. We streamline Bochman's causal I/O
logics via proof-search-oriented sequent calculi. Our calculi establish a
natural syntactic link between the derivability in these logics and in the
original I/O logics. As a consequence of our results, we obtain new, simple
semantics for all these logics, complexity bounds, embeddings into normal modal
logics, and efficient deduction methods. Our work encompasses many scattered
results and provides uniform solutions to various unresolved problems.


------------------------------------------------------------------------------

Title:
Edit-DiffNeRF: Editing 3D Neural Radiance Fields using 2D Diffusion  Model

Abstract: Recent research has demonstrated that the combination of pretrained diffusion
models with neural radiance fields (NeRFs) has emerged as a promising approach
for text-to-3D generation. Simply coupling NeRF with diffusion models will
result in cross-view inconsistency and degradation of stylized view syntheses.
To address this challenge, we propose the Edit-DiffNeRF framework, which is
composed of a frozen diffusion model, a proposed delta module to edit the
latent semantic space of the diffusion model, and a NeRF. Instead of training
the entire diffusion for each scene, our method focuses on editing the latent
semantic space in frozen pretrained diffusion models by the delta module. This
fundamental change to the standard diffusion framework enables us to make
fine-grained modifications to the rendered views and effectively consolidate
these instructions in a 3D scene via NeRF training. As a result, we are able to
produce an edited 3D scene that faithfully aligns to input text instructions.
Furthermore, to ensure semantic consistency across different viewpoints, we
propose a novel multi-view semantic consistency loss that extracts a latent
semantic embedding from the input view as a prior, and aim to reconstruct it in
different views. Our proposed method has been shown to effectively edit
real-world 3D scenes, resulting in 25% improvement in the alignment of the
performed 3D edits with text instructions compared to prior work.


------------------------------------------------------------------------------

Title:
Reconfigurable Intelligent Surface Assisted Semantic Communication  Systems

Abstract: Semantic communication, which focuses on conveying the meaning of information
rather than exact bit reconstruction, has gained considerable attention in
recent years. Meanwhile, reconfigurable intelligent surface (RIS) is a
promising technology that can achieve high spectral and energy efficiency by
dynamically reflecting incident signals through programmable passive
components. In this paper, we put forth a semantic communication scheme aided
by RIS. Using text transmission as an example, experimental results demonstrate
that the RIS-assisted semantic communication system outperforms the
point-to-point semantic communication system in terms of BLEU scores in
Rayleigh fading channels, especially at low signal-to-noise ratio (SNR)
regimes. In addition, the RIS-assisted semantic communication system exhibits
superior robustness against channel estimation errors compared to its
point-to-point counterpart. RIS can improve performance as it provides extra
line-of-sight (LoS) paths and enhances signal propagation conditions compared
to point-to-point systems.


------------------------------------------------------------------------------

Title:
MedFMC: A Real-world Dataset and Benchmark For Foundation Model  Adaptation in Medical Image Classification

Abstract: Foundation models, often pre-trained with large-scale data, have achieved
paramount success in jump-starting various vision and language applications.
Recent advances further enable adapting foundation models in downstream tasks
efficiently using only a few training samples, e.g., in-context learning. Yet,
the application of such learning paradigms in medical image analysis remains
scarce due to the shortage of publicly accessible data and benchmarks. In this
paper, we aim at approaches adapting the foundation models for medical image
classification and present a novel dataset and benchmark for the evaluation,
i.e., examining the overall performance of accommodating the large-scale
foundation models downstream on a set of diverse real-world clinical tasks. We
collect five sets of medical imaging data from multiple institutes targeting a
variety of real-world clinical tasks (22,349 images in total), i.e., thoracic
diseases screening in X-rays, pathological lesion tissue screening, lesion
detection in endoscopy images, neonatal jaundice evaluation, and diabetic
retinopathy grading. Results of multiple baseline methods are demonstrated
using the proposed dataset from both accuracy and cost-effective perspectives.


------------------------------------------------------------------------------

Title:
Understanding Parameter Sharing in Transformers

Abstract: Parameter sharing has proven to be a parameter-efficient approach. Previous
work on Transformers has focused on sharing parameters in different layers,
which can improve the performance of models with limited parameters by
increasing model depth. In this paper, we study why this approach works from
two perspectives. First, increasing model depth makes the model more complex,
and we hypothesize that the reason is related to model complexity (referring to
FLOPs). Secondly, since each shared parameter will participate in the network
computation several times in forward propagation, its corresponding gradient
will have a different range of values from the original model, which will
affect the model convergence. Based on this, we hypothesize that training
convergence may also be one of the reasons. Through further analysis, we show
that the success of this approach can be largely attributed to better
convergence, with only a small part due to the increased model complexity.
Inspired by this, we tune the training hyperparameters related to model
convergence in a targeted manner. Experiments on 8 machine translation tasks
show that our model achieves competitive performance with only half the model
complexity of parameter sharing models.


------------------------------------------------------------------------------

Title:
Towards Practical Federated Causal Structure Learning

Abstract: Understanding causal relations is vital in scientific discovery. The process
of causal structure learning involves identifying causal graphs from
observational data to understand such relations. Usually, a central server
performs this task, but sharing data with the server poses privacy risks.
Federated learning can solve this problem, but existing solutions for federated
causal structure learning make unrealistic assumptions about data and lack
convergence guarantees. FedC2SL is a federated constraint-based causal
structure learning scheme that learns causal graphs using a federated
conditional independence test, which examines conditional independence between
two variables under a condition set without collecting raw data from clients.
FedC2SL requires weaker and more realistic assumptions about data and offers
stronger resistance to data variability among clients. FedPC and FedFCI are the
two variants of FedC2SL for causal structure learning in causal sufficiency and
causal insufficiency, respectively. The study evaluates FedC2SL using both
synthetic datasets and real-world data against existing solutions and finds it
demonstrates encouraging performance and strong resilience to data
heterogeneity among clients.


------------------------------------------------------------------------------

Title:
A Hybrid Feature Selection and Construction Method for Detection of Wind  Turbine Generator Heating Faults

Abstract: Preprocessing of information is an essential step for the effective design of
machine learning applications. Feature construction and selection are powerful
techniques used for this aim. In this paper, a feature selection and
construction approach is presented for the detection of wind turbine generator
heating faults. Data were collected from Supervisory Control and Data
Acquisition (SCADA) system of a wind turbine. The original features directly
collected from the data collection system consist of wind characteristics,
operational data, temperature measurements and status information. In addition
to these original features, new features were created in the feature
construction step to obtain information that can be more powerful indications
of the faults. After the construction of new features, a hybrid feature
selection technique was implemented to find out the most relevant features in
the overall set to increase the classification accuracy and decrease the
computational burden. Feature selection step consists of filter and
wrapper-based parts. Filter based feature selection was applied to exclude the
features which are non-discriminative and wrapper-based method was used to
determine the final features considering the redundancies and mutual relations
amongst them. Artificial Neural Networks were used both in the detection phase
and as the induction algorithm of the wrapper-based feature selection part. The
results show that, the proposed approach contributes to the fault detection
system to be more reliable especially in terms of reducing the number of false
fault alarms.


------------------------------------------------------------------------------

Title:
Modularizing while Training: a New Paradigm for Modularizing DNN Models

Abstract: Deep neural network (DNN) models have become increasingly crucial components
in intelligent software systems. However, training a DNN model is typically
expensive in terms of both time and money. To address this issue, researchers
have recently focused on reusing existing DNN models - borrowing the idea of
code reuse in software engineering. However, reusing an entire model could
cause extra overhead or inherits the weakness from the undesired
functionalities. Hence, existing work proposes to decompose an already trained
model into modules, i.e., modularizing-after-training, and enable module reuse.
Since trained models are not built for modularization,
modularizing-after-training incurs huge overhead and model accuracy loss. In
this paper, we propose a novel approach that incorporates modularization into
the model training process, i.e., modularizing-while-training (MwT). We train a
model to be structurally modular through two loss functions that optimize
intra-module cohesion and inter-module coupling. We have implemented the
proposed approach for modularizing Convolutional Neural Network (CNN) models in
this work. The evaluation results on representative models demonstrate that MwT
outperforms the state-of-the-art approach. Specifically, the accuracy loss
caused by MwT is only 1.13 percentage points, which is 1.76 percentage points
less than that of the baseline. The kernel retention rate of the modules
generated by MwT is only 14.58%, with a reduction of 74.31% over the
state-of-the-art approach. Furthermore, the total time cost required for
training and modularizing is only 108 minutes, half of the baseline.


------------------------------------------------------------------------------

Title:
A Simple Data Augmentation for Feature Distribution Skewed Federated  Learning

Abstract: Federated learning (FL) facilitates collaborative learning among multiple
clients in a distributed manner, while ensuring privacy protection. However,
its performance is inevitably degraded as suffering data heterogeneity, i.e.,
non-IID data. In this paper, we focus on the feature distribution skewed FL
scenario, which is widespread in real-world applications. The main challenge
lies in the feature shift caused by the different underlying distributions of
local datasets. While the previous attempts achieved progress, few studies pay
attention to the data itself, the root of this issue. Therefore, the primary
goal of this paper is to develop a general data augmentation technique at the
input level, to mitigate the feature shift. To achieve this goal, we propose
FedRDN, a simple yet remarkably effective data augmentation method for feature
distribution skewed FL, which randomly injects the statistics of the dataset
from the entire federation into the client's data. By this, our method can
effectively improve the generalization of features, thereby mitigating the
feature shift. Moreover, FedRDN is a plug-and-play component, which can be
seamlessly integrated into the data augmentation flow with only a few lines of
code. Extensive experiments on several datasets show that the performance of
various representative FL works can be further improved by combining them with
FedRDN, which demonstrates the strong scalability and generalizability of
FedRDN. The source code will be released.


------------------------------------------------------------------------------

Title:
Explore, Establish, Exploit: Red Teaming Language Models from Scratch

Abstract: Deploying Large language models (LLMs) can pose hazards from harmful outputs
such as toxic or dishonest speech. Prior work has introduced tools that elicit
harmful outputs in order to identify and mitigate these risks. While this is a
valuable step toward securing language models, these approaches typically rely
on a pre-existing classifier for undesired outputs. This limits their
application to situations where the type of harmful behavior is known with
precision beforehand. However, this skips a central challenge of red teaming:
developing a contextual understanding of the behaviors that a model can
exhibit. Furthermore, when such a classifier already exists, red teaming has
limited marginal value because the classifier could simply be used to filter
training data or model outputs. In this work, we consider red teaming under the
assumption that the adversary is working from a high-level, abstract
specification of undesired behavior. The red team is expected to refine/extend
this specification and identify methods to elicit this behavior from the model.
Our red teaming framework consists of three steps: 1) Exploring the model's
behavior in the desired context; 2) Establishing a measurement of undesired
behavior (e.g., a classifier trained to reflect human evaluations); and 3)
Exploiting the model's flaws using this measure and an established red teaming
methodology. We apply this approach to red team GPT-2 and GPT-3 models to
systematically discover classes of prompts that elicit toxic and dishonest
statements. In doing so, we also construct and release the CommonClaim dataset
of 20,000 statements that have been labeled by human subjects as
common-knowledge-true, common-knowledge-false, or neither. Code is available at
this https URL CommonClaim
is available at this https URL


------------------------------------------------------------------------------

Title:
Complete Trigger Selection in Satisfiability modulo first-order Theories

Abstract: Let T be an SMT solver with no theory solvers except for Quantifier
Instantiation. Given a set of first-order clauses S saturated by Resolution
(with a valid literal selection function) we show that T is complete if its
Trigger function is the same as the literal selection function. So if T halts
with a ground model G, then G can be extended to a model in the theory of S. In
addition for a suitable ordering, if all maximal literals are selected in each
clause, then T will halt on G, so it is a decision procedure for the theory S.
Also, for a suitable ordering, if all clauses are Horn, or all clauses are
2SAT, then T solves the theory S in polynomial time.


------------------------------------------------------------------------------

Title:
A Multi-Level, Multi-Scale Visual Analytics Approach to Assessment of  Multifidelity HPC Systems

Abstract: The ability to monitor and interpret of hardware system events and behaviors
are crucial to improving the robustness and reliability of these systems,
especially in a supercomputing facility. The growing complexity and scale of
these systems demand an increase in monitoring data collected at multiple
fidelity levels and varying temporal resolutions. In this work, we aim to build
a holistic analytical system that helps make sense of such massive data, mainly
the hardware logs, job logs, and environment logs collected from disparate
subsystems and components of a supercomputer system. This end-to-end log
analysis system, coupled with visual analytics support, allows users to glean
and promptly extract supercomputer usage and error patterns at varying temporal
and spatial resolutions. We use multiresolution dynamic mode decomposition
(mrDMD), a technique that depicts high-dimensional data as correlated
spatial-temporal variations patterns or modes, to extract variation patterns
isolated at specified frequencies. Our improvements to the mrDMD algorithm help
promptly reveal useful information in the massive environment log dataset,
which is then associated with the processed hardware and job log datasets using
our visual analytics system. Furthermore, our system can identify the usage and
error patterns filtered at user, project, and subcomponent levels. We exemplify
the effectiveness of our approach with two use scenarios with the Cray XC40
supercomputer.


------------------------------------------------------------------------------

Title:
A study on group fairness in healthcare outcomes for nursing home  residents during the COVID-19 pandemic in the Basque Country

Abstract: We explore the effect of nursing home status on healthcare outcomes such as
hospitalisation, mortality and in-hospital mortality during the COVID-19
pandemic. Some claim that in specific Autonomous Communities (geopolitical
divisions) in Spain, elderly people in nursing homes had restrictions on access
to hospitals and treatments, which raised a public outcry about the fairness of
such measures. In this work, the case of the Basque Country is studied under a
rigorous statistical approach and a physician's perspective. As
fairness/unfairness is hard to model mathematically and has strong real-world
implications, this work concentrates on the following simplification:
establishing if the nursing home status had a direct effect on healthcare
outcomes once accounted for other meaningful patients' information such as age,
health status and period of the pandemic, among others. The methods followed
here are a combination of established techniques as well as new proposals from
the fields of causality and fair learning. The current analysis suggests that
as a group, people in nursing homes were significantly less likely to be
hospitalised, and considerably more likely to die, even in hospitals, compared
to their non-residents counterparts during most of the pandemic. Further data
collection and analysis are needed to guarantee that this is solely/mainly due
to nursing home status.


------------------------------------------------------------------------------

Title:
Opportunistic Transmission of Distributed Learning Models in Mobile UAVs

Abstract: In this paper, we propose an opportunistic scheme for the transmission of
model updates from Federated Learning (FL) clients to the server, where clients
are wireless mobile users. This proposal aims to opportunistically take
advantage of the proximity of users to the base station or the general
condition of the wireless transmission channel, rather than traditional
synchronous transmission. In this scheme, during the training, intermediate
model parameters are uploaded to the server, opportunistically and based on the
wireless channel condition. Then, the proactively-transmitted model updates are
used for the global aggregation if the final local model updates are delayed.
We apply this novel model transmission scheme to one of our previous work,
which is a hybrid split and federated learning (HSFL) framework for UAVs.
Simulation results confirm the superiority of using proactive transmission over
the conventional asynchronous aggregation scheme for the staled model by
obtaining higher accuracy and more stable training performance. Test accuracy
increases by up to 13.47% with just one round of extra transmission.


------------------------------------------------------------------------------

Title:
Residual Q-Learning: Offline and Online Policy Customization without  Value

Abstract: Imitation Learning (IL) is a widely used framework for learning imitative
behavior from demonstrations. It is especially appealing for solving complex
real-world tasks where handcrafting reward function is difficult, or when the
goal is to mimic human expert behavior. However, the learned imitative policy
can only follow the behavior in the demonstration. When applying the imitative
policy, we may need to customize the policy behavior to meet different
requirements coming from diverse downstream tasks. Meanwhile, we still want the
customized policy to maintain its imitative nature. To this end, we formulate a
new problem setting called policy customization. It defines the learning task
as training a policy that inherits the characteristics of the prior policy
while satisfying some additional requirements imposed by a target downstream
task. We propose a novel and principled approach to interpret and determine the
trade-off between the two task objectives. Specifically, we formulate the
customization problem as a Markov Decision Process (MDP) with a reward function
that combines 1) the inherent reward of the demonstration; and 2) the add-on
reward specified by the downstream task. We propose a novel framework, Residual
Q-learning, which can solve the formulated MDP by leveraging the prior policy
without knowing the inherent reward or value function of the prior policy. We
derive a family of residual Q-learning algorithms that can realize offline and
online policy customization, and show that the proposed algorithms can
effectively accomplish policy customization tasks in various environments.


------------------------------------------------------------------------------

Title:
Motion Comfort Optimization for Autonomous Vehicles: Concepts, Methods,  and Techniques

Abstract: This article outlines the architecture of autonomous driving and related
complementary frameworks from the perspective of human comfort. The technical
elements for measuring Autonomous Vehicle (AV) user comfort and psychoanalysis
are listed here. At the same time, this article introduces the technology
related to the structure of automatic driving and the reaction time of
automatic driving. We also discuss the technical details related to the
automatic driving comfort system, the response time of the AV driver, the
comfort level of the AV, motion sickness, and related optimization
technologies. The function of the sensor is affected by various factors. Since
the sensor of automatic driving mainly senses the environment around a vehicle,
including "the weather" which introduces the challenges and limitations of
second-hand sensors in autonomous vehicles under different weather conditions.
The comfort and safety of autonomous driving are also factors that affect the
development of autonomous driving technologies. This article further analyzes
the impact of autonomous driving on the user's physical and psychological
states and how the comfort factors of autonomous vehicles affect the automotive
market. Also, part of our focus is on the benefits and shortcomings of
autonomous driving. The goal is to present an exhaustive overview of the most
relevant technical matters to help researchers and application developers
comprehend the different comfort factors and systems of autonomous driving.
Finally, we provide detailed automated driving comfort use cases to illustrate
the comfort-related issues of autonomous driving. Then, we provide implications
and insights for the future of autonomous driving.


------------------------------------------------------------------------------

Title:
Runtime Construction of Large-Scale Spiking Neuronal Network Models on  GPU Devices

Abstract: Simulation speed matters for neuroscientific research: this includes not only
how quickly the simulated model time of a large-scale spiking neuronal network
progresses, but also how long it takes to instantiate the network model in
computer memory. On the hardware side, acceleration via highly parallel GPUs is
being increasingly utilized. On the software side, code generation approaches
ensure highly optimized code, at the expense of repeated code regeneration and
recompilation after modifications to the network model. Aiming for a greater
flexibility with respect to iterative model changes, here we propose a new
method for creating network connections interactively, dynamically, and
directly in GPU memory through a set of commonly used high-level connection
rules. We validate the simulation performance with both consumer and data
center GPUs on two neuroscientifically relevant models: a cortical microcircuit
of about 77,000 leaky-integrate-and-fire neuron models and 300 million static
synapses, and a two-population network recurrently connected using a variety of
connection rules. With our proposed ad hoc network instantiation, both network
construction and simulation times are comparable or shorter than those obtained
with other state-of-the-art simulation technologies, while still meeting the
flexibility demands of explorative network modeling.


------------------------------------------------------------------------------

Title:
FFB: A Fair Fairness Benchmark for In-Processing Group Fairness Methods

Abstract: This paper introduces the Fair Fairness Benchmark (\textsf{FFB}), a
benchmarking framework for in-processing group fairness methods. Ensuring
fairness in machine learning is critical for ethical and legal compliance.
However, there exist challenges in comparing and developing of fairness methods
due to inconsistencies in experimental settings, lack of accessible algorithmic
implementations, and limited extensibility of current fairness packages and
tools. To address these issues, we introduce an open-source, standardized
benchmark for evaluating in-processing group fairness methods and provide a
comprehensive analysis of state-of-the-art methods to ensure different notions
of group fairness. This work offers the following key contributions: the
provision of flexible, extensible, minimalistic, and research-oriented
open-source code; the establishment of unified fairness method benchmarking
pipelines; and extensive benchmarking, which yields key insights from
$\mathbf{45,079}$ experiments. We believe our work will significantly
facilitate the growth and development of the fairness research community. The
benchmark, including code and running logs, is available at
this https URL


------------------------------------------------------------------------------

Title:
Trained Transformers Learn Linear Models In-Context

Abstract: Attention-based neural networks such as transformers have demonstrated a
remarkable ability to exhibit in-context learning (ICL): Given a short prompt
sequence of tokens from an unseen task, they can formulate relevant per-token
and next-token predictions without any parameter updates. By embedding a
sequence of labeled training data and unlabeled test data as a prompt, this
allows for transformers to behave like supervised learning algorithms. Indeed,
recent work has shown that when training transformer architectures over random
instances of linear regression problems, these models' predictions mimic those
of ordinary least squares.
Towards understanding the mechanisms underlying this phenomenon, we
investigate the dynamics of ICL in transformers with a single linear
self-attention layer trained by gradient flow on linear regression tasks. We
show that despite non-convexity, gradient flow with a suitable random
initialization finds a global minimum of the objective function. At this global
minimum, when given a test prompt of labeled examples from a new prediction
task, the transformer achieves prediction error competitive with the best
linear predictor over the test prompt distribution. We additionally
characterize the robustness of the trained transformer to a variety of
distribution shifts and show that although a number of shifts are tolerated,
shifts in the covariate distribution of the prompts are not. Motivated by this,
we consider a generalized ICL setting where the covariate distributions can
vary across prompts. We show that although gradient flow succeeds at finding a
global minimum in this setting, the trained transformer is still brittle under
mild covariate shifts.


------------------------------------------------------------------------------

Title:
Super-Resolution Radar Imaging with Sparse Arrays Using a Deep Neural  Network Trained with Enhanced Virtual Data

Abstract: This paper introduces a method based on a deep neural network (DNN) that is
perfectly capable of processing radar data from extremely thinned radar
apertures. The proposed DNN processing can provide both aliasing-free radar
imaging and super-resolution. The results are validated by measuring the
detection performance on realistic simulation data and by evaluating the
Point-Spread-function (PSF) and the target-separation performance on measured
point-like targets. Also, a qualitative evaluation of a typical automotive
scene is conducted. It is shown that this approach can outperform
state-of-the-art subspace algorithms and also other existing machine learning
solutions. The presented results suggest that machine learning approaches
trained with sufficiently sophisticated virtual input data are a very promising
alternative to compressed sensing and subspace approaches in radar signal
processing. The key to this performance is that the DNN is trained using
realistic simulation data that perfectly mimic a given sparse antenna radar
array hardware as the input. As ground truth, ultra-high resolution data from
an enhanced virtual radar are simulated. Contrary to other work, the DNN
utilizes the complete radar cube and not only the antenna channel information
at certain range-Doppler detections. After training, the proposed DNN is
capable of sidelobe- and ambiguity-free imaging. It simultaneously delivers
nearly the same resolution and image quality as would be achieved with a fully
occupied array.


------------------------------------------------------------------------------

Title:
Arbitrariness Lies Beyond the Fairness-Accuracy Frontier

Abstract: Machine learning tasks may admit multiple competing models that achieve
similar performance yet produce conflicting outputs for individual samples -- a
phenomenon known as predictive multiplicity. We demonstrate that fairness
interventions in machine learning optimized solely for group fairness and
accuracy can exacerbate predictive multiplicity. Consequently, state-of-the-art
fairness interventions can mask high predictive multiplicity behind favorable
group fairness and accuracy metrics. We argue that a third axis of
``arbitrariness'' should be considered when deploying models to aid
decision-making in applications of individual-level impact. To address this
challenge, we propose an ensemble algorithm applicable to any fairness
intervention that provably ensures more consistent predictions.


------------------------------------------------------------------------------

Title:
A Vision-based Autonomous Perching Approach for Nano Aerial Vehicles

Abstract: Over the past decades, quadcopters have been investigated, due to their
mobility and flexibility to operate in a wide range of environments. They have
been used in various areas, including surveillance and monitoring. During a
mission, drones do not have to remain active once they have reached a target
location. To conserve energy and maintain a static position, it is possible to
perch and stop the motors in such situations. The problem of achieving a
reliable and highly accurate perching method remains a challenge and promising.
In this paper, a vision-based autonomous perching approach for nano quadcopters
onto a predefined perching target on horizontal surfaces is proposed. First, a
perching target with a small marker inside a larger one is designed to improve
detection capability at a variety of ranges. Second, a monocular camera is used
to calculate the relative poses of the flying vehicle from the markers
detected. Then, a Kalman filter is applied to determine the pose more reliably,
especially when measurement data is missing. Next, we introduce an algorithm
for merging the pose data from multiple markers. Finally, the poses are sent to
the perching planner to conduct the real flight test to align the drone with
the target's center and steer it there. Based on the experimental results, the
approach proved to be effective and feasible. The drone can successfully perch
on the center of markers within two centimeters of precision.


------------------------------------------------------------------------------

Title:
Existence and Construction of a Gröbner Basis for a Polynomial Ideal

Abstract: This extended abstract gives a construction for lifting a Gr\"obner basis
algorithm for an ideal in a polynomial ring over a commutative ring R under the
condition that R also admits a Gr\"obner basis for every ideal in R.


------------------------------------------------------------------------------

Title:
Equitable Multi-task Learning

Abstract: Multi-task learning (MTL) has achieved great success in various research
domains, such as CV, NLP and IR etc. Due to the complex and competing task
correlation, naive training all tasks may lead to inequitable learning, i.e.
some tasks are learned well while others are overlooked. Multi-task
optimization (MTO) aims to improve all tasks at same time, but conventional
methods often perform poor when tasks with large loss scale or gradient norm
magnitude difference. To solve the issue, we in-depth investigate the equity
problem for MTL and find that regularizing relative contribution of different
tasks (i.e. value of task-specific loss divides its raw gradient norm) in
updating shared parameter can improve generalization performance of MTL. Based
on our theoretical analysis, we propose a novel multi-task optimization method,
named EMTL, to achieve equitable MTL. Specifically, we efficiently add variance
regularization to make different tasks' relative contribution closer. Extensive
experiments have been conduct to evaluate EMTL, our method stably outperforms
state-of-the-art methods on the public benchmark datasets of two different
research domains. Furthermore, offline and online A/B test on multi-task
recommendation are conducted too. EMTL improves multi-task recommendation
significantly, demonstrating the superiority and practicability of our method
in industrial landscape.


------------------------------------------------------------------------------

Title:
Understanding the Role of Feedback in Online Learning with Switching  Costs

Abstract: In this paper, we study the role of feedback in online learning with
switching costs. It has been shown that the minimax regret is
$\widetilde{\Theta}(T^{2/3})$ under bandit feedback and improves to
$\widetilde{\Theta}(\sqrt{T})$ under full-information feedback, where $T$ is
the length of the time horizon. However, it remains largely unknown how the
amount and type of feedback generally impact regret. To this end, we first
consider the setting of bandit learning with extra observations; that is, in
addition to the typical bandit feedback, the learner can freely make a total of
$B_{\mathrm{ex}}$ extra observations. We fully characterize the minimax regret
in this setting, which exhibits an interesting phase-transition phenomenon:
when $B_{\mathrm{ex}} = O(T^{2/3})$, the regret remains
$\widetilde{\Theta}(T^{2/3})$, but when $B_{\mathrm{ex}} = \Omega(T^{2/3})$, it
becomes $\widetilde{\Theta}(T/\sqrt{B_{\mathrm{ex}}})$, which improves as the
budget $B_{\mathrm{ex}}$ increases. To design algorithms that can achieve the
minimax regret, it is instructive to consider a more general setting where the
learner has a budget of $B$ total observations. We fully characterize the
minimax regret in this setting as well and show that it is
$\widetilde{\Theta}(T/\sqrt{B})$, which scales smoothly with the total budget
$B$. Furthermore, we propose a generic algorithmic framework, which enables us
to design different learning algorithms that can achieve matching upper bounds
for both settings based on the amount and type of feedback. One interesting
finding is that while bandit feedback can still guarantee optimal regret when
the budget is relatively limited, it no longer suffices to achieve optimal
regret when the budget is relatively large.


------------------------------------------------------------------------------

Title:
A comprehensive review of 3D convolutional neural network-based  classification techniques of diseased and defective crops using non-UAV-based  hyperspectral images

Abstract: Hyperspectral imaging (HSI) is a non-destructive and contactless technology
that provides valuable information about the structure and composition of an
object. It can capture detailed information about the chemical and physical
properties of agricultural crops. Due to its wide spectral range, compared with
multispectral- or RGB-based imaging methods, HSI can be a more effective tool
for monitoring crop health and productivity. With the advent of this imaging
tool in agrotechnology, researchers can more accurately address issues related
to the detection of diseased and defective crops in the agriculture industry.
This allows to implement the most suitable and accurate farming solutions, such
as irrigation and fertilization before crops enter a damaged and
difficult-to-recover phase of growth in the field. While HSI provides valuable
insights into the object under investigation, the limited number of HSI
datasets for crop evaluation presently poses a bottleneck. Dealing with the
curse of dimensionality presents another challenge due to the abundance of
spectral and spatial information in each hyperspectral cube. State-of-the-art
methods based on 1D- and 2D-CNNs struggle to efficiently extract spectral and
spatial information. On the other hand, 3D-CNN-based models have shown
significant promise in achieving better classification and detection results by
leveraging spectral and spatial features simultaneously. Despite the apparent
benefits of 3D-CNN-based models, their usage for classification purposes in
this area of research has remained limited. This paper seeks to address this
gap by reviewing 3D-CNN-based architectures and the typical deep learning
pipeline, including preprocessing and visualization of results, for the
classification of hyperspectral images of diseased and defective crops.
Furthermore, we discuss open research areas and challenges when utilizing
3D-CNNs with HSI data.


------------------------------------------------------------------------------

Title:
TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series  Forecasting

Abstract: Transformers have gained popularity in time series forecasting for their
ability to capture long-sequence interactions. However, their high memory and
computing requirements pose a critical bottleneck for long-term forecasting. To
address this, we propose TSMixer, a lightweight neural architecture exclusively
composed of multi-layer perceptron (MLP) modules. TSMixer is designed for
multivariate forecasting and representation learning on patched time series,
providing an efficient alternative to Transformers. Our model draws inspiration
from the success of MLP-Mixer models in computer vision. We demonstrate the
challenges involved in adapting Vision MLP-Mixer for time series and introduce
empirically validated components to enhance accuracy. This includes a novel
design paradigm of attaching online reconciliation heads to the MLP-Mixer
backbone, for explicitly modeling the time-series properties such as hierarchy
and channel-correlations. We also propose a Hybrid channel modeling approach to
effectively handle noisy channel interactions and generalization across diverse
datasets, a common challenge in existing patch channel-mixing methods.
Additionally, a simple gated attention mechanism is introduced in the backbone
to prioritize important features. By incorporating these lightweight
components, we significantly enhance the learning capability of simple MLP
structures, outperforming complex Transformer models with minimal computing
usage. Moreover, TSMixer's modular design enables compatibility with both
supervised and masked self-supervised learning methods, making it a promising
building block for time-series Foundation Models. TSMixer outperforms
state-of-the-art MLP and Transformer models in forecasting by a considerable
margin of 8-60%. It also outperforms the latest strong benchmarks of
Patch-Transformer models (by 1-2%) with a significant reduction in memory and
runtime (2-3X).


------------------------------------------------------------------------------

Title:
Segment Any Point Cloud Sequences by Distilling Vision Foundation Models

Abstract: Recent advancements in vision foundation models (VFMs) have opened up new
possibilities for versatile and efficient visual perception. In this work, we
introduce Seal, a novel framework that harnesses VFMs for segmenting diverse
automotive point cloud sequences. Seal exhibits three appealing properties: i)
Scalability: VFMs are directly distilled into point clouds, eliminating the
need for annotations in either 2D or 3D during pretraining. ii) Consistency:
Spatial and temporal relationships are enforced at both the camera-to-LiDAR and
point-to-segment stages, facilitating cross-modal representation learning. iii)
Generalizability: Seal enables knowledge transfer in an off-the-shelf manner to
downstream tasks involving diverse point clouds, including those from
real/synthetic, low/high-resolution, large/small-scale, and clean/corrupted
datasets. Extensive experiments conducted on eleven different point cloud
datasets showcase the effectiveness and superiority of Seal. Notably, Seal
achieves a remarkable 45.0% mIoU on nuScenes after linear probing, surpassing
random initialization by 36.9% mIoU and outperforming prior arts by 6.1% mIoU.
Moreover, Seal demonstrates significant performance gains over existing methods
across 20 different few-shot fine-tuning tasks on all eleven tested point cloud
datasets.


------------------------------------------------------------------------------

Title:
Making an agent's trust stable in a series of success and failure tasks  through empathy

Abstract: As AI technology develops, trust in AI agents is becoming more important for
more AI applications in human society. Possible ways to improve the trust
relationship include empathy, success-failure series, and capability
(performance). Appropriate trust is less likely to cause deviations between
actual and ideal performance. In this study, we focus on the agent's empathy
and success-failure series to increase trust in AI agents. We experimentally
examine the effect of empathy from agent to person on changes in trust over
time. The experiment was conducted with a two-factor mixed design: empathy
(available, not available) and success-failure series (phase 1 to phase 5). An
analysis of variance (ANOVA) was conducted using data from 198 participants.
The results showed an interaction between the empathy factor and the
success-failure series factor, with trust in the agent stabilizing when empathy
was present. This result supports our hypothesis. This study shows that
designing AI agents to be empathetic is an important factor for trust and helps
humans build appropriate trust relationships with AI agents.


------------------------------------------------------------------------------

Title:
QH9: A Quantum Hamiltonian Prediction Benchmark for QM9 Molecules

Abstract: Supervised machine learning approaches have been increasingly used in
accelerating electronic structure prediction as surrogates of first-principle
computational methods, such as density functional theory (DFT). While numerous
quantum chemistry datasets focus on chemical properties and atomic forces, the
ability to achieve accurate and efficient prediction of the Hamiltonian matrix
is highly desired, as it is the most important and fundamental physical
quantity that determines the quantum states of physical systems and chemical
properties. In this work, we generate a new Quantum Hamiltonian dataset, named
as QH9, to provide precise Hamiltonian matrices for 2,399 molecular dynamics
trajectories and 130,831 stable molecular geometries, based on the QM9 dataset.
By designing benchmark tasks with various molecules, we show that current
machine learning models have the capacity to predict Hamiltonian matrices for
arbitrary molecules. Both the QH9 dataset and the baseline models are provided
to the community through an open-source benchmark, which can be highly valuable
for developing machine learning methods and accelerating molecular and
materials design for scientific and technological applications. Our benchmark
is publicly available at
this https URL


------------------------------------------------------------------------------

Title:
Sound Demixing Challenge 2023 -- Music Demixing Track Technical Report

Abstract: In this report, we present our award-winning solutions for the Music Demixing
Track of Sound Demixing Challenge 2023. We focus on two methods designed for
this challenge: a time-efficient source separation network that achieves
state-of-the-art results on the MUSDB benchmark and a loss masking method for
noise-robust source separation. Code for reproducing model training and final
submissions is available at github.com/kuielab/sdx23.


------------------------------------------------------------------------------

Title:
Prevention of cyberattacks in WSN and packet drop by CI framework and  information processing protocol using AI and Big Data

Abstract: As the reliance on wireless sensor networks (WSNs) rises in numerous sectors,
cyberattack prevention and data transmission integrity become essential
problems. This study provides a complete framework to handle these difficulties
by integrating a cognitive intelligence (CI) framework, an information
processing protocol, and sophisticated artificial intelligence (AI) and big
data analytics approaches. The CI architecture is intended to improve WSN
security by dynamically reacting to an evolving threat scenario. It employs
artificial intelligence algorithms to continuously monitor and analyze network
behavior, identifying and mitigating any intrusions in real time. Anomaly
detection algorithms are also included in the framework to identify packet drop
instances caused by attacks or network congestion. To support the CI
architecture, an information processing protocol focusing on efficient and
secure data transfer within the WSN is introduced. To protect data integrity
and prevent unwanted access, this protocol includes encryption and
authentication techniques. Furthermore, it enhances the routing process with
the use of AI and big data approaches, providing reliable and timely packet
delivery. Extensive simulations and tests are carried out to assess the
efficiency of the suggested framework. The findings show that it is capable of
detecting and preventing several forms of assaults, including as
denial-of-service (DoS) attacks, node compromise, and data tampering.
Furthermore, the framework is highly resilient to packet drop occurrences,
which improves the WSN's overall reliability and performance


------------------------------------------------------------------------------

Title:
Crowdsourcing and Evaluating Text-Based Audio Retrieval Relevances

Abstract: This paper explores grading text-based audio retrieval relevances with
crowdsourcing assessments. Given a free-form text (e.g., a caption) as a query,
crowdworkers are asked to grade audio clips using numeric scores (between 0 and
100) to indicate their judgements of how much the sound content of an audio
clip matches the text, where 0 indicates no content match at all and 100
indicates perfect content match. We integrate the crowdsourced relevances into
training and evaluating text-based audio retrieval systems, and evaluate the
effect of using them together with binary relevances from audio captioning.
Conventionally, these binary relevances are defined by captioning-based
audio-caption pairs, where being positive indicates that the caption describes
the paired audio, and being negative applies to all other pairs. Experimental
results indicate that there is no clear benefit from incorporating crowdsourced
relevances alongside binary relevances when the crowdsourced relevances are
binarized for contrastive learning. Conversely, the results suggest that using
only binary relevances defined by captioning-based audio-caption pairs is
sufficient for contrastive learning.


------------------------------------------------------------------------------

Title:
Employing Multimodal Machine Learning for Stress Detection

Abstract: In the current age, human lifestyle has become more knowledge oriented
leading to generation of sedentary employment. This has given rise to a number
of health and mental disorders. Mental wellness is one of the most neglected
but crucial aspects of today's world. Mental health issues can, both directly
and indirectly, affect other sections of human physiology and impede an
individual's day-to-day activities and performance. However, identifying the
stress and finding the stress trend for an individual leading to serious mental
ailments is challenging and involves multiple factors. Such identification can
be achieved accurately by fusing these multiple modalities (due to various
factors) arising from behavioral patterns. Certain techniques are identified in
the literature for this purpose; however, very few machine learning-based
methods are proposed for such multimodal fusion tasks. In this work, a
multimodal AI-based framework is proposed to monitor a person's working
behavior and stress levels. We propose a methodology for efficiently detecting
stress due to workload by concatenating heterogeneous raw sensor data streams
(e.g., face expressions, posture, heart rate, computer interaction). This data
can be securely stored and analyzed to understand and discover personalized
unique behavioral patterns leading to mental strain and fatigue. The
contribution of this work is twofold; proposing a multimodal AI-based strategy
for fusion to detect stress and its level and secondly identify a stress
pattern over a period of time. We were able to achieve 96.09% accuracy on the
test set in stress detection and classification. Further, we reduce the stress
scale prediction model loss to 0.036 using these modalities. This work can
prove important for the community at large, specifically those working
sedentary jobs to monitor and identify stress levels, especially in current
times of COVID-19.


------------------------------------------------------------------------------

Title:
Symmetry-Informed Geometric Representation for Molecules, Proteins, and  Crystalline Materials

Abstract: Artificial intelligence for scientific discovery has recently generated
significant interest within the machine learning and scientific communities,
particularly in the domains of chemistry, biology, and material discovery. For
these scientific problems, molecules serve as the fundamental building blocks,
and machine learning has emerged as a highly effective and powerful tool for
modeling their geometric structures. Nevertheless, due to the rapidly evolving
process of the field and the knowledge gap between science (e.g., physics,
chemistry, & biology) and machine learning communities, a benchmarking study on
geometrical representation for such data has not been conducted. To address
such an issue, in this paper, we first provide a unified view of the current
symmetry-informed geometric methods, classifying them into three main
categories: invariance, equivariance with spherical frame basis, and
equivariance with vector frame basis. Then we propose a platform, coined
Geom3D, which enables benchmarking the effectiveness of geometric strategies.
Geom3D contains 16 advanced symmetry-informed geometric representation models
and 14 geometric pretraining methods over 46 diverse datasets, including small
molecules, proteins, and crystalline materials. We hope that Geom3D can, on the
one hand, eliminate barriers for machine learning researchers interested in
exploring scientific problems; and, on the other hand, provide valuable
guidance for researchers in computational chemistry, structural biology, and
materials science, aiding in the informed selection of representation
techniques for specific applications.


------------------------------------------------------------------------------

Title:
Language Aligned Visual Representations Predict Human Behavior in  Naturalistic Learning Tasks

Abstract: Humans possess the ability to identify and generalize relevant features of
natural objects, which aids them in various situations. To investigate this
phenomenon and determine the most effective representations for predicting
human behavior, we conducted two experiments involving category learning and
reward learning. Our experiments used realistic images as stimuli, and
participants were tasked with making accurate decisions based on novel stimuli
for all trials, thereby necessitating generalization. In both tasks, the
underlying rules were generated as simple linear functions using stimulus
dimensions extracted from human similarity judgments. Notably, participants
successfully identified the relevant stimulus features within a few trials,
demonstrating effective generalization. We performed an extensive model
comparison, evaluating the trial-by-trial predictive accuracy of diverse deep
learning models' representations of human choices. Intriguingly,
representations from models trained on both text and image data consistently
outperformed models trained solely on images, even surpassing models using the
features that generated the task itself. These findings suggest that
language-aligned visual representations possess sufficient richness to describe
human generalization in naturalistic settings and emphasize the role of
language in shaping human cognition.


------------------------------------------------------------------------------

Title:
Simplified Temporal Consistency Reinforcement Learning

Abstract: Reinforcement learning is able to solve complex sequential decision-making
tasks but is currently limited by sample efficiency and required computation.
To improve sample efficiency, recent work focuses on model-based RL which
interleaves model learning with planning. Recent methods further utilize policy
learning, value estimation, and, self-supervised learning as auxiliary
objectives. In this paper we show that, surprisingly, a simple representation
learning approach relying only on a latent dynamics model trained by latent
temporal consistency is sufficient for high-performance RL. This applies when
using pure planning with a dynamics model conditioned on the representation,
but, also when utilizing the representation as policy and value function
features in model-free RL. In experiments, our approach learns an accurate
dynamics model to solve challenging high-dimensional locomotion tasks with
online planners while being 4.1 times faster to train compared to
ensemble-based methods. With model-free RL without planning, especially on
high-dimensional tasks, such as the DeepMind Control Suite Humanoid and Dog
tasks, our approach outperforms model-free methods by a large margin and
matches model-based methods' sample efficiency while training 2.4 times faster.


------------------------------------------------------------------------------

Title:
Identifying key players in dark web marketplaces

Abstract: Dark web marketplaces have been a significant outlet for illicit trade,
serving millions of users worldwide for over a decade. However, not all users
are the same. This paper aims to identify the key players in Bitcoin
transaction networks linked to dark markets and assess their role by analysing
a dataset of 40 million Bitcoin transactions involving 31 markets in the period
2011-2021. First, we propose an algorithm that categorizes users either as
buyers or sellers and shows that a large fraction of the traded volume is
concentrated in a small group of elite market participants. Then, we
investigate both market star-graphs and user-to-user networks and highlight the
importance of a new class of users, namely `multihomers' who operate on
multiple marketplaces concurrently. Specifically, we show how the networks of
multihomers and seller-to-seller interactions can shed light on the resilience
of the dark market ecosystem against external shocks. Our findings suggest that
understanding the behavior of key players in dark web marketplaces is critical
to effectively disrupting illegal activities.


------------------------------------------------------------------------------

Title:
The influence of user personality and rating scale features on rating  behaviour: an empirical study

Abstract: User ratings are widely used in web systems and applications to provide
personalized interaction and to help other users make better choices. Previous
research has shown that rating scale features and user personality can both
influence users' rating behaviour, but relatively little work has been devoted
to understanding if the effects of rating scale features may vary depending on
users' personality.
In this paper, we study the impact of scale granularity and colour on the
ratings of individuals with different personalities, represented according to
the Big Five model. To this aim, we carried out a user study with 203
participants, in the context of a web-based survey where users were assigned an
image rating task.
Our results confirm that both colour and granularity can affect user ratings,
but their specific effects also depend on user scores for certain personality
traits, in particular agreeableness, openness to experience and
conscientiousness.


------------------------------------------------------------------------------

Title:
Host-Based Network Intrusion Detection via Feature Flattening and  Two-stage Collaborative Classifier

Abstract: Network Intrusion Detection Systems (NIDS) have been extensively investigated
by monitoring real network traffic and analyzing suspicious activities.
However, there are limitations in detecting specific types of attacks with
NIDS, such as Advanced Persistent Threats (APT). Additionally, NIDS is
restricted in observing complete traffic information due to encrypted traffic
or a lack of authority. To address these limitations, a Host-based Intrusion
Detection system (HIDS) evaluates resources in the host, including logs, files,
and folders, to identify APT attacks that routinely inject malicious files into
victimized nodes. In this study, a hybrid network intrusion detection system
that combines NIDS and HIDS is proposed to improve intrusion detection
performance. The feature flattening technique is applied to flatten
two-dimensional host-based features into one-dimensional vectors, which can be
directly used by traditional Machine Learning (ML) models. A two-stage
collaborative classifier is introduced that deploys two levels of ML algorithms
to identify network intrusions. In the first stage, a binary classifier is used
to detect benign samples. All detected attack types undergo a multi-class
classifier to reduce the complexity of the original problem and improve the
overall detection performance. The proposed method is shown to generalize
across two well-known datasets, CICIDS 2018 and NDSec-1. Performance of
XGBoost, which represents conventional ML, is evaluated. Combining host and
network features enhances attack detection performance (macro average F1 score)
by 8.1% under the CICIDS 2018 dataset and 3.7% under the NDSec-1 dataset.
Meanwhile, the two-stage collaborative classifier improves detection
performance for most single classes, especially for DoS-LOIC-UDP and
DoS-SlowHTTPTest, with improvements of 30.7% and 84.3%, respectively, when
compared with the traditional ML XGBoost.


------------------------------------------------------------------------------

Title:
Leveraging Human Salience to Improve Calorie Estimation

Abstract: The following paper investigates the effectiveness of incorporating human
salience into the task of calorie prediction from images of food. We observe a
32.2% relative improvement when incorporating saliency maps on the images of
food highlighting the most calorie regions. We also attempt to further improve
the accuracy by starting the best models using pre-trained weights on similar
tasks of mass estimation and food classification. However, we observe no
improvement. Surprisingly, we also find that our best model was not able to
surpass the original performance published alongside the test dataset,
Nutrition5k. We use ResNet50 and Xception as the base models for our
experiment.


------------------------------------------------------------------------------

Title:
FLAIR: A Metric for Liquidity Provider Competitiveness in Automated  Market Makers

Abstract: This paper aims to enhance the understanding of liquidity provider (LP)
returns in automated market makers (AMMs). LPs face market risk as well as
adverse selection due to risky asset holdings in the pool that they provide
liquidity to and the informational asymmetry between informed traders
(arbitrageurs) and AMMs. Loss-versus-rebalancing (LVR) quantifies the adverse
selection cost (Milionis et al., 2022a), and is a popular metric to evaluate
the flow toxicity to an AMM. However, individual LP returns are critically
affected by another factor orthogonal to the above: the competitiveness among
LPs. This work introduces a novel metric for LP competitiveness, called FLAIR
(short for fee liquidity-adjusted instantaneous returns), that aims to
supplement LVR in assessments of LP performance to capture the dynamic behavior
of LPs in a pool. Our metric reflects the characteristics of fee
return-on-capital, and differentiates active liquidity provisioning strategies
in AMMs. To illustrate how both flow toxicity, accounting for the
sophistication of the counterparty of LPs, as well as LP competitiveness,
accounting for the sophistication of the competition among LPs, affect
individual LP returns, we propose a quadrant interpretation where all of these
characteristics may be readily visualized. We examine LP competitiveness in an
ex-post fashion, and show example cases in all of which our metric confirms the
expected nuances and intuition of competitiveness among LPs. FLAIR has
particular merit in empirical analyses, and is able to better inform practical
assessments of AMM pools.


------------------------------------------------------------------------------

Title:
Multi-Polynomial Monte Carlo for Trace Estimation in Lattice QCD

Abstract: Estimating the trace of the inverse of a large matrix is an important problem
in lattice quantum chromodynamics. A multilevel Monte Carlo method is proposed
for this problem that uses different degree polynomials for the levels. The
polynomials are developed from the GMRES algorithm for solving linear
equations. To reduce orthogonalization expense, the highest degree polynomial
is a composite or double polynomial found with a polynomial preconditioned
GMRES iteration. Added to some of the Monte Carlo pieces is deflation of
eigenvalues that reduces the variance. Deflation is also used for finding a
reduced degree deflated polynomial. The new Multipolynomial Monte Carlo method
can significantly improve the trace computation for matrices that have a
difficult spectrum due to small eigenvalues.


------------------------------------------------------------------------------

Title:
Private Federated Frequency Estimation: Adapting to the Hardness of the  Instance

Abstract: In federated frequency estimation (FFE), multiple clients work together to
estimate the frequencies of their collective data by communicating with a
server that respects the privacy constraints of Secure Summation (SecSum), a
cryptographic multi-party computation protocol that ensures that the server can
only access the sum of client-held vectors. For single-round FFE, it is known
that count sketching is nearly information-theoretically optimal for achieving
the fundamental accuracy-communication trade-offs [Chen et al., 2022]. However,
we show that under the more practical multi-round FEE setting, simple
adaptations of count sketching are strictly sub-optimal, and we propose a novel
hybrid sketching algorithm that is provably more accurate. We also address the
following fundamental question: how should a practitioner set the sketch size
in a way that adapts to the hardness of the underlying problem? We propose a
two-phase approach that allows for the use of a smaller sketch size for simpler
problems (e.g. near-sparse or light-tailed distributions). We conclude our work
by showing how differential privacy can be added to our algorithm and verifying
its superior performance through extensive experiments conducted on large-scale
datasets.


------------------------------------------------------------------------------

Title:
ControlPULP: A RISC-V On-Chip Parallel Power Controller for Many-Core  HPC Processors with FPGA-Based Hardware-In-The-Loop Power and Thermal  Emulation

Abstract: High-Performance Computing (HPC) processors are nowadays integrated
Cyber-Physical Systems demanding complex and high-bandwidth closed-loop power
and thermal control strategies. To efficiently satisfy real-time multi-input
multi-output (MIMO) optimal power requirements, high-end processors integrate
an on-die power controller system (PCS).
While traditional PCSs are based on a simple microcontroller (MCU)-class
core, more scalable and flexible PCS architectures are required to support
advanced MIMO control algorithms for managing the ever-increasing number of
cores, power states, and process, voltage, and temperature variability.
This paper presents ControlPULP, an open-source, HW/SW RISC-V parallel PCS
platform consisting of a single-core MCU with fast interrupt handling coupled
with a scalable multi-core programmable cluster accelerator and a specialized
DMA engine for the parallel acceleration of real-time power management
policies. ControlPULP relies on FreeRTOS to schedule a reactive power control
firmware (PCF) application layer.
We demonstrate ControlPULP in a power management use-case targeting a
next-generation 72-core HPC processor. We first show that the multi-core
cluster accelerates the PCF, achieving 4.9x speedup compared to single-core
execution, enabling more advanced power management algorithms within the
control hyper-period at a shallow area overhead, about 0.1% the area of a
modern HPC CPU die. We then assess the PCS and PCF by designing an FPGA-based,
closed-loop emulation framework that leverages the heterogeneous SoCs paradigm,
achieving DVFS tracking with a mean deviation within 3% the plant's thermal
design power (TDP) against a software-equivalent model-in-the-loop approach.
Finally, we show that the proposed PCF compares favorably with an
industry-grade control algorithm under computational-intensive workloads.


------------------------------------------------------------------------------

Title:
Graph Extraction for Assisting Crash Simulation Data Analysis

Abstract: In this work, we establish a method for abstracting information from Computer
Aided Engineering (CAE) into graphs. Such graph representations of CAE data can
improve design guidelines and support recommendation systems by enabling the
comparison of simulations, highlighting unexplored experimental designs, and
correlating different designs. We focus on the load-path in crashworthiness
analysis, a complex sub-discipline in vehicle design. The load-path is the
sequence of parts that absorb most of the energy caused by the impact. To
detect the load-path, we generate a directed weighted graph from the CAE data.
The vertices represent the vehicle's parts, and the edges are an abstraction of
the connectivity of the parts. The edge direction follows the temporal
occurrence of the collision, where the edge weights reflect aspects of the
energy absorption. We introduce and assess three methods for graph extraction
and an additional method for further updating each graph with the sequences of
absorption. Based on longest-path calculations, we introduce an automated
detection of the load-path, which we analyse for the different graph extraction
methods and weights. Finally, we show how our method for the detection of
load-paths helps in the classification and labelling of CAE simulations.


------------------------------------------------------------------------------

Title:
SSL4EO-L: Datasets and Foundation Models for Landsat Imagery

Abstract: The Landsat program is the longest-running Earth observation program in
history, with 50+ years of data acquisition by 8 satellites. The multispectral
imagery captured by sensors onboard these satellites is critical for a wide
range of scientific fields. Despite the increasing popularity of deep learning
and remote sensing, the majority of researchers still use decision trees and
random forests for Landsat image analysis due to the prevalence of small
labeled datasets and lack of foundation models. In this paper, we introduce
SSL4EO-L, the first ever dataset designed for Self-Supervised Learning for
Earth Observation for the Landsat family of satellites (including 3 sensors and
2 product levels) and the largest Landsat dataset in history (5M image
patches). Additionally, we modernize and re-release the L7 Irish and L8 Biome
cloud detection datasets, and introduce the first ML benchmark datasets for
Landsats 4-5 TM and Landsat 7 ETM+ SR. Finally, we pre-train the first
foundation models for Landsat imagery using SSL4EO-L and evaluate their
performance on multiple semantic segmentation tasks. All datasets and model
weights are available via the TorchGeo (this https URL)
library, making reproducibility and experimentation easy, and enabling
scientific advancements in the burgeoning field of remote sensing for a myriad
of downstream applications.


------------------------------------------------------------------------------

Title:
Diff-TTSG: Denoising probabilistic integrated speech and gesture  synthesis

Abstract: With read-aloud speech synthesis achieving high naturalness scores, there is
a growing research interest in synthesising spontaneous speech. However, human
spontaneous face-to-face conversation has both spoken and non-verbal aspects
(here, co-speech gestures). Only recently has research begun to explore the
benefits of jointly synthesising these two modalities in a single system. The
previous state of the art used non-probabilistic methods, which fail to capture
the variability of human speech and motion, and risk producing oversmoothing
artefacts and sub-optimal synthesis quality. We present the first
diffusion-based probabilistic model, called Diff-TTSG, that jointly learns to
synthesise speech and gestures together. Our method can be trained on small
datasets from scratch. Furthermore, we describe a set of careful uni- and
multi-modal subjective tests for evaluating integrated speech and gesture
synthesis systems, and use them to validate our proposed approach. For
synthesised examples please see this https URL


------------------------------------------------------------------------------

Title:
BN-DRISHTI: Bangla Document Recognition through Instance-level  Segmentation of Handwritten Text Images

Abstract: Handwriting recognition remains challenging for some of the most spoken
languages, like Bangla, due to the complexity of line and word segmentation
brought by the curvilinear nature of writing and lack of quality datasets. This
paper solves the segmentation problem by introducing a state-of-the-art method
(BN-DRISHTI) that combines a deep learning-based object detection framework
(YOLO) with Hough and Affine transformation for skew correction. However,
training deep learning models requires a massive amount of data. Thus, we also
present an extended version of the BN-HTRd dataset comprising 786 full-page
handwritten Bangla document images, line and word-level annotation for
segmentation, and corresponding ground truths for word recognition. Evaluation
on the test portion of our dataset resulted in an F-score of 99.97% for line
and 98% for word segmentation. For comparative analysis, we used three external
Bangla handwritten datasets, namely BanglaWriting, WBSUBNdb_text, and ICDAR
2013, where our system outperformed by a significant margin, further justifying
the performance of our approach on completely unseen samples.


------------------------------------------------------------------------------

Title:
Second order quantitative bounds for unadjusted generalized Hamiltonian  Monte Carlo

Abstract: This paper provides a convergence analysis for generalized Hamiltonian Monte
Carlo samplers, a family of Markov Chain Monte Carlo methods based on leapfrog
integration of Hamiltonian dynamics and kinetic Langevin diffusion, that
encompasses the unadjusted Hamiltonian Monte Carlo method. Assuming that the
target distribution $\pi$ satisfies a log-Sobolev inequality and mild
conditions on the corresponding potential function, we establish quantitative
bounds on the relative entropy of the iterates defined by the algorithm, with
respect to $\pi$. Our approach is based on a perturbative and discrete version
of the modified entropy method developed to establish hypocoercivity for the
continuous-time kinetic Langevin process. As a corollary of our main result, we
are able to derive complexity bounds for the class of algorithms at hand. In
particular, we show that the total number of iterations to achieve a target
accuracy $\varepsilon >0$ is of order $d/\varepsilon^{1/4}$, where $d$ is the
dimension of the problem. This result can be further improved in the case of
weakly interacting mean field potentials, for which we find a total number of
iterations of order $(d/\varepsilon)^{1/4}$.


------------------------------------------------------------------------------

Title:
FedMultimodal: A Benchmark For Multimodal Federated Learning

Abstract: Over the past few years, Federated Learning (FL) has become an emerging
machine learning technique to tackle data privacy challenges through
collaborative training. In the Federated Learning algorithm, the clients submit
a locally trained model, and the server aggregates these parameters until
convergence. Despite significant efforts that have been made to FL in fields
like computer vision, audio, and natural language processing, the FL
applications utilizing multimodal data streams remain largely unexplored. It is
known that multimodal learning has broad real-world applications in emotion
recognition, healthcare, multimedia, and social media, while user privacy
persists as a critical concern. Specifically, there are no existing FL
benchmarks targeting multimodal applications or related tasks. In order to
facilitate the research in multimodal FL, we introduce FedMultimodal, the first
FL benchmark for multimodal learning covering five representative multimodal
applications from ten commonly used datasets with a total of eight unique
modalities. FedMultimodal offers a systematic FL pipeline, enabling end-to-end
modeling framework ranging from data partition and feature extraction to FL
benchmark algorithms and model evaluation. Unlike existing FL benchmarks,
FedMultimodal provides a standardized approach to assess the robustness of FL
against three common data corruptions in real-life multimodal applications:
missing modalities, missing labels, and erroneous labels. We hope that
FedMultimodal can accelerate numerous future research directions, including
designing multimodal FL algorithms toward extreme data heterogeneity,
robustness multimodal FL, and efficient multimodal FL. The datasets and
benchmark results can be accessed at:
this https URL


------------------------------------------------------------------------------

Title:
Warpformer: A Multi-scale Modeling Approach for Irregular Clinical Time  Series

Abstract: Irregularly sampled multivariate time series are ubiquitous in various
fields, particularly in healthcare, and exhibit two key characteristics:
intra-series irregularity and inter-series discrepancy. Intra-series
irregularity refers to the fact that time-series signals are often recorded at
irregular intervals, while inter-series discrepancy refers to the significant
variability in sampling rates among diverse series. However, recent advances in
irregular time series have primarily focused on addressing intra-series
irregularity, overlooking the issue of inter-series discrepancy. To bridge this
gap, we present Warpformer, a novel approach that fully considers these two
characteristics. In a nutshell, Warpformer has several crucial designs,
including a specific input representation that explicitly characterizes both
intra-series irregularity and inter-series discrepancy, a warping module that
adaptively unifies irregular time series in a given scale, and a customized
attention module for representation learning. Additionally, we stack multiple
warping and attention modules to learn at different scales, producing
multi-scale representations that balance coarse-grained and fine-grained
signals for downstream tasks. We conduct extensive experiments on widely used
datasets and a new large-scale benchmark built from clinical databases. The
results demonstrate the superiority of Warpformer over existing
state-of-the-art approaches.


------------------------------------------------------------------------------

Title:
Wikibio: a Semantic Resource for the Intersectional Analysis of  Biographical Events

Abstract: Biographical event detection is a relevant task for the exploration and
comparison of the ways in which people's lives are told and represented. In
this sense, it may support several applications in digital humanities and in
works aimed at exploring bias about minoritized groups. Despite that, there are
no corpora and models specifically designed for this task. In this paper we
fill this gap by presenting a new corpus annotated for biographical event
detection. The corpus, which includes 20 Wikipedia biographies, was compared
with five existing corpora to train a model for the biographical event
detection task. The model was able to detect all mentions of the target-entity
in a biography with an F-score of 0.808 and the entity-related events with an
F-score of 0.859. Finally, the model was used for performing an analysis of
biases about women and non-Western people in Wikipedia biographies.


------------------------------------------------------------------------------

Title:
Investigating the Utility of Surprisal from Large Language Models for  Speech Synthesis Prosody

Abstract: This paper investigates the use of word surprisal, a measure of the
predictability of a word in a given context, as a feature to aid speech
synthesis prosody. We explore how word surprisal extracted from large language
models (LLMs) correlates with word prominence, a signal-based measure of the
salience of a word in a given discourse. We also examine how context length and
LLM size affect the results, and how a speech synthesizer conditioned with
surprisal values compares with a baseline system. To evaluate these factors, we
conducted experiments using a large corpus of English text and LLMs of varying
sizes. Our results show that word surprisal and word prominence are moderately
correlated, suggesting that they capture related but distinct aspects of
language use. We find that length of context and size of the LLM impact the
correlations, but not in the direction anticipated, with longer contexts and
larger LLMs generally underpredicting prominent words in a nearly linear
manner. We demonstrate that, in line with these findings, a speech synthesizer
conditioned with surprisal values provides a minimal improvement over the
baseline with the results suggesting a limited effect of using surprisal values
for eliciting appropriate prominence patterns.


------------------------------------------------------------------------------

Title:
Understanding Optimization of Deep Learning

Abstract: This article provides a comprehensive understanding of optimization in deep
learning, with a primary focus on the challenges of gradient vanishing and
gradient exploding, which normally lead to diminished model representational
ability and training instability, respectively. We analyze these two challenges
through several strategic measures, including the improvement of gradient flow
and the imposition of constraints on a network's Lipschitz constant. To help
understand the current optimization methodologies, we categorize them into two
classes: explicit optimization and implicit optimization. Explicit optimization
methods involve direct manipulation of optimizer parameters, including weight,
gradient, learning rate, and weight decay. Implicit optimization methods, by
contrast, focus on improving the overall landscape of a network by enhancing
its modules, such as residual shortcuts, normalization methods, attention
mechanisms, and activations. In this article, we provide an in-depth analysis
of these two optimization classes and undertake a thorough examination of the
Jacobian matrices and the Lipschitz constants of many widely used deep learning
modules, highlighting existing issues as well as potential improvements.
Moreover, we also conduct a series of analytical experiments to substantiate
our theoretical discussions. This article does not aim to propose a new
optimizer or network. Rather, our intention is to present a comprehensive
understanding of optimization in deep learning. We hope that this article will
assist readers in gaining a deeper insight in this field and encourages the
development of more robust, efficient, and high-performing models.


------------------------------------------------------------------------------

Title:
Magnetic Resonance Spectroscopy Quantification Aided by Deep Estimations  of Imperfection Factors and Overall Macromolecular Signal

Abstract: Magnetic Resonance Spectroscopy (MRS) is an important non-invasive technique
for in vivo biomedical detection. However, it is still challenging to
accurately quantify metabolites with proton MRS due to three problems: Serious
overlaps of metabolite signals, signal distortions due to non-ideal acquisition
conditions and interference with strong background signals including
macromolecule signals. The most popular software, LCModel, adopts the
non-linear least square to quantify metabolites and addresses these problems by
introducing regularization terms, imperfection factors of non-ideal acquisition
conditions, and designing several empirical priors such as basissets of both
metabolites and macromolecules. However, solving such a large non-linear
quantitative problem is complicated. Moreover, when the signal-to-noise ratio
of an input MRS signal is low, the solution may have a large deviation. In this
work, deep learning is introduced to reduce the complexity of solving this
overall quantitative problem. Deep learning is designed to predict directly the
imperfection factors and the overall signal from macromolecules. Then, the
remaining part of the quantification problem becomes a much simpler effective
fitting and is easily solved by Linear Least Squares (LLS), which greatly
improves the generalization to unseen concentration of metabolites in the
training data. Experimental results show that compared with LCModel, the
proposed method has smaller quantification errors for 700 sets of simulated
test data, and presents more stable quantification results for 20 sets of
healthy in vivo data at a wide range of signal-to-noise ratio. Qnet also
outperforms other deep learning methods in terms of lower quantification error
on most metabolites. Finally, QNet has been deployed on a cloud computing
platform, CloudBrain-MRS, which is open accessed at
this https URL


------------------------------------------------------------------------------

Title:
R2-Diff: Denoising by diffusion as a refinement of retrieved motion for  image-based motion prediction

Abstract: Image-based motion prediction is one of the essential techniques for robot
manipulation. Among the various prediction models, we focus on diffusion models
because they have achieved state-of-the-art performance in various
applications. In image-based motion prediction, diffusion models stochastically
predict contextually appropriate motion by gradually denoising random Gaussian
noise based on the image context. While diffusion models are able to predict
various motions by changing the random noise, they sometimes fail to predict a
contextually appropriate motion based on the image because the random noise is
sampled independently of the image context. To solve this problem, we propose
R2-Diff. In R2-Diff, a motion retrieved from a dataset based on image
similarity is fed into a diffusion model instead of random noise. Then, the
retrieved motion is refined through the denoising process of the diffusion
model. Since the retrieved motion is almost appropriate to the context, it
becomes easier to predict contextually appropriate motion. However, traditional
diffusion models are not optimized to refine the retrieved motion. Therefore,
we propose the method of tuning the hyperparameters based on the distance of
the nearest neighbor motion among the dataset to optimize the diffusion model
for refinement. Furthermore, we propose an image-based retrieval method to
retrieve the nearest neighbor motion in inference. Our proposed retrieval
efficiently computes the similarity based on the image features along the
motion trajectory. We demonstrate that R2-Diff accurately predicts appropriate
motions and achieves high task success rates compared to recent
state-of-the-art models in robot manipulation.


------------------------------------------------------------------------------

Title:
Seeing the World through Your Eyes

Abstract: The reflective nature of the human eye is an underappreciated source of
information about what the world around us looks like. By imaging the eyes of a
moving person, we can collect multiple views of a scene outside the camera's
direct line of sight through the reflections in the eyes. In this paper, we
reconstruct a 3D scene beyond the camera's line of sight using portrait images
containing eye reflections. This task is challenging due to 1) the difficulty
of accurately estimating eye poses and 2) the entangled appearance of the eye
iris and the scene reflections. Our method jointly refines the cornea poses,
the radiance field depicting the scene, and the observer's eye iris texture. We
further propose a simple regularization prior on the iris texture pattern to
improve reconstruction quality. Through various experiments on synthetic and
real-world captures featuring people with varied eye colors, we demonstrate the
feasibility of our approach to recover 3D scenes using eye reflections.


------------------------------------------------------------------------------

Title:
Deep learning techniques for blind image super-resolution: A high-scale  multi-domain perspective evaluation

Abstract: Despite several solutions and experiments have been conducted recently
addressing image super-resolution (SR), boosted by deep learning (DL)
techniques, they do not usually design evaluations with high scaling factors,
capping it at 2x or 4x. Moreover, the datasets are generally benchmarks which
do not truly encompass significant diversity of domains to proper evaluate the
techniques. It is also interesting to remark that blind SR is attractive for
real-world scenarios since it is based on the idea that the degradation process
is unknown, and hence techniques in this context rely basically on
low-resolution (LR) images. In this article, we present a high-scale (8x)
controlled experiment which evaluates five recent DL techniques tailored for
blind image SR: Adaptive Pseudo Augmentation (APA), Blind Image SR with
Spatially Variant Degradations (BlindSR), Deep Alternating Network (DAN),
FastGAN, and Mixture of Experts Super-Resolution (MoESR). We consider 14 small
datasets from five different broader domains which are: aerial, fauna, flora,
medical, and satellite. Another distinctive characteristic of our evaluation is
that some of the DL approaches were designed for single-image SR but others
not. Two no-reference metrics were selected, being the classical natural image
quality evaluator (NIQE) and the recent transformer-based multi-dimension
attention network for no-reference image quality assessment (MANIQA) score, to
assess the techniques. Overall, MoESR can be regarded as the best solution
although the perceptual quality of the created HR images of all the techniques
still needs to improve. Supporting code: this https URL
Datasets:
this https URL


------------------------------------------------------------------------------

Title:
Numerical study of the Serre-Green-Naghdi equations in 2D

Abstract: A numerical approach for the Serre-Green-Naghdi (SGN) equations in 2D based
on a Fourier spectral method with a Krylov subspace technique is presented. The
code is used to study the transverse stability of line solitary waves, 1D
solitary waves being exact solutions of the 2D waves independent of the second
variable. The study of localised initial data as well as crossing 1D solitary
waves does not give an indication of stable structures in SGN solutions
localised in two spatial dimensions.


------------------------------------------------------------------------------

Title:
Adaptive Hierarchical SpatioTemporal Network for Traffic Forecasting

Abstract: Accurate traffic forecasting is vital to intelligent transportation systems,
which are widely adopted to solve urban traffic issues. Existing traffic
forecasting studies focus on modeling spatial-temporal dynamics in traffic
data, among which the graph convolution network (GCN) is at the center for
exploiting the spatial dependency embedded in the road network graphs. However,
these GCN-based methods operate intrinsically on the node level (e.g., road and
intersection) only whereas overlooking the spatial hierarchy of the whole city.
Nodes such as intersections and road segments can form clusters (e.g.,
regions), which could also have interactions with each other and share
similarities at a higher level. In this work, we propose an Adaptive
Hierarchical SpatioTemporal Network (AHSTN) to promote traffic forecasting by
exploiting the spatial hierarchy and modeling multi-scale spatial correlations.
Apart from the node-level spatiotemporal blocks, AHSTN introduces the adaptive
spatiotemporal downsampling module to infer the spatial hierarchy for
spatiotemporal modeling at the cluster level. Then, an adaptive spatiotemporal
upsampling module is proposed to upsample the cluster-level representations to
the node-level and obtain the multi-scale representations for generating
predictions. Experiments on two real-world datasets show that AHSTN achieves
better performance over several strong baselines.


------------------------------------------------------------------------------

Title:
Adversarially robust clustering with optimality guarantees

Abstract: We consider the problem of clustering data points coming from sub-Gaussian
mixtures. Existing methods that provably achieve the optimal mislabeling error,
such as the Lloyd algorithm, are usually vulnerable to outliers. In contrast,
clustering methods seemingly robust to adversarial perturbations are not known
to satisfy the optimal statistical guarantees. We propose a simple algorithm
that obtains the optimal mislabeling rate even when we allow adversarial
outliers to be present. Our algorithm achieves the optimal error rate in
constant iterations when a weak initialization condition is satisfied. In the
absence of outliers, in fixed dimensions, our theoretical guarantees are
similar to that of the Lloyd algorithm. Extensive experiments on various
simulated data sets are conducted to support the theoretical guarantees of our
method.


------------------------------------------------------------------------------

Title:
Cross-Modal Video to Body-joints Augmentation for Rehabilitation  Exercise Quality Assessment

Abstract: Exercise-based rehabilitation programs have been shown to enhance quality of
life and reduce mortality and rehospitalizations. AI-driven virtual
rehabilitation programs enable patients to complete exercises independently at
home while AI algorithms can analyze exercise data to provide feedback to
patients and report their progress to clinicians. This paper introduces a novel
approach to assessing the quality of rehabilitation exercises using RGB video.
Sequences of skeletal body joints are extracted from consecutive RGB video
frames and analyzed by many-to-one sequential neural networks to evaluate
exercise quality. Existing datasets for exercise rehabilitation lack adequate
samples for training deep sequential neural networks to generalize effectively.
A cross-modal data augmentation approach is proposed to resolve this problem.
Visual augmentation techniques are applied to video data, and body joints
extracted from the resulting augmented videos are used for training sequential
neural networks. Extensive experiments conducted on the KInematic assessment of
MOvement and clinical scores for remote monitoring of physical REhabilitation
(KIMORE) dataset, demonstrate the superiority of the proposed method over
previous baseline approaches. The ablation study highlights a significant
enhancement in exercise quality assessment following cross-modal augmentation.


------------------------------------------------------------------------------

Title:
Improving Path Planning Performance through Multimodal Generative Models  with Local Critics

Abstract: This paper presents a novel method for accelerating path planning tasks in
unknown scenes with obstacles by utilizing Wasserstein Generative Adversarial
Networks (WGANs) with Gradient Penalty (GP) to approximate the distribution of
the free conditioned configuration space. Our proposed approach involves
conditioning the WGAN-GP with a Variational Auto-Encoder in a continuous latent
space to handle multimodal datasets. However, training a Variational
Auto-Encoder with WGAN-GP can be challenging for image-to-configuration-space
problems, as the Kullback-Leibler loss function often converges to a random
distribution. To overcome this issue, we simplify the configuration space as a
set of Gaussian distributions and divide the dataset into several local models.
This enables us to not only learn the model but also speed up its convergence.
We evaluate the reconstructed configuration space using the homology rank of
manifolds for datasets with the geometry score. Furthermore, we propose a novel
transformation of the robot's configuration space that enables us to measure
how well collision-free regions are reconstructed, which could be used with
other rank of homology metrics. Our experiments show promising results for
accelerating path planning tasks in unknown scenes while generating
quasi-optimal paths with our WGAN-GP. The source code is openly available.


------------------------------------------------------------------------------

Title:
DoubleAdapt: A Meta-learning Approach to Incremental Learning for Stock  Trend Forecasting

Abstract: Stock trend forecasting is a fundamental task of quantitative investment
where precise predictions of price trends are indispensable. As an online
service, stock data continuously arrive over time. It is practical and
efficient to incrementally update the forecast model with the latest data which
may reveal some new patterns recurring in the future stock market. However,
incremental learning for stock trend forecasting still remains under-explored
due to the challenge of distribution shifts (a.k.a. concept drifts). With the
stock market dynamically evolving, the distribution of future data can slightly
or significantly differ from incremental data, hindering the effectiveness of
incremental updates. To address this challenge, we propose DoubleAdapt, an
end-to-end framework with two adapters, which can effectively adapt the data
and the model to mitigate the effects of distribution shifts. Our key insight
is to automatically learn how to adapt stock data into a locally stationary
distribution in favor of profitable updates. Complemented by data adaptation,
we can confidently adapt the model parameters under mitigated distribution
shifts. We cast each incremental learning task as a meta-learning task and
automatically optimize the adapters for desirable data adaptation and parameter
initialization. Experiments on real-world stock datasets demonstrate that
DoubleAdapt achieves state-of-the-art predictive performance and shows
considerable efficiency.


------------------------------------------------------------------------------

Title:
Attention-based Open RAN Slice Management using Deep Reinforcement  Learning

Abstract: As emerging networks such as Open Radio Access Networks (O-RAN) and 5G
continue to grow, the demand for various services with different requirements
is increasing. Network slicing has emerged as a potential solution to address
the different service requirements. However, managing network slices while
maintaining quality of services (QoS) in dynamic environments is a challenging
task. Utilizing machine learning (ML) approaches for optimal control of dynamic
networks can enhance network performance by preventing Service Level Agreement
(SLA) violations. This is critical for dependable decision-making and
satisfying the needs of emerging networks. Although RL-based control methods
are effective for real-time monitoring and controlling network QoS,
generalization is necessary to improve decision-making reliability. This paper
introduces an innovative attention-based deep RL (ADRL) technique that
leverages the O-RAN disaggregated modules and distributed agent cooperation to
achieve better performance through effective information extraction and
implementing generalization. The proposed method introduces a value-attention
network between distributed agents to enable reliable and optimal
decision-making. Simulation results demonstrate significant improvements in
network performance compared to other DRL baseline methods.


------------------------------------------------------------------------------

Title:
A Post-Quantum Associative Memory

Abstract: Associative memories are devices storing information that can be fully
retrieved given partial disclosure of it. We examine a toy model of associative
memory and the ultimate limitations it is subjected to within the framework of
general probabilistic theories (GPTs), which represent the most general class
of physical theories satisfying some basic operational axioms. We ask ourselves
how large the dimension of a GPT should be so that it can accommodate $2^m$
states with the property that any $N$ of them are perfectly distinguishable.
Call $d(N,m)$ the minimal such dimension. Invoking an old result by Danzer and
Gr\"unbaum, we prove that $d(2,m)=m+1$, to be compared with $O(2^m)$ when the
GPT is required to be either classical or quantum. This yields an example of a
task where GPTs outperform both classical and quantum theory exponentially.
More generally, we resolve the case of fixed $N$ and asymptotically large $m$,
proving that $d(N,m) \leq m^{1+o_N(1)}$ (as $m\to\infty$) for every $N\geq 2$,
which yields again an exponential improvement over classical and quantum
theories. Finally, we develop a numerical approach to the general problem of
finding the largest $N$-wise mutually distinguishable set for a given GPT,
which can be seen as an instance of the maximum clique problem on $N$-regular
hypergraphs.


------------------------------------------------------------------------------

Title:
Large-Scale Quantum Separability Through a Reproducible Machine Learning  Lens

Abstract: The quantum separability problem consists in deciding whether a bipartite
density matrix is entangled or separable. In this work, we propose a machine
learning pipeline for finding approximate solutions for this NP-hard problem in
large-scale scenarios. We provide an efficient Frank-Wolfe-based algorithm to
approximately seek the nearest separable density matrix and derive a systematic
way for labeling density matrices as separable or entangled, allowing us to
treat quantum separability as a classification problem. Our method is
applicable to any two-qudit mixed states. Numerical experiments with quantum
states of 3- and 7-dimensional qudits validate the efficiency of the proposed
procedure, and demonstrate that it scales up to thousands of density matrices
with a high quantum entanglement detection accuracy. This takes a step towards
benchmarking quantum separability to support the development of more powerful
entanglement detection techniques.


------------------------------------------------------------------------------

Title:
1st Solution Places for CVPR 2023 UG$^2$+ Challenge Track 2.2-Coded  Target Restoration through Atmospheric Turbulence

Abstract: In this technical report, we briefly introduce the solution of our team
VIELab-HUST for coded target restoration through atmospheric turbulence in CVPR
2023 UG$^2$+ Track 2.2. In this task, we propose an efficient multi-stage
framework to restore a high quality image from distorted frames. Specifically,
each distorted frame is initially aligned using image registration to suppress
geometric distortion. We subsequently select the sharpest set of registered
frames by employing a frame selection approach based on image sharpness, and
average them to produce an image that is largely free of geometric distortion,
albeit with blurriness. A learning-based deblurring method is then applied to
remove the residual blur in the averaged image. Finally, post-processing
techniques are utilized to further enhance the quality of the output image. Our
framework is capable of handling different kinds of coded target dataset
provided in the final testing phase, and ranked 1st on the final leaderboard.
Our code will be available at this https URL


------------------------------------------------------------------------------

Title:
Perturbed Initial Orbit Determination

Abstract: An algorithm for robust initial orbit determination (IOD) under perturbed
orbital dynamics is presented. By leveraging map inversion techniques defined
in the algebra of Taylor polynomials, this tool is capable of not only
returning an highly accurate solution to the IOD problem, but also estimating a
range of validity for the aforementioned solution in which the true orbit state
should lie. Automatic domain splitting is then used on top of the IOD routines
to ensure the local truncation error introduced by a polynomial representation
of the state estimate remains below a predefined threshold to meet the
specified requirements in accuracy. The algorithm is adapted to three types of
ground based sensors, namely range radars, Doppler-only radars and optical
telescopes by taking into account their different constraints in terms of
available measurements and sensor noise. Its improved performance with respect
to a Keplerian based IOD solution is finally demonstrated with large scale
numerical simulations over a subset of tracked objects in low Earth orbit.


------------------------------------------------------------------------------

Title:
MuMFiM: Multiscale Modeling of Fibrous Materials

Abstract: This article presents MuMFiM, an open source application for multiscale
modeling of fibrous materials on massively parallel computers. MuMFiM uses two
scales to represent fibrous materials such as biological network materials
(extracellular matrix, connective tissue, etc.). It is designed to make use of
multiple levels of parallelism, including distributed parallelism of the macro
and microscales as well as GPU accelerated data-parallelism of the microscale.
Scaling results of the GPU accelerated microscale show that solving microscale
problems concurrently on the GPU can lead to a 1000x speedup over the solution
of a single RVE on the GPU. In addition, we show nearly optimal strong and weak
scaling results of MuMFiM on up to 128 nodes of AiMOS (Rensselaer Polytechnic
Institute) which is composed of IBM AC922 nodes with 6 Volta V100 GPU and 2 20
core Power 9 CPUs each. We also show how MuMFiM can be used to solve problems
of interest to the broader engineering community, in particular providing an
example of the facet capsule ligament (FCL) of the human spine undergoing
uniaxial extension.


------------------------------------------------------------------------------

Title:
Transforming Observations of Ocean Temperature with a Deep Convolutional  Residual Regressive Neural Network

Abstract: Sea surface temperature (SST) is an essential climate variable that can be
measured via ground truth, remote sensing, or hybrid model methodologies. Here,
we celebrate SST surveillance progress via the application of a few relevant
technological advances from the late 20th and early 21st century. We further
develop our existing water cycle observation framework, Flux to Flow (F2F), to
fuse AMSR-E and MODIS into a higher resolution product with the goal of
capturing gradients and filling cloud gaps that are otherwise unavailable. Our
neural network architecture is constrained to a deep convolutional residual
regressive neural network. We utilize three snapshots of twelve monthly SST
measurements in 2010 as measured by the passive microwave radiometer AMSR-E,
the visible and infrared monitoring MODIS instrument, and the in situ Argo
dataset ISAS. The performance of the platform and success of this approach is
evaluated using the root mean squared error (RMSE) metric. We determine that
the 1:1 configuration of input and output data and a large observation region
is too challenging for the single compute node and dcrrnn structure as is. When
constrained to a single 100 x 100 pixel region and a small training dataset,
the algorithm improves from the baseline experiment covering a much larger
geography. For next discrete steps, we envision the consideration of a large
input range with a very small output range. Furthermore, we see the need to
integrate land and sea variables before performing computer vision tasks like
those within. Finally, we see parallelization as necessary to overcome the
compute obstacles we encountered.


------------------------------------------------------------------------------

Title:
Vacant Holes for Unsupervised Detection of the Outliers in Compact  Latent Representation

Abstract: Detection of the outliers is pivotal for any machine learning model deployed
and operated in real-world. It is essential for the Deep Neural Networks that
were shown to be overconfident with such inputs. Moreover, even deep generative
models that allow estimation of the probability density of the input fail in
achieving this task. In this work, we concentrate on the specific type of these
models: Variational Autoencoders (VAEs). First, we unveil a significant
theoretical flaw in the assumption of the classical VAE model. Second, we
enforce an accommodating topological property to the image of the deep neural
mapping to the latent space: compactness to alleviate the flaw and obtain the
means to provably bound the image within the determined limits by squeezing
both inliers and outliers together. We enforce compactness using two
approaches: (i) Alexandroff extension and (ii) fixed Lipschitz continuity
constant on the mapping of the encoder of the VAEs. Finally and most
importantly, we discover that the anomalous inputs predominantly tend to land
on the vacant latent holes within the compact space, enabling their successful
identification. For that reason, we introduce a specifically devised score for
hole detection and evaluate the solution against several baseline benchmarks
achieving promising results.


------------------------------------------------------------------------------

Title:
Learning from Local Experience: Informed Sampling Distributions for High  Dimensional Motion Planning

Abstract: This paper presents a sampling-based motion planning framework that leverages
the geometry of obstacles in a workspace as well as prior experiences from
motion planning problems. Previous studies have demonstrated the benefits of
utilizing prior solutions to motion planning problems for improving planning
efficiency. However, particularly for high-dimensional systems, achieving high
performance across randomized environments remains a technical challenge for
experience-based approaches due to the substantial variance between each query.
To address this challenge, we propose a novel approach that involves decoupling
the problem into subproblems through algorithmic workspace decomposition and
graph search. Additionally, we capitalize on prior experience within each
subproblem. This approach effectively reduces the variance across different
problems, leading to improved performance for experience-based planners. To
validate the effectiveness of our framework, we conduct experiments using 2D
and 6D robotic systems. The experimental results demonstrate that our framework
outperforms existing algorithms in terms of planning time and cost.


------------------------------------------------------------------------------

Title:
Composite CAN XL-Ethernet Networks for Next-Gen Automotive and  Automation Systems

Abstract: New generation electrified and self-driving vehicles require much higher
performance and flexibility for onboard digital communications than Controller
Area Networks may offer. For this reason, automotive Ethernet is often regarded
as the next de facto standard technology in these contexts, and by extension
for networked embedded systems as well. However, an abrupt and drastic move
from CAN to Ethernet is likely to cause further cost increases, which can be
hardly tolerated by buyers. This paper analyzes the third generation of CAN,
termed CAN XL, and studies how interoperability can be ensured with Ethernet.
Likely, composite CAN XL-Ethernet networks are the key for getting the best of
both worlds, not only in the automotive domain but also for sensing and control
in scenarios like building automation, wired sensor networks, and low-cost
networked embedded systems with real-time constraints.


------------------------------------------------------------------------------

Title:
Non-Asymptotic Performance of Social Machine Learning Under Limited Data

Abstract: This paper studies the probability of error associated with the social
machine learning framework, which involves an independent training phase
followed by a cooperative decision-making phase over a graph. This framework
addresses the problem of classifying a stream of unlabeled data in a
distributed manner. We consider two kinds of classification tasks with limited
observations in the prediction phase, namely, the statistical classification
task and the single-sample classification task. For each task, we describe the
distributed learning rule and analyze the probability of error accordingly. To
do so, we first introduce a stronger consistent training condition that
involves the margin distributions generated by the trained classifiers. Based
on this condition, we derive an upper bound on the probability of error for
both tasks, which depends on the statistical properties of the data and the
combination policy used to combine the distributed classifiers. For the
statistical classification problem, we employ the geometric social learning
rule and conduct a non-asymptotic performance analysis. An exponential decay of
the probability of error with respect to the number of unlabeled samples is
observed in the upper bound. For the single-sample classification task, a
distributed learning rule that functions as an ensemble classifier is
constructed. An upper bound on the probability of error of this ensemble
classifier is established.


------------------------------------------------------------------------------

Title:
Granger-Causal Hierarchical Skill Discovery

Abstract: Reinforcement Learning (RL) has shown promising results learning policies for
complex tasks, but can often suffer from low sample efficiency and limited
transfer. We introduce the Hierarchy of Interaction Skills (HIntS) algorithm,
which uses learned interaction detectors to discover and train a hierarchy of
skills that manipulate factors in factored environments. Inspired by Granger
causality, these unsupervised detectors capture key events between factors to
sample efficiently learn useful skills and transfer those skills to other
related tasks -- tasks where many reinforcement learning techniques struggle.
We evaluate HIntS on a robotic pushing task with obstacles -- a challenging
domain where other RL and HRL methods fall short. The learned skills not only
demonstrate transfer using variants of Breakout, a common RL benchmark, but
also show 2-3x improvement in both sample efficiency and final performance
compared to comparable RL baselines. Together, HIntS demonstrates a proof of
concept for using Granger-causal relationships for skill discovery.


------------------------------------------------------------------------------

Title:
On the Giant Component of Geometric Inhomogeneous Random Graphs

Abstract: In this paper we study the threshold model of \emph{geometric inhomogeneous
random graphs} (GIRGs); a generative random graph model that is closely related
to \emph{hyperbolic random graphs} (HRGs). These models have been observed to
capture complex real-world networks well with respect to the structural and
algorithmic properties. Following comprehensive studies regarding their
\emph{connectivity}, i.e., which parts of the graphs are connected, we have a
good understanding under which circumstances a \emph{giant} component
(containing a constant fraction of the graph) emerges. While previous results
are rather technical and challenging to work with, the goal of this paper is to
provide more accessible proofs. At the same time we significantly improve the
previously known probabilistic guarantees, showing that GIRGs contain a giant
component with probability $1 - \exp(-\Omega(n^{(3-\tau)/2}))$ for graph size
$n$ and a degree distribution with power-law exponent $\tau \in (2, 3)$. Based
on that we additionally derive insights about the connectivity of certain
induced subgraphs of GIRGs.


------------------------------------------------------------------------------

Title:
FALL-E: A Foley Sound Synthesis Model and Strategies

Abstract: This paper introduces FALL-E, a foley synthesis system and its
training/inference strategies. The FALL-E model employs a cascaded approach
comprising low-resolution spectrogram generation, spectrogram super-resolution,
and a vocoder. We trained every sound-related model from scratch using our
extensive datasets, and utilized a pre-trained language model. We conditioned
the model with dataset-specific texts, enabling it to learn sound quality and
recording environment based on text input. Moreover, we leveraged external
language models to improve text descriptions of our datasets and performed
prompt engineering for quality, coherence, and diversity. FALL-E was evaluated
by an objective measure as well as listening tests in the DCASE 2023 challenge
Task 7. The submission achieved the second place on average, while achieving
the best score for diversity, second place for audio quality, and third place
for class fitness.


------------------------------------------------------------------------------

Title:
Stable Tomography for Structured Quantum States

Abstract: The reconstruction of quantum states from experimental measurements, often
achieved using quantum state tomography (QST), is crucial for the verification
and benchmarking of quantum devices. However, performing QST for a generic
unstructured quantum state requires an enormous number of state copies that
grows \emph{exponentially} with the number of individual quanta in the system,
even for the most optimal measurement settings. Fortunately, many physical
quantum states, such as states generated by noisy, intermediate-scale quantum
computers, are usually structured. In one dimension, such states are expected
to be well approximated by matrix product operators (MPOs) with a finite
matrix/bond dimension independent of the number of qubits, therefore enabling
efficient state representation. Nevertheless, it is still unclear whether
efficient QST can be performed for these states in general.
In this paper, we attempt to bridge this gap and establish theoretical
guarantees for the stable recovery of MPOs using tools from compressive sensing
and the theory of empirical processes. We begin by studying two types of random
measurement settings: Gaussian measurements and Haar random rank-one Positive
Operator Valued Measures (POVMs). We show that the information contained in an
MPO with a finite bond dimension can be preserved using a number of random
measurements that depends only \emph{linearly} on the number of qubits,
assuming no statistical error of the measurements. We then study MPO-based QST
with physical quantum measurements through Haar random rank-one POVMs that can
be implemented on quantum computers. We prove that only a \emph{polynomial}
number of state copies in the number of qubits is required to guarantee bounded
recovery error of an MPO state.


------------------------------------------------------------------------------

Title:
Multi-omics Prediction from High-content Cellular Imaging with Deep  Learning

Abstract: High-content cellular imaging, transcriptomics, and proteomics data provide
rich and complementary views on the molecular layers of biology that influence
cellular states and function. However, the biological determinants through
which changes in multi-omics measurements influence cellular morphology have
not yet been systematically explored, and the degree to which cell imaging
could potentially enable the prediction of multi-omics directly from cell
imaging data is therefore currently unclear. Here, we address the question of
whether it is possible to predict bulk multi-omics measurements directly from
cell images using Image2Omics -- a deep learning approach that predicts
multi-omics in a cell population directly from high-content images stained with
multiplexed fluorescent dyes. We perform an experimental evaluation in
gene-edited macrophages derived from human induced pluripotent stem cell
(hiPSC) under multiple stimulation conditions and demonstrate that Image2Omics
achieves significantly better performance in predicting transcriptomics and
proteomics measurements directly from cell images than predictors based on the
mean observed training set abundance. We observed significant predictability of
abundances for 5903 (22.43%; 95% CI: 8.77%, 38.88%) and 5819 (22.11%; 95% CI:
10.40%, 38.08%) transcripts out of 26137 in M1 and M2-stimulated macrophages
respectively and for 1933 (38.77%; 95% CI: 36.94%, 39.85%) and 2055 (41.22%;
95% CI: 39.31%, 42.42%) proteins out of 4986 in M1 and M2-stimulated
macrophages respectively. Our results show that some transcript and protein
abundances are predictable from cell imaging and that cell imaging may
potentially, in some settings and depending on the mechanisms of interest and
desired performance threshold, even be a scalable and resource-efficient
substitute for multi-omics measurements.


------------------------------------------------------------------------------

Title:
Human Preference Score v2: A Solid Benchmark for Evaluating Human  Preferences of Text-to-Image Synthesis

Abstract: Recent text-to-image generative models can generate high-fidelity images from
text inputs, but the quality of these generated images cannot be accurately
evaluated by existing evaluation metrics. To address this issue, we introduce
Human Preference Dataset v2 (HPD v2), a large-scale dataset that captures human
preferences on images from a wide range of sources. HPD v2 comprises 798,090
human preference choices on 430,060 pairs of images, making it the largest
dataset of its kind. The text prompts and images are deliberately collected to
eliminate potential bias, which is a common issue in previous datasets. By
fine-tuning CLIP on HPD v2, we obtain Human Preference Score v2 (HPS v2), a
scoring model that can more accurately predict text-generated images' human
preferences. Our experiments demonstrate that HPS v2 generalizes better than
previous metrics across various image distributions and is responsive to
algorithmic improvements of text-to-image generative models, making it a
preferable evaluation metric for these models. We also investigate the design
of the evaluation prompts for text-to-image generative models, to make the
evaluation stable, fair and easy-to-use. Finally, we establish a benchmark for
text-to-image generative models using HPS v2, which includes a set of recent
text-to-image models from the academia, community and industry. The code and
dataset is / will be available at this https URL


------------------------------------------------------------------------------

Title:
Recurrent Memory Decision Transformer

Abstract: Transformative models, originally developed for natural language problems,
have recently been widely used in offline reinforcement learning tasks. This is
due to the fact that the agent's history can be represented as a sequence, and
the whole task can be reduced to the sequence modeling task. However, the
quadratic complexity of the transformer operation limits the potential increase
in context. Therefore, to work with long sequences in a natural language,
different versions of the memory mechanism are used. In this paper, we propose
the Recurrent Memory Decision Transformer (RMDT), a model that uses a recurrent
memory mechanism for reinforcement learning problems. We conduct thorough
experiments on Atari games and MoJoCo control problems, and show that our
proposed model is significantly superior to its counterparts without the
recurrent memory mechanism on Atari games. We also carefully study the effect
of memory on the performance of the proposed model. These findings shed light
on the potential of incorporating recurrent memory mechanisms to improve the
performance of large-scale transformer models in offline reinforcement learning
tasks. The Recurrent Memory Decision Transformer code is publicly available in
repository \url{this https URL}.


------------------------------------------------------------------------------

Title:
Modular Controllers Facilitate the Co-Optimization of Morphology and  Control in Soft Robots

Abstract: Soft robotics is a rapidly growing area of robotics research that would
benefit greatly from design automation, given the challenges of manually
engineering complex, compliant, and generally non-intuitive robot body plans
and behaviors. It has been suggested that a major hurdle currently limiting
soft robot brain-body co-optimization is the fragile specialization between a
robot's controller and the particular body plan it controls, resulting in
premature convergence. Here we posit that modular controllers are more robust
to changes to a robot's body plan. We demonstrate a decreased reduction in
locomotion performance after morphological mutations to soft robots with
modular controllers, relative to those with similar global controllers -
leading to fitter offspring. Moreover, we show that the increased
transferability of modular controllers to similar body plans enables more
effective brain-body co-optimization of soft robots, resulting in an increased
rate of positive morphological mutations and higher overall performance of
evolved robots. We hope that this work helps provide specific methods to
improve soft robot design automation in this particular setting, while also
providing evidence to support our understanding of the challenges of brain-body
co-optimization more generally.


------------------------------------------------------------------------------

Title:
QuadSwarm: A Modular Multi-Quadrotor Simulator for Deep Reinforcement  Learning with Direct Thrust Control

Abstract: Reinforcement learning (RL) has shown promise in creating robust policies for
robotics tasks. However, contemporary RL algorithms are data-hungry, often
requiring billions of environment transitions to train successful policies.
This necessitates the use of fast and highly-parallelizable simulators. In
addition to speed, such simulators need to model the physics of the robots and
their interaction with the environment to a level acceptable for transferring
policies learned in simulation to reality. We present QuadSwarm, a fast,
reliable simulator for research in single and multi-robot RL for quadrotors
that addresses both issues. QuadSwarm, with fast forward-dynamics propagation
decoupled from rendering, is designed to be highly parallelizable such that
throughput scales linearly with additional compute. It provides multiple
components tailored toward multi-robot RL, including diverse training
scenarios, and provides domain randomization to facilitate the development and
sim2real transfer of multi-quadrotor control policies. Initial experiments
suggest that QuadSwarm achieves over 48,500 simulation samples per second (SPS)
on a single quadrotor and over 62,000 SPS on eight quadrotors on a 16-core CPU.
The code can be found in this https URL


------------------------------------------------------------------------------

Title:
HiNeRV: Video Compression with Hierarchical Encoding based Neural  Representation

Abstract: Learning-based video compression is currently one of the most popular
research topics, offering the potential to compete with conventional standard
video codecs. In this context, Implicit Neural Representations (INRs) have
previously been used to represent and compress image and video content,
demonstrating relatively high decoding speed compared to other methods.
However, existing INR-based methods have failed to deliver rate quality
performance comparable with the state of the art in video compression. This is
mainly due to the simplicity of the employed network architectures, which limit
their representation capability. In this paper, we propose HiNeRV, an INR that
combines bilinear interpolation with novel hierarchical positional encoding.
This structure employs depth-wise convolutional and MLP layers to build a deep
and wide network architecture with much higher capacity. We further build a
video codec based on HiNeRV and a refined pipeline for training, pruning and
quantization that can better preserve HiNeRV's performance during lossy model
compression. The proposed method has been evaluated on both UVG and MCL-JCV
datasets for video compression, demonstrating significant improvement over all
existing INRs baselines and competitive performance when compared to
learning-based codecs (72.3% overall bit rate saving over HNeRV and 43.4% over
DCVC on the UVG dataset, measured in PSNR).


------------------------------------------------------------------------------

Title:
ChatGPT for Suicide Risk Assessment on Social Media: Quantitative  Evaluation of Model Performance, Potentials and Limitations

Abstract: This paper presents a novel framework for quantitatively evaluating the
interactive ChatGPT model in the context of suicidality assessment from social
media posts, utilizing the University of Maryland Reddit suicidality dataset.
We conduct a technical evaluation of ChatGPT's performance on this task using
Zero-Shot and Few-Shot experiments and compare its results with those of two
fine-tuned transformer-based models. Additionally, we investigate the impact of
different temperature parameters on ChatGPT's response generation and discuss
the optimal temperature based on the inconclusiveness rate of ChatGPT. Our
results indicate that while ChatGPT attains considerable accuracy in this task,
transformer-based models fine-tuned on human-annotated datasets exhibit
superior performance. Moreover, our analysis sheds light on how adjusting the
ChatGPT's hyperparameters can improve its ability to assist mental health
professionals in this critical task.


------------------------------------------------------------------------------

Title:
Unsupervised Anomaly Detection via Nonlinear Manifold Learning

Abstract: Anomalies are samples that significantly deviate from the rest of the data
and their detection plays a major role in building machine learning models that
can be reliably used in applications such as data-driven design and novelty
detection. The majority of existing anomaly detection methods either are
exclusively developed for (semi) supervised settings, or provide poor
performance in unsupervised applications where there is no training data with
labeled anomalous samples. To bridge this research gap, we introduce a robust,
efficient, and interpretable methodology based on nonlinear manifold learning
to detect anomalies in unsupervised settings. The essence of our approach is to
learn a low-dimensional and interpretable latent representation (aka manifold)
for all the data points such that normal samples are automatically clustered
together and hence can be easily and robustly identified. We learn this
low-dimensional manifold by designing a learning algorithm that leverages
either a latent map Gaussian process (LMGP) or a deep autoencoder (AE). Our
LMGP-based approach, in particular, provides a probabilistic perspective on the
learning task and is ideal for high-dimensional applications with scarce data.
We demonstrate the superior performance of our approach over existing
technologies via multiple analytic examples and real-world datasets.


------------------------------------------------------------------------------

Title:
DreamSim: Learning New Dimensions of Human Visual Similarity using  Synthetic Data

Abstract: Current perceptual similarity metrics operate at the level of pixels and
patches. These metrics compare images in terms of their low-level colors and
textures, but fail to capture mid-level similarities and differences in image
layout, object pose, and semantic content. In this paper, we develop a
perceptual metric that assesses images holistically. Our first step is to
collect a new dataset of human similarity judgments over image pairs that are
alike in diverse ways. Critical to this dataset is that judgments are nearly
automatic and shared by all observers. To achieve this we use recent
text-to-image models to create synthetic pairs that are perturbed along various
dimensions. We observe that popular perceptual metrics fall short of explaining
our new data, and we introduce a new metric, DreamSim, tuned to better align
with human perception. We analyze how our metric is affected by different
visual attributes, and find that it focuses heavily on foreground objects and
semantic content while also being sensitive to color and layout. Notably,
despite being trained on synthetic data, our metric generalizes to real images,
giving strong results on retrieval and reconstruction tasks. Furthermore, our
metric outperforms both prior learned metrics and recent large vision models on
these tasks.


------------------------------------------------------------------------------

Title:
The temporal dynamics of group interactions in higher-order social  networks

Abstract: The structure and behaviour of many social systems are shaped by the
interactions among their individuals. Representing them as complex networks has
shed light on the mechanisms that govern their formation and evolution. Such
representations, however, focus on interactions between pairs of individuals
(represented by the edges of the network), although many social interactions
involve instead groups: these can be represented as hyperedges, that can
comprehend any number of individuals, leading to the use of higher-order
network representations. While recent research has investigated the structure
of static higher-order networks, little is known about the mechanisms that
govern their evolution. How do groups form and develop? How do people move
between different groups? Here, we investigate the temporal dynamics of group
change, using empirical social interactions collected in different settings. We
leverage proximity records from two data-collection efforts that have tracked
the temporal evolution of social interactions among students of a university
and children in a preschool. We study the mechanisms governing the temporal
dynamics both at the node and group level, characterising how individuals
navigate groups and how groups form and disaggregate, finding robust patterns
across contexts. We then propose a dynamical hypergraph model that closely
reproduces the empirical observations. These results represent a further step
in understanding the social dynamics of higher-order interactions, stressing
the importance of considering their temporal aspect. The proposed model
moreover opens up research directions to study the impact of higher-order
temporal network patterns on dynamical processes that evolve on top of them.


------------------------------------------------------------------------------

Title:
Evaluating Data Attribution for Text-to-Image Models

Abstract: While large text-to-image models are able to synthesize "novel" images, these
images are necessarily a reflection of the training data. The problem of data
attribution in such models -- which of the images in the training set are most
responsible for the appearance of a given generated image -- is a difficult yet
important one. As an initial step toward this problem, we evaluate attribution
through "customization" methods, which tune an existing large-scale model
toward a given exemplar object or style. Our key insight is that this allows us
to efficiently create synthetic images that are computationally influenced by
the exemplar by construction. With our new dataset of such exemplar-influenced
images, we are able to evaluate various data attribution algorithms and
different possible feature spaces. Furthermore, by training on our dataset, we
can tune standard models, such as DINO, CLIP, and ViT, toward the attribution
problem. Even though the procedure is tuned towards small exemplar sets, we
show generalization to larger sets. Finally, by taking into account the
inherent uncertainty of the problem, we can assign soft attribution scores over
a set of training images.


------------------------------------------------------------------------------

Title:
MobileASR: A resource-aware on-device personalisation framework for  automatic speech recognition in mobile phones

Abstract: We describe a comprehensive methodology for developing user-voice
personalised ASR models by effectively training models on mobile phones,
allowing user data and models to be stored and used locally. To achieve this,
we propose a resource-aware sub-model based training approach that considers
the RAM, and battery capabilities of mobile phones. We also investigate the
relationship between available resources and training time, highlighting the
effectiveness of using sub-models in such scenarios. By taking into account the
evaluation metric and battery constraints of the mobile phones, we are able to
perform efficient training and halt the process accordingly. To simulate real
users, we use speakers with various accents. The entire on-device training and
evaluation framework was then tested on various mobile phones across brands. We
show that fine-tuning the models and selecting the right hyperparameter values
is a trade-off between the lowest achievable performance metric, on-device
training time, and memory consumption. Overall, our methodology offers a
comprehensive solution for developing personalized ASR models while leveraging
the capabilities of mobile phones, and balancing the need for accuracy with
resource constraints.


------------------------------------------------------------------------------

Title:
Pengembangan Domain Specific Language Untuk Pengelolaan Data Warehouse

Abstract: Efforts to improve the performance of services on the transaction at a bank
can be done by performing data retention, reduce the volume of data in the
database production by cutting the historical data in accordance with the rules
in a bank to a data warehouse. Design and implementation of applications Domain
Specific Language (DSL) for handling the data transfer on the data warehouse is
divided into lexical analysis, syntax analysis, semantic analysis and code
generation. Each part has different characteristics to produce an executable
command. Has been developed an application with the DSL method, which is
beneficial to reduce the error of writing a command for a normal
(non-technical) way to transfer data. From the test result in a decision Oracle
transfer method according to the size scale of a particular data.


------------------------------------------------------------------------------

Title:
Understanding and Mitigating Extrapolation Failures in Physics-Informed  Neural Networks

Abstract: Physics-informed Neural Networks (PINNs) have recently gained popularity in
the scientific community due to their effective approximation of partial
differential equations (PDEs) using deep neural networks. However, their
application has been generally limited to interpolation scenarios, where
predictions rely on inputs within the support of the training set. In
real-world applications, extrapolation is often required, but the out of domain
behavior of PINNs is understudied. In this paper, we provide a detailed
investigation of PINNs' extrapolation behavior and provide evidence against
several previously held assumptions: we study the effects of different model
choices on extrapolation and find that once the model can achieve zero
interpolation error, further increases in architecture size or in the number of
points sampled have no effect on extrapolation behavior. We also show that for
some PDEs, PINNs perform nearly as well in extrapolation as in interpolation.
By analyzing the Fourier spectra of the solution functions, we characterize the
PDEs that yield favorable extrapolation behavior, and show that the presence of
high frequencies in the solution function is not to blame for poor
extrapolation behavior. Finally, we propose a transfer learning-based strategy
based on our Fourier results, which decreases extrapolation errors in PINNs by
up to $82 \%$.


------------------------------------------------------------------------------

Title:
PaReprop: Fast Parallelized Reversible Backpropagation

Abstract: The growing size of datasets and deep learning models has made faster and
memory-efficient training crucial. Reversible transformers have recently been
introduced as an exciting new method for extremely memory-efficient training,
but they come with an additional computation overhead of activation
re-computation in the backpropagation phase. We present PaReprop, a fast
Parallelized Reversible Backpropagation algorithm that parallelizes the
additional activation re-computation overhead in reversible training with the
gradient computation itself in backpropagation phase. We demonstrate the
effectiveness of the proposed PaReprop algorithm through extensive benchmarking
across model families (ViT, MViT, Swin and RoBERTa), data modalities (Vision &
NLP), model sizes (from small to giant), and training batch sizes. Our
empirical results show that PaReprop achieves up to 20% higher training
throughput than vanilla reversible training, largely mitigating the theoretical
overhead of 25% lower throughput from activation recomputation in reversible
training. Project page: this https URL


------------------------------------------------------------------------------

Title:
Rosetta Neurons: Mining the Common Units in a Model Zoo

Abstract: Do different neural networks, trained for various vision tasks, share some
common representations? In this paper, we demonstrate the existence of common
features we call "Rosetta Neurons" across a range of models with different
architectures, different tasks (generative and discriminative), and different
types of supervision (class-supervised, text-supervised, self-supervised). We
present an algorithm for mining a dictionary of Rosetta Neurons across several
popular vision models: Class Supervised-ResNet50, DINO-ResNet50, DINO-ViT, MAE,
CLIP-ResNet50, BigGAN, StyleGAN-2, StyleGAN-XL. Our findings suggest that
certain visual concepts and structures are inherently embedded in the natural
world and can be learned by different models regardless of the specific task or
architecture, and without the use of semantic labels. We can visualize shared
concepts directly due to generative models included in our analysis. The
Rosetta Neurons facilitate model-to-model translation enabling various
inversion-based manipulations, including cross-class alignments, shifting,
zooming, and more, without the need for specialized training.


------------------------------------------------------------------------------

Title:
Parallel-in-time integration of the shallow water equations on the  rotating sphere using Parareal and MGRIT

Abstract: Despite the growing interest in parallel-in-time methods as an approach to
accelerate numerical simulations in atmospheric modelling, improving their
stability and convergence remains a substantial challenge for their application
to operational models. In this work, we study the temporal parallelization of
the shallow water equations on the rotating sphere combined with time-stepping
schemes commonly used in atmospheric modelling due to their stability
properties, namely an Eulerian implicit-explicit (IMEX) method and a
semi-Lagrangian semi-implicit method (SL-SI-SETTLS). The main goal is to
investigate the performance of parallel-in-time methods, namely Parareal and
Multigrid Reduction in Time (MGRIT), when these well-established schemes are
used on the coarse discretization levels and provide insights on how they can
be improved for better performance. We begin by performing an analytical
stability study of Parareal and MGRIT applied to a linearized ordinary
differential equation depending on the choice of coarse scheme. Next, we
perform numerical simulations of two standard tests to evaluate the stability,
convergence and speedup provided by the parallel-in-time methods compared to a
fine reference solution computed serially. We also conduct a detailed
investigation on the influence of artificial viscosity and hyperviscosity
approaches, applied on the coarse discretization levels, on the performance of
the temporal parallelization. Both the analytical stability study and the
numerical simulations indicate a poorer stability behaviour when SL-SI-SETTLS
is used on the coarse levels, compared to the IMEX scheme. With the IMEX
scheme, a better trade-off between convergence, stability and speedup compared
to serial simulations can be obtained under proper parameters and artificial
viscosity choices, opening the perspective of the potential competitiveness for
realistic models.


------------------------------------------------------------------------------

Title:
Neural Volumetric Reconstruction for Coherent Synthetic Aperture Sonar

Abstract: Synthetic aperture sonar (SAS) measures a scene from multiple views in order
to increase the resolution of reconstructed imagery. Image reconstruction
methods for SAS coherently combine measurements to focus acoustic energy onto
the scene. However, image formation is typically under-constrained due to a
limited number of measurements and bandlimited hardware, which limits the
capabilities of existing reconstruction methods. To help meet these challenges,
we design an analysis-by-synthesis optimization that leverages recent advances
in neural rendering to perform coherent SAS imaging. Our optimization enables
us to incorporate physics-based constraints and scene priors into the image
formation process. We validate our method on simulation and experimental
results captured in both air and water. We demonstrate both quantitatively and
qualitatively that our method typically produces superior reconstructions than
existing approaches. We share code and data for reproducibility.


------------------------------------------------------------------------------

Title:
SIGHT: A Large Annotated Dataset on Student Insights Gathered from  Higher Education Transcripts

Abstract: Lectures are a learning experience for both students and teachers. Students
learn from teachers about the subject material, while teachers learn from
students about how to refine their instruction. However, online student
feedback is unstructured and abundant, making it challenging for teachers to
learn and improve. We take a step towards tackling this challenge. First, we
contribute a dataset for studying this problem: SIGHT is a large dataset of 288
math lecture transcripts and 15,784 comments collected from the Massachusetts
Institute of Technology OpenCourseWare (MIT OCW) YouTube channel. Second, we
develop a rubric for categorizing feedback types using qualitative analysis.
Qualitative analysis methods are powerful in uncovering domain-specific
insights, however they are costly to apply to large data sources. To overcome
this challenge, we propose a set of best practices for using large language
models (LLMs) to cheaply classify the comments at scale. We observe a striking
correlation between the model's and humans' annotation: Categories with
consistent human annotations (>$0.9$ inter-rater reliability, IRR) also display
higher human-model agreement (>$0.7$), while categories with less consistent
human annotations ($0.7$-$0.8$ IRR) correspondingly demonstrate lower
human-model agreement ($0.3$-$0.5$). These techniques uncover useful student
feedback from thousands of comments, costing around $\$0.002$ per comment. We
conclude by discussing exciting future directions on using online student
feedback and improving automated annotation techniques for qualitative
research.


------------------------------------------------------------------------------

Title:
From BERT to GPT-3 Codex: Harnessing the Potential of Very Large  Language Models for Data Management

Abstract: Large language models have recently advanced the state of the art on many
natural language processing benchmarks. The newest generation of models can be
applied to a variety of tasks with little to no specialized training. This
technology creates various opportunities for applications in the context of
data management.
The tutorial will introduce participants to basic background on language
models, discuss different methods to use language models, and give an overview
and short demonstration of available libraries and APIs. Models for generating
natural language will be considered as well as models, such as GPT-3 Codex,
which complete program code or generate code from natural language
instructions. Finally, the tutorial will discuss recent research in the
database community that exploits language models in the context of traditional
database systems or proposes novel system architectures that are based on them.
The tutorial is targeted at database researchers. No prior background on
language models is required. The goal of the tutorial is to introduce database
researchers to the latest generation of language models, and to their use cases
in the domain of data management.


------------------------------------------------------------------------------

Title:
Fusing Structural and Functional Connectivities using Disentangled VAE  for Detecting MCI

Abstract: Brain network analysis is a useful approach to studying human brain disorders
because it can distinguish patients from healthy people by detecting abnormal
connections. Due to the complementary information from multiple modal
neuroimages, multimodal fusion technology has a lot of potential for improving
prediction performance. However, effective fusion of multimodal medical images
to achieve complementarity is still a challenging problem. In this paper, a
novel hierarchical structural-functional connectivity fusing (HSCF) model is
proposed to construct brain structural-functional connectivity matrices and
predict abnormal brain connections based on functional magnetic resonance
imaging (fMRI) and diffusion tensor imaging (DTI). Specifically, the prior
knowledge is incorporated into the separators for disentangling each modality
of information by the graph convolutional networks (GCN). And a disentangled
cosine distance loss is devised to ensure the disentanglement's effectiveness.
Moreover, the hierarchical representation fusion module is designed to
effectively maximize the combination of relevant and effective features between
modalities, which makes the generated structural-functional connectivity more
robust and discriminative in the cognitive disease analysis. Results from a
wide range of tests performed on the public Alzheimer's Disease Neuroimaging
Initiative (ADNI) database show that the proposed model performs better than
competing approaches in terms of classification evaluation. In general, the
proposed HSCF model is a promising model for generating brain
structural-functional connectivities and identifying abnormal brain connections
as cognitive disease progresses.


------------------------------------------------------------------------------

Title:
Personalized Image Enhancement Featuring Masked Style Modeling

Abstract: We address personalized image enhancement in this study, where we enhance
input images for each user based on the user's preferred images. Previous
methods apply the same preferred style to all input images (i.e., only one
style for each user); in contrast to these methods, we aim to achieve
content-aware personalization by applying different styles to each image
considering the contents. For content-aware personalization, we make two
contributions. First, we propose a method named masked style modeling, which
can predict a style for an input image considering the contents by using the
framework of masked language modeling. Second, to allow this model to consider
the contents of images, we propose a novel training scheme where we download
images from Flickr and create pseudo input and retouched image pairs using a
degrading model. We conduct quantitative evaluations and a user study, and our
method trained using our training scheme successfully achieves content-aware
personalization; moreover, our method outperforms other previous methods in
this field. Our source code is available at
this https URL


------------------------------------------------------------------------------

Title:
SAFER: Situation Aware Facial Emotion Recognition

Abstract: In this paper, we present SAFER, a novel system for emotion recognition from
facial expressions. It employs state-of-the-art deep learning techniques to
extract various features from facial images and incorporates contextual
information, such as background and location type, to enhance its performance.
The system has been designed to operate in an open-world setting, meaning it
can adapt to unseen and varied facial expressions, making it suitable for
real-world applications. An extensive evaluation of SAFER against existing
works in the field demonstrates improved performance, achieving an accuracy of
91.4% on the CAER-S dataset. Additionally, the study investigates the effect of
novelty such as face masks during the Covid-19 pandemic on facial emotion
recognition and critically examines the limitations of mainstream facial
expressions datasets. To address these limitations, a novel dataset for facial
emotion recognition is proposed. The proposed dataset and the system are
expected to be useful for various applications such as human-computer
interaction, security, and surveillance.


------------------------------------------------------------------------------

Title:
Explaining Legal Concepts with Augmented Large Language Models (GPT-4)

Abstract: Interpreting the meaning of legal open-textured terms is a key task of legal
professionals. An important source for this interpretation is how the term was
applied in previous court cases. In this paper, we evaluate the performance of
GPT-4 in generating factually accurate, clear and relevant explanations of
terms in legislation. We compare the performance of a baseline setup, where
GPT-4 is directly asked to explain a legal term, to an augmented approach,
where a legal information retrieval module is used to provide relevant context
to the model, in the form of sentences from case law. We found that the direct
application of GPT-4 yields explanations that appear to be of very high quality
on their surface. However, detailed analysis uncovered limitations in terms of
the factual accuracy of the explanations. Further, we found that the
augmentation leads to improved quality, and appears to eliminate the issue of
hallucination, where models invent incorrect statements. These findings open
the door to the building of systems that can autonomously retrieve relevant
sentences from case law and condense them into a useful explanation for legal
scholars, educators or practicing lawyers alike.


------------------------------------------------------------------------------

Title:
The 2023 Video Similarity Dataset and Challenge

Abstract: This work introduces a dataset, benchmark, and challenge for the problem of
video copy detection and localization. The problem comprises two distinct but
related tasks: determining whether a query video shares content with a
reference video ("detection"), and additionally temporally localizing the
shared content within each video ("localization"). The benchmark is designed to
evaluate methods on these two tasks, and simulates a realistic
needle-in-haystack setting, where the majority of both query and reference
videos are "distractors" containing no copied content. We propose a metric that
reflects both detection and localization accuracy. The associated challenge
consists of two corresponding tracks, each with restrictions that reflect
real-world settings. We provide implementation code for evaluation and
baselines. We also analyze the results and methods of the top submissions to
the challenge. The dataset, baseline methods and evaluation code is publicly
available and will be discussed at a dedicated CVPR'23 workshop.


------------------------------------------------------------------------------

Title:
Linear convergence of Nesterov-1983 with the strong convexity

Abstract: For modern gradient-based optimization, a developmental landmark is
Nesterov's accelerated gradient descent method, which is proposed in [Nesterov,
1983], so shorten as Nesterov-1983. Afterward, one of the important progresses
is its proximal generalization, named the fast iterative shrinkage-thresholding
algorithm (FISTA), which is widely used in image science and engineering.
However, it is unknown whether both Nesterov-1983 and FISTA converge linearly
on the strongly convex function, which has been listed as the open problem in
the comprehensive review [Chambolle and Pock, 2016, Appendix B]. In this paper,
we answer this question by the use of the high-resolution differential equation
framework. Along with the phase-space representation previously adopted, the
key difference here in constructing the Lyapunov function is that the
coefficient of the kinetic energy varies with the iteration. Furthermore, we
point out that the linear convergence of both the two algorithms above has no
dependence on the parameter $r$ on the strongly convex function. Meanwhile, it
is also obtained that the proximal subgradient norm converges linearly.


------------------------------------------------------------------------------

Title:
From Database Repairs to Causality in Databases and Beyond

Abstract: We describe some recent approaches to score-based explanations for query
answers in databases. The focus is on work done by the author and
collaborators. Special emphasis is placed on the use of counterfactual
reasoning for score specification and computation. Several examples that
illustrate the flexibility of these methods are shown.


------------------------------------------------------------------------------

Title:
Hierarchical Planning and Control for Box Loco-Manipulation

Abstract: Humans perform everyday tasks using a combination of locomotion and
manipulation skills. Building a system that can handle both skills is essential
to creating virtual humans. We present a physically-simulated human capable of
solving box rearrangement tasks, which requires a combination of both skills.
We propose a hierarchical control architecture, where each level solves the
task at a different level of abstraction, and the result is a physics-based
simulated virtual human capable of rearranging boxes in a cluttered
environment. The control architecture integrates a planner, diffusion models,
and physics-based motion imitation of sparse motion clips using deep
reinforcement learning. Boxes can vary in size, weight, shape, and placement
height. Code and trained control policies are provided.


------------------------------------------------------------------------------

Title:
Span-Selective Linear Attention Transformers for Effective and Robust  Schema-Guided Dialogue State Tracking

Abstract: In schema-guided dialogue state tracking models estimate the current state of
a conversation using natural language descriptions of the service schema for
generalization to unseen services. Prior generative approaches which decode
slot values sequentially do not generalize well to variations in schema, while
discriminative approaches separately encode history and schema and fail to
account for inter-slot and intent-slot dependencies. We introduce SPLAT, a
novel architecture which achieves better generalization and efficiency than
prior approaches by constraining outputs to a limited prediction space. At the
same time, our model allows for rich attention among descriptions and history
while keeping computation costs constrained by incorporating linear-time
attention. We demonstrate the effectiveness of our model on the Schema-Guided
Dialogue (SGD) and MultiWOZ datasets. Our approach significantly improves upon
existing models achieving 85.3 JGA on the SGD dataset. Further, we show
increased robustness on the SGD-X benchmark: our model outperforms the more
than 30$\times$ larger D3ST-XXL model by 5.0 points.


------------------------------------------------------------------------------

Title:
Online Heavy-tailed Change-point detection

Abstract: We study algorithms for online change-point detection (OCPD), where samples
that are potentially heavy-tailed, are presented one at a time and a change in
the underlying mean must be detected as early as possible. We present an
algorithm based on clipped Stochastic Gradient Descent (SGD), that works even
if we only assume that the second moment of the data generating process is
bounded. We derive guarantees on worst-case, finite-sample false-positive rate
(FPR) over the family of all distributions with bounded second moment. Thus,
our method is the first OCPD algorithm that guarantees finite-sample FPR, even
if the data is high dimensional and the underlying distributions are
heavy-tailed. The technical contribution of our paper is to show that
clipped-SGD can estimate the mean of a random vector and simultaneously provide
confidence bounds at all confidence values. We combine this robust estimate
with a union bound argument and construct a sequential change-point algorithm
with finite-sample FPR guarantees. We show empirically that our algorithm works
well in a variety of situations, whether the underlying data are heavy-tailed,
light-tailed, high dimensional or discrete. No other algorithm achieves bounded
FPR theoretically or empirically, over all settings we study simultaneously.


------------------------------------------------------------------------------

Title:
Power-law Dynamic arising from machine learning

Abstract: We study a kind of new SDE that was arisen from the research on optimization
in machine learning, we call it power-law dynamic because its stationary
distribution cannot have sub-Gaussian tail and obeys power-law. We prove that
the power-law dynamic is ergodic with unique stationary distribution, provided
the learning rate is small enough. We investigate its first exist time. In
particular, we compare the exit times of the (continuous) power-law dynamic and
its discretization. The comparison can help guide machine learning algorithm.


------------------------------------------------------------------------------

Title:
Update on the Verification and Validation Efforts for the Stakeholder  Tool for Assessing Radioactive Transportation

Abstract: The United States Department of Energy (U.S. DOE) is planning for the
transportation, storage, and disposal of spent nuclear fuel (SNF) and
high-level radioactive waste (HLW) from commercial nuclear power plants and
other U.S. DOE sites. The Stakeholder Tool for Assessing Radioactive
Transportation (START) is a web-based, geospatial decision-support tool
developed for evaluating routing options and other aspects of transporting SNF
and HLW via barge, train, truck, and intermodal surface transport in the
continental United States. The verification and validation (V&V) effort is
intended to independently assess START to provide confidence in the ability of
the tool to accurately provide intended outputs. The results selected for the
V&V effort of the START code include those identified as crucial outputs by
subject matter experts.
Outputs from START such as shape files and keyhole markup language (KML)
files are analyzed using a geodesic computation using the WSG-84 ellipsoid
model. Most of the V&V efforts are aimed towards examining and comparing the
total length reported in the various files in the START tool. This work also
focuses on the development of V&V methodologies for various outputs that could
be replicated by the end user on a set of user-defined routes. Over 150 origin
destination pairs were run as part of this effort to test the functionality of
the START tool. In addition to presenting results using an independent geodesic
computation, this work will provide a comparison of the total route lengths
between START version 3.3 and the previous release of START (version 3.2.2).


------------------------------------------------------------------------------

Title:
UrbanIR: Large-Scale Urban Scene Inverse Rendering from a Single Video

Abstract: We show how to build a model that allows realistic, free-viewpoint renderings
of a scene under novel lighting conditions from video. Our method -- UrbanIR:
Urban Scene Inverse Rendering -- computes an inverse graphics representation
from the video. UrbanIR jointly infers shape, albedo, visibility, and sun and
sky illumination from a single video of unbounded outdoor scenes with unknown
lighting. UrbanIR uses videos from cameras mounted on cars (in contrast to many
views of the same points in typical NeRF-style estimation). As a result,
standard methods produce poor geometry estimates (for example, roofs), and
there are numerous ''floaters''. Errors in inverse graphics inference can
result in strong rendering artifacts. UrbanIR uses novel losses to control
these and other sources of error. UrbanIR uses a novel loss to make very good
estimates of shadow volumes in the original scene. The resulting
representations facilitate controllable editing, delivering photorealistic
free-viewpoint renderings of relit scenes and inserted objects. Qualitative
evaluation demonstrates strong improvements over the state-of-the-art.


------------------------------------------------------------------------------

Title:
Generative Proxemics: A Prior for 3D Social Interaction from Images

Abstract: Social interaction is a fundamental aspect of human behavior and
communication. The way individuals position themselves in relation to others,
also known as proxemics, conveys social cues and affects the dynamics of social
interaction. We present a novel approach that learns a 3D proxemics prior of
two people in close social interaction. Since collecting a large 3D dataset of
interacting people is a challenge, we rely on 2D image collections where social
interactions are abundant. We achieve this by reconstructing pseudo-ground
truth 3D meshes of interacting people from images with an optimization approach
using existing ground-truth contact maps. We then model the proxemics using a
novel denoising diffusion model called BUDDI that learns the joint distribution
of two people in close social interaction directly in the SMPL-X parameter
space. Sampling from our generative proxemics model produces realistic 3D human
interactions, which we validate through a user study. Additionally, we
introduce a new optimization method that uses the diffusion prior to
reconstruct two people in close proximity from a single image without any
contact annotation. Our approach recovers more accurate and plausible 3D social
interactions from noisy initial estimates and outperforms state-of-the-art
methods. See our project site for code, data, and model:
muelea.github.io/buddi.


------------------------------------------------------------------------------

Title:
Competitive and Resource Efficient Factored Hybrid HMM Systems are  Simpler Than You Think

Abstract: Building competitive hybrid hidden Markov model~(HMM) systems for automatic
speech recognition~(ASR) requires a complex multi-stage pipeline consisting of
several training criteria. The recent sequence-to-sequence models offer the
advantage of having simpler pipelines that can start from-scratch. We propose a
purely neural based single-stage from-scratch pipeline for a context-dependent
hybrid HMM that offers similar simplicity. We use an alignment from a full-sum
trained zero-order posterior HMM with a BLSTM encoder. We show that with this
alignment we can build a Conformer factored hybrid that performs even better
than both a state-of-the-art classic hybrid and a factored hybrid trained with
alignments taken from more complex Gaussian mixture based systems. Our finding
is confirmed on Switchboard 300h and LibriSpeech 960h tasks with comparable
results to other approaches in the literature, and by additionally relying on a
responsible choice of available computational resources.


------------------------------------------------------------------------------

Title:
Deep Learning-Based Spatiotemporal Multi-Event Reconstruction for Delay  Line Detectors

Abstract: Accurate observation of two or more particles within a very narrow time
window has always been a challenge in modern physics. It creates the
possibility of correlation experiments, such as the ground-breaking Hanbury
Brown-Twiss experiment, leading to new physical insights. For low-energy
electrons, one possibility is to use a microchannel plate with subsequent delay
lines for the readout of the incident particle hits, a setup called a Delay
Line Detector. The spatial and temporal coordinates of more than one particle
can be fully reconstructed outside a region called the dead radius. For
interesting events, where two electrons are close in space and time, the
determination of the individual positions of the electrons requires elaborate
peak finding algorithms. While classical methods work well with single particle
hits, they fail to identify and reconstruct events caused by multiple nearby
particles. To address this challenge, we present a new spatiotemporal machine
learning model to identify and reconstruct the position and time of such
multi-hit particle signals. This model achieves a much better resolution for
nearby particle hits compared to the classical approach, removing some of the
artifacts and reducing the dead radius by half. We show that machine learning
models can be effective in improving the spatiotemporal performance of delay
line detectors.


------------------------------------------------------------------------------

Title:
MFAS: Emotion Recognition through Multiple Perspectives Fusion  Architecture Search Emulating Human Cognition

Abstract: Speech emotion recognition aims to identify and analyze emotional states in
target speech similar to humans. Perfect emotion recognition can greatly
benefit a wide range of human-machine interaction tasks. Inspired by the human
process of understanding emotions, we demonstrate that compared to quantized
modeling, understanding speech content from a continuous perspective, akin to
human-like comprehension, enables the model to capture more comprehensive
emotional information. Additionally, considering that humans adjust their
perception of emotional words in textual semantic based on certain cues present
in speech, we design a novel search space and search for the optimal fusion
strategy for the two types of information. Experimental results further
validate the significance of this perception adjustment. Building on these
observations, we propose a novel framework called Multiple perspectives Fusion
Architecture Search (MFAS). Specifically, we utilize continuous-based knowledge
to capture speech semantic and quantization-based knowledge to learn textual
semantic. Then, we search for the optimal fusion strategy for them.
Experimental results demonstrate that MFAS surpasses existing models in
comprehensively capturing speech emotion information and can automatically
adjust fusion strategy.


------------------------------------------------------------------------------

Title:
Semantic HELM: An Interpretable Memory for Reinforcement Learning

Abstract: Reinforcement learning agents deployed in the real world often have to cope
with partially observable environments. Therefore, most agents employ memory
mechanisms to approximate the state of the environment. Recently, there have
been impressive success stories in mastering partially observable environments,
mostly in the realm of computer games like Dota 2, StarCraft II, or MineCraft.
However, none of these methods are interpretable in the sense that it is not
comprehensible for humans how the agent decides which actions to take based on
its inputs. Yet, human understanding is necessary in order to deploy such
methods in high-stake domains like autonomous driving or medical applications.
We propose a novel memory mechanism that operates on human language to
illuminate the decision-making process. First, we use CLIP to associate visual
inputs with language tokens. Then we feed these tokens to a pretrained language
model that serves the agent as memory and provides it with a coherent and
interpretable representation of the past. Our memory mechanism achieves
state-of-the-art performance in environments where memorizing the past is
crucial to solve tasks. Further, we present situations where our memory
component excels or fails to demonstrate strengths and weaknesses of our new
approach.


------------------------------------------------------------------------------

Title:
Diffusion Models for Zero-Shot Open-Vocabulary Segmentation

Abstract: The variety of objects in the real world is nearly unlimited and is thus
impossible to capture using models trained on a fixed set of categories. As a
result, in recent years, open-vocabulary methods have attracted the interest of
the community. This paper proposes a new method for zero-shot open-vocabulary
segmentation. Prior work largely relies on contrastive training using
image-text pairs, leveraging grouping mechanisms to learn image features that
are both aligned with language and well-localised. This however can introduce
ambiguity as the visual appearance of images with similar captions often
varies. Instead, we leverage the generative properties of large-scale
text-to-image diffusion models to sample a set of support images for a given
textual category. This provides a distribution of appearances for a given text
circumventing the ambiguity problem. We further propose a mechanism that
considers the contextual background of the sampled images to better localise
objects and segment the background directly. We show that our method can be
used to ground several existing pre-trained self-supervised feature extractors
in natural language and provide explainable predictions by mapping back to
regions in the support set. Our proposal is training-free, relying on
pre-trained components only, yet, shows strong performance on a range of
open-vocabulary segmentation benchmarks, obtaining a lead of more than 10% on
the Pascal VOC benchmark.


------------------------------------------------------------------------------

Title:
ArtFusion: Controllable Arbitrary Style Transfer using Dual Conditional  Latent Diffusion Models

Abstract: Arbitrary Style Transfer (AST) aims to transform images by adopting the style
from any selected artwork. Nonetheless, the need to accommodate diverse and
subjective user preferences poses a significant challenge. While some users
wish to preserve distinct content structures, others might favor a more
pronounced stylization. Despite advances in feed-forward AST methods, their
limited customizability hinders their practical application. We propose a new
approach, ArtFusion, which provides a flexible balance between content and
style. In contrast to traditional methods reliant on biased similarity losses,
ArtFusion utilizes our innovative Dual Conditional Latent Diffusion
Probabilistic Models (Dual-cLDM). This approach mitigates repetitive patterns
and enhances subtle artistic aspects like brush strokes and genre-specific
features. Despite the promising results of conditional diffusion probabilistic
models (cDM) in various generative tasks, their introduction to style transfer
is challenging due to the requirement for paired training data. ArtFusion
successfully navigates this issue, offering more practical and controllable
stylization. A key element of our approach involves using a single image for
both content and style during model training, all the while maintaining
effective stylization during inference. ArtFusion outperforms existing
approaches on outstanding controllability and faithful presentation of artistic
details, providing evidence of its superior style transfer capabilities.
Furthermore, the Dual-cLDM utilized in ArtFusion carries the potential for a
variety of complex multi-condition generative tasks, thus greatly broadening
the impact of our research.


------------------------------------------------------------------------------

Title:
Open Source-based Over-The-Air 5G New Radio Sidelink Testbed

Abstract: The focus of this paper is the prototype development for 5G new radio (NR)
sidelink communications, which enables NR UEs to transfer data independently
without the assistance of a base station (gNB), designated as sidelink mode 2.
Our design leverages open-source software operating on software-defined radios
(SDRs), which can be easily extended for multiple UE scenarios. The software
includes all signal processing components specified by 5G sidelink standards,
including Low -Density Parity Check (LDPC) encoding/decoding, polar
encoding/decoding, data and control multiplexing, modulation/demodulation, and
orthogonal frequency-division multiplexing (OFDM) modulation/demodulation. It
can be configured to operate with different bands, bandwidths, and multiple
antenna settings. One method to demonstrate the completed Physical Sidelink
Broadcast Channel (PSBCH) development is to show synchronization between a
SyncRef UE and a nearby UE. The SyncRef UE broadcasts a sidelink
synchronization signal block (S-SSB) periodically, which the nearby UE detects
and uses to synchronize its timing and frequency components with the SyncRef
UE. Once a connection is established, the SyncRef UE acts as a transmitter and
shares data with the receiver UE (nearby UE) via the physical sidelink share
channel (PSSCH). Our physical sidelink framework is tested using both an RF
simulator and an over-the-air (OTA) testbed. In this work, we show both
synchronization and data transmission/reception with 5G sidelink mode 2, where
our OTA experimental results align well with our simulation results.


------------------------------------------------------------------------------

Title:
Infinite Photorealistic Worlds using Procedural Generation

Abstract: We introduce Infinigen, a procedural generator of photorealistic 3D scenes of
the natural world. Infinigen is entirely procedural: every asset, from shape to
texture, is generated from scratch via randomized mathematical rules, using no
external source and allowing infinite variation and composition. Infinigen
offers broad coverage of objects and scenes in the natural world including
plants, animals, terrains, and natural phenomena such as fire, cloud, rain, and
snow. Infinigen can be used to generate unlimited, diverse training data for a
wide range of computer vision tasks including object detection, semantic
segmentation, optical flow, and 3D reconstruction. We expect Infinigen to be a
useful resource for computer vision research and beyond. Please visit
this https URL for videos, code and pre-generated data.


------------------------------------------------------------------------------

Title:
Neural Relighting with Subsurface Scattering by Learning the Radiance  Transfer Gradient

Abstract: Reconstructing and relighting objects and scenes under varying lighting
conditions is challenging: existing neural rendering methods often cannot
handle the complex interactions between materials and light. Incorporating
pre-computed radiance transfer techniques enables global illumination, but
still struggles with materials with subsurface scattering effects. We propose a
novel framework for learning the radiance transfer field via volume rendering
and utilizing various appearance cues to refine geometry end-to-end. This
framework extends relighting and reconstruction capabilities to handle a wider
range of materials in a data-driven fashion. The resulting models produce
plausible rendering results in existing and novel conditions. We will release
our code and a novel light stage dataset of objects with subsurface scattering
effects publicly available.


------------------------------------------------------------------------------

Title:
WizMap: Scalable Interactive Visualization for Exploring Large Machine  Learning Embeddings

Abstract: Machine learning models often learn latent embedding representations that
capture the domain semantics of their training data. These embedding
representations are valuable for interpreting trained models, building new
models, and analyzing new datasets. However, interpreting and using embeddings
can be challenging due to their opaqueness, high dimensionality, and the large
size of modern datasets. To tackle these challenges, we present WizMap, an
interactive visualization tool to help researchers and practitioners easily
explore large embeddings. With a novel multi-resolution embedding summarization
method and a familiar map-like interaction design, WizMap enables users to
navigate and interpret embedding spaces with ease. Leveraging modern web
technologies such as WebGL and Web Workers, WizMap scales to millions of
embedding points directly in users' web browsers and computational notebooks
without the need for dedicated backend servers. WizMap is open-source and
available at the following public demo link: this https URL


------------------------------------------------------------------------------

Title:
Radars for Autonomous Driving: A Review of Deep Learning Methods and  Challenges

Abstract: Radar is a key component of the suite of perception sensors used for safe and
reliable navigation of autonomous vehicles. Its unique capabilities include
high-resolution velocity imaging, detection of agents in occlusion and over
long ranges, and robust performance in adverse weather conditions. However, the
usage of radar data presents some challenges: it is characterized by low
resolution, sparsity, clutter, high uncertainty, and lack of good datasets.
These challenges have limited radar deep learning research. As a result,
current radar models are often influenced by lidar and vision models, which are
focused on optical features that are relatively weak in radar data, thus
resulting in under-utilization of radar's capabilities and diminishing its
contribution to autonomous perception. This review seeks to encourage further
deep learning research on autonomous radar data by 1) identifying key research
themes, and 2) offering a comprehensive overview of current opportunities and
challenges in the field. Topics covered include early and late fusion,
occupancy flow estimation, uncertainty modeling, and multipath detection. The
paper also discusses radar fundamentals and data representation, presents a
curated list of recent radar datasets, and reviews state-of-the-art lidar and
vision models relevant for radar research. For a summary of the paper and more
results, visit the website: autonomous-radars.github.io.


------------------------------------------------------------------------------

Title:
Harvard Glaucoma Fairness: A Retinal Nerve Disease Dataset for Fairness  Learning and Fair Identity Normalization

Abstract: Fairness in machine learning is important for societal well-being, but
limited public datasets hinder its progress. Currently, no dedicated public
medical datasets with imaging data for fairness learning are available, though
minority groups suffer from more health issues. To address this gap, we
introduce Harvard Glaucoma Fairness (Harvard-GF), a retinal nerve disease
dataset with both 2D and 3D imaging data and balanced racial groups for
glaucoma detection. Glaucoma is the leading cause of irreversible blindness
globally with Blacks having doubled glaucoma prevalence than other races. We
also propose a fair identity normalization (FIN) approach to equalize the
feature importance between different identity groups. Our FIN approach is
compared with various the-state-of-the-arts fairness learning methods with
superior performance in both racial and gender fairness tasks with 2D and 3D
imaging data, which demonstrate the utilities of our dataset Harvard-GF for
fairness learning. To facilitate fairness comparisons between different models,
we propose an equity-scaled performance measure, which can be flexibly used to
compare all kinds of performance metrics in the context of fairness. The
dataset and code are publicly accessible via this https URL
and this https URL, respectively.


------------------------------------------------------------------------------

Title:
Fit Like You Sample: Sample-Efficient Generalized Score Matching from  Fast Mixing Markov Chains

Abstract: Score matching is an approach to learning probability distributions
parametrized up to a constant of proportionality (e.g. Energy-Based Models).
The idea is to fit the score of the distribution, rather than the likelihood,
thus avoiding the need to evaluate the constant of proportionality. While
there's a clear algorithmic benefit, the statistical "cost'' can be steep:
recent work by Koehler et al. 2022 showed that for distributions that have poor
isoperimetric properties (a large Poincar\'e or log-Sobolev constant), score
matching is substantially statistically less efficient than maximum likelihood.
However, many natural realistic distributions, e.g. multimodal distributions as
simple as a mixture of two Gaussians in one dimension -- have a poor Poincar\'e
constant.
In this paper, we show a close connection between the mixing time of an
arbitrary Markov process with generator $\mathcal{L}$ and an appropriately
chosen generalized score matching loss that tries to fit $\frac{\mathcal{O}
p}{p}$. If $\mathcal{L}$ corresponds to a Markov process corresponding to a
continuous version of simulated tempering, we show the corresponding
generalized score matching loss is a Gaussian-convolution annealed score
matching loss, akin to the one proposed in Song and Ermon 2019. Moreover, we
show that if the distribution being learned is a finite mixture of Gaussians in
$d$ dimensions with a shared covariance, the sample complexity of annealed
score matching is polynomial in the ambient dimension, the diameter the means,
and the smallest and largest eigenvalues of the covariance -- obviating the
Poincar\'e constant-based lower bounds of the basic score matching loss shown
in Koehler et al. 2022. This is the first result characterizing the benefits of
annealing for score matching -- a crucial component in more sophisticated
score-based approaches like Song and Ermon 2019.


------------------------------------------------------------------------------

Title:
Language-Guided Music Recommendation for Video via Prompt Analogies

Abstract: We propose a method to recommend music for an input video while allowing a
user to guide music selection with free-form natural language. A key challenge
of this problem setting is that existing music video datasets provide the
needed (video, music) training pairs, but lack text descriptions of the music.
This work addresses this challenge with the following three contributions.
First, we propose a text-synthesis approach that relies on an analogy-based
prompting procedure to generate natural language music descriptions from a
large-scale language model (BLOOM-176B) given pre-trained music tagger outputs
and a small number of human text descriptions. Second, we use these synthesized
music descriptions to train a new trimodal model, which fuses text and video
input representations to query music samples. For training, we introduce a text
dropout regularization mechanism which we show is critical to model
performance. Our model design allows for the retrieved music audio to agree
with the two input modalities by matching visual style depicted in the video
and musical genre, mood, or instrumentation described in the natural language
query. Third, to evaluate our approach, we collect a testing dataset for our
problem by annotating a subset of 4k clips from the YT8M-MusicVideo dataset
with natural language music descriptions which we make publicly available. We
show that our approach can match or exceed the performance of prior methods on
video-to-music retrieval while significantly improving retrieval accuracy when
using text guidance.


------------------------------------------------------------------------------

Title:
Are ChatGPT and Other Similar Systems the Modern Lernaean Hydras of AI?

Abstract: The rise of Generative Artificial Intelligence systems (``AI systems'') has
created unprecedented social engagement. AI code generation systems provide
responses (output) to questions or requests by accessing the vast library of
open-source code created by developers over decades. However, they do so by
allegedly stealing the open-source code stored in virtual libraries, known as
repositories. How all this happens and whether there is a solution short of
years of litigation that can protect innovation is the focus of this article.
We also peripherally touch upon the array of issues raised by the relationship
between AI and copyright. Looking ahead, we propose the following: (a)
immediate changes to the licenses for open-source code created by developers
that will allow access and/or use of any open-source code to humans only; (b)
we suggest revisions to the Massachusetts Institute of Technology (``MIT'')
license so that AI systems procure appropriate licenses from open-source code
developers, which we believe will harmonize standards and build social
consensus for the benefit of all of humanity rather than profit-driven centers
of innovation; (c) We call for urgent legislative action to protect the future
of AI systems while also promoting innovation; and (d) we propose that there is
a shift in the burden of proof to AI systems in obfuscation cases.


------------------------------------------------------------------------------

Title:
MinMax Networks

Abstract: While much progress has been achieved over the last decades in neuro-inspired
machine learning, there are still fundamental theoretical problems in
gradient-based learning using combinations of neurons. These problems, such as
saddle points and suboptimal plateaus of the cost function, can lead in theory
and practice to failures of learning. In addition, the discrete step size
selection of the gradient is problematic since too large steps can lead to
instability and too small steps slow down the learning. This paper describes an
alternative discrete MinMax learning approach for continuous piece-wise linear
functions. Global exponential convergence of the algorithm is established using
Contraction Theory with Inequality Constraints, which is extended from the
continuous to the discrete case in this paper:
The parametrization of each linear function piece is, in contrast to deep
learning, linear in the proposed MinMax network. This allows a linear
regression stability proof as long as measurements do not transit from one
linear region to its neighbouring linear region.
The step size of the discrete gradient descent is Lagrangian limited
orthogonal to the edge of two neighbouring linear functions. It will be shown
that this Lagrangian step limitation does not decrease the convergence of the
unconstrained system dynamics in contrast to a step size limitation in the
direction of the gradient.
We show that the convergence rate of a constrained piece-wise linear function
learning is equivalent to the exponential convergence rates of the individual
local linear regions.


------------------------------------------------------------------------------

Title:
DreamHuman: Animatable 3D Avatars from Text

Abstract: We present DreamHuman, a method to generate realistic animatable 3D human
avatar models solely from textual descriptions. Recent text-to-3D methods have
made considerable strides in generation, but are still lacking in important
aspects. Control and often spatial resolution remain limited, existing methods
produce fixed rather than animated 3D human models, and anthropometric
consistency for complex structures like people remains a challenge. DreamHuman
connects large text-to-image synthesis models, neural radiance fields, and
statistical human body models in a novel modeling and optimization framework.
This makes it possible to generate dynamic 3D human avatars with high-quality
textures and learned, instance-specific, surface deformations. We demonstrate
that our method is capable to generate a wide variety of animatable, realistic
3D human models from text. Our 3D models have diverse appearance, clothing,
skin tones and body shapes, and significantly outperform both generic
text-to-3D approaches and previous text-based 3D avatar generators in visual
fidelity. For more results and animations please check our website at
this https URL


------------------------------------------------------------------------------

Title:
Matching Pairs: Attributing Fine-Tuned Models to their Pre-Trained Large  Language Models

Abstract: The wide applicability and adaptability of generative large language models
(LLMs) has enabled their rapid adoption. While the pre-trained models can
perform many tasks, such models are often fine-tuned to improve their
performance on various downstream applications. However, this leads to issues
over violation of model licenses, model theft, and copyright infringement.
Moreover, recent advances show that generative technology is capable of
producing harmful content which exacerbates the problems of accountability
within model supply chains. Thus, we need a method to investigate how a model
was trained or a piece of text was generated and what their pre-trained base
model was. In this paper we take the first step to address this open problem by
tracing back the origin of a given fine-tuned LLM to its corresponding
pre-trained base model. We consider different knowledge levels and attribution
strategies, and find that we can correctly trace back 8 out of the 10 fine
tuned models with our best method.


------------------------------------------------------------------------------

Title:
Quality and Efficiency of Manual Annotation: Pre-annotation Bias

Abstract: This paper presents an analysis of annotation using an automatic
pre-annotation for a mid-level annotation complexity task -- dependency syntax
annotation. It compares the annotation efforts made by annotators using a
pre-annotated version (with a high-accuracy parser) and those made by fully
manual annotation. The aim of the experiment is to judge the final annotation
quality when pre-annotation is used. In addition, it evaluates the effect of
automatic linguistically-based (rule-formulated) checks and another annotation
on the same data available to the annotators, and their influence on annotation
quality and efficiency. The experiment confirmed that the pre-annotation is an
efficient tool for faster manual syntactic annotation which increases the
consistency of the resulting annotation without reducing its quality.


------------------------------------------------------------------------------

Title:
Datasets and Benchmarks for Offline Safe Reinforcement Learning

Abstract: This paper presents a comprehensive benchmarking suite tailored to offline
safe reinforcement learning (RL) challenges, aiming to foster progress in the
development and evaluation of safe learning algorithms in both the training and
deployment phases. Our benchmark suite contains three packages: 1) expertly
crafted safe policies, 2) D4RL-styled datasets along with environment wrappers,
and 3) high-quality offline safe RL baseline implementations. We feature a
methodical data collection pipeline powered by advanced safe RL algorithms,
which facilitates the generation of diverse datasets across 38 popular safe RL
tasks, from robot control to autonomous driving. We further introduce an array
of data post-processing filters, capable of modifying each dataset's diversity,
thereby simulating various data collection conditions. Additionally, we provide
elegant and extensible implementations of prevalent offline safe RL algorithms
to accelerate research in this area. Through extensive experiments with over
50000 CPU and 800 GPU hours of computations, we evaluate and compare the
performance of these baseline algorithms on the collected datasets, offering
insights into their strengths, limitations, and potential areas of improvement.
Our benchmarking framework serves as a valuable resource for researchers and
practitioners, facilitating the development of more robust and reliable offline
safe RL solutions in safety-critical applications. The benchmark website is
available at \url{www.offline-saferl.org}.


------------------------------------------------------------------------------

Title:
Seeing the Pose in the Pixels: Learning Pose-Aware Representations in  Vision Transformers

Abstract: Human perception of surroundings is often guided by the various poses present
within the environment. Many computer vision tasks, such as human action
recognition and robot imitation learning, rely on pose-based entities like
human skeletons or robotic arms. However, conventional Vision Transformer (ViT)
models uniformly process all patches, neglecting valuable pose priors in input
videos. We argue that incorporating poses into RGB data is advantageous for
learning fine-grained and viewpoint-agnostic representations. Consequently, we
introduce two strategies for learning pose-aware representations in ViTs. The
first method, called Pose-aware Attention Block (PAAB), is a plug-and-play ViT
block that performs localized attention on pose regions within videos. The
second method, dubbed Pose-Aware Auxiliary Task (PAAT), presents an auxiliary
pose prediction task optimized jointly with the primary ViT task. Although
their functionalities differ, both methods succeed in learning pose-aware
representations, enhancing performance in multiple diverse downstream tasks.
Our experiments, conducted across seven datasets, reveal the efficacy of both
pose-aware methods on three video analysis tasks, with PAAT holding a slight
edge over PAAB. Both PAAT and PAAB surpass their respective backbone
Transformers by up to 9.8% in real-world action recognition and 21.8% in
multi-view robotic video alignment. Code is available at
this https URL


------------------------------------------------------------------------------

Title:
Conditional Human Sketch Synthesis with Explicit Abstraction Control

Abstract: This paper presents a novel free-hand sketch synthesis approach addressing
explicit abstraction control in class-conditional and photo-to-sketch
synthesis. Abstraction is a vital aspect of sketches, as it defines the
fundamental distinction between a sketch and an image. Previous works relied on
implicit control to achieve different levels of abstraction, leading to
inaccurate control and synthesized sketches deviating from human sketches. To
resolve this challenge, we propose two novel abstraction control mechanisms,
state embeddings and the stroke token, integrated into a transformer-based
latent diffusion model (LDM). These mechanisms explicitly provide the required
amount of points or strokes to the model, enabling accurate point-level and
stroke-level control in synthesized sketches while preserving recognizability.
Outperforming state-of-the-art approaches, our method effectively generates
diverse, non-rigid and human-like sketches. The proposed approach enables
coherent sketch synthesis and excels in representing human habits with desired
abstraction levels, highlighting the potential of sketch synthesis for
real-world applications.


------------------------------------------------------------------------------

Title:
Inroads into Autonomous Network Defence using Explained Reinforcement  Learning

Abstract: Computer network defence is a complicated task that has necessitated a high
degree of human involvement. However, with recent advancements in machine
learning, fully autonomous network defence is becoming increasingly plausible.
This paper introduces an end-to-end methodology for studying attack strategies,
designing defence agents and explaining their operation. First, using state
diagrams, we visualise adversarial behaviour to gain insight about potential
points of intervention and inform the design of our defensive models. We opt to
use a set of deep reinforcement learning agents trained on different parts of
the task and organised in a shallow hierarchy. Our evaluation shows that the
resulting design achieves a substantial performance improvement compared to
prior work. Finally, to better investigate the decision-making process of our
agents, we complete our analysis with a feature ablation and importance study.


------------------------------------------------------------------------------

Title:
Shaping digital transformation for a sustainable society --  Contributions from Bits & Bäume

Abstract: This companion book to the "Bits & B\"aume" conference 2022 compiles the
insights, work, research and opinions of more than 65 authors with a "Bits &
B\"aume" background, including practitioners, researchers and activists. The
articles demonstrate the progress made in merging "Bits" and "B\"aume" (Trees)
topics since our first publication in 2019 by addressing different sub-areas of
the intersections between digitalisation and sustainability. Encompassing a
wide range of topics, the articles delve into pressing challenges such as the
resource consumption, power implications and democratic governance of digital
infrastructures, AI, blockchains, mobile apps, and other software applications,
as well as the need to address the unsustainable practices and paradigms of
e.g., the platform economy. Offering not only transparency but also solutions,
the journal presents practical approaches and concepts related to the necessary
transformation, such as the Computer Science for Future programme. It also
contains articles commenting on current political developments, such as the EU
legislation on sustainability and freedom-related aspects of ICT devices.
Further articles highlight the power of and need for an active civil society,
aiming to inspire activism. This journal caters for everyone: Are you just
getting into the topics around Bits & B\"aume? Have you been involved in this
field for many years, or are you an expert in one of the areas touched on here?
In this journal you will find both introductory topics, such as illustrations
on the challenges of today's digitalised society, and also advanced topics,
such as conceptual and regulatory discussions. Whatever your background, we
think you'll enjoy the read, learn something new on the way, and get inspired.
Ultimately, we are all united by the overarching goal of shaping digitalisation
as part of a necessary socio-ecological change.


------------------------------------------------------------------------------

Title:
Challenges of Using Real-World Sensory Inputs for Motion Forecasting in  Autonomous Driving

Abstract: Motion forecasting plays a critical role in enabling robots to anticipate
future trajectories of surrounding agents and plan accordingly. However,
existing forecasting methods often rely on curated datasets that are not
faithful to what real-world perception pipelines can provide. In reality,
upstream modules that are responsible for detecting and tracking agents, and
those that gather road information to build the map, can introduce various
errors, including misdetections, tracking errors, and difficulties in being
accurate for distant agents and road elements. This paper aims to uncover the
challenges of bringing motion forecasting models to this more realistic setting
where inputs are provided by perception modules. In particular, we quantify the
impacts of the domain gap through extensive evaluation. Furthermore, we design
synthetic perturbations to better characterize their consequences, thus
providing insights into areas that require improvement in upstream perception
modules and guidance toward the development of more robust forecasting methods.


------------------------------------------------------------------------------

Title:
Single-Stage Visual Query Localization in Egocentric Videos

Abstract: Visual Query Localization on long-form egocentric videos requires
spatio-temporal search and localization of visually specified objects and is
vital to build episodic memory systems. Prior work develops complex multi-stage
pipelines that leverage well-established object detection and tracking methods
to perform VQL. However, each stage is independently trained and the complexity
of the pipeline results in slow inference speeds. We propose VQLoC, a novel
single-stage VQL framework that is end-to-end trainable. Our key idea is to
first build a holistic understanding of the query-video relationship and then
perform spatio-temporal localization in a single shot manner. Specifically, we
establish the query-video relationship by jointly considering query-to-frame
correspondences between the query and each video frame and frame-to-frame
correspondences between nearby video frames. Our experiments demonstrate that
our approach outperforms prior VQL methods by 20% accuracy while obtaining a
10x improvement in inference speed. VQLoC is also the top entry on the Ego4D
VQ2D challenge leaderboard. Project page: this https URL


------------------------------------------------------------------------------

Title:
Relation-Aware Diffusion Model for Controllable Poster Layout Generation

Abstract: Poster layout is a crucial aspect of poster design. Prior methods primarily
focus on the correlation between visual content and graphic elements. However,
a pleasant layout should also consider the relationship between visual and
textual contents and the relationship between elements. In this study, we
introduce a relation-aware diffusion model for poster layout generation that
incorporates these two relationships in the generation process. Firstly, we
devise a visual-textual relation-aware module that aligns the visual and
textual representations across modalities, thereby enhancing the layout's
efficacy in conveying textual information. Subsequently, we propose a geometry
relation-aware module that learns the geometry relationship between elements by
comprehensively considering contextual information. Additionally, the proposed
method can generate diverse layouts based on user constraints. To advance
research in this field, we have constructed a poster layout dataset named
CGL-Dataset V2. Our proposed method outperforms state-of-the-art methods on
CGL-Dataset V2. The data and code will be available at
this https URL


------------------------------------------------------------------------------

Title:
Relational Temporal Graph Reasoning for Dual-task Dialogue Language  Understanding

Abstract: Dual-task dialog language understanding aims to tackle two correlative dialog
language understanding tasks simultaneously via leveraging their inherent
correlations. In this paper, we put forward a new framework, whose core is
relational temporal graph reasoning.We propose a speaker-aware temporal graph
(SATG) and a dual-task relational temporal graph (DRTG) to facilitate
relational temporal modeling in dialog understanding and dual-task reasoning.
Besides, different from previous works that only achieve implicit
semantics-level interactions, we propose to model the explicit dependencies via
integrating prediction-level interactions. To implement our framework, we first
propose a novel model Dual-tAsk temporal Relational rEcurrent Reasoning network
(DARER), which first generates the context-, speaker- and temporal-sensitive
utterance representations through relational temporal modeling of SATG, then
conducts recurrent dual-task relational temporal graph reasoning on DRTG, in
which process the estimated label distributions act as key clues in
prediction-level interactions. And the relational temporal modeling in DARER is
achieved by relational convolutional networks (RGCNs). Then we further propose
Relational Temporal Transformer (ReTeFormer), which achieves fine-grained
relational temporal modeling via Relation- and Structure-aware Disentangled
Multi-head Attention. Accordingly, we propose DARER with ReTeFormer (DARER2),
which adopts two variants of ReTeFormer to achieve the relational temporal
modeling of SATG and DTRG, respectively. The extensive experiments on different
scenarios verify that our models outperform state-of-the-art models by a large
margin. Remarkably, on the dialog sentiment classification task in the Mastodon
dataset, DARER and DARER2 gain relative improvements of about 28% and 34% over
the previous best model in terms of F1.


------------------------------------------------------------------------------

Title:
Neural Fine-Tuning Search for Few-Shot Learning

Abstract: In few-shot recognition, a classifier that has been trained on one set of
classes is required to rapidly adapt and generalize to a disjoint, novel set of
classes. To that end, recent studies have shown the efficacy of fine-tuning
with carefully crafted adaptation architectures. However this raises the
question of: How can one design the optimal adaptation strategy? In this paper,
we study this question through the lens of neural architecture search (NAS).
Given a pre-trained neural network, our algorithm discovers the optimal
arrangement of adapters, which layers to keep frozen and which to fine-tune. We
demonstrate the generality of our NAS method by applying it to both residual
networks and vision transformers and report state-of-the-art performance on
Meta-Dataset and Meta-Album.


------------------------------------------------------------------------------

Title:
KoLA: Carefully Benchmarking World Knowledge of Large Language Models

Abstract: The unprecedented performance of large language models (LLMs) necessitates
improvements in evaluations. Rather than merely exploring the breadth of LLM
abilities, we believe meticulous and thoughtful designs are essential to
thorough, unbiased, and applicable evaluations. Given the importance of world
knowledge to LLMs, we construct a Knowledge-oriented LLM Assessment benchmark
(KoLA), in which we carefully design three crucial factors: (1) For ability
modeling, we mimic human cognition to form a four-level taxonomy of
knowledge-related abilities, covering $19$ tasks. (2) For data, to ensure fair
comparisons, we use both Wikipedia, a corpus prevalently pre-trained by LLMs,
along with continuously collected emerging corpora, aiming to evaluate the
capacity to handle unseen data and evolving knowledge. (3) For evaluation
criteria, we adopt a contrastive system, including overall standard scores for
better numerical comparability across tasks and models and a unique
self-contrast metric for automatically evaluating knowledge hallucination. We
evaluate $21$ open-source and commercial LLMs and obtain some intriguing
findings. The KoLA dataset and open-participation leaderboard are publicly
released at this https URL and will be continuously updated to provide
references for developing LLMs and knowledge-related systems.


------------------------------------------------------------------------------

Title:
DaMuEL: A Large Multilingual Dataset for Entity Linking

Abstract: We present DaMuEL, a large Multilingual Dataset for Entity Linking containing
data in 53 languages. DaMuEL consists of two components: a knowledge base that
contains language-agnostic information about entities, including their claims
from Wikidata and named entity types (PER, ORG, LOC, EVENT, BRAND, WORK_OF_ART,
MANUFACTURED); and Wikipedia texts with entity mentions linked to the knowledge
base, along with language-specific text from Wikidata such as labels, aliases,
and descriptions, stored separately for each language. The Wikidata QID is used
as a persistent, language-agnostic identifier, enabling the combination of the
knowledge base with language-specific texts and information for each entity.
Wikipedia documents deliberately annotate only a single mention for every
entity present; we further automatically detect all mentions of named entities
linked from each document. The dataset contains 27.9M named entities in the
knowledge base and 12.3G tokens from Wikipedia texts. The dataset is published
under the CC BY-SA license at this https URL


------------------------------------------------------------------------------

Title:
Knowledge Guided Representation Learning and Causal Structure Learning  in Soil Science

Abstract: An improved understanding of soil can enable more sustainable land-use
practices. Nevertheless, soil is called a complex, living medium due to the
complex interaction of different soil processes that limit our understanding of
soil. Process-based models and analyzing observed data provide two avenues for
improving our understanding of soil processes. Collecting observed data is
cost-prohibitive but reflects real-world behavior, while process-based models
can be used to generate ample synthetic data which may not be representative of
reality. We propose a framework, knowledge-guided representation learning, and
causal structure learning (KGRCL), to accelerate scientific discoveries in soil
science. The framework improves representation learning for simulated soil
processes via conditional distribution matching with observed soil processes.
Simultaneously, the framework leverages both observed and simulated data to
learn a causal structure among the soil processes. The learned causal graph is
more representative of ground truth than other graphs generated from other
causal discovery methods. Furthermore, the learned causal graph is leveraged in
a supervised learning setup to predict the impact of fertilizer use and
changing weather on soil carbon. We present the results in five different
locations to show the improvement in the prediction performance in
out-of-sample and few-shots setting.


------------------------------------------------------------------------------

Title:
Crowd-Powered Photo Enhancement Featuring an Active Learning Based Local  Filter

Abstract: In this study, we address local photo enhancement to improve the aesthetic
quality of an input image by applying different effects to different regions.
Existing photo enhancement methods are either not content-aware or not local;
therefore, we propose a crowd-powered local enhancement method for
content-aware local enhancement, which is achieved by asking crowd workers to
locally optimize parameters for image editing functions. To make it easier to
locally optimize the parameters, we propose an active learning based local
filter. The parameters need to be determined at only a few key pixels selected
by an active learning method, and the parameters at the other pixels are
automatically predicted using a regression model. The parameters at the
selected key pixels are independently optimized, breaking down the optimization
problem into a sequence of single-slider adjustments. Our experiments show that
the proposed filter outperforms existing filters, and our enhanced results are
more visually pleasing than the results by the existing enhancement methods.
Our source code and results are available at
this https URL


------------------------------------------------------------------------------

Title:
Undetectable Watermarks for Language Models

Abstract: Recent advances in the capabilities of large language models such as GPT-4
have spurred increasing concern about our ability to detect AI-generated text.
Prior works have suggested methods of embedding watermarks in model outputs, by
noticeably altering the output distribution. We ask: Is it possible to
introduce a watermark without incurring any detectable change to the output
distribution?
To this end we introduce a cryptographically-inspired notion of undetectable
watermarks for language models. That is, watermarks can be detected only with
the knowledge of a secret key; without the secret key, it is computationally
intractable to distinguish watermarked outputs from those of the original
model. In particular, it is impossible for a user to observe any degradation in
the quality of the text. Crucially, watermarks should remain undetectable even
when the user is allowed to adaptively query the model with arbitrarily chosen
prompts. We construct undetectable watermarks based on the existence of one-way
functions, a standard assumption in cryptography.


------------------------------------------------------------------------------

Title:
ANOVA approximation with mixed tensor product basis on scattered points

Abstract: In this paper we consider an orthonormal basis, generated by a tensor product
of Fourier basis functions, half period cosine basis functions, and the
Chebyshev basis functions. We deal with the approximation problem in high
dimensions related to this basis and design a fast algorithm to multiply with
the underlying matrix, consisting of rows of the non-equidistant Fourier
matrix, the non-equidistant cosine matrix and the non-equidistant Chebyshev
matrix, and its transposed. This leads us to an ANOVA (analysis of variance)
decomposition for functions with partially periodic boundary conditions through
using the Fourier basis in some dimensions and the half period cosine basis or
the Chebyshev basis in others. We consider sensitivity analysis in this
setting, in order to find an adapted basis for the underlying approximation
problem. More precisely, we find the underlying index set of the
multidimensional series expansion. Additionally, we test this ANOVA
approximation with mixed basis at numerical experiments, and refer to the
advantage of interpretable results.


------------------------------------------------------------------------------

Title:
Enlarged Large Margin Loss for Imbalanced Classification

Abstract: We propose a novel loss function for imbalanced classification. LDAM loss,
which minimizes a margin-based generalization bound, is widely utilized for
class-imbalanced image classification. Although, by using LDAM loss, it is
possible to obtain large margins for the minority classes and small margins for
the majority classes, the relevance to a large margin, which is included in the
original softmax cross entropy loss, is not be clarified yet. In this study, we
reconvert the formula of LDAM loss using the concept of the large margin
softmax cross entropy loss based on the softplus function and confirm that LDAM
loss includes a wider large margin than softmax cross entropy loss.
Furthermore, we propose a novel Enlarged Large Margin (ELM) loss, which can
further widen the large margin of LDAM loss. ELM loss utilizes the large margin
for the maximum logit of the incorrect class in addition to the basic margin
used in LDAM loss. Through experiments conducted on imbalanced CIFAR datasets
and large-scale datasets with long-tailed distribution, we confirmed that
classification accuracy was much improved compared with LDAM loss and
conventional losses for imbalanced classification.


------------------------------------------------------------------------------

Title:
Bayesian Game Formulation of Power Allocation in Multiple Access Wiretap  Channel with Incomplete CSI

Abstract: In this paper, we address the problem of distributed power allocation in a
$K$ user fading multiple access wiretap channel, where global channel state
information is limited, i.e., each user has knowledge of their own channel
state with respect to Bob and Eve but only knows the distribution of other
users' channel states. We model this problem as a Bayesian game, where each
user is assumed to selfishly maximize his average \emph{secrecy capacity} with
partial channel state information. In this work, we first prove that there is a
unique Bayesian equilibrium in the proposed game. Additionally, the price of
anarchy is calculated to measure the efficiency of the equilibrium solution. We
also propose a fast convergent iterative algorithm for power allocation.
Finally, the results are validated using simulation results.


------------------------------------------------------------------------------

Title:
Probabilistic Learning of Multivariate Time Series with Temporal  Irregularity

Abstract: Multivariate sequential data collected in practice often exhibit temporal
irregularities, including nonuniform time intervals and component misalignment.
However, if uneven spacing and asynchrony are endogenous characteristics of the
data rather than a result of insufficient observation, the information content
of these irregularities plays a defining role in characterizing the
multivariate dependence structure. Existing approaches for probabilistic
forecasting either overlook the resulting statistical heterogeneities, are
susceptible to imputation biases, or impose parametric assumptions on the data
distribution. This paper proposes an end-to-end solution that overcomes these
limitations by allowing the observation arrival times to play the central role
of model construction, which is at the core of temporal irregularities. To
acknowledge temporal irregularities, we first enable unique hidden states for
components so that the arrival times can dictate when, how, and which hidden
states to update. We then develop a conditional flow representation to
non-parametrically represent the data distribution, which is typically
non-Gaussian, and supervise this representation by carefully factorizing the
log-likelihood objective to select conditional information that facilitates
capturing time variation and path dependency. The broad applicability and
superiority of the proposed solution are confirmed by comparing it with
existing approaches through ablation studies and testing on real-world
datasets.


------------------------------------------------------------------------------

Title:
Encyclopedic VQA: Visual questions about detailed properties of  fine-grained categories

Abstract: We propose Encyclopedic-VQA, a large scale visual question answering (VQA)
dataset featuring visual questions about detailed properties of fine-grained
categories and instances. It contains 221k unique question+answer pairs each
matched with (up to) 5 images, resulting in a total of 1M VQA samples.
Moreover, our dataset comes with a controlled knowledge base derived from
Wikipedia, marking the evidence to support each answer. Empirically, we show
that our dataset poses a hard challenge for large vision+language models as
they perform poorly on our dataset: PaLI [14] is state-of-the-art on OK-VQA
[37], yet it only achieves 13.0% accuracy on our dataset. Moreover, we
experimentally show that progress on answering our encyclopedic questions can
be achieved by augmenting large models with a mechanism that retrieves relevant
information from the knowledge base. An oracle experiment with perfect
retrieval achieves 87.0% accuracy on the single-hop portion of our dataset, and
an automatic retrieval-augmented prototype yields 48.8%. We believe that our
dataset enables future research on retrieval-augmented vision+language models.


------------------------------------------------------------------------------

Title:
Temporally-Extended Prompts Optimization for SAM in Interactive Medical  Image Segmentation

Abstract: The Segmentation Anything Model (SAM) has recently emerged as a foundation
model for addressing image segmentation. Owing to the intrinsic complexity of
medical images and the high annotation cost, the medical image segmentation
(MIS) community has been encouraged to investigate SAM's zero-shot capabilities
to facilitate automatic annotation. Inspired by the extraordinary
accomplishments of interactive medical image segmentation (IMIS) paradigm, this
paper focuses on assessing the potential of SAM's zero-shot capabilities within
the IMIS paradigm to amplify its benefits in the MIS domain. Regrettably, we
observe that SAM's vulnerability to prompt forms (e.g., points, bounding boxes)
becomes notably pronounced in IMIS. This leads us to develop a framework that
adaptively offers suitable prompt forms for human experts. We refer to the
framework above as temporally-extended prompts optimization (TEPO) and model it
as a Markov decision process, solvable through reinforcement learning.
Numerical experiments on the standardized benchmark BraTS2020 demonstrate that
the learned TEPO agent can further enhance SAM's zero-shot capability in the
MIS context.


------------------------------------------------------------------------------

Title:
Training Diffusion Classifiers with Denoising Assistance

Abstract: Score-matching and diffusion models have emerged as state-of-the-art
generative models for both conditional and unconditional generation.
Classifier-guided diffusion models are created by training a classifier on
samples obtained from the forward-diffusion process (i.e., from data to noise).
In this paper, we propose denoising-assisted (DA) classifiers wherein the
diffusion classifier is trained using both noisy and denoised examples as
simultaneous inputs to the model. We differentiate between denoising-assisted
(DA) classifiers and noisy classifiers, which are diffusion classifiers that
are only trained on noisy examples. Our experiments on Cifar10 and Imagenet
show that DA-classifiers improve over noisy classifiers both quantitatively in
terms of generalization to test data and qualitatively in terms of
perceptually-aligned classifier-gradients and generative modeling metrics.
Finally, we describe a semi-supervised framework for training diffusion
classifiers and our experiments, that also include positive-unlabeled settings,
demonstrate improved generalization of DA-classifiers over noisy classifiers.


------------------------------------------------------------------------------

Title:
Action Sensitivity Learning for the Ego4D Episodic Memory Challenge 2023

Abstract: This report presents ReLER submission to two tracks in the Ego4D Episodic
Memory Benchmark in CVPR 2023, including Natural Language Queries and Moment
Queries. This solution inherits from our proposed Action Sensitivity Learning
framework (ASL) to better capture discrepant information of frames. Further, we
incorporate a series of stronger video features and fusion strategies. Our
method achieves an average mAP of 29.34, ranking 1st in Moment Queries
Challenge, and garners 19.79 mean R1, ranking 2nd in Natural Language Queries
Challenge. Our code will be released.


------------------------------------------------------------------------------

Title:
Your Room is not Private: Gradient Inversion Attack for Deep Q-Learning

Abstract: The prominence of embodied Artificial Intelligence (AI), which empowers
robots to navigate, perceive, and engage within virtual environments, has
attracted significant attention, owing to the remarkable advancements in
computer vision and large language models. Privacy emerges as a pivotal concern
within the realm of embodied AI, as the robot access substantial personal
information. However, the issue of privacy leakage in embodied AI tasks,
particularly in relation to decision-making algorithms, has not received
adequate consideration in research. This paper aims to address this gap by
proposing an attack on the Deep Q-Learning algorithm, utilizing gradient
inversion to reconstruct states, actions, and Q-values. The choice of using
gradients for the attack is motivated by the fact that commonly employed
federated learning techniques solely utilize gradients computed based on
private user data to optimize models, without storing or transmitting the data
to public servers. Nevertheless, these gradients contain sufficient information
to potentially expose private data. To validate our approach, we conduct
experiments on the AI2THOR simulator and evaluate our algorithm on active
perception, a prevalent task in embodied AI. The experimental results
convincingly demonstrate the effectiveness of our method in successfully
recovering all information from the data across all 120 room layouts.


------------------------------------------------------------------------------

Title:
Statutory Professions in AI governance and their consequences for  explainable AI

Abstract: Intentional and accidental harms arising from the use of AI have impacted the
health, safety and rights of individuals. While regulatory frameworks are being
developed, there remains a lack of consensus on methods necessary to deliver
safe AI. The potential for explainable AI (XAI) to contribute to the
effectiveness of the regulation of AI is being increasingly examined.
Regulation must include methods to ensure compliance on an ongoing basis,
though there is an absence of practical proposals on how to achieve this. For
XAI to be successfully incorporated into a regulatory system, the individuals
who are engaged in interpreting/explaining the model to stakeholders should be
sufficiently qualified for the role. Statutory professionals are prevalent in
domains in which harm can be done to the health, safety and rights of
individuals. The most obvious examples are doctors, engineers and lawyers.
Those professionals are required to exercise skill and judgement and to defend
their decision making process in the event of harm occurring. We propose that a
statutory profession framework be introduced as a necessary part of the AI
regulatory framework for compliance and monitoring purposes. We will refer to
this new statutory professional as an AI Architect (AIA). This AIA would be
responsible to ensure the risk of harm is minimised and accountable in the
event that harms occur. The AIA would also be relied on to provide appropriate
interpretations/explanations of XAI models to stakeholders. Further, in order
to satisfy themselves that the models have been developed in a satisfactory
manner, the AIA would require models to have appropriate transparency.
Therefore it is likely that the introduction of an AIA system would lead to an
increase in the use of XAI to enable AIA to discharge their professional
obligations.


------------------------------------------------------------------------------

Title:
Estimating Generic 3D Room Structures from 2D Annotations

Abstract: Indoor rooms are among the most common use cases in 3D scene understanding.
Current state-of-the-art methods for this task are driven by large annotated
datasets. Room layouts are especially important, consisting of structural
elements in 3D, such as wall, floor, and ceiling. However, they are difficult
to annotate, especially on pure RGB video. We propose a novel method to produce
generic 3D room layouts just from 2D segmentation masks, which are easy to
annotate for humans. Based on these 2D annotations, we automatically
reconstruct 3D plane equations for the structural elements and their spatial
extent in the scene, and connect adjacent elements at the appropriate contact
edges. We annotate and publicly release 2266 3D room layouts on the
RealEstate10k dataset, containing YouTube videos. We demonstrate the high
quality of these 3D layouts annotations with extensive experiments.


------------------------------------------------------------------------------

Title:
Image encryption for Offshore wind power based on 2D-LCLM and Zhou Yi  Eight Trigrams

Abstract: Offshore wind power is an important part of the new power system, due to the
complex and changing situation at ocean, its normal operation and maintenance
cannot be done without information such as images, therefore, it is especially
important to transmit the correct image in the process of information
transmission. In this paper, we propose a new encryption algorithm for offshore
wind power based on two-dimensional lagged complex logistic mapping (2D-LCLM)
and Zhou Yi Eight Trigrams. Firstly, the initial value of the 2D-LCLM is
constructed by the Sha-256 to associate the 2D-LCLM with the plaintext.
Secondly, a new encryption rule is proposed from the Zhou Yi Eight Trigrams to
obfuscate the pixel values and generate the round key. Then, 2D-LCLM is
combined with the Zigzag to form an S-box. Finally, the simulation experiment
of the algorithm is accomplished. The experimental results demonstrate that the
algorithm can resistant common attacks and has prefect encryption performance.


------------------------------------------------------------------------------

Title:
Combinatorial Pure Exploration of Multi-Armed Bandit with a Real Number  Action Class

Abstract: The combinatorial pure exploration (CPE) in the stochastic multi-armed bandit
setting (MAB) is a well-studied online decision-making problem: A player wants
to find the optimal \emph{action} $\boldsymbol{\pi}^*$ from \emph{action class}
$\mathcal{A}$, which is a collection of subsets of arms with certain
combinatorial structures. Though CPE can represent many combinatorial
structures such as paths, matching, and spanning trees, most existing works
focus only on binary action class $\mathcal{A}\subseteq\{0, 1\}^d$ for some
positive integer $d$. This binary formulation excludes important problems such
as the optimal transport, knapsack, and production planning problems. To
overcome this limitation, we extend the binary formulation to real,
$\mathcal{A}\subseteq\mathbb{R}^d$, and propose a new algorithm. The only
assumption we make is that the number of actions in $\mathcal{A}$ is polynomial
in $d$. We show an upper bound of the sample complexity for our algorithm and
the action class-dependent lower bound for R-CPE-MAB, by introducing a quantity
that characterizes the problem's difficulty, which is a generalization of the
notion \emph{width} introduced in Chen et al.[2014].


------------------------------------------------------------------------------

Title:
ATLAS: Automatically Detecting Discrepancies Between Privacy Policies  and Privacy Labels

Abstract: Privacy policies are long, complex documents that end-users seldom read.
Privacy labels aim to ameliorate these issues by providing succinct summaries
of salient data practices. In December 2020, Apple began requiring that app
developers submit privacy labels describing their apps' data practices. Yet,
research suggests that app developers often struggle to do so. In this paper,
we automatically identify possible discrepancies between mobile app privacy
policies and their privacy labels. Such discrepancies could be indicators of
potential privacy compliance issues.
We introduce the Automated Privacy Label Analysis System (ATLAS). ATLAS
includes three components: a pipeline to systematically retrieve iOS App Store
listings and privacy policies; an ensemble-based classifier capable of
predicting privacy labels from the text of privacy policies with 91.3% accuracy
using state-of-the-art NLP techniques; and a discrepancy analysis mechanism
that enables a large-scale privacy analysis of the iOS App Store.
Our system has enabled us to analyze 354,725 iOS apps. We find several
interesting trends. For example, only 40.3% of apps in the App Store provide
easily accessible privacy policies, and only 29.6% of apps provide both
accessible privacy policies and privacy labels. Among apps that provide both,
88.0% have at least one possible discrepancy between the text of their privacy
policy and their privacy label, which could be indicative of a potential
compliance issue. We find that, on average, apps have 5.32 such potential
compliance issues.
We hope that ATLAS will help app developers, researchers, regulators, and
mobile app stores alike. For example, app developers could use our classifier
to check for discrepancies between their privacy policies and privacy labels,
and regulators could use our system to help review apps at scale for potential
compliance issues.


------------------------------------------------------------------------------

Title:
E-Calib: A Fast, Robust and Accurate Calibration Toolbox for Event  Cameras

Abstract: Event cameras triggered a paradigm shift in the computer vision community
delineated by their asynchronous nature, low latency, and high dynamic range.
Calibration of event cameras is always essential to account for the sensor
intrinsic parameters and for 3D perception. However, conventional image-based
calibration techniques are not applicable due to the asynchronous, binary
output of the sensor. The current standard for calibrating event cameras relies
on either blinking patterns or event-based image reconstruction algorithms.
These approaches are difficult to deploy in factory settings and are affected
by noise and artifacts degrading the calibration performance. To bridge these
limitations, we present E-Calib, a novel, fast, robust, and accurate
calibration toolbox for event cameras utilizing the asymmetric circle grid, for
its robustness to out-of-focus scenes. The proposed method is tested in a
variety of rigorous experiments for different event camera models, on circle
grids with different geometric properties, and under challenging illumination
conditions. The results show that our approach outperforms the state-of-the-art
in detection success rate, reprojection error, and estimation accuracy of
extrinsic parameters.


------------------------------------------------------------------------------

Title:
Transferring Knowledge for Food Image Segmentation using Transformers  and Convolutions

Abstract: Food image segmentation is an important task that has ubiquitous
applications, such as estimating the nutritional value of a plate of food.
Although machine learning models have been used for segmentation in this
domain, food images pose several challenges. One challenge is that food items
can overlap and mix, making them difficult to distinguish. Another challenge is
the degree of inter-class similarity and intra-class variability, which is
caused by the varying preparation methods and dishes a food item may be served
in. Additionally, class imbalance is an inevitable issue in food datasets. To
address these issues, two models are trained and compared, one based on
convolutional neural networks and the other on Bidirectional Encoder
representation for Image Transformers (BEiT). The models are trained and
valuated using the FoodSeg103 dataset, which is identified as a robust
benchmark for food image segmentation. The BEiT model outperforms the previous
state-of-the-art model by achieving a mean intersection over union of 49.4 on
FoodSeg103. This study provides insights into transfering knowledge using
convolution and Transformer-based approaches in the food image domain.


------------------------------------------------------------------------------

Title:
How are the people in the photos judged? Analysis of brain activity when  assessing levels of trust and attractiveness

Abstract: Trust is the foundation of every area of life. Without it, it is difficult to
build lasting relationships. Unfortunately, in recent years, trust has been
severely damaged by the spread of fake news and disinformation, which has
become a serious social problem. In addition to trust, the factor influencing
interpersonal relationships is perceived attractiveness, which is currently
created to a large extent by digital media. Understanding the principles of
judging others can be helpful in fighting prejudice and rebuilding trust in
society. One way to learn about people's choices is to record their brain
activity as they make choices. The article presents an experiment in which the
faces of different people were presented, and the participants' task was to
assess how much they can trust a given person and how attractive they are.
During the study, the EEG signal was recorded, which was used to build models
of logistic regression classifiers. In addition, the most active areas of the
brain that participate in the assessment of trust and attractiveness of the
face were indicated.


------------------------------------------------------------------------------

Title:
Coding for the Gaussian Channel in the Finite Blocklength Regime Using a  CNN-Autoencoder

Abstract: The development of delay-sensitive applications that require ultra high
reliability created an additional challenge for wireless networks. This led to
Ultra-Reliable Low-Latency Communications, as a use case that 5G and beyond 5G
systems must support. However, supporting low latency communications requires
the use of short codes, while attaining vanishing frame error probability (FEP)
requires long codes. Thus, developing codes for the finite blocklength regime
(FBR) achieving certain reliability requirements is necessary. This paper
investigates the potential of Convolutional Neural Networks autoencoders
(CNN-AE) in approaching the theoretical maximum achievable rate over a Gaussian
channel for a range of signal-to-noise ratios at a fixed blocklength and target
FEP, which is a different perspective compared to existing works that explore
the use of CNNs from bit-error and symbol-error rate perspectives. We explain
the studied CNN-AE architecture, evaluate it numerically, and compare it to the
theoretical maximum achievable rate and the achievable rates of polar coded
quadrature amplitude modulation (QAM), Reed-Muller coded QAM, multilevel polar
coded modulation, and a TurboAE-MOD scheme from the literature. Numerical
results show that the CNN-AE outperforms these benchmark schemes and approaches
the theoretical maximum rate, demonstrating the capability of CNN-AEs in
learning good codes for delay-constrained applications.


------------------------------------------------------------------------------

Title:
Mitigating Cold-start Forecasting using Cold Causal Demand Forecasting  Model

Abstract: Forecasting multivariate time series data, which involves predicting future
values of variables over time using historical data, has significant practical
applications. Although deep learning-based models have shown promise in this
field, they often fail to capture the causal relationship between dependent
variables, leading to less accurate forecasts. Additionally, these models
cannot handle the cold-start problem in time series data, where certain
variables lack historical data, posing challenges in identifying dependencies
among variables. To address these limitations, we introduce the Cold Causal
Demand Forecasting (CDF-cold) framework that integrates causal inference with
deep learning-based models to enhance the forecasting accuracy of multivariate
time series data affected by the cold-start problem. To validate the
effectiveness of the proposed approach, we collect 15 multivariate time-series
datasets containing the network traffic of different Google data centers. Our
experiments demonstrate that the CDF-cold framework outperforms
state-of-the-art forecasting models in predicting future values of multivariate
time series data.


------------------------------------------------------------------------------

Title:
Opportunities for Large Language Models and Discourse in Engineering  Design

Abstract: In recent years, large language models have achieved breakthroughs on a wide
range of benchmarks in natural language processing and continue to increase in
performance. Recently, the advances of large language models have raised
interest outside the natural language processing community and could have a
large impact on daily life. In this paper, we pose the question: How will large
language models and other foundation models shape the future product
development process? We provide the reader with an overview of the subject by
summarizing both recent advances in natural language processing and the use of
information technology in the engineering design process. We argue that
discourse should be regarded as the core of engineering design processes, and
therefore should be represented in a digital artifact. On this basis, we
describe how foundation models such as large language models could contribute
to the design discourse by automating parts thereof that involve creativity and
reasoning, and were previously reserved for humans. We describe how
simulations, experiments, topology optimizations, and other process steps can
be integrated into a machine-actionable, discourse-centric design process.
Finally, we outline the future research that will be necessary for the
implementation of the conceptualized framework.


------------------------------------------------------------------------------

Title:
A Learning Assisted Method for Uncovering Power Grid Generation and  Distribution System Vulnerabilities

Abstract: Intelligent attackers can suitably tamper sensor/actuator data at various
Smart grid surfaces causing intentional power oscillations, which if left
undetected, can lead to voltage disruptions. We develop a novel combination of
formal methods and machine learning tools that learns power system dynamics
with the objective of generating unsafe yet stealthy false data based attack
sequences. We enable the grid with anomaly detectors in a generalized manner so
that it is difficult for an attacker to remain undetected. Our methodology,
when applied on an IEEE 14 bus power grid model, uncovers stealthy attack
vectors even in presence of such detectors.


------------------------------------------------------------------------------

Title:
Rolling control and dynamics model of two section articulated-wing  ornithopter

Abstract: This paper invented a new rolling control mechanism of two section
articulated-wing ornithopter, which is analogues to aileron control in plane,
however, similar control mechanism leads to opposite result, indicating the
ornithopter supposed to go left now go right instead. This research gives a
qualitative dynamics model which explains this new phenomenon. Because of wing
folding, the differential rotation of outer-section wing (analogues to aileron
in plane, left aileron up and right aileron down make left turn) around pitch
axis becomes common mode rotation around yaw axis,leading its rotating torque
changing from left-handed rotation (using left-handed as example, right-handed
is the same) around roll axis to a common mode force pointing to front-right
(northeast, NE) direction from first player's view of the ornithopter.Because
most of the flapping movement is in the upper hemisphere from ornithopter's
view, the NE force is above on the center of mass of the orthopter, generating
a right-handed moment around roll axis. Therefore, the ornithopter supposed to
go left now goes right. This phenomenon is a unique and only observed in two
section articulated-wing ornithopter by far. Many field tests conducted by
authors confirm it is highly repetitive.


------------------------------------------------------------------------------

Title:
The Split Matters: Flat Minima Methods for Improving the Performance of  GNNs

Abstract: When training a Neural Network, it is optimized using the available training
data with the hope that it generalizes well to new or unseen testing data. At
the same absolute value, a flat minimum in the loss landscape is presumed to
generalize better than a sharp minimum. Methods for determining flat minima
have been mostly researched for independent and identically distributed (i. i.
d.) data such as images. Graphs are inherently non-i. i. d. since the vertices
are edge-connected. We investigate flat minima methods and combinations of
those methods for training graph neural networks (GNNs). We use GCN and GAT as
well as extend Graph-MLP to work with more layers and larger graphs. We conduct
experiments on small and large citation, co-purchase, and protein datasets with
different train-test splits in both the transductive and inductive training
procedure. Results show that flat minima methods can improve the performance of
GNN models by over 2 points, if the train-test split is randomized. Following
Shchur et al., randomized splits are essential for a fair evaluation of GNNs,
as other (fixed) splits like 'Planetoid' are biased. Overall, we provide
important insights for improving and fairly evaluating flat minima methods on
GNNs. We recommend practitioners to always use weight averaging techniques, in
particular EWA when using early stopping. While weight averaging techniques are
only sometimes the best performing method, they are less sensitive to
hyperparameters, need no additional training, and keep the original model
unchanged. All source code is available in
this https URL


------------------------------------------------------------------------------

Title:
Propagating Knowledge Updates to LMs Through Distillation

Abstract: Modern language models have the capacity to store and use immense amounts of
knowledge about real-world entities, but it remains unclear how to update their
implicit "knowledge bases.'' While prior methods for updating knowledge in LMs
successfully inject facts, updated LMs then fail to make inferences based on
these injected facts. In this work, we demonstrate that a context
distillation-based approach can both impart knowledge about entities and
propagate that knowledge to enable broader inferences. Our approach consists of
two stages: transfer set generation and distillation on the transfer set. We
first generate a transfer set by simply prompting a language model to generate
a continuation from the entity definition. Then, we update the model parameters
so that the distribution of the LM (the student) matches the distribution of
the LM conditioned on the definition (the teacher) on the transfer set. Our
experiments demonstrate that this approach is more effective in propagating
knowledge updates compared to fine-tuning and other gradient-based
knowledge-editing methods without compromising performance in other contexts,
even when injecting the definitions of up to 150 entities at once.


------------------------------------------------------------------------------

Title:
Can Language Models Teach Weaker Agents? Teacher Explanations Improve  Students via Theory of Mind

Abstract: Large Language Models (LLMs) perform complex reasoning by generating
explanations for their predictions. However, a complementary goal of
explanations is to also communicate useful knowledge that improves weaker
agents. Hence, we investigate whether LLMs also make good teachers for weaker
agents. In particular, we consider a student-teacher framework between two LLM
agents and study if, when, and how the teacher should intervene with natural
language explanations to improve the student's performance. Since communication
is expensive, we define a budget such that the teacher only communicates
explanations for a fraction of the data, after which the student should perform
well on its own. We decompose the teaching problem along four axes: (1) if
teacher's test time intervention improve student predictions, (2) when it is
worth explaining a data point, (3) how the teacher should personalize
explanations to better teach the student, and (4) if teacher explanations also
improve student performance on future unexplained data. We first show that
teacher LLMs can indeed intervene on student reasoning to improve their
performance. Next, we propose a Theory of Mind approach, in which the teacher
builds two few-shot mental models of the student. The first model defines an
Intervention Function that simulates the utility of an intervention, allowing
the teacher to intervene when this utility is the highest and improving student
performance at lower budgets. The second model enables the teacher to
personalize explanations for a particular student and outperform unpersonalized
teachers. We also demonstrate that in multi-turn interactions, teacher
explanations generalize and learning from explained data improves student
performance on future unexplained data. Finally, we also verify that misaligned
teachers can lower student performance to random chance by intentionally
misleading them.


------------------------------------------------------------------------------

Title:
Generalizable Resource Scaling of 5G Slices using Constrained  Reinforcement Learning

Abstract: Network slicing is a key enabler for 5G to support various applications.
Slices requested by service providers (SPs) have heterogeneous quality of
service (QoS) requirements, such as latency, throughput, and jitter. It is
imperative that the 5G infrastructure provider (InP) allocates the right amount
of resources depending on the slice's traffic, such that the specified QoS
levels are maintained during the slice's lifetime while maximizing resource
efficiency. However, there is a non-trivial relationship between the QoS and
resource allocation. In this paper, this relationship is learned using a
regression-based model. We also leverage a risk-constrained reinforcement
learning agent that is trained offline using this model and domain
randomization for dynamically scaling slice resources while maintaining the
desired QoS level. Our novel approach reduces the effects of network modeling
errors since it is model-free and does not require QoS metrics to be
mathematically formulated in terms of traffic. In addition, it provides
robustness against uncertain network conditions, generalizes to different
real-world traffic patterns, and caters to various QoS metrics. The results
show that the state-of-the-art approaches can lead to QoS degradation as high
as 44.5% when tested on previously unseen traffic. On the other hand, our
approach maintains the QoS degradation below a preset 10% threshold on such
traffic, while minimizing the allocated resources. Additionally, we demonstrate
that the proposed approach is robust against varying network conditions and
inaccurate traffic predictions.


------------------------------------------------------------------------------

Title:
A Survey of Some Density Based Clustering Techniques

Abstract: Density Based Clustering are a type of Clustering methods using in data
mining for extracting previously unknown patterns from data sets. There are a
number of density based clustering methods such as DBSCAN, OPTICS, DENCLUE,
VDBSCAN, DVBSCAN, DBCLASD and ST-DBSCAN. In this paper, a study of these
methods is done along with their characteristics, advantages and disadvantages
and most importantly, their applicability to different types of data sets to
mine useful and appropriate patterns.


------------------------------------------------------------------------------

Title:
Design and performance of a space-time virtual element method for the  heat equation on prismatic meshes

Abstract: We present a space-time virtual element method for the discretization of the
heat equation, which is defined on general prismatic meshes and variable
degrees of accuracy. Strategies to handle efficiently the space-time mesh
structure are discussed. We perform convergence tests for the $h$- and
$hp$-versions of the method in case of smooth and singular solutions, and test
space-time adaptive mesh refinements driven by a residual-type error indicator.


------------------------------------------------------------------------------

Title:
MuRS: Mutant Ranking and Suppression using Identifier Templates

Abstract: Diff-based mutation testing is a mutation testing approach that only mutates
lines affected by a code change under review. Google's mutation testing service
integrates diff-based mutation testing into the code review process and
continuously gathers developer feedback on mutants surfaced during code review.
To enhance the developer experience, the mutation testing service implements a
number of suppression rules, which target not-useful mutants-that is, mutants
that have consistently received negative developer feedback. However, while
effective, manually implementing suppression rules require significant
engineering time. An automatic system to rank and suppress mutants would
facilitate the maintenance of the mutation testing service. This paper proposes
and evaluates MuRS, an automated approach that groups mutants by patterns in
the source code under test and uses these patterns to rank and suppress future
mutants based on historical developer feedback on mutants in the same group. To
evaluate MuRS, we conducted an A/B testing study, comparing MuRS to the
existing mutation testing service. Despite the strong baseline, which uses
manually developed suppression rules, the results show a statistically
significantly lower negative feedback ratio of 11.45% for MuRS versus 12.41%
for the baseline. The results also show that MuRS is able to recover existing
suppression rules implemented in the baseline. Finally, the results show that
statement-deletion mutant groups received both the most positive and negative
developer feedback, suggesting a need for additional context that can
distinguish between useful and not-useful mutants in these groups. Overall,
MuRS has the potential to substantially reduce the development and maintenance
cost for an effective mutation testing service by automatically learning
suppression rules.


------------------------------------------------------------------------------

Title:
Contrast, Stylize and Adapt: Unsupervised Contrastive Learning Framework  for Domain Adaptive Semantic Segmentation

Abstract: To overcome the domain gap between synthetic and real-world datasets,
unsupervised domain adaptation methods have been proposed for semantic
segmentation. Majority of the previous approaches have attempted to reduce the
gap either at the pixel or feature level, disregarding the fact that the two
components interact positively. To address this, we present CONtrastive FEaTure
and pIxel alignment (CONFETI) for bridging the domain gap at both the pixel and
feature levels using a unique contrastive formulation. We introduce
well-estimated prototypes by including category-wise cross-domain information
to link the two alignments: the pixel-level alignment is achieved using the
jointly trained style transfer module with the prototypical semantic
consistency, while the feature-level alignment is enforced to cross-domain
features with the \textbf{pixel-to-prototype contrast}. Our extensive
experiments demonstrate that our method outperforms existing state-of-the-art
methods using DeepLabV2. Our code is available at
this https URL


------------------------------------------------------------------------------

Title:
Fast and Examination-agnostic Reciprocal Recommendation in Matching  Markets

Abstract: In matching markets such as job posting and online dating platforms, the
recommender system plays a critical role in the success of the platform. Unlike
standard recommender systems that suggest items to users, reciprocal
recommender systems (RRSs) that suggest other users must take into account the
mutual interests of users. In addition, ensuring that recommendation
opportunities do not disproportionately favor popular users is essential for
the total number of matches and for fairness among users. Existing
recommendation methods in matching markets, however, face computational
challenges on large-scale platforms and depend on specific examination
functions in the position-based model (PBM). In this paper, we introduce the
reciprocal recommendation method based on the matching with transferable
utility (TU matching) model in the context of ranking recommendations in
matching markets and propose a fast and examination-model-free algorithm.
Furthermore, we evaluate our approach on experiments with synthetic data and
real-world data from an online dating platform in Japan. Our method performs
better than or as well as existing methods in terms of the total number of
matches and works well even in a large-scale dataset for which one existing
method does not work.


------------------------------------------------------------------------------

Title:
UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric  and Semantic Rendering

Abstract: In this technical report, we present our solution, named UniOCC, for the
Vision-Centric 3D occupancy prediction track in the nuScenes Open Dataset
Challenge at CVPR 2023. Existing methods for occupancy prediction primarily
focus on optimizing projected features on 3D volume space using 3D occupancy
labels. However, the generation process of these labels is complex and
expensive (relying on 3D semantic annotations), and limited by voxel
resolution, they cannot provide fine-grained spatial semantics. To address this
limitation, we propose a novel Unifying Occupancy (UniOcc) prediction method,
explicitly imposing spatial geometry constraint and complementing fine-grained
semantic supervision through volume ray rendering. Our method significantly
enhances model performance and demonstrates promising potential in reducing
human annotation costs. Given the laborious nature of annotating 3D occupancy,
we further introduce a Depth-aware Teacher Student (DTS) framework to enhance
prediction accuracy using unlabeled data. Our solution achieves 51.27\% mIoU on
the official leaderboard with single model, placing 3rd in this challenge.


------------------------------------------------------------------------------

Title:
ChessGPT: Bridging Policy Learning and Language Modeling

Abstract: When solving decision-making tasks, humans typically depend on information
from two key sources: (1) Historical policy data, which provides interaction
replay from the environment, and (2) Analytical insights in natural language
form, exposing the invaluable thought process or strategic considerations.
Despite this, the majority of preceding research focuses on only one source:
they either use historical replay exclusively to directly learn policy or value
functions, or engaged in language model training utilizing mere language
corpus. In this paper, we argue that a powerful autonomous agent should cover
both sources. Thus, we propose ChessGPT, a GPT model bridging policy learning
and language modeling by integrating data from these two sources in Chess
games. Specifically, we build a large-scale game and language dataset related
to chess. Leveraging the dataset, we showcase two model examples ChessCLIP and
ChessGPT, integrating policy learning and language modeling. Finally, we
propose a full evaluation framework for evaluating language model's chess
ability. Experimental results validate our model and dataset's effectiveness.
We open source our code, model, and dataset at
this https URL


------------------------------------------------------------------------------

Title:
Stochastic Re-weighted Gradient Descent via Distributionally Robust  Optimization

Abstract: We develop a re-weighted gradient descent technique for boosting the
performance of deep neural networks. Our algorithm involves the importance
weighting of data points during each optimization step. Our approach is
inspired by distributionally robust optimization with $f$-divergences, which
has been known to result in models with improved generalization guarantees. Our
re-weighting scheme is simple, computationally efficient, and can be combined
with any popular optimization algorithms such as SGD and Adam. Empirically, we
demonstrate our approach's superiority on various tasks, including vanilla
classification, classification with label imbalance, noisy labels, domain
adaptation, and tabular representation learning. Notably, we obtain
improvements of +0.7% and +1.44% over SOTA on DomainBed and Tabular benchmarks,
respectively. Moreover, our algorithm boosts the performance of BERT on GLUE
benchmarks by +1.94%, and ViT on ImageNet-1K by +0.9%. These results
demonstrate the effectiveness of the proposed approach, indicating its
potential for improving performance in diverse domains.


------------------------------------------------------------------------------

Title:
Zero-Shot Anomaly Detection with Pre-trained Segmentation Models

Abstract: This technical report outlines our submission to the zero-shot track of the
Visual Anomaly and Novelty Detection (VAND) 2023 Challenge. Building on the
performance of the WINCLIP framework, we aim to enhance the system's
localization capabilities by integrating zero-shot segmentation models. In
addition, we perform foreground instance segmentation which enables the model
to focus on the relevant parts of the image, thus allowing the models to better
identify small or subtle deviations. Our pipeline requires no external data or
information, allowing for it to be directly applied to new datasets. Our team
(Variance Vigilance Vanguard) ranked third in the zero-shot track of the VAND
challenge, and achieve an average F1-max score of 81.5/24.2 at a sample/pixel
level on the VisA dataset.


------------------------------------------------------------------------------

Title:
Dis-AE: Multi-domain & Multi-task Generalisation on Real-World Clinical  Data

Abstract: Clinical data is often affected by clinically irrelevant factors such as
discrepancies between measurement devices or differing processing methods
between sites. In the field of machine learning (ML), these factors are known
as domains and the distribution differences they cause in the data are known as
domain shifts. ML models trained using data from one domain often perform
poorly when applied to data from another domain, potentially leading to wrong
predictions. As such, developing machine learning models that can generalise
well across multiple domains is a challenging yet essential task in the
successful application of ML in clinical practice. In this paper, we propose a
novel disentangled autoencoder (Dis-AE) neural network architecture that can
learn domain-invariant data representations for multi-label classification of
medical measurements even when the data is influenced by multiple interacting
domain shifts at once. The model utilises adversarial training to produce data
representations from which the domain can no longer be determined. We evaluate
the model's domain generalisation capabilities on synthetic datasets and full
blood count (FBC) data from blood donors as well as primary and secondary care
patients, showing that Dis-AE improves model generalisation on multiple domains
simultaneously while preserving clinically relevant information.


------------------------------------------------------------------------------

Title:
KUCST at CheckThat 2023: How good can we be with a generic model?

Abstract: In this paper we present our method for tasks 2 and 3A at the CheckThat2023
shared task. We make use of a generic approach that has been used to tackle a
diverse set of tasks, inspired by authorship attribution and profiling. We
train a number of Machine Learning models and our results show that Gradient
Boosting performs the best for both tasks. Based on the official ranking
provided by the shared task organizers, our model shows an average performance
compared to other teams.


------------------------------------------------------------------------------

Title:
Predictive Maneuver Planning with Deep Reinforcement Learning (PMP-DRL)  for comfortable and safe autonomous driving

Abstract: This paper presents a Predictive Maneuver Planning with Deep Reinforcement
Learning (PMP-DRL) model for maneuver planning. Traditional rule-based maneuver
planning approaches often have to improve their abilities to handle the
variabilities of real-world driving scenarios. By learning from its experience,
a Reinforcement Learning (RL)-based driving agent can adapt to changing driving
conditions and improve its performance over time. Our proposed approach
combines a predictive model and an RL agent to plan for comfortable and safe
maneuvers. The predictive model is trained using historical driving data to
predict the future positions of other surrounding vehicles. The surrounding
vehicles' past and predicted future positions are embedded in context-aware
grid maps. At the same time, the RL agent learns to make maneuvers based on
this spatio-temporal context information. Performance evaluation of PMP-DRL has
been carried out using simulated environments generated from publicly available
NGSIM US101 and I80 datasets. The training sequence shows the continuous
improvement in the driving experiences. It shows that proposed PMP-DRL can
learn the trade-off between safety and comfortability. The decisions generated
by the recent imitation learning-based model are compared with the proposed
PMP-DRL for unseen scenarios. The results clearly show that PMP-DRL can handle
complex real-world scenarios and make better comfortable and safe maneuver
decisions than rule-based and imitative models.


------------------------------------------------------------------------------

Title:
Unbalanced Diffusion Schrödinger Bridge

Abstract: Schr\"odinger bridges (SBs) provide an elegant framework for modeling the
temporal evolution of populations in physical, chemical, or biological systems.
Such natural processes are commonly subject to changes in population size over
time due to the emergence of new species or birth and death events. However,
existing neural parameterizations of SBs such as diffusion Schr\"odinger
bridges (DSBs) are restricted to settings in which the endpoints of the
stochastic process are both probability measures and assume conservation of
mass constraints. To address this limitation, we introduce unbalanced DSBs
which model the temporal evolution of marginals with arbitrary finite mass.
This is achieved by deriving the time reversal of stochastic differential
equations with killing and birth terms. We present two novel algorithmic
schemes that comprise a scalable objective function for training unbalanced
DSBs and provide a theoretical analysis alongside challenging applications on
predicting heterogeneous molecular single-cell responses to various cancer
drugs and simulating the emergence and spread of new viral variants.


------------------------------------------------------------------------------

Title:
Evolutionary Curriculum Training for DRL-Based Navigation Systems

Abstract: In recent years, Deep Reinforcement Learning (DRL) has emerged as a promising
method for robot collision avoidance. However, such DRL models often come with
limitations, such as adapting effectively to structured environments containing
various pedestrians. In order to solve this difficulty, previous research has
attempted a few approaches, including training an end-to-end solution by
integrating a waypoint planner with DRL and developing a multimodal solution to
mitigate the drawbacks of the DRL model. However, these approaches have
encountered several issues, including slow training times, scalability
challenges, and poor coordination among different models. To address these
challenges, this paper introduces a novel approach called evolutionary
curriculum training to tackle these challenges. The primary goal of
evolutionary curriculum training is to evaluate the collision avoidance model's
competency in various scenarios and create curricula to enhance its
insufficient skills. The paper introduces an innovative evaluation technique to
assess the DRL model's performance in navigating structured maps and avoiding
dynamic obstacles. Additionally, an evolutionary training environment generates
all the curriculum to improve the DRL model's inadequate skills tested in the
previous evaluation. We benchmark the performance of our model across five
structured environments to validate the hypothesis that this evolutionary
training environment leads to a higher success rate and a lower average number
of collisions. Further details and results at our project website.


------------------------------------------------------------------------------

Title:
Tensor BM-Decomposition for Compression and Analysis of Spatio-Temporal  Third-order Data

Abstract: Given tensors $\boldsymbol{\mathscr{A}}, \boldsymbol{\mathscr{B}},
\boldsymbol{\mathscr{C}}$ of size $m \times 1 \times n$, $m \times p \times 1$,
and $1\times p \times n$, respectively, their Bhattacharya-Mesner (BM) product
will result in a third order tensor of dimension $m \times p \times n$ and
BM-rank of 1 (Mesner and Bhattacharya, 1990). Thus, if a third-order tensor can
be written as a sum of a small number of such BM-rank 1 terms, this
BM-decomposition (BMD) offers an implicitly compressed representation of the
tensor. Therefore, in this paper, we give a generative model which illustrates
that spatio-temporal video data can be expected to have low BM-rank. Then, we
discuss non-uniqueness properties of the BMD and give an improved bound on the
BM-rank of a third-order tensor. We present and study properties of an
iterative algorithm for computing an approximate BMD, including convergence
behavior and appropriate choices for starting guesses that allow for the
decomposition of our spatial-temporal data into stationary and non-stationary
components. Several numerical experiments show the impressive ability of our
BMD algorithm to extract important temporal information from video data while
simultaneously compressing the data. In particular, we compare our approach
with dynamic mode decomposition (DMD): first, we show how the matrix-based DMD
can be reinterpreted in tensor BMP form, then we explain why the low BM-rank
decomposition can produce results with superior compression properties while
simultaneously providing better separation of stationary and non-stationary
features in the data. We conclude with a comparison of our low BM-rank
decomposition to two other tensor decompositions, CP and the t-SVDM.


------------------------------------------------------------------------------

Title:
Multi-Objective Optimization of Electrical Machines using a Hybrid  Data-and Physics-Driven Approach

Abstract: Magneto-static finite element (FE) simulations make numerical optimization of
electrical machines very time-consuming and computationally intensive during
the design stage. In this paper, we present the application of a hybrid
data-and physics-driven model for numerical optimization of permanent magnet
synchronous machines (PMSM). Following the data-driven supervised training,
deep neural network (DNN) will act as a meta-model to characterize the
electromagnetic behavior of PMSM by predicting intermediate FE measures. These
intermediate measures are then post-processed with various physical models to
compute the required key performance indicators (KPIs), e.g., torque, shaft
power, and material costs. We perform multi-objective optimization with both
classical FE and a hybrid approach using a nature-inspired evolutionary
algorithm. We show quantitatively that the hybrid approach maintains the
quality of Pareto results better or close to conventional FE simulation-based
optimization while being computationally very cheap.


------------------------------------------------------------------------------

Title:
Localization with Anticipation for Autonomous Urban Driving in Rain

Abstract: This paper presents a localization algorithm for autonomous urban vehicles
under rain weather conditions. In adverse weather, human drivers anticipate the
location of the ego-vehicle based on the control inputs they provide and
surrounding road contextual information. Similarly, in our approach for
localization in rain weather, we use visual data, along with a global reference
path and vehicle motion model for anticipating and better estimating the pose
of the ego-vehicle in each frame. The global reference path contains useful
road contextual information such as the angle of turn which can be potentially
used to improve the localization accuracy especially when sensors are
compromised. We experimented on the Oxford Robotcar Dataset and our internal
dataset from Singapore to validate our localization algorithm in both clear and
rain weather conditions. Our method improves localization accuracy by 50.83% in
rain weather and 34.32% in clear weather when compared to baseline algorithms.


------------------------------------------------------------------------------

Title:
OpenOOD v1.5: Enhanced Benchmark for Out-of-Distribution Detection

Abstract: Out-of-Distribution (OOD) detection is critical for the reliable operation of
open-world intelligent systems. Despite the emergence of an increasing number
of OOD detection methods, the evaluation inconsistencies present challenges for
tracking the progress in this field. OpenOOD v1 initiated the unification of
the OOD detection evaluation but faced limitations in scalability and
usability. In response, this paper presents OpenOOD v1.5, a significant
improvement from its predecessor that ensures accurate, standardized, and
user-friendly evaluation of OOD detection methodologies. Notably, OpenOOD v1.5
extends its evaluation capabilities to large-scale datasets such as ImageNet,
investigates full-spectrum OOD detection which is important yet underexplored,
and introduces new features including an online leaderboard and an easy-to-use
evaluator. This work also contributes in-depth analysis and insights derived
from comprehensive experimental results, thereby enriching the knowledge pool
of OOD detection methodologies. With these enhancements, OpenOOD v1.5 aims to
drive advancements and offer a more robust and comprehensive evaluation
benchmark for OOD detection research.


------------------------------------------------------------------------------

Title:
Why does Stereo Triangulation Not Work in UAV Distance Estimation

Abstract: UAV distance estimation plays an important role for path planning of swarm
UAVs and collision avoidance. However, the lack of annotated data seriously
hinder the related studies. In this paper, we build and present a UAVDE dataset
for UAV distance estimation, in which distance between two UAVs is obtained by
UWB sensors. During experiments, we surprisingly observe that the commonly used
stereo triangulation can not stand for UAV scenes. The core reason is the
position deviation issue of UAVs due to long shooting distance and camera
vibration, which is common in UAV scenes. To tackle this issue, we propose a
novel position correction module (PCM), which can directly predict the offset
between the image positions and the actual ones of UAVs and perform calculation
compensation in stereo triangulation. Besides, to further boost performance on
hard samples, we propose a dynamic iterative correction mechanism, which is
composed of multiple stacked PCMs and a gating mechanism to adaptively
determine whether further correction is required according to the difficulty of
data samples. Consequently, the position deviation issue can be effectively
alleviated. We conduct extensive experiments on UAVDE, and our proposed method
can achieve a 38.84% performance improvement, which demonstrates its
effectiveness and superiority. The code and dataset would be released.


------------------------------------------------------------------------------

Title:
Performance Evaluation and Comparison of a New Regression Algorithm

Abstract: In recent years, Machine Learning algorithms, in particular supervised
learning techniques, have been shown to be very effective in solving regression
problems. We compare the performance of a newly proposed regression algorithm
against four conventional machine learning algorithms namely, Decision Trees,
Random Forest, k-Nearest Neighbours and XG Boost. The proposed algorithm was
presented in detail in a previous paper but detailed comparisons were not
included. We do an in-depth comparison, using the Mean Absolute Error (MAE) as
the performance metric, on a diverse set of datasets to illustrate the great
potential and robustness of the proposed approach. The reader is free to
replicate our results since we have provided the source code in a GitHub
repository while the datasets are publicly available.


------------------------------------------------------------------------------

Title:
Robustness Analysis on Foundational Segmentation Models

Abstract: Due to the increase in computational resources and accessibility of data, an
increase in large, deep learning models trained on copious amounts of data
using self-supervised or semi-supervised learning have emerged. These
"foundation" models are often adapted to a variety of downstream tasks like
classification, object detection, and segmentation with little-to-no training
on the target dataset. In this work, we perform a robustness analysis of Visual
Foundation Models (VFMs) for segmentation tasks and compare them to supervised
models of smaller scale. We focus on robustness against real-world distribution
shift perturbations.We benchmark four state-of-the-art segmentation
architectures using 2 different datasets, COCO and ADE20K, with 17 different
perturbations with 5 severity levels each. We find interesting insights that
include (1) VFMs are not robust to compression-based corruptions, (2) while the
selected VFMs do not significantly outperform or exhibit more robustness
compared to non-VFM models, they remain competitively robust in zero-shot
evaluations, particularly when non-VFM are under supervision and (3) selected
VFMs demonstrate greater resilience to specific categories of objects, likely
due to their open-vocabulary training paradigm, a feature that non-VFM models
typically lack. We posit that the suggested robustness evaluation introduces
new requirements for foundational models, thus sparking further research to
enhance their performance.


------------------------------------------------------------------------------

Title:
Position-Based Nonlinear Gauss-Seidel for Quasistatic Hyperelasticity

Abstract: Position based dynamics is a powerful technique for simulating a variety of
materials. Its primary strength is its robustness when run with limited
computational budget. We develop a novel approach to address problems with PBD
for quasistatic hyperelastic materials. Even though PBD is based on the
projection of static constraints, PBD is best suited for dynamic simulations.
This is particularly relevant since the efficient creation of large data sets
of plausible, but not necessarily accurate elastic equilibria is of increasing
importance with the emergence of quasistatic neural networks. Furthermore, PBD
projects one constraint at a time. We show that ignoring the effects of
neighboring constraints limits its convergence and stability properties. Recent
works have shown that PBD can be related to the Gauss-Seidel approximation of a
Lagrange multiplier formulation of backward Euler time stepping, where each
constraint is solved/projected independently of the others in an iterative
fashion. We show that a position-based, rather than constraint-based nonlinear
Gauss-Seidel approach solves these problems. Our approach retains the essential
PBD feature of stable behavior with constrained computational budgets, but also
allows for convergent behavior with expanded budgets. We demonstrate the
efficacy of our method on a variety of representative hyperelastic problems and
show that both successive over relaxation (SOR) and Chebyshev acceleration can
be easily applied.


------------------------------------------------------------------------------

Title:
Pushing the Limits of Unsupervised Unit Discovery for SSL Speech  Representation

Abstract: The excellent generalization ability of self-supervised learning (SSL) for
speech foundation models has garnered significant attention. HuBERT is a
successful example that utilizes offline clustering to convert speech features
into discrete units for a masked language modeling pretext task. However,
simply clustering features as targets by k-means does not fully inspire the
model's performance. In this work, we present an unsupervised method to improve
SSL targets. Two models are proposed, MonoBERT and PolyBERT, which leverage
context-independent and context-dependent phoneme-based units for pre-training.
Our models outperform other SSL models significantly on the LibriSpeech
benchmark without the need for iterative re-clustering and re-training.
Furthermore, our models equipped with context-dependent units even outperform
target-improvement models that use labeled data during pre-training. How we
progressively improve the unit discovery process is demonstrated through
experiments.


------------------------------------------------------------------------------

Title:
Exploiting Uncertainty for Querying Inconsistent Description Logics  Knowledge Bases

Abstract: The necessity to manage inconsistency in Description Logics Knowledge Bases
(KBs) has come to the fore with the increasing importance gained by the
Semantic Web, where information comes from different sources that constantly
change their content and may contain contradictory descriptions when considered
either alone or together. Classical reasoning algorithms do not handle
inconsistent KBs, forcing the debugging of the KB in order to remove the
inconsistency. In this paper, we exploit an existing probabilistic semantics
called DISPONTE to overcome this problem and allow queries also in case of
inconsistent KBs. We implemented our approach in the reasoners TRILL and BUNDLE
and empirically tested the validity of our proposal. Moreover, we formally
compare the presented approach to that of the repair semantics, one of the most
established semantics when considering DL reasoning tasks.


------------------------------------------------------------------------------

Title:
Web of Things and Trends in Agriculture: A Systematic Literature Review

Abstract: In the past few years, the Web of Things (WOT) became a beneficial
game-changing technology within the Agriculture domain as it introduces
innovative and promising solutions to the Internet of Things (IoT) agricultural
applications problems by providing its services. WOT provides the support for
integration, interoperability for heterogeneous devices, infrastructures,
platforms, and the emergence of various other technologies. The main aim of
this study is about understanding and providing a growing and existing research
content, issues, and directions for the future regarding WOT-based agriculture.
Therefore, a systematic literature review (SLR) of research articles is
presented by categorizing the selected studies published between 2010 and 2020
into the following categories: research type, approaches, and their application
domains. Apart from reviewing the state-of-the-art articles on WOT solutions
for the agriculture field, a taxonomy of WOT-base agriculture application
domains has also been presented in this study. A model has also presented to
show the picture of WOT based Smart Agriculture. Lastly, the findings of this
SLR and the research gaps in terms of open issues have been presented to
provide suggestions on possible future directions for the researchers for
future research.


------------------------------------------------------------------------------

Title:
Text Promptable Surgical Instrument Segmentation with Vision-Language  Models

Abstract: In this paper, we propose a novel text promptable surgical instrument
segmentation approach to overcome challenges associated with diversity and
differentiation of surgical instruments in minimally invasive surgeries. We
redefine the task as text promptable, thereby enabling a more nuanced
comprehension of surgical instruments and adaptability to new instrument types.
Inspired by recent advancements in vision-language models, we leverage
pretrained image and text encoders as our model backbone and design a text
promptable mask decoder consisting of attention- and convolution-based
prompting schemes for surgical instrument segmentation prediction. Our model
leverages multiple text prompts for each surgical instrument through a new
mixture of prompts mechanism, resulting in enhanced segmentation performance.
Additionally, we introduce a hard instrument area reinforcement module to
improve image feature comprehension and segmentation precision. Extensive
experiments on EndoVis2017 and EndoVis2018 datasets demonstrate our model's
superior performance and promising generalization capability. To our knowledge,
this is the first implementation of a promptable approach to surgical
instrument segmentation, offering significant potential for practical
application in the field of robotic-assisted surgery.


------------------------------------------------------------------------------

Title:
Node Cardinality Estimation in a Heterogeneous Wireless Network Deployed  Over a Large Region Using a Mobile Base Station

Abstract: We consider the problem of estimation of the node cardinality of each node
type in a heterogeneous wireless network with $T$ types of nodes deployed over
a large region, where $T \ge 2$ is an integer. A mobile base station (MBS),
such as that mounted on an unmanned aerial vehicle, is used in such cases since
a single static base station is not sufficient to cover such a large region.
The MBS moves around in the region and makes multiple stops, and at the last
stop, it is able to estimate the node cardinalities for the entire region. In
this paper, we propose two schemes, viz., HSRC-M1 and HSRC-M2, to rapidly
estimate the number of nodes of each type. Both schemes have two phases, and
they are performed at each stop. We prove that the node cardinality estimates
computed using our proposed schemes are equal to, and hence as accurate as, the
estimates that would have been obtained if a well-known estimation protocol
designed for homogeneous networks in prior work were separately executed $T$
times. We compute closed-form expressions for the expected number of slots
required by HSRC-M1 to execute and the expected energy consumption of a node
under HSRC-M1. We formulate the problem of finding the optimal tour of the MBS
around the region, which covers all the nodes and minimizes the travel cost of
the MBS, show that it is NP-complete, and provide a greedy algorithm to solve
it. Using simulations, we show that the numbers of slots required by the
proposed schemes, HSRC-M1 and HSRC-M2, for computing node cardinality estimates
are significantly less than the number of slots required for $T$ separate
executions of the above estimation protocol for homogeneous networks.


------------------------------------------------------------------------------

Title:
Recursive First-order Syntactic Unification Modulo Variable Classes

Abstract: We present a generalization of first-order syntactic unification to a term
algebra where variables belong to disjoint, total, linearly ordered sets
referred to as variable classes. Unlike First-order syntactic unification, the
number of variables within a given problem is not finitely bound as some
variable classes are associated with self-referencing recursive substitutions
allowing the construction of infinitely deep terms containing infinitely many
variables, what we refer to as arithmetic progressive terms. Such constructions
are related to inductive reasoning. We show that unifiability is decidable for
so-called simple linear loops and conjecture decidability for less restrictive
classes.


------------------------------------------------------------------------------

Title:
Lakat: An open and permissionless architecture for continuous  integration academic publishing

Abstract: In this paper, we present three contributions to the field of academic
publishing. Firstly, we introduce Lakat, a novel base layer for a publishing
system that fosters collaboration, pluralism and permissionless participation.
Drawing inspiration from the philosophy of Imre Lakatos, Lakat is designed as a
peer-to-peer process- and conflict-oriented system that supports continuous
integration across multiple branches. This architecture provides a robust
foundation for the integration of existing reputation systems and incentive
structures or the development of new ones. Secondly, we propose a new consensus
mechanism, called Proof of Review, which ensures the integrity and quality of
the content while promoting active participation from the community. Lastly, we
present Lignification, a new finality gadget specifically designed for
branched, permissionless systems. Lignification provides a deterministic way to
find the consensual state in these systems, ensuring the system's robustness
and reliability in handling complex scenarios where multiple contributors may
be proposing changes simultaneously. Together, these contributions aim to
provide a convenient starting point to tackle some of the issues in traditional
paper-formatted publishing of research output. By prioritizing collaboration,
process-orientation, and pluralism, Lakat aims to improve the way research is
conducted and disseminated and ultimately hopes to contribute to a healthier
and more productive academic culture.


------------------------------------------------------------------------------

Title:
Fix Fairness, Don't Ruin Accuracy: Performance Aware Fairness Repair  using AutoML

Abstract: Machine learning (ML) is increasingly being used in critical decision-making
software, but incidents have raised questions about the fairness of ML
predictions. To address this issue, new tools and methods are needed to
mitigate bias in ML-based software. Previous studies have proposed bias
mitigation algorithms that only work in specific situations and often result in
a loss of accuracy. Our proposed solution is a novel approach that utilizes
automated machine learning (AutoML) techniques to mitigate bias. Our approach
includes two key innovations: a novel optimization function and a
fairness-aware search space. By improving the default optimization function of
AutoML and incorporating fairness objectives, we are able to mitigate bias with
little to no loss of accuracy. Additionally, we propose a fairness-aware search
space pruning method for AutoML to reduce computational cost and repair time.
Our approach, built on the state-of-the-art Auto-Sklearn tool, is designed to
reduce bias in real-world scenarios. In order to demonstrate the effectiveness
of our approach, we evaluated our approach on four fairness problems and 16
different ML models, and our results show a significant improvement over the
baseline and existing bias mitigation techniques. Our approach, Fair-AutoML,
successfully repaired 60 out of 64 buggy cases, while existing bias mitigation
techniques only repaired up to 44 out of 64 cases.


------------------------------------------------------------------------------

Title:
Emotional Speech-Driven Animation with Content-Emotion Disentanglement

Abstract: To be widely adopted, 3D facial avatars need to be animated easily,
realistically, and directly, from speech signals. While the best recent methods
generate 3D animations that are synchronized with the input audio, they largely
ignore the impact of emotions on facial expressions. Instead, their focus is on
modeling the correlations between speech and facial motion, resulting in
animations that are unemotional or do not match the input emotion. We observe
that there are two contributing factors resulting in facial animation - the
speech and the emotion. We exploit these insights in EMOTE (Expressive Model
Optimized for Talking with Emotion), which generates 3D talking head avatars
that maintain lip sync while enabling explicit control over the expression of
emotion. Due to the absence of high-quality aligned emotional 3D face datasets
with speech, EMOTE is trained from an emotional video dataset (i.e., MEAD). To
achieve this, we match speech-content between generated sequences and target
videos differently from emotion content. Specifically, we train EMOTE with
additional supervision in the form of a lip-reading objective to preserve the
speech-dependent content (spatially local and high temporal frequency), while
utilizing emotion supervision on a sequence-level (spatially global and low
frequency). Furthermore, we employ a content-emotion exchange mechanism in
order to supervise different emotion on the same audio, while maintaining the
lip motion synchronized with the speech. To employ deep perceptual losses
without getting undesirable artifacts, we devise a motion prior in form of a
temporal VAE. Extensive qualitative, quantitative, and perceptual evaluations
demonstrate that EMOTE produces state-of-the-art speech-driven facial
animations, with lip sync on par with the best methods while offering
additional, high-quality emotional control.


------------------------------------------------------------------------------

Title:
MLonMCU: TinyML Benchmarking with Fast Retargeting

Abstract: While there exist many ways to deploy machine learning models on
microcontrollers, it is non-trivial to choose the optimal combination of
frameworks and targets for a given application. Thus, automating the end-to-end
benchmarking flow is of high relevance nowadays. A tool called MLonMCU is
proposed in this paper and demonstrated by benchmarking the state-of-the-art
TinyML frameworks TFLite for Microcontrollers and TVM effortlessly with a large
number of configurations in a low amount of time.


------------------------------------------------------------------------------

Title:
An Efficient and Multi-private Key Secure Aggregation for Federated  Learning

Abstract: With the emergence of privacy leaks in federated learning, secure aggregation
protocols that mainly adopt either homomorphic encryption or threshold secret
sharing have been widely developed for federated learning to protect the
privacy of the local training data of each client. However, these existing
protocols suffer from many shortcomings, such as the dependence on a trusted
third party, the vulnerability to clients being corrupted, low efficiency, the
trade-off between security and fault tolerance, etc. To solve these
disadvantages, we propose an efficient and multi-private key secure aggregation
scheme for federated learning. Specifically, we skillfully modify the variant
ElGamal encryption technique to achieve homomorphic addition operation, which
has two important advantages: 1) The server and each client can freely select
public and private keys without introducing a trust third party and 2) Compared
to the variant ElGamal encryption, the plaintext space is relatively large,
which is more suitable for the deep model. Besides, for the high dimensional
deep model parameter, we introduce a super-increasing sequence to compress
multi-dimensional data into 1-D, which can greatly reduce encryption and
decryption times as well as communication for ciphertext transmission. Detailed
security analyses show that our proposed scheme achieves the semantic security
of both individual local gradients and the aggregated result while achieving
optimal robustness in tolerating both client collusion and dropped clients.
Extensive simulations demonstrate that the accuracy of our scheme is almost the
same as the non-private approach, while the efficiency of our scheme is much
better than the state-of-the-art homomorphic encryption-based secure
aggregation schemes. More importantly, the efficiency advantages of our scheme
will become increasingly prominent as the number of model parameters increases.


------------------------------------------------------------------------------

Title:
Feed Two Birds with One Scone: Exploiting Wild Data for Both  Out-of-Distribution Generalization and Detection

Abstract: Modern machine learning models deployed in the wild can encounter both
covariate and semantic shifts, giving rise to the problems of
out-of-distribution (OOD) generalization and OOD detection respectively. While
both problems have received significant research attention lately, they have
been pursued independently. This may not be surprising, since the two tasks
have seemingly conflicting goals. This paper provides a new unified approach
that is capable of simultaneously generalizing to covariate shifts while
robustly detecting semantic shifts. We propose a margin-based learning
framework that exploits freely available unlabeled data in the wild that
captures the environmental test-time OOD distributions under both covariate and
semantic shifts. We show both empirically and theoretically that the proposed
margin constraint is the key to achieving both OOD generalization and
detection. Extensive experiments show the superiority of our framework,
outperforming competitive baselines that specialize in either OOD
generalization or OOD detection. Code is publicly available at
this https URL


------------------------------------------------------------------------------

Title:
Training Multimedia Event Extraction With Generated Images and Captions

Abstract: Contemporary news reporting increasingly features multimedia content,
motivating research on multimedia event extraction. However, the task lacks
annotated multimodal training data and artificially generated training data
suffer from the distribution shift from the real-world data. In this paper, we
propose Cross-modality Augmented Multimedia Event Learning (CAMEL), which
successfully utilizes artificially generated multimodal training data and
achieves state-of-the-art performance. Conditioned on unimodal training data,
we generate multimodal training data using off-the-shelf image generators like
Stable Diffusion and image captioners like BLIP. In order to learn robust
features that are effective across domains, we devise an iterative and gradual
annealing training strategy. Substantial experiments show that CAMEL surpasses
state-of-the-art (SOTA) baselines on the M2E2 benchmark. On multimedia events
in particular, we outperform the prior SOTA by 4.2\% F1 on event mention
identification and by 9.8\% F1 on argument identification, which demonstrates
that CAMEL learns synergistic representations from the two modalities.


------------------------------------------------------------------------------

Title:
Tecnicas Avanzadas de Ciberseguridad: Integracion y Evolucion de la Kill  Chain en Diversos Escenarios

Abstract: The document provides an in-depth analysis of the main attack chain models
used in cybersecurity, including the Lockheed Martin Cyber Kill Chain
framework, the MITER ATT&CK framework, the Diamond model, and the IoTKC,
focusing on their strengths and weaknesses. Subsequently, the need for greater
adaptability and comprehensiveness in attack analysis is highlighted, which has
led to the growing preference for frameworks such as MITRE ATT&CK and the
Diamond model. A review of insider attacks in cloud computing shows how the
combination of attack trees and kill chains can offer an effective methodology
to identify and detect these types of threats, focusing detection and defense
efforts on critical nodes. Likewise, emphasis is placed on the importance of
advanced analysis models, such as BACCER, in the identification and detection
of attack patterns and decision logic using intelligence techniques and
defensive and offensive actions.


------------------------------------------------------------------------------

Title:
Offline Multi-Agent Reinforcement Learning with Coupled Value  Factorization

Abstract: Offline reinforcement learning (RL) that learns policies from offline
datasets without environment interaction has received considerable attention in
recent years. Compared with the rich literature in the single-agent case,
offline multi-agent RL is still a relatively underexplored area. Most existing
methods directly apply offline RL ingredients in the multi-agent setting
without fully leveraging the decomposable problem structure, leading to less
satisfactory performance in complex tasks. We present OMAC, a new offline
multi-agent RL algorithm with coupled value factorization. OMAC adopts a
coupled value factorization scheme that decomposes the global value function
into local and shared components, and also maintains the credit assignment
consistency between the state-value and Q-value functions. Moreover, OMAC
performs in-sample learning on the decomposed local state-value functions,
which implicitly conducts max-Q operation at the local level while avoiding
distributional shift caused by evaluating out-of-distribution actions. Based on
the comprehensive evaluations of the offline multi-agent StarCraft II
micro-management tasks, we demonstrate the superior performance of OMAC over
the state-of-the-art offline multi-agent RL methods.


------------------------------------------------------------------------------

Title:
Artificial intelligence adoption in the physical sciences, natural  sciences, life sciences, social sciences and the arts and humanities: A  bibliometric analysis of research publications from 1960-2021

Abstract: Analysing historical patterns of artificial intelligence (AI) adoption can
inform decisions about AI capability uplift, but research to date has provided
a limited view of AI adoption across various fields of research. In this study
we examine worldwide adoption of AI technology within 333 fields of research
during 1960-2021. We do this by using bibliometric analysis with 137 million
peer-reviewed publications captured in The Lens database. We define AI using a
list of 214 phrases developed by expert working groups at the Organisation for
Economic Cooperation and Development (OECD). We found that 3.1 million of the
137 million peer-reviewed research publications during the entire period were
AI-related, with a surge in AI adoption across practically all research fields
(physical science, natural science, life science, social science and the arts
and humanities) in recent years. The diffusion of AI beyond computer science
was early, rapid and widespread. In 1960 14% of 333 research fields were
related to AI (many in computer science), but this increased to cover over half
of all research fields by 1972, over 80% by 1986 and over 98% in current times.
We note AI has experienced boom-bust cycles historically: the AI "springs" and
"winters". We conclude that the context of the current surge appears different,
and that interdisciplinary AI application is likely to be sustained.


------------------------------------------------------------------------------

Title:
Neural World Models for Computer Vision

Abstract: Humans navigate in their environment by learning a mental model of the world
through passive observation and active interaction. Their world model allows
them to anticipate what might happen next and act accordingly with respect to
an underlying objective. Such world models hold strong promises for planning in
complex environments like in autonomous driving. A human driver, or a
self-driving system, perceives their surroundings with their eyes or their
cameras. They infer an internal representation of the world which should: (i)
have spatial memory (e.g. occlusions), (ii) fill partially observable or noisy
inputs (e.g. when blinded by sunlight), and (iii) be able to reason about
unobservable events probabilistically (e.g. predict different possible
futures). They are embodied intelligent agents that can predict, plan, and act
in the physical world through their world model. In this thesis we present a
general framework to train a world model and a policy, parameterised by deep
neural networks, from camera observations and expert demonstrations. We
leverage important computer vision concepts such as geometry, semantics, and
motion to scale world models to complex urban driving scenes.
First, we propose a model that predicts important quantities in computer
vision: depth, semantic segmentation, and optical flow. We then use 3D geometry
as an inductive bias to operate in the bird's-eye view space. We present for
the first time a model that can predict probabilistic future trajectories of
dynamic agents in bird's-eye view from 360{\deg} surround monocular cameras
only. Finally, we demonstrate the benefits of learning a world model in
closed-loop driving. Our model can jointly predict static scene, dynamic scene,
and ego-behaviour in an urban driving environment.


------------------------------------------------------------------------------

Title:
Few-shot bioacoustic event detection at the DCASE 2023 challenge

Abstract: Few-shot bioacoustic event detection consists in detecting sound events of
specified types, in varying soundscapes, while having access to only a few
examples of the class of interest. This task ran as part of the DCASE challenge
for the third time this year with an evaluation set expanded to include new
animal species, and a new rule: ensemble models were no longer allowed. The
2023 few shot task received submissions from 6 different teams with F-scores
reaching as high as 63% on the evaluation set. Here we describe the task,
focusing on describing the elements that differed from previous years. We also
take a look back at past editions to describe how the task has evolved. Not
only have the F-score results steadily improved (40% to 60% to 63%), but the
type of systems proposed have also become more complex. Sound event detection
systems are no longer simple variations of the baselines provided: multiple
few-shot learning methodologies are still strong contenders for the task.


------------------------------------------------------------------------------

Title:
Audio Tagging on an Embedded Hardware Platform

Abstract: Convolutional neural networks (CNNs) have exhibited state-of-the-art
performance in various audio classification tasks. However, their real-time
deployment remains a challenge on resource-constrained devices like embedded
systems. In this paper, we analyze how the performance of large-scale
pretrained audio neural networks designed for audio pattern recognition changes
when deployed on a hardware such as Raspberry Pi. We empirically study the role
of CPU temperature, microphone quality and audio signal volume on performance.
Our experiments reveal that the continuous CPU usage results in an increased
temperature that can trigger an automated slowdown mechanism in the Raspberry
Pi, impacting inference latency. The quality of a microphone, specifically with
affordable devices like the Google AIY Voice Kit, and audio signal volume, all
affect the system performance. In the course of our investigation, we encounter
substantial complications linked to library compatibility and the unique
processor architecture requirements of the Raspberry Pi, making the process
less straightforward compared to conventional computers (PCs). Our
observations, while presenting challenges, pave the way for future researchers
to develop more compact machine learning models, design heat-dissipative
hardware, and select appropriate microphones when AI models are deployed for
real-time applications on edge devices. All related assets and an interactive
demo can be found on GitHub


------------------------------------------------------------------------------

Title:
Concealing CAN Message Sequences to Prevent Schedule-based Bus-off  Attacks

Abstract: This work focuses on eliminating timing-side channels in real-time
safety-critical cyber-physical network protocols like Controller Area Networks
(CAN). Automotive Electronic Control Units (ECUs) implement predictable
scheduling decisions based on task level response time estimation. Such levels
of determinism exposes timing information about task executions and therefore
corresponding message transmissions via the network buses (that connect the
ECUs and actuators). With proper analysis, such timing side channels can be
utilized to launch several schedule-based attacks that can lead to eventual
denial-of-service or man-in-the-middle-type attacks. To eliminate this
determinism, we propose a novel schedule obfuscation strategy by skipping
certain control task executions and related data transmissions along with
random shifting of the victim task instance. While doing this, our strategy
contemplates the performance of the control task as well by bounding the number
of control execution skips. We analytically demonstrate how the attack success
probability (ASP) is reduced under this proposed attack-aware skipping and
randomization. We also demonstrate the efficacy and real-time applicability of
our attack-aware schedule obfuscation strategy Hide-n-Seek by applying it to
synthesized automotive task sets in a real-time Hardware-in-loop (HIL) setup.


------------------------------------------------------------------------------

Title:
A Bayesian approach to uncertainty in word embedding bias estimation

Abstract: Multiple measures, such as WEAT or MAC, attempt to quantify the magnitude of
bias present in word embeddings in terms of a single-number metric. However,
such metrics and the related statistical significance calculations rely on
treating pre-averaged data as individual data points and employing
bootstrapping techniques with low sample sizes. We show that similar results
can be easily obtained using such methods even if the data are generated by a
null model lacking the intended bias. Consequently, we argue that this approach
generates false confidence. To address this issue, we propose a Bayesian
alternative: hierarchical Bayesian modeling, which enables a more
uncertainty-sensitive inspection of bias in word embeddings at different levels
of granularity. To showcase our method, we apply it to Religion, Gender, and
Race word lists from the original research, together with our control neutral
word lists. We deploy the method using Google, Glove, and Reddit embeddings.
Further, we utilize our approach to evaluate a debiasing technique applied to
Reddit word embedding. Our findings reveal a more complex landscape than
suggested by the proponents of single-number metrics. The datasets and source
code for the paper are publicly available.


------------------------------------------------------------------------------

Title:
DEYOv2: Rank Feature with Greedy Matching for End-to-End Object  Detection

Abstract: This paper presents a novel object detector called DEYOv2, an improved
version of the first-generation DEYO (DETR with YOLO) model. DEYOv2, similar to
its predecessor, DEYOv2 employs a progressive reasoning approach to accelerate
model training and enhance performance. The study delves into the limitations
of one-to-one matching in optimization and proposes solutions to effectively
address the issue, such as Rank Feature and Greedy Matching. This approach
enables the third stage of DEYOv2 to maximize information acquisition from the
first and second stages without needing NMS, achieving end-to-end optimization.
By combining dense queries, sparse queries, one-to-many matching, and
one-to-one matching, DEYOv2 leverages the advantages of each method. It
outperforms all existing query-based end-to-end detectors under the same
settings. When using ResNet-50 as the backbone and multi-scale features on the
COCO dataset, DEYOv2 achieves 51.1 AP and 51.8 AP in 12 and 24 epochs,
respectively. Compared to the end-to-end model DINO, DEYOv2 provides
significant performance gains of 2.1 AP and 1.4 AP in the two epoch settings.
To the best of our knowledge, DEYOv2 is the first fully end-to-end object
detector that combines the respective strengths of classical detectors and
query-based detectors.


------------------------------------------------------------------------------

Title:
Chatbots to ChatGPT in a Cybersecurity Space: Evolution,  Vulnerabilities, Attacks, Challenges, and Future Recommendations

Abstract: Chatbots shifted from rule-based to artificial intelligence techniques and
gained traction in medicine, shopping, customer services, food delivery,
education, and research. OpenAI developed ChatGPT blizzard on the Internet as
it crossed one million users within five days of its launch. However, with the
enhanced popularity, chatbots experienced cybersecurity threats and
vulnerabilities. This paper discussed the relevant literature, reports, and
explanatory incident attacks generated against chatbots. Our initial point is
to explore the timeline of chatbots from ELIZA (an early natural language
processing computer program) to GPT-4 and provide the working mechanism of
ChatGPT. Subsequently, we explored the cybersecurity attacks and
vulnerabilities in chatbots. Besides, we investigated the ChatGPT, specifically
in the context of creating the malware code, phishing emails, undetectable
zero-day attacks, and generation of macros and LOLBINs. Furthermore, the
history of cyberattacks and vulnerabilities exploited by cybercriminals are
discussed, particularly considering the risk and vulnerabilities in ChatGPT.
Addressing these threats and vulnerabilities requires specific strategies and
measures to reduce the harmful consequences. Therefore, the future directions
to address the challenges were presented.


------------------------------------------------------------------------------

Title:
COSA: Concatenated Sample Pretrained Vision-Language Foundation Model

Abstract: Due to the limited scale and quality of video-text training corpus, most
vision-language foundation models employ image-text datasets for pretraining
and primarily focus on modeling visually semantic representations while
disregarding temporal semantic representations and correlations. To address
this issue, we propose COSA, a COncatenated SAmple pretrained vision-language
foundation model. COSA jointly models visual contents and event-level temporal
cues using only image-text corpora. We achieve this by sequentially
concatenating multiple image-text pairs as inputs for pretraining. This
transformation effectively converts existing image-text corpora into a pseudo
long-form video-paragraph corpus, enabling richer scene transformations and
explicit event-description correspondence. Extensive experiments demonstrate
that COSA consistently improves performance across a broad range of downstream
tasks, including long-form/short-form video-text tasks and image-text tasks
such as retrieval, captioning, and question answering. Notably, COSA achieves
state-of-the-art results on various competitive benchmarks. Code and model are
released at this https URL


------------------------------------------------------------------------------

Title:
NAVI: Category-Agnostic Image Collections with High-Quality 3D Shape and  Pose Annotations

Abstract: Recent advances in neural reconstruction enable high-quality 3D object
reconstruction from casually captured image collections. Current techniques
mostly analyze their progress on relatively simple image collections where
Structure-from-Motion (SfM) techniques can provide ground-truth (GT) camera
poses. We note that SfM techniques tend to fail on in-the-wild image
collections such as image search results with varying backgrounds and
illuminations. To enable systematic research progress on 3D reconstruction from
casual image captures, we propose NAVI: a new dataset of category-agnostic
image collections of objects with high-quality 3D scans along with per-image
2D-3D alignments providing near-perfect GT camera parameters. These 2D-3D
alignments allow us to extract accurate derivative annotations such as dense
pixel correspondences, depth and segmentation maps. We demonstrate the use of
NAVI image collections on different problem settings and show that NAVI enables
more thorough evaluations that were not possible with existing datasets. We
believe NAVI is beneficial for systematic research progress on 3D
reconstruction and correspondence estimation. Project page:
this https URL


------------------------------------------------------------------------------

Title:
DIFFender: Diffusion-Based Adversarial Defense against Patch Attacks in  the Physical World

Abstract: Adversarial attacks in the physical world, particularly patch attacks, pose
significant threats to the robustness and reliability of deep learning models.
Developing reliable defenses against patch attacks is crucial for real-world
applications, yet current research in this area is severely lacking. In this
paper, we propose DIFFender, a novel defense method that leverages the
pre-trained diffusion model to perform both localization and defense against
potential adversarial patch attacks. DIFFender is designed as a pipeline
consisting of two main stages: patch localization and restoration. In the
localization stage, we exploit the intriguing properties of a diffusion model
to effectively identify the locations of adversarial patches. In the
restoration stage, we employ a text-guided diffusion model to eliminate
adversarial regions in the image while preserving the integrity of the visual
content. Additionally, we design a few-shot prompt-tuning algorithm to
facilitate simple and efficient tuning, enabling the learned representations to
easily transfer to downstream tasks, which optimize two stages jointly. We
conduct extensive experiments on image classification and face recognition to
demonstrate that DIFFender exhibits superior robustness under strong adaptive
attacks and generalizes well across various scenarios, diverse classifiers, and
multiple attack methods.


------------------------------------------------------------------------------

Title:
Active Representation Learning for General Task Space with Applications  in Robotics

Abstract: Representation learning based on multi-task pretraining has become a powerful
approach in many domains. In particular, task-aware representation learning
aims to learn an optimal representation for a specific target task by sampling
data from a set of source tasks, while task-agnostic representation learning
seeks to learn a universal representation for a class of tasks. In this paper,
we propose a general and versatile algorithmic and theoretic framework for
\textit{active representation learning}, where the learner optimally chooses
which source tasks to sample from. This framework, along with a tractable meta
algorithm, allows most arbitrary target and source task spaces (from discrete
to continuous), covers both task-aware and task-agnostic settings, and is
compatible with deep representation learning practices. We provide several
instantiations under this framework, from bilinear and feature-based nonlinear
to general nonlinear cases. In the bilinear case, by leveraging the non-uniform
spectrum of the task representation and the calibrated source-target relevance,
we prove that the sample complexity to achieve $\varepsilon$-excess risk on
target scales with $ (k^*)^2 \|v^*\|_2^2 \varepsilon^{-2}$ where $k^*$ is the
effective dimension of the target and $\|v^*\|_2^2 \in (0,1]$ represents the
connection between source and target space. Compared to the passive one, this
can save up to $\frac{1}{d_W}$ of sample complexity, where $d_W$ is the task
space dimension. Finally, we demonstrate different instantiations of our meta
algorithm in synthetic datasets and robotics problems, from pendulum
simulations to real-world drone flight datasets. On average, our algorithms
outperform baselines by $20\%-70\%$.


------------------------------------------------------------------------------

Title:
On the $k$-Hamming and $k$-Edit Distances

Abstract: In this paper we consider the weighted $k$-Hamming and $k$-Edit distances,
that are natural generalizations of the classical Hamming and Edit distances.
As main results of this paper we prove that for any $k\geq 2$ the
DECIS-$k$-Hamming problem is $\mathbb{P}$-SPACE-complete and the DECIS-$k$-Edit
problem is NEXPTIME-complete.


------------------------------------------------------------------------------

Title:
Interleaving Pre-Trained Language Models and Large Language Models for  Zero-Shot NL2SQL Generation

Abstract: Zero-shot NL2SQL is crucial in achieving natural language to SQL that is
adaptive to new environments (e.g., new databases, new linguistic phenomena or
SQL structures) with zero annotated NL2SQL samples from such environments.
Existing approaches either fine-tune pre-trained language models (PLMs) based
on annotated data or use prompts to guide fixed large language models (LLMs)
such as ChatGPT. PLMs can perform well in schema alignment but struggle to
achieve complex reasoning, while LLMs is superior in complex reasoning tasks
but cannot achieve precise schema alignment. In this paper, we propose a
ZeroNL2SQL framework that combines the complementary advantages of PLMs and
LLMs for supporting zero-shot NL2SQL. ZeroNL2SQL first uses PLMs to generate an
SQL sketch via schema alignment, then uses LLMs to fill the missing information
via complex reasoning. Moreover, in order to better align the generated SQL
queries with values in the given database instances, we design a predicate
calibration method to guide the LLM in completing the SQL sketches based on the
database instances and select the optimal SQL query via an execution-based
strategy. Comprehensive experiments show that ZeroNL2SQL can achieve the best
zero-shot NL2SQL performance on real-world benchmarks. Specifically, ZeroNL2SQL
outperforms the state-of-the-art PLM-based methods by 3.2% to 13% and exceeds
LLM-based methods by 10% to 20% on execution accuracy.


------------------------------------------------------------------------------

Title:
Winning Solution for the CVPR2023 Visual Anomaly and Novelty Detection  Challenge: Multimodal Prompting for Data-centric Anomaly Detection

Abstract: This technical report introduces the winning solution of the team
\textit{Segment Any Anomaly} for the CVPR2023 Visual Anomaly and Novelty
Detection (VAND) challenge. Going beyond uni-modal prompt, \textit{e.g.},
language prompt, we present a novel framework, \textit{i.e.}, Segment Any
Anomaly + (SAA$+$), for zero-shot anomaly segmentation with multi-modal prompts
for the regularization of cascaded modern foundation models. Inspired by the
great zero-shot generalization ability of foundation models like Segment
Anything, we first explore their assembly (SAA) to leverage diverse multi-modal
prior knowledge for anomaly localization. Subsequently, we further introduce
multimodal prompts (SAA$+$) derived from domain expert knowledge and target
image context to enable the non-parameter adaptation of foundation models to
anomaly segmentation. The proposed SAA$+$ model achieves state-of-the-art
performance on several anomaly segmentation benchmarks, including VisA and
MVTec-AD, in the zero-shot setting. We will release the code of our winning
solution for the CVPR2023 VAND challenge at
\href{Segment-Any-Anomaly}{this https URL}
\footnote{The extended-version paper with more details is available at
~\cite{cao2023segment}.}


------------------------------------------------------------------------------

Title:
Deep learning based Meta-modeling for Multi-objective Technology  Optimization of Electrical Machines

Abstract: Optimization of rotating electrical machines is both time- and
computationally expensive. Because of the different parametrization, design
optimization is commonly executed separately for each machine technology. In
this paper, we present the application of a variational auto-encoder (VAE) to
optimize two different machine technologies simultaneously, namely an
asynchronous machine and a permanent magnet synchronous machine. After
training, we employ a deep neural network and a decoder as meta-models to
predict global key performance indicators (KPIs) and generate associated new
designs, respectively, through unified latent space in the optimization loop.
Numerical results demonstrate concurrent parametric multi-objective technology
optimization in the high-dimensional design space. The VAE-based approach is
quantitatively compared to a classical deep learning-based direct approach for
KPIs prediction.


------------------------------------------------------------------------------

Title:
SCALE: Scaling up the Complexity for Advanced Language Model Evaluation

Abstract: Recent strides in Large Language Models (LLMs) have saturated many NLP
benchmarks (even professional domain-specific ones), emphasizing the need for
novel, more challenging novel ones to properly assess LLM capabilities. In this
paper, we introduce a novel NLP benchmark that poses challenges to current LLMs
across four key dimensions: processing long documents (up to 50K tokens),
utilizing domain specific knowledge (embodied in legal texts), multilingual
understanding (covering five languages), and multitasking (comprising legal
document to document Information Retrieval, Court View Generation, Leading
Decision Summarization, Citation Extraction, and eight challenging Text
Classification tasks). Our benchmark comprises diverse legal NLP datasets from
the Swiss legal system, allowing for a comprehensive study of the underlying
Non-English, inherently multilingual, federal legal system. Despite recent
advances, efficiently processing long documents for intense review/analysis
tasks remains an open challenge for language models. Also, comprehensive,
domain-specific benchmarks requiring high expertise to develop are rare, as are
multilingual benchmarks. This scarcity underscores our contribution's value,
considering most public models are trained predominantly on English corpora,
while other languages remain understudied, particularly for practical
domain-specific NLP tasks. Our benchmark allows for testing and advancing the
state-of-the-art LLMs. As part of our study, we evaluate several pre-trained
multilingual language models on our benchmark to establish strong baselines as
a point of reference. Despite the large size of our datasets (tens to hundreds
of thousands of examples), existing publicly available models struggle with
most tasks, even after in-domain pretraining. We publish all resources
(benchmark suite, pre-trained models, code) under a fully permissive open CC
BY-SA license.


------------------------------------------------------------------------------

Title:
The Upper Bound of Information Diffusion in Code Review

Abstract: Background: Code review, the discussion around a code change among humans,
forms a communication network that enables its participants to exchange and
spread information. Although reported by qualitative studies, our understanding
of the capability of code review as a communication network is still limited.
Objective: In this article, we report on a first step towards evaluating the
capability of code review as a communication network by quantifying how fast
and how far information can spread through code review: the upper bound of
information diffusion in code review.
Method: In an in-silico experiment, we simulate an artificial information
diffusion within large (Microsoft), mid-sized (Spotify), and small code review
systems (Trivago) modelled as communication networks. We then measure the
minimal topological and temporal distances between the participants to quantify
how far and how fast information can spread in code review.
Results: An average code review participants in the small and mid-sized code
review systems can spread information to between 72% and 85% of all code review
participants within four weeks independently of network size and tooling; for
the large code review systems, we found an absolute boundary of about 11000
reachable participants. On average (median), information can spread between two
participants in code review in less than five hops and less than five days.
Conclusion: We found evidence that the communication network emerging from
code review scales well and spreads information fast and broadly, corroborating
the findings of prior qualitative work. The study lays the foundation for
understanding and improving code review as a communication network.


------------------------------------------------------------------------------

Title:
Modularity Trumps Invariance for Compositional Robustness

Abstract: By default neural networks are not robust to changes in data distribution.
This has been demonstrated with simple image corruptions, such as blurring or
adding noise, degrading image classification performance. Many methods have
been proposed to mitigate these issues but for the most part models are
evaluated on single corruptions. In reality, visual space is compositional in
nature, that is, that as well as robustness to elemental corruptions,
robustness to compositions of corruptions is also needed. In this work we
develop a compositional image classification task where, given a few elemental
corruptions, models are asked to generalize to compositions of these
corruptions. That is, to achieve compositional robustness. We experimentally
compare empirical risk minimization with an invariance building pairwise
contrastive loss and, counter to common intuitions in domain generalization,
achieve only marginal improvements in compositional robustness by encouraging
invariance. To move beyond invariance, following previously proposed inductive
biases that model architectures should reflect data structure, we introduce a
modular architecture whose structure replicates the compositional nature of the
task. We then show that this modular approach consistently achieves better
compositional robustness than non-modular approaches. We additionally find
empirical evidence that the degree of invariance between representations of
'in-distribution' elemental corruptions fails to correlate with robustness to
'out-of-distribution' compositions of corruptions.


------------------------------------------------------------------------------

Title:
Team AcieLee: Technical Report for EPIC-SOUNDS Audio-Based Interaction  Recognition Challenge 2023

Abstract: In this report, we describe the technical details of our submission to the
EPIC-SOUNDS Audio-Based Interaction Recognition Challenge 2023, by Team
"AcieLee" (username: Yuqi\_Li). The task is to classify the audio caused by
interactions between objects, or from events of the camera wearer. We conducted
exhaustive experiments and found learning rate step decay, backbone frozen,
label smoothing and focal loss contribute most to the performance improvement.
After training, we combined multiple models from different stages and
integrated them into a single model by assigning fusion weights. This proposed
method allowed us to achieve 3rd place in the CVPR 2023 workshop of EPIC-SOUNDS
Audio-Based Interaction Recognition Challenge.


------------------------------------------------------------------------------

Title:
Quantum Cryptography for Enhanced Network Security: A Comprehensive  Survey of Research, Developments, and Future Directions

Abstract: With the ever-growing concern for internet security, the field of quantum
cryptography emerges as a promising solution for enhancing the security of
networking systems. In this paper, 20 notable papers from leading conferences
and journals are reviewed and categorized based on their focus on various
aspects of quantum cryptography, including key distribution, quantum bit
commitment, post quantum cryptography, and counterfactual quantum key
distribution. The paper explores the motivations and challenges of employing
quantum cryptography, addressing security and privacy concerns along with
existing solutions. Secure key distribution, a critical component in ensuring
the confidentiality and integrity of transmitted information over a network, is
emphasized in the discussion. The survey examines the potential of quantum
cryptography to enable secure key exchange between parties, even when faced
with eavesdropping, and other applications of quantum cryptography.
Additionally, the paper analyzes the methodologies, findings, and limitations
of each reviewed study, pinpointing trends such as the increasing focus on
practical implementation of quantum cryptography protocols and the growing
interest in postquantum cryptography research. Furthermore, the survey
identifies challenges and open research questions, including the need for more
efficient quantum repeater networks, improved security proofs for continuous
variable quantum key distribution, and the development of quantum resistant
cryptographic algorithms.


------------------------------------------------------------------------------

Title:
Optimal Exploration for Model-Based RL in Nonlinear Systems

Abstract: Learning to control unknown nonlinear dynamical systems is a fundamental
problem in reinforcement learning and control theory. A commonly applied
approach is to first explore the environment (exploration), learn an accurate
model of it (system identification), and then compute an optimal controller
with the minimum cost on this estimated system (policy optimization). While
existing work has shown that it is possible to learn a uniformly good model of
the system~\citep{mania2020active}, in practice, if we aim to learn a good
controller with a low cost on the actual system, certain system parameters may
be significantly more critical than others, and we therefore ought to focus our
exploration on learning such parameters.
In this work, we consider the setting of nonlinear dynamical systems and seek
to formally quantify, in such settings, (a) which parameters are most relevant
to learning a good controller, and (b) how we can best explore so as to
minimize uncertainty in such parameters. Inspired by recent work in linear
systems~\citep{wagenmaker2021task}, we show that minimizing the controller loss
in nonlinear systems translates to estimating the system parameters in a
particular, task-dependent metric. Motivated by this, we develop an algorithm
able to efficiently explore the system to reduce uncertainty in this metric,
and prove a lower bound showing that our approach learns a controller at a
near-instance-optimal rate. Our algorithm relies on a general reduction from
policy optimization to optimal experiment design in arbitrary systems, and may
be of independent interest. We conclude with experiments demonstrating the
effectiveness of our method in realistic nonlinear robotic systems.


------------------------------------------------------------------------------

Title:
Behaviorally Typed State Machines in TypeScript for Heterogeneous Swarms

Abstract: A heterogeneous swarm system is a distributed system where participants come
and go, communication topology may change at any time, data replication is
asynchronous and partial, and local agents behave differently between nodes.
These systems are hard to design and reason about, mainly because we desire a
particular class of behaviors to emerge from the interplay of heterogeneous
individual agents. Nevertheless, mission-critical operations like manufacturing
process orchestration in factories use such systems due to their uncompromising
availability and resilience of computing services.
This paper presents a set of TypeScript libraries to model peer-to-peer
workflows as state machines, execute them using the Actyx middleware, and check
the shape of these machines for conformance to a swarm protocol. The swarm
protocol describes an idealized global view of the cooperation of machines of
different roles. It directly corresponds to a diagram a product manager would
sketch on a whiteboard; this allows for verifying that the coded state machines
correctly implement the product specification. A well-formed swarm protocol
also guarantees that conforming machines will achieve eventual consensus on the
overall state progression even in the absence of further coordination. This
tool is for developers of business logic for heterogeneous swarm systems,
helping them verify that their protocols and implementations are correct.
Tool repo: this https URL


------------------------------------------------------------------------------

Title:
CoverHunter: Cover Song Identification with Refined Attention and  Alignments

Abstract: Abstract: Cover song identification (CSI) focuses on finding the same music
with different versions in reference anchors given a query track. In this
paper, we propose a novel system named CoverHunter that overcomes the
shortcomings of existing detection schemes by exploring richer features with
refined attention and alignments. CoverHunter contains three key modules: 1) A
convolution-augmented transformer (i.e., Conformer) structure that captures
both local and global feature interactions in contrast to previous methods
mainly relying on convolutional neural networks; 2) An attention-based time
pooling module that further exploits the attention in the time dimension; 3) A
novel coarse-to-fine training scheme that first trains a network to roughly
align the song chunks and then refines the network by training on the aligned
chunks. At the same time, we also summarize some important training tricks used
in our system that help achieve better results. Experiments on several standard
CSI datasets show that our method significantly improves over state-of-the-art
methods with an embedding size of 128 (2.3% on SHS100K-TEST and 17.7% on
DaTacos).


------------------------------------------------------------------------------

Title:
Learning by Analogy: Diverse Questions Generation in Math Word Problem

Abstract: Solving math word problem (MWP) with AI techniques has recently made great
progress with the success of deep neural networks (DNN), but it is far from
being solved. We argue that the ability of learning by analogy is essential for
an MWP solver to better understand same problems which may typically be
formulated in diverse ways. However most existing works exploit the shortcut
learning to train MWP solvers simply based on samples with a single question.
In lack of diverse questions, these methods merely learn shallow heuristics. In
this paper, we make a first attempt to solve MWPs by generating diverse yet
consistent questions/equations. Given a typical MWP including the scenario
description, question, and equation (i.e., answer), we first generate multiple
consistent equations via a group of heuristic rules. We then feed them to a
question generator together with the scenario to obtain the corresponding
diverse questions, forming a new MWP with a variety of questions and equations.
Finally we engage a data filter to remove those unreasonable MWPs, keeping the
high-quality augmented ones. To evaluate the ability of learning by analogy for
an MWP solver, we generate a new MWP dataset (called DiverseMath23K) with
diverse questions by extending the current benchmark Math23K. Extensive
experimental results demonstrate that our proposed method can generate
high-quality diverse questions with corresponding equations, further leading to
performance improvement on Diverse-Math23K. The code and dataset is available
at: this https URL


------------------------------------------------------------------------------

Title:
Counterfactuals Modulo Temporal Logics

Abstract: Lewis' theory of counterfactuals is the foundation of many contemporary
notions of causality. In this paper, we extend this theory in the temporal
direction to enable symbolic counterfactual reasoning on infinite sequences,
such as counterexamples found by a model checker and trajectories produced by a
reinforcement learning agent. In particular, our extension considers a more
relaxed notion of similarity between worlds and proposes two additional
counterfactual operators that close a semantic gap between the previous two in
this more general setting. Further, we consider versions of counterfactuals
that minimize the distance to the witnessing counterfactual worlds, a common
requirement in causal analysis. To automate counterfactual reasoning in the
temporal domain, we introduce a logic that combines temporal and counterfactual
operators, and outline decision procedures for the satisfiability and
trace-checking problems of this logic.


------------------------------------------------------------------------------

Title:
Fast Algorithms for Directed Graph Partitioning Using Flows and  Reweighted Eigenvalues

Abstract: We consider a new semidefinite programming relaxation for directed edge
expansion, which is obtained by adding triangle inequalities to the reweighted
eigenvalue formulation. Applying the matrix multiplicative weight update method
to this relaxation, we derive almost linear-time algorithms to achieve
$O(\sqrt{\log{n}})$-approximation and Cheeger-type guarantee for directed edge
expansion, as well as an improved cut-matching game for directed graphs. This
provides a primal-dual flow-based framework to obtain the best known algorithms
for directed graph partitioning. The same approach also works for vertex
expansion and for hypergraphs, providing a simple and unified approach to
achieve the best known results for different expansion problems and different
algorithmic techniques.


------------------------------------------------------------------------------

Title:
Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and  Text Integration

Abstract: Although instruction-tuned large language models (LLMs) have exhibited
remarkable capabilities across various NLP tasks, their effectiveness on other
data modalities beyond text has not been fully studied. In this work, we
propose Macaw-LLM, a novel multi-modal LLM that seamlessly integrates visual,
audio, and textual information. Macaw-LLM consists of three main components: a
modality module for encoding multi-modal data, a cognitive module for
harnessing pretrained LLMs, and an alignment module for harmonizing diverse
representations. Our novel alignment module seamlessly bridges multi-modal
features to textual features, simplifying the adaptation process from the
modality modules to the cognitive module. In addition, we construct a
large-scale multi-modal instruction dataset in terms of multi-turn dialogue,
including 69K image instances and 50K video instances. We have made our data,
code and model publicly available, which we hope can pave the way for future
research in multi-modal LLMs and expand the capabilities of LLMs to handle
diverse data modalities and address complex real-world scenarios.


------------------------------------------------------------------------------

Title:
Rethinking Document-Level Relation Extraction: A Reality Check

Abstract: Recently, numerous efforts have continued to push up performance boundaries
of document-level relation extraction (DocRE) and have claimed significant
progress in DocRE. In this paper, we do not aim at proposing a novel model for
DocRE. Instead, we take a closer look at the field to see if these performance
gains are actually true. By taking a comprehensive literature review and a
thorough examination of popular DocRE datasets, we find that these performance
gains are achieved upon a strong or even untenable assumption in common: all
named entities are perfectly localized, normalized, and typed in advance. Next,
we construct four types of entity mention attacks to examine the robustness of
typical DocRE models by behavioral probing. We also have a close check on model
usability in a more realistic setting. Our findings reveal that most of current
DocRE models are vulnerable to entity mention attacks and difficult to be
deployed in real-world end-user NLP applications. Our study calls more
attentions for future research to stop simplifying problem setups, and to model
DocRE in the wild rather than in an unrealistic Utopian world.


------------------------------------------------------------------------------

Title:
Voting Booklet Bias: Stance Detection in Swiss Federal Communication

Abstract: In this study, we use recent stance detection methods to study the stance
(for, against or neutral) of statements in official information booklets for
voters. Our main goal is to answer the fundamental question: are topics to be
voted on presented in a neutral way?
To this end, we first train and compare several models for stance detection
on a large dataset about Swiss politics. We find that fine-tuning an M-BERT
model leads to the best accuracy. We then use our best model to analyze the
stance of utterances extracted from the Swiss federal voting booklet concerning
the Swiss popular votes of September 2022, which is the main goal of this
project.
We evaluated the models in both a multilingual as well as a monolingual
context for German, French, and Italian. Our analysis shows that some issues
are heavily favored while others are more balanced, and that the results are
largely consistent across languages.
Our findings have implications for the editorial process of future voting
booklets and the design of better automated systems for analyzing political
discourse. The data and code accompanying this paper are available at
this https URL


------------------------------------------------------------------------------

Title:
CMMLU: Measuring massive multitask language understanding in Chinese

Abstract: As the capabilities of large language models (LLMs) continue to advance,
evaluating their performance becomes increasingly crucial and challenging. This
paper aims to bridge this gap by introducing CMMLU, a comprehensive Chinese
benchmark that covers various subjects, including natural science, social
sciences, engineering, and humanities. We conduct a thorough evaluation of 18
advanced multilingual- and Chinese-oriented LLMs, assessing their performance
across different subjects and settings. The results reveal that most existing
LLMs struggle to achieve an average accuracy of 50%, even when provided with
in-context examples and chain-of-thought prompts, whereas the random baseline
stands at 25%. This highlights significant room for improvement in LLMs.
Additionally, we conduct extensive experiments to identify factors impacting
the models' performance and propose directions for enhancing LLMs. CMMLU fills
the gap in evaluating the knowledge and reasoning capabilities of large
language models within the Chinese context.


------------------------------------------------------------------------------

Title:
Who Let the Smart Toaster Hack the House? An Investigation into the  Security Vulnerabilities of Consumer IoT Devices

Abstract: For smart homes to be safe homes, they must be designed with security in
mind. Yet, despite the widespread proliferation of connected digital
technologies in the home environment, there is a lack of research evaluating
the security vulnerabilities and potential risks present within these systems.
Our research presents a comprehensive methodology for conducting systematic IoT
security attacks, intercepting network traffic and evaluating the security
risks of smart home devices. We perform thousands of automated experiments
using 11 popular commercial IoT devices when deployed in a testbed, exposed to
a series of real deployed attacks (flooding, port scanning and OS scanning).
Our findings indicate that these devices are vulnerable to security attacks and
our results are relevant to the security research community, device engineers
and the users who rely on these technologies in their daily lives.


------------------------------------------------------------------------------

Title:
Improving Explainability of Disentangled Representations using  Multipath-Attribution Mappings

Abstract: Explainable AI aims to render model behavior understandable by humans, which
can be seen as an intermediate step in extracting causal relations from
correlative patterns. Due to the high risk of possible fatal decisions in
image-based clinical diagnostics, it is necessary to integrate explainable AI
into these safety-critical systems. Current explanatory methods typically
assign attribution scores to pixel regions in the input image, indicating their
importance for a model's decision. However, they fall short when explaining why
a visual feature is used. We propose a framework that utilizes interpretable
disentangled representations for downstream-task prediction. Through
visualizing the disentangled representations, we enable experts to investigate
possible causation effects by leveraging their domain knowledge. Additionally,
we deploy a multi-path attribution mapping for enriching and validating
explanations. We demonstrate the effectiveness of our approach on a synthetic
benchmark suite and two medical datasets. We show that the framework not only
acts as a catalyst for causal relation extraction but also enhances model
robustness by enabling shortcut detection without the need for testing under
distribution shifts.


------------------------------------------------------------------------------

Title:
A Graphical Formalism for Commonsense Reasoning with Recipes

Abstract: Whilst cooking is a very important human activity, there has been little
consideration given to how we can formalize recipes for use in a reasoning
framework. We address this need by proposing a graphical formalization that
captures the comestibles (ingredients, intermediate food items, and final
products), and the actions on comestibles in the form of a labelled bipartite
graph. We then propose formal definitions for comparing recipes, for composing
recipes from subrecipes, and for deconstructing recipes into subrecipes. We
also introduce and compare two formal definitions for substitution into recipes
which are required when there are missing ingredients, or some actions are not
possible, or because there is a need to change the final product somehow.


------------------------------------------------------------------------------

Title:
High-Resolution Convolutional Neural Networks on Homomorphically  Encrypted Data via Sharding Ciphertexts

Abstract: Recently, Deep Convolutional Neural Networks (DCNNs) including the ResNet-20
architecture have been privately evaluated on encrypted, low-resolution data
with the Residue-Number-System Cheon-Kim-Kim-Song (RNS-CKKS) homomorphic
encryption scheme. We extend methods for evaluating DCNNs on images with larger
dimensions and many channels, beyond what can be stored in single ciphertexts.
Additionally, we simplify and improve the efficiency of the recently introduced
multiplexed image format, demonstrating that homomorphic evaluation can work
with standard, row-major matrix packing and results in encrypted inference time
speedups by $4.6-6.5\times$. We also show how existing DCNN models can be
regularized during the training process to further improve efficiency and
accuracy. These techniques are applied to homomorphically evaluate a DCNN with
high accuracy on the high-resolution ImageNet dataset for the first time,
achieving $80.2\%$ top-1 accuracy. We also achieve the highest reported
accuracy of homomorphically evaluated CNNs on the CIFAR-10 dataset of $98.3\%$.


------------------------------------------------------------------------------

Title:
Multi-Loss Convolutional Network with Time-Frequency Attention for  Speech Enhancement

Abstract: The Dual-Path Convolution Recurrent Network (DPCRN) was proposed to
effectively exploit time-frequency domain information. By combining the DPRNN
module with Convolution Recurrent Network (CRN), the DPCRN obtained a promising
performance in speech separation with a limited model size. In this paper, we
explore self-attention in the DPCRN module and design a model called Multi-Loss
Convolutional Network with Time-Frequency Attention(MNTFA) for speech
enhancement. We use self-attention modules to exploit the long-time
information, where the intra-chunk self-attentions are used to model the
spectrum pattern and the inter-chunk self-attention are used to model the
dependence between consecutive frames. Compared to DPRNN, axial self-attention
greatly reduces the need for memory and computation, which is more suitable for
long sequences of speech signals. In addition, we propose a joint training
method of a multi-resolution STFT loss and a WavLM loss using a pre-trained
WavLM network. Experiments show that with only 0.23M parameters, the proposed
model achieves a better performance than DPCRN.


------------------------------------------------------------------------------

Title:
Sampling-Based Techniques for Training Deep Neural Networks with Limited  Computational Resources: A Scalability Evaluation

Abstract: Deep neural networks are superior to shallow networks in learning complex
representations. As such, there is a fast-growing interest in utilizing them in
large-scale settings. The training process of neural networks is already known
to be time-consuming, and having a deep architecture only aggravates the issue.
This process consists mostly of matrix operations, among which matrix
multiplication is the bottleneck. Several sampling-based techniques have been
proposed for speeding up the training time of deep neural networks by
approximating the matrix products. These techniques fall under two categories:
(i) sampling a subset of nodes in every hidden layer as active at every
iteration and (ii) sampling a subset of nodes from the previous layer to
approximate the current layer's activations using the edges from the sampled
nodes. In both cases, the matrix products are computed using only the selected
samples. In this paper, we evaluate the scalability of these approaches on CPU
machines with limited computational resources. Making a connection between the
two research directions as special cases of approximating matrix
multiplications in the context of neural networks, we provide a negative
theoretical analysis that shows feedforward approximation is an obstacle
against scalability. We conduct comprehensive experimental evaluations that
demonstrate the most pressing challenges and limitations associated with the
studied approaches. We observe that the hashing-based node selection method is
not scalable to a large number of layers, confirming our theoretical analysis.
Finally, we identify directions for future research.


------------------------------------------------------------------------------

Title:
Can ChatGPT pass the Vietnamese National High School Graduation  Examination?

Abstract: This research article highlights the potential of AI-powered chatbots in
education and presents the results of using ChatGPT, a large language model, to
complete the Vietnamese National High School Graduation Examination (VNHSGE).
The study dataset included 30 essays in the literature test case and 1,700
multiple-choice questions designed for other subjects. The results showed that
ChatGPT was able to pass the examination with an average score of 6-7,
demonstrating the technology's potential to revolutionize the educational
landscape. The analysis of ChatGPT performance revealed its proficiency in a
range of subjects, including mathematics, English, physics, chemistry, biology,
history, geography, civic education, and literature, which suggests its
potential to provide effective support for learners. However, further research
is needed to assess ChatGPT performance on more complex exam questions and its
potential to support learners in different contexts. As technology continues to
evolve and improve, we can expect to see the use of AI tools like ChatGPT
become increasingly common in educational settings, ultimately enhancing the
educational experience for both students and educators.


------------------------------------------------------------------------------

Title:
Transformer-aided Wireless Image Transmission with Channel Feedback

Abstract: This paper presents a novel wireless image transmission paradigm that can
exploit feedback from the receiver, called DeepJSCC-ViT-f. We consider a block
feedback channel model, where the transmitter receives noiseless/noisy channel
output feedback after each block. The proposed scheme employs a single encoder
to facilitate transmission over multiple blocks, refining the receiver's
estimation at each block. Specifically, the unified encoder of DeepJSCC-ViT-f
can leverage the semantic information from the source image, and acquire
channel state information and the decoder's current belief about the source
image from the feedback signal to generate coded symbols at each block.
Numerical experiments show that our DeepJSCC-ViT-f scheme achieves
state-of-the-art transmission performance with robustness to noise in the
feedback link. Additionally, DeepJSCC-ViT-f can adapt to the channel condition
directly through feedback without the need for separate channel estimation. We
further extend the scope of the DeepJSCC-ViT-f approach to include the
broadcast channel, which enables the transmitter to generate broadcast codes in
accordance with signal semantics and channel feedback from individual
receivers.


------------------------------------------------------------------------------

Title:
X-Rel: Energy-Efficient and Low-Overhead Approximate Reliability  Framework for Error-Tolerant Applications Deployed in Critical Systems

Abstract: Triple Modular Redundancy (TMR) is one of the most common techniques in
fault-tolerant systems, in which the output is determined by a majority voter.
However, the design diversity of replicated modules and/or soft errors that are
more likely to happen in the nanoscale era may affect the majority voting
scheme. Besides, the significant overheads of the TMR scheme may limit its
usage in energy consumption and area-constrained critical systems. However, for
most inherently error-resilient applications such as image processing and
vision deployed in critical systems (like autonomous vehicles and robotics),
achieving a given level of reliability has more priority than precise results.
Therefore, these applications can benefit from the approximate computing
paradigm to achieve higher energy efficiency and a lower area. This paper
proposes an energy-efficient approximate reliability (X-Rel) framework to
overcome the aforementioned challenges of the TMR systems and get the full
potential of approximate computing without sacrificing the desired reliability
constraint and output quality. The X-Rel framework relies on relaxing the
precision of the voter based on a systematical error bounding method that
leverages user-defined quality and reliability constraints. Afterward, the size
of the achieved voter is used to approximate the TMR modules such that the
overall area and energy consumption are minimized. The effectiveness of
employing the proposed X-Rel technique in a TMR structure, for different
quality constraints as well as with various reliability bounds are evaluated in
a 15-nm FinFET technology. The results of the X-Rel voter show delay, area, and
energy consumption reductions of up to 86%, 87%, and 98%, respectively, when
compared to those of the state-of-the-art approximate TMR voters.


------------------------------------------------------------------------------

Title:
When and Why Momentum Accelerates SGD:An Empirical Study

Abstract: Momentum has become a crucial component in deep learning optimizers,
necessitating a comprehensive understanding of when and why it accelerates
stochastic gradient descent (SGD). To address the question of ''when'', we
establish a meaningful comparison framework that examines the performance of
SGD with Momentum (SGDM) under the \emph{effective learning rates} $\eta_{ef}$,
a notion unifying the influence of momentum coefficient $\mu$ and batch size
$b$ over learning rate $\eta$. In the comparison of SGDM and SGD with the same
effective learning rate and the same batch size, we observe a consistent
pattern: when $\eta_{ef}$ is small, SGDM and SGD experience almost the same
empirical training losses; when $\eta_{ef}$ surpasses a certain threshold, SGDM
begins to perform better than SGD. Furthermore, we observe that the advantage
of SGDM over SGD becomes more pronounced with a larger batch size. For the
question of ``why'', we find that the momentum acceleration is closely related
to \emph{abrupt sharpening} which is to describe a sudden jump of the
directional Hessian along the update direction. Specifically, the misalignment
between SGD and SGDM happens at the same moment that SGD experiences abrupt
sharpening and converges slower. Momentum improves the performance of SGDM by
preventing or deferring the occurrence of abrupt sharpening. Together, this
study unveils the interplay between momentum, learning rates, and batch sizes,
thus improving our understanding of momentum acceleration.


------------------------------------------------------------------------------

Title:
A9 Intersection Dataset: All You Need for Urban 3D Camera-LiDAR Roadside  Perception

Abstract: Intelligent Transportation Systems (ITS) allow a drastic expansion of the
visibility range and decrease occlusions for autonomous driving. To obtain
accurate detections, detailed labeled sensor data for training is required.
Unfortunately, high-quality 3D labels of LiDAR point clouds from the
infrastructure perspective of an intersection are still rare. Therefore, we
provide the A9 Intersection Dataset, which consists of labeled LiDAR point
clouds and synchronized camera images. Here, we recorded the sensor output from
two roadside cameras and LiDARs mounted on intersection gantry bridges. The
point clouds were labeled in 3D by experienced annotators. Furthermore, we
provide calibration data between all sensors, which allow the projection of the
3D labels into the camera images and an accurate data fusion. Our dataset
consists of 4.8k images and point clouds with more than 57.4k manually labeled
3D boxes. With ten object classes, it has a high diversity of road users in
complex driving maneuvers, such as left and right turns, overtaking, and
U-turns. In experiments, we provided multiple baselines for the perception
tasks. Overall, our dataset is a valuable contribution to the scientific
community to perform complex 3D camera-LiDAR roadside perception tasks. Find
data, code, and more information at this https URL


------------------------------------------------------------------------------

Title:
Detecting Misuses of Security APIs: A Systematic Review

Abstract: Security Application Programming Interfaces (APIs) play a vital role in
ensuring software security. However, misuse of security APIs may introduce
vulnerabilities that can be exploited by hackers. API design complexities,
inadequate documentation and insufficient security training are some of the
reasons for misusing security APIs. In order to help developers and
organizations, software security community have devised and evaluated several
approaches to detecting misuses of security APIs. We rigorously analyzed and
synthesized the literature on security APIs misuses for building a body of
knowledge on the topic. Our review has identified and discussed the security
APIs studied from misuse perspective, the types of reported misuses and the
approaches developed to detect misuses and how the proposed approaches have
been evaluated. Our review has also highlighted the open research issues for
advancing the state-of-the-art of detecting misuse of security APIs.


------------------------------------------------------------------------------

Title:
Opinion Tree Parsing for Aspect-based Sentiment Analysis

Abstract: Extracting sentiment elements using pre-trained generative models has
recently led to large improvements in aspect-based sentiment analysis
benchmarks. However, these models always need large-scale computing resources,
and they also ignore explicit modeling of structure between sentiment elements.
To address these challenges, we propose an opinion tree parsing model, aiming
to parse all the sentiment elements from an opinion tree, which is much faster,
and can explicitly reveal a more comprehensive and complete aspect-level
sentiment structure. In particular, we first introduce a novel context-free
opinion grammar to normalize the opinion tree structure. We then employ a
neural chart-based opinion tree parser to fully explore the correlations among
sentiment elements and parse them into an opinion tree structure. Extensive
experiments show the superiority of our proposed model and the capacity of the
opinion tree parser with the proposed context-free opinion grammar. More
importantly, the results also prove that our model is much faster than previous
models.


------------------------------------------------------------------------------

Title:
1st Solution Places for CVPR 2023 UG$^{\textbf{2}}$+ Challenge Track  2.1-Text Recognition through Atmospheric Turbulence

Abstract: In this technical report, we present the solution developed by our team
VIELab-HUST for text recognition through atmospheric turbulence in Track 2.1 of
the CVPR 2023 UG$^{2}$+ challenge. Our solution involves an efficient
multi-stage framework that restores a high-quality image from distorted frames.
Specifically, a frame selection algorithm based on sharpness is first utilized
to select the sharpest set of distorted frames. Next, each frame in the
selected frames is aligned to suppress geometric distortion through
optical-flow-based image registration. Then, a region-based image fusion method
with DT-CWT is utilized to mitigate the blur caused by the turbulence. Finally,
a learning-based deartifacts method is applied to remove the artifacts in the
fused image, generating a high-quality outuput. Our framework can handle both
hot-air text dataset and turbulence text dataset provided in the final testing
phase and achieved 1st place in text recognition accuracy. Our code will be
available at this https URL


------------------------------------------------------------------------------

Title:
BED: Bi-Encoder-Based Detectors for Out-of-Distribution Detection

Abstract: This paper introduces a novel method leveraging bi-encoder-based detectors
along with a comprehensive study comparing different out-of-distribution (OOD)
detection methods in NLP using different feature extractors. The feature
extraction stage employs popular methods such as Universal Sentence Encoder
(USE), BERT, MPNET, and GLOVE to extract informative representations from
textual data. The evaluation is conducted on several datasets, including
CLINC150, ROSTD-Coarse, SNIPS, and YELLOW. Performance is assessed using
metrics such as F1-Score, MCC, FPR@90, FPR@95, AUPR, an AUROC. The experimental
results demonstrate that the proposed bi-encoder-based detectors outperform
other methods, both those that require OOD labels in training and those that do
not, across all datasets, showing great potential for OOD detection in NLP. The
simplicity of the training process and the superior detection performance make
them applicable to real-world scenarios. The presented methods and benchmarking
metrics serve as a valuable resource for future research in OOD detection,
enabling further advancements in this field. The code and implementation
details can be found on our GitHub repository:
this https URL


------------------------------------------------------------------------------

Title:
Med-MMHL: A Multi-Modal Dataset for Detecting Human- and LLM-Generated  Misinformation in the Medical Domain

Abstract: The pervasive influence of misinformation has far-reaching and detrimental
effects on both individuals and society. The COVID-19 pandemic has witnessed an
alarming surge in the dissemination of medical misinformation. However,
existing datasets pertaining to misinformation predominantly focus on textual
information, neglecting the inclusion of visual elements, and tend to center
solely on COVID-19-related misinformation, overlooking misinformation
surrounding other diseases. Furthermore, the potential of Large Language Models
(LLMs), such as the ChatGPT developed in late 2022, in generating
misinformation has been overlooked in previous works. To overcome these
limitations, we present Med-MMHL, a novel multi-modal misinformation detection
dataset in a general medical domain encompassing multiple diseases. Med-MMHL
not only incorporates human-generated misinformation but also includes
misinformation generated by LLMs like ChatGPT. Our dataset aims to facilitate
comprehensive research and development of methodologies for detecting
misinformation across diverse diseases and various scenarios, including human
and LLM-generated misinformation detection at the sentence, document, and
multi-modal levels. To access our dataset and code, visit our GitHub
repository: \url{this https URL}.


------------------------------------------------------------------------------

Title:
STARSS23: An Audio-Visual Dataset of Spatial Recordings of Real Scenes  with Spatiotemporal Annotations of Sound Events

Abstract: While direction of arrival (DOA) of sound events is generally estimated from
multichannel audio data recorded in a microphone array, sound events usually
derive from visually perceptible source objects, e.g., sounds of footsteps come
from the feet of a walker. This paper proposes an audio-visual sound event
localization and detection (SELD) task, which uses multichannel audio and video
information to estimate the temporal activation and DOA of target sound events.
Audio-visual SELD systems can detect and localize sound events using signals
from a microphone array and audio-visual correspondence. We also introduce an
audio-visual dataset, Sony-TAu Realistic Spatial Soundscapes 2023 (STARSS23),
which consists of multichannel audio data recorded with a microphone array,
video data, and spatiotemporal annotation of sound events. Sound scenes in
STARSS23 are recorded with instructions, which guide recording participants to
ensure adequate activity and occurrences of sound events. STARSS23 also serves
human-annotated temporal activation labels and human-confirmed DOA labels,
which are based on tracking results of a motion capture system. Our benchmark
results show that the audio-visual SELD system achieves lower localization
error than the audio-only system. The data is available at
this https URL


------------------------------------------------------------------------------

Title:
Network Architecture Design toward Convergence of Mobile Applications  and Networks

Abstract: With the quick proliferation of extended reality (XR) services, the mobile
communications networks are faced with gigantic challenges to meet the
diversified and challenging service requirements. A tight coordination or even
convergence of applications and mobile networks is highly motivated. In this
paper, a multi-domain (e.g. application layer, transport layer, the core
network, radio access network, user equipment) coordination scheme is first
proposed, which facilitates a tight coordination between applications and
networks based on the current 5G networks. Toward the convergence of
applications and networks, a network architectures with cross-domain joint
processing capability is further proposed for 6G mobile communications and
beyond. Both designs are able to provide more accurate information of the
quality of experience (QoE) and quality of service (QoS), thus paving the path
for the joint optimization of applications and networks. The benefits of the
QoE assisted scheduling are further investigated via simulations. A new
QoE-oriented fairness metric is further proposed, which is capable of ensuring
better fairness when different services are scheduled. Future research
directions and their standardization impacts are also identified. Toward
optimized end-to-end service provision, the paradigm shift from loosely coupled
to converged design of applications and wireless communication networks is
indispensable.


------------------------------------------------------------------------------

Title:
RecFusion: A Binomial Diffusion Process for 1D Data for Recommendation

Abstract: In this paper we propose RecFusion, which comprise a set of diffusion models
for recommendation. Unlike image data which contain spatial correlations, a
user-item interaction matrix, commonly utilized in recommendation, lacks
spatial relationships between users and items. We formulate diffusion on a 1D
vector and propose binomial diffusion, which explicitly models binary user-item
interactions with a Bernoulli process. We show that RecFusion approaches the
performance of complex VAE baselines on the core recommendation setting (top-n
recommendation for binary non-sequential feedback) and the most common datasets
(MovieLens and Netflix). Our proposed diffusion models that are specialized for
1D and/or binary setups have implications beyond recommendation systems, such
as in the medical domain with MRI and CT scans.


------------------------------------------------------------------------------

Title:
Datasheets for Machine Learning Sensors

Abstract: Machine learning (ML) sensors offer a new paradigm for sensing that enables
intelligence at the edge while empowering end-users with greater control of
their data. As these ML sensors play a crucial role in the development of
intelligent devices, clear documentation of their specifications,
functionalities, and limitations is pivotal. This paper introduces a standard
datasheet template for ML sensors and discusses its essential components
including: the system's hardware, ML model and dataset attributes, end-to-end
performance metrics, and environmental impact. We provide an example datasheet
for our own ML sensor and discuss each section in detail. We highlight how
these datasheets can facilitate better understanding and utilization of sensor
data in ML applications, and we provide objective measures upon which system
performance can be evaluated and compared. Together, ML sensors and their
datasheets provide greater privacy, security, transparency, explainability,
auditability, and user-friendliness for ML-enabled embedded systems. We
conclude by emphasizing the need for standardization of datasheets across the
broader ML community to ensure the responsible and effective use of sensor
data.


------------------------------------------------------------------------------

Title:
Category Theory in Isabelle/HOL as a Basis for Meta-logical  Investigation

Abstract: This paper presents meta-logical investigations based on category theory
using the proof assistant Isabelle/HOL. We demonstrate the potential of a free
logic based shallow semantic embedding of category theory by providing a
formalization of the notion of elementary topoi. Additionally, we formalize
symmetrical monoidal closed categories expressing the denotational semantic
model of intuitionistic multiplicative linear logic. Next to these
meta-logical-investigations, we contribute to building an Isabelle category
theory library, with a focus on ease of use in the formalization beyond
category theory itself. This work paves the way for future formalizations based
on category theory and demonstrates the power of automated reasoning in
investigating meta-logical questions.


------------------------------------------------------------------------------

Title:
Hyperbolic Representation Learning: Revisiting and Advancing

Abstract: The non-Euclidean geometry of hyperbolic spaces has recently garnered
considerable attention in the realm of representation learning. Current
endeavors in hyperbolic representation largely presuppose that the underlying
hierarchies can be automatically inferred and preserved through the adaptive
optimization process. This assumption, however, is questionable and requires
further validation. In this work, we first introduce a position-tracking
mechanism to scrutinize existing prevalent \hlms, revealing that the learned
representations are sub-optimal and unsatisfactory. To address this, we propose
a simple yet effective method, hyperbolic informed embedding (HIE), by
incorporating cost-free hierarchical information deduced from the hyperbolic
distance of the node to origin (i.e., induced hyperbolic norm) to advance
existing \hlms. The proposed method HIE is both task-agnostic and
model-agnostic, enabling its seamless integration with a broad spectrum of
models and tasks. Extensive experiments across various models and different
tasks demonstrate the versatility and adaptability of the proposed method.
Remarkably, our method achieves a remarkable improvement of up to 21.4\%
compared to the competing baselines.


------------------------------------------------------------------------------

Title:
DiAReL: Reinforcement Learning with Disturbance Awareness for Robust  Sim2Real Policy Transfer in Robot Control

Abstract: Delayed Markov decision processes fulfill the Markov property by augmenting
the state space of agents with a finite time window of recently committed
actions. In reliance with these state augmentations, delay-resolved
reinforcement learning algorithms train policies to learn optimal interactions
with environments featured with observation or action delays. Although such
methods can directly be trained on the real robots, due to sample inefficiency,
limited resources or safety constraints, a common approach is to transfer
models trained in simulation to the physical robot. However, robotic
simulations rely on approximated models of the physical systems, which hinders
the sim2real transfer. In this work, we consider various uncertainties in the
modelling of the robot's dynamics as unknown intrinsic disturbances applied on
the system input. We introduce a disturbance-augmented Markov decision process
in delayed settings as a novel representation to incorporate disturbance
estimation in training on-policy reinforcement learning algorithms. The
proposed method is validated across several metrics on learning a robotic
reaching task and compared with disturbance-unaware baselines. The results show
that the disturbance-augmented models can achieve higher stabilization and
robustness in the control response, which in turn improves the prospects of
successful sim2real transfer.


------------------------------------------------------------------------------

Title:
Entanglement Distribution in Satellite-based Dynamic Quantum Networks

Abstract: Low Earth Orbit (LEO) satellites present a compelling opportunity for the
establishment of a global quantum information network. However, satellite-based
entanglement distribution from a networking perspective has not been fully
investigated. Existing works often do not account for satellite movement over
time when distributing entanglement and/or often do not permit entanglement
distribution along inter-satellite links, which are two shortcomings we address
in this paper. We first define a system model which considers both satellite
movement over time and inter-satellite links. We next formulate the optimal
entanglement distribution (OED) problem under this system model and show how to
convert the OED problem in a dynamic physical network to one in a static
logical graph which can be used to solve the OED problem in the dynamic
physical network. We then propose a polynomial time greedy algorithm for
computing satellite-assisted multi-hop entanglement paths. We also design an
integer linear programming (ILP)-based algorithm to compute optimal solutions
as a baseline to study the performance of our greedy algorithm. We present
evaluation results to demonstrate the advantage of our model and algorithms.


------------------------------------------------------------------------------

Title:
Quantum Game Theory meets Quantum Networks

Abstract: Classical game theory is a powerful tool focusing on optimized resource
distribution, allocation and sharing in classical wired and wireless networks.
As quantum networks are emerging as a means of providing true connectivity
between quantum computers, it is imperative and crucial to exploit game theory
for addressing challenges like entanglement distribution and access, routing,
topology extraction and inference for quantum networks. Quantum networks
provide the promising opportunity of employing quantum games owing to their
inherent capability of generating and sharing quantum states. Besides, quantum
games offer enhanced payoffs and winning probabilities, new strategies and
equilibria, which are unimaginable in classical games. Employing quantum game
theory to solve fundamental challenges in quantum networks opens a new
fundamental research direction necessitating inter-disciplinary efforts. In
this article, we introduce a novel game-theoretical framework for exploiting
quantum strategies to solve, as archetypal example, one of the key
functionality of a quantum network, namely, the entanglement distribution. We
compare the quantum strategies with classical ones by showing the quantum
advantages in terms of link fidelity improvement and latency decrease in
communication. In future, we will generalize our game framework to optimize
entanglement distribution and access over any quantum network topology. We will
also explore how quantum games can be leveraged to address other challenges
like routing, optimization of quantum operations and topology design.


------------------------------------------------------------------------------

Title:
DiPlomat: A Dialogue Dataset for Situated Pragmatic Reasoning

Abstract: Pragmatic reasoning plays a pivotal role in deciphering implicit meanings
that frequently arise in real-life conversations and is essential for the
development of communicative social agents. In this paper, we introduce a novel
challenge, DiPlomat, aiming at benchmarking machines' capabilities on pragmatic
reasoning and situated conversational understanding. Compared with previous
works that treat different figurative expressions (e.g. metaphor, sarcasm) as
individual tasks, DiPlomat provides a cohesive framework towards general
pragmatic understanding. Our dataset is created through the utilization of
Amazon Mechanical Turk ( AMT ), resulting in a total of 4, 177 multi-turn
dialogues. In conjunction with the dataset, we propose two tasks, Pragmatic
Identification and Reasoning (PIR) and Conversational Question Answering (CQA).
Experimental results with state-of-the-art (SOTA) neural architectures reveal
several significant findings: 1) large language models ( LLMs) exhibit poor
performance in tackling this subjective domain; 2) comprehensive comprehension
of context emerges as a critical factor for establishing benign human-machine
interactions; 3) current models defect in the application of pragmatic
reasoning. As a result, we call on more attention to improve the ability of
context understanding, reasoning, and implied meaning modeling.


------------------------------------------------------------------------------

Title:
A surface finite element method for the Navier-Stokes equations on  evolving surfaces

Abstract: We introduce a surface finite element method for the numerical solution of
Navier-Stokes equations on evolving surfaces with a prescribed deformation of
the surface in normal direction. The method is based on approaches for the full
surface Navier-Stokes equations in the context of fluid-deformable surfaces and
adds a penalization of the normal component. Numerical results demonstrate the
same optimal order as proposed for surface (Navier-)Stokes equations on
stationary surfaces. The approach is applied to high-resolution 3D scans of
clothed bodies in motion to provide interactive virtual fluid-like clothing.


------------------------------------------------------------------------------

Title:
Context-Aware Change Detection With Semi-Supervised Learning

Abstract: Change detection using earth observation data plays a vital role in
quantifying the impact of disasters in affected areas. While data sources like
Sentinel-2 provide rich optical information, they are often hindered by cloud
cover, limiting their usage in disaster scenarios. However, leveraging
pre-disaster optical data can offer valuable contextual information about the
area such as landcover type, vegetation cover, soil types, enabling a better
understanding of the disaster's impact. In this study, we develop a model to
assess the contribution of pre-disaster Sentinel-2 data in change detection
tasks, focusing on disaster-affected areas. The proposed Context-Aware Change
Detection Network (CACDN) utilizes a combination of pre-disaster Sentinel-2
data, pre and post-disaster Sentinel-1 data and ancillary Digital Elevation
Models (DEM) data. The model is validated on flood and landslide detection and
evaluated using three metrics: Area Under the Precision-Recall Curve (AUPRC),
Intersection over Union (IoU), and mean IoU. The preliminary results show
significant improvement (4\%, AUPRC, 3-7\% IoU, 3-6\% mean IoU) in model's
change detection capabilities when incorporated with pre-disaster optical data
reflecting the effectiveness of using contextual information for accurate flood
and landslide detection.


------------------------------------------------------------------------------

Title:
SSCBench: A Large-Scale 3D Semantic Scene Completion Benchmark for  Autonomous Driving

Abstract: Semantic scene completion (SSC) is crucial for holistic 3D scene
understanding by jointly estimating semantics and geometry from sparse
observations. However, progress in SSC, particularly in autonomous driving
scenarios, is hindered by the scarcity of high-quality datasets. To overcome
this challenge, we introduce SSCBench, a comprehensive benchmark that
integrates scenes from widely-used automotive datasets (e.g., KITTI-360,
nuScenes, and Waymo). SSCBench follows an established setup and format in the
community, facilitating the easy exploration of the camera- and LiDAR-based SSC
across various real-world scenarios. We present quantitative and qualitative
evaluations of state-of-the-art algorithms on SSCBench and commit to
continuously incorporating novel automotive datasets and SSC algorithms to
drive further advancements in this field. Our resources are released on
this https URL


------------------------------------------------------------------------------

Title:
Description-Enhanced Label Embedding Contrastive Learning for Text  Classification

Abstract: Text Classification is one of the fundamental tasks in natural language
processing, which requires an agent to determine the most appropriate category
for input sentences. Recently, deep neural networks have achieved impressive
performance in this area, especially Pre-trained Language Models (PLMs).
Usually, these methods concentrate on input sentences and corresponding
semantic embedding generation. However, for another essential component:
labels, most existing works either treat them as meaningless one-hot vectors or
use vanilla embedding methods to learn label representations along with model
training, underestimating the semantic information and guidance that these
labels reveal. To alleviate this problem and better exploit label information,
in this paper, we employ Self-Supervised Learning (SSL) in model learning
process and design a novel self-supervised Relation of Relation (R2)
classification task for label utilization from a one-hot manner perspective.
Then, we propose a novel Relation of Relation Learning Network (R2-Net) for
text classification, in which text classification and R2 classification are
treated as optimization targets. Meanwhile, triplet loss is employed to enhance
the analysis of differences and connections among labels. Moreover, considering
that one-hot usage is still short of exploiting label information, we
incorporate external knowledge from WordNet to obtain multi-aspect descriptions
for label semantic learning and extend R2-Net to a novel Description-Enhanced
Label Embedding network (DELE) from a label embedding perspective. ...


------------------------------------------------------------------------------

Title:
Real-Time Network-Level Traffic Signal Control: An Explicit Multiagent  Coordination Method

Abstract: Efficient traffic signal control (TSC) has been one of the most useful ways
for reducing urban road congestion. Key to the challenge of TSC includes 1) the
essential of real-time signal decision, 2) the complexity in traffic dynamics,
and 3) the network-level coordination. Recent efforts that applied
reinforcement learning (RL) methods can query policies by mapping the traffic
state to the signal decision in real-time, however, is inadequate for
unexpected traffic flows. By observing real traffic information, online
planning methods can compute the signal decisions in a responsive manner. We
propose an explicit multiagent coordination (EMC)-based online planning methods
that can satisfy adaptive, real-time and network-level TSC. By multiagent, we
model each intersection as an autonomous agent, and the coordination efficiency
is modeled by a cost (i.e., congestion index) function between neighbor
intersections. By network-level coordination, each agent exchanges messages
with respect to cost function with its neighbors in a fully decentralized
manner. By real-time, the message passing procedure can interrupt at any time
when the real time limit is reached and agents select the optimal signal
decisions according to the current message. Moreover, we prove our EMC method
can guarantee network stability by borrowing ideas from transportation domain.
Finally, we test our EMC method in both synthetic and real road network
datasets. Experimental results are encouraging: compared to RL and conventional
transportation baselines, our EMC method performs reasonably well in terms of
adapting to real-time traffic dynamics, minimizing vehicle travel time and
scalability to city-scale road networks.


------------------------------------------------------------------------------

Title:
Improving Image Tracing with Convolutional Autoencoders by High-Pass  Filter Preprocessing

Abstract: The process of transforming a raster image into a vector representation is
known as image tracing. This study looks into several processing methods that
include high-pass filtering, autoencoding, and vectorization to extract an
abstract representation of an image. According to the findings, rebuilding an
image with autoencoders, high-pass filtering it, and then vectorizing it can
represent the image more abstractly while increasing the effectiveness of the
vectorization process.


------------------------------------------------------------------------------

Title:
The Least Squares Finite Element Method for Elasticity Interface Problem  on Unfitted Mesh

Abstract: In this paper, we propose and analyze the least squares finite element
methods for the linear elasticity interface problem in the stress-displacement
system on unfitted meshes. We consider the cases that the interface is $C^2$ or
polygonal, and the exact solution $(\sigma,u)$ belongs to $H^s(div; \Omega_0
\cup \Omega_1) \times $H^{1+s}(\Omega_0 \cup \Omega_1)$ with $s > 1/2$. Two
types of least squares functionals are defined to seek the numerical solution.
The first is defined by simply applying the $L^2$ norm least squares principle,
and requires the condition $s \geq 1$. The second is defined with a discrete
minus norm, which is related to the inner product in $H^{-1/2}(\Gamma)$. The
use of this discrete minus norm results in a method of optimal convergence
rates and allows the exact solution has the regularity of any $s > 1/2$. The
stability near the interface for both methods is guaranteed by the ghost
penalty bilinear forms and we can derive the robust condition number estimates.
The convergence rates under $L^2$ norm and the energy norm are derived for both
methods. We illustrate the accuracy and the robustness of the proposed methods
by a series of numerical experiments for test problems in two and three
dimensions.


------------------------------------------------------------------------------

Title:
Infrastructure Crack Segmentation: Boundary Guidance Method and  Benchmark Dataset

Abstract: Cracks provide an essential indicator of infrastructure performance
degradation, and achieving high-precision pixel-level crack segmentation is an
issue of concern. Unlike the common research paradigms that adopt novel
artificial intelligence (AI) methods directly, this paper examines the inherent
characteristics of cracks so as to introduce boundary features into crack
identification and then builds a boundary guidance crack segmentation model
(BGCrack) with targeted structures and modules, including a high frequency
module, global information modeling module, joint optimization module, etc.
Extensive experimental results verify the feasibility of the proposed designs
and the effectiveness of the edge information in improving segmentation
results. In addition, considering that notable open-source datasets mainly
consist of asphalt pavement cracks because of ease of access, there is no
standard and widely recognized dataset yet for steel structures, one of the
primary structural forms in civil infrastructure. This paper provides a steel
crack dataset that establishes a unified and fair benchmark for the
identification of steel cracks.


------------------------------------------------------------------------------

Title:
Re-Benchmarking Pool-Based Active Learning for Binary Classification

Abstract: Active learning is a paradigm that significantly enhances the performance of
machine learning models when acquiring labeled data is expensive. While several
benchmarks exist for evaluating active learning strategies, their findings
exhibit some misalignment. This discrepancy motivates us to develop a
transparent and reproducible benchmark for the community. Our efforts result in
an open-sourced implementation
(this https URL) that is reliable and
extensible for future research. By conducting thorough re-benchmarking
experiments, we have not only rectified misconfigurations in existing benchmark
but also shed light on the under-explored issue of model compatibility, which
directly causes the observed discrepancy. Resolving the discrepancy reassures
that the uncertainty sampling strategy of active learning remains an effective
and preferred choice for most datasets. Our experience highlights the
importance of dedicating research efforts towards re-benchmarking existing
benchmarks to produce more credible results and gain deeper insights.


------------------------------------------------------------------------------

Title:
ArchGym: An Open-Source Gymnasium for Machine Learning Assisted  Architecture Design

Abstract: Machine learning is a prevalent approach to tame the complexity of design
space exploration for domain-specific architectures. Using ML for design space
exploration poses challenges. First, it's not straightforward to identify the
suitable algorithm from an increasing pool of ML methods. Second, assessing the
trade-offs between performance and sample efficiency across these methods is
inconclusive. Finally, lack of a holistic framework for fair, reproducible, and
objective comparison across these methods hinders progress of adopting ML-aided
architecture design space exploration and impedes creating repeatable
artifacts. To mitigate these challenges, we introduce ArchGym, an open-source
gym and easy-to-extend framework that connects diverse search algorithms to
architecture simulators. To demonstrate utility, we evaluate ArchGym across
multiple vanilla and domain-specific search algorithms in designing custom
memory controller, deep neural network accelerators, and custom SoC for AR/VR
workloads, encompassing over 21K experiments. Results suggest that with
unlimited samples, ML algorithms are equally favorable to meet user-defined
target specification if hyperparameters are tuned; no solution is necessarily
better than another (e.g., reinforcement learning vs. Bayesian methods). We
coin the term hyperparameter lottery to describe the chance for a search
algorithm to find an optimal design provided meticulously selected
hyperparameters. The ease of data collection and aggregation in ArchGym
facilitates research in ML-aided architecture design space exploration. As a
case study, we show this advantage by developing a proxy cost model with an
RMSE of 0.61% that offers a 2,000-fold reduction in simulation time. Code and
data for ArchGym is available at this https URL


------------------------------------------------------------------------------

Title:
Multilingual End to End Entity Linking

Abstract: Entity Linking is one of the most common Natural Language Processing tasks in
practical applications, but so far efficient end-to-end solutions with
multilingual coverage have been lacking, leading to complex model stacks. To
fill this gap, we release and open source BELA, the first fully end-to-end
multilingual entity linking model that efficiently detects and links entities
in texts in any of 97 languages. We provide here a detailed description of the
model and report BELA's performance on four entity linking datasets covering
high- and low-resource languages.


------------------------------------------------------------------------------

Title:
Langevin Thompson Sampling with Logarithmic Communication: Bandits and  Reinforcement Learning

Abstract: Thompson sampling (TS) is widely used in sequential decision making due to
its ease of use and appealing empirical performance. However, many existing
analytical and empirical results for TS rely on restrictive assumptions on
reward distributions, such as belonging to conjugate families, which limits
their applicability in realistic scenarios. Moreover, sequential decision
making problems are often carried out in a batched manner, either due to the
inherent nature of the problem or to serve the purpose of reducing
communication and computation costs. In this work, we jointly study these
problems in two popular settings, namely, stochastic multi-armed bandits (MABs)
and infinite-horizon reinforcement learning (RL), where TS is used to learn the
unknown reward distributions and transition dynamics, respectively. We propose
batched $\textit{Langevin Thompson Sampling}$ algorithms that leverage MCMC
methods to sample from approximate posteriors with only logarithmic
communication costs in terms of batches. Our algorithms are computationally
efficient and maintain the same order-optimal regret guarantees of
$\mathcal{O}(\log T)$ for stochastic MABs, and $\mathcal{O}(\sqrt{T})$ for RL.
We complement our theoretical findings with experimental results.


------------------------------------------------------------------------------

Title:
Towards Benchmarking and Improving the Temporal Reasoning Capability of  Large Language Models

Abstract: Reasoning about time is of fundamental importance. Many facts are
time-dependent. For example, athletes change teams from time to time, and
different government officials are elected periodically. Previous
time-dependent question answering (QA) datasets tend to be biased in either
their coverage of time spans or question types. In this paper, we introduce a
comprehensive probing dataset \tempreason to evaluate the temporal reasoning
capability of large language models. Our dataset includes questions of three
temporal reasoning levels. In addition, we also propose a novel learning
framework to improve the temporal reasoning capability of large language
models, based on temporal span extraction and time-sensitive reinforcement
learning. We conducted experiments in closed book QA, open book QA, and
reasoning QA settings and demonstrated the effectiveness of our approach. Our
code and data are released on this https URL


------------------------------------------------------------------------------

Title:
IsoEx: an explainable unsupervised approach to process event logs cyber  investigation

Abstract: 39 seconds. That is the timelapse between two consecutive cyber attacks as of
2023. Meaning that by the time you are done reading this abstract, about 1 or 2
additional cyber attacks would have occurred somewhere in the world. In this
context of highly increased frequency of cyber threats, Security Operation
Centers (SOC) and Computer Emergency Response Teams (CERT) can be overwhelmed.
In order to relieve the cybersecurity teams in their investigative effort and
help them focus on more added-value tasks, machine learning approaches and
methods started to emerge. This paper introduces a novel method, IsoEx, for
detecting anomalous and potentially problematic command lines during the
investigation of contaminated devices. IsoEx is built around a set of features
that leverages the log structure of the command line, as well as its
parent/child relationship, to achieve a greater accuracy than traditional
methods. To detect anomalies, IsoEx resorts to an unsupervised anomaly
detection technique that is both highly sensitive and lightweight. A key
contribution of the paper is its emphasis on interpretability, achieved through
the features themselves and the application of eXplainable Artificial
Intelligence (XAI) techniques and visualizations. This is critical to ensure
the adoption of the method by SOC and CERT teams, as the paper argues that the
current literature on machine learning for log investigation has not adequately
addressed the issue of explainability. This method was proven efficient in a
real-life environment as it was built to support a company\'s SOC and CERT


------------------------------------------------------------------------------

Title:
SF-TMN: SlowFast Temporal Modeling Network for Surgical Phase  Recognition

Abstract: Automatic surgical phase recognition is one of the key technologies to
support Video-Based Assessment (VBA) systems for surgical education. Utilizing
temporal information is crucial for surgical phase recognition, hence various
recent approaches extract frame-level features to conduct full video temporal
modeling. For better temporal modeling, we propose SlowFast Temporal Modeling
Network (SF-TMN) for surgical phase recognition that can not only achieve
frame-level full video temporal modeling but also achieve segment-level full
video temporal modeling. We employ a feature extraction network, pre-trained on
the target dataset, to extract features from video frames as the training data
for SF-TMN. The Slow Path in SF-TMN utilizes all frame features for frame
temporal modeling. The Fast Path in SF-TMN utilizes segment-level features
summarized from frame features for segment temporal modeling. The proposed
paradigm is flexible regarding the choice of temporal modeling networks. We
explore MS-TCN and ASFormer models as temporal modeling networks and experiment
with multiple combination strategies for Slow and Fast Paths. We evaluate
SF-TMN on Cholec80 surgical phase recognition task and demonstrate that SF-TMN
can achieve state-of-the-art results on all considered metrics. SF-TMN with
ASFormer backbone outperforms the state-of-the-art Not End-to-End(TCN) method
by 2.6% in accuracy and 7.4% in the Jaccard score. We also evaluate SF-TMN on
action segmentation datasets including 50salads, GTEA, and Breakfast, and
achieve state-of-the-art results. The improvement in the results shows that
combining temporal information from both frame level and segment level by
refining outputs with temporal refinement stages is beneficial for the temporal
modeling of surgical phases.


------------------------------------------------------------------------------

Title:
Exploring the MIT Mathematics and EECS Curriculum Using Large Language  Models

Abstract: We curate a comprehensive dataset of 4,550 questions and solutions from
problem sets, midterm exams, and final exams across all MIT Mathematics and
Electrical Engineering and Computer Science (EECS) courses required for
obtaining a degree. We evaluate the ability of large language models to fulfill
the graduation requirements for any MIT major in Mathematics and EECS. Our
results demonstrate that GPT-3.5 successfully solves a third of the entire MIT
curriculum, while GPT-4, with prompt engineering, achieves a perfect solve rate
on a test set excluding questions based on images. We fine-tune an open-source
large language model on this dataset. We employ GPT-4 to automatically grade
model responses, providing a detailed performance breakdown by course,
question, and answer type. By embedding questions in a low-dimensional space,
we explore the relationships between questions, topics, and classes and
discover which questions and classes are required for solving other questions
and classes through few-shot learning. Our analysis offers valuable insights
into course prerequisites and curriculum design, highlighting language models'
potential for learning and improving Mathematics and EECS education.


------------------------------------------------------------------------------

Title:
Digital signature schemes using non-square matrices or scrap  automorphisms

Abstract: We offer two very transparent digital signature schemes: one using non-square
matrices and the other using scrap automorphisms. The former can be easily
converted to a public key encryption scheme.


------------------------------------------------------------------------------

Title:
Exploring Isolated Musical Notes as Pre-training Data for Predominant  Instrument Recognition in Polyphonic Music

Abstract: With the growing amount of musical data available, automatic instrument
recognition, one of the essential problems in Music Information Retrieval
(MIR), is drawing more and more attention. While automatic recognition of
single instruments has been well-studied, it remains challenging for
polyphonic, multi-instrument musical recordings. This work presents our efforts
toward building a robust end-to-end instrument recognition system for
polyphonic multi-instrument music. We train our model using a pre-training and
fine-tuning approach: we use a large amount of monophonic musical data for
pre-training and subsequently fine-tune the model for the polyphonic ensemble.
In pre-training, we apply data augmentation techniques to alleviate the domain
gap between monophonic musical data and real-world music. We evaluate our
method on the IRMAS testing data, a polyphonic musical dataset comprising
professionally-produced commercial music recordings. Experimental results show
that our best model achieves a micro F1-score of 0.674 and an LRAP of 0.814,
meaning 10.9% and 8.9% relative improvement compared with the previous
state-of-the-art end-to-end approach. Also, we are able to build a lightweight
model, achieving competitive performance with only 519K trainable parameters.


------------------------------------------------------------------------------

Title:
Linguistic Binding in Diffusion Models: Enhancing Attribute  Correspondence through Attention Map Alignment

Abstract: Text-conditioned image generation models often generate incorrect
associations between entities and their visual attributes. This reflects an
impaired mapping between linguistic binding of entities and modifiers in the
prompt and visual binding of the corresponding elements in the generated image.
As one notable example, a query like ``a pink sunflower and a yellow flamingo''
may incorrectly produce an image of a yellow sunflower and a pink flamingo. To
remedy this issue, we propose SynGen, an approach which first syntactically
analyses the prompt to identify entities and their modifiers, and then uses a
novel loss function that encourages the cross-attention maps to agree with the
linguistic binding reflected by the syntax. Specifically, we encourage large
overlap between attention maps of entities and their modifiers, and small
overlap with other entities and modifier words. The loss is optimized during
inference, without retraining or fine-tuning the model. Human evaluation on
three datasets, including one new and challenging set, demonstrate significant
improvements of SynGen compared with current state of the art methods. This
work highlights how making use of sentence structure during inference can
efficiently and substantially improve the faithfulness of text-to-image
generation.


------------------------------------------------------------------------------

Title:
A Self-Supervised Miniature One-Shot Texture Segmentation (MOSTS) Model  for Real-Time Robot Navigation and Embedded Applications

Abstract: Determining the drivable area, or free space segmentation, is critical for
mobile robots to navigate indoor environments safely. However, the lack of
coherent markings and structures (e.g., lanes, curbs, etc.) in indoor spaces
places the burden of traversability estimation heavily on the mobile robot.
This paper explores the use of a self-supervised one-shot texture segmentation
framework and an RGB-D camera to achieve robust drivable area segmentation.
With a fast inference speed and compact size, the developed model, MOSTS is
ideal for real-time robot navigation and various embedded applications. A
benchmark study was conducted to compare MOSTS's performance with existing
one-shot texture segmentation models to evaluate its performance. Additionally,
a validation dataset was built to assess MOSTS's ability to perform texture
segmentation in the wild, where it effectively identified small low-lying
objects that were previously undetectable by depth measurements. Further, the
study also compared MOSTS's performance with two State-Of-The-Art (SOTA) indoor
semantic segmentation models, both quantitatively and qualitatively. The
results showed that MOSTS offers comparable accuracy with up to eight times
faster inference speed in indoor drivable area segmentation.


------------------------------------------------------------------------------

Title:
Towards Interpretability in Audio and Visual Affective Machine Learning:  A Review

Abstract: Machine learning is frequently used in affective computing, but presents
challenges due the opacity of state-of-the-art machine learning methods.
Because of the impact affective machine learning systems may have on an
individual's life, it is important that models be made transparent to detect
and mitigate biased decision making. In this regard, affective machine learning
could benefit from the recent advancements in explainable artificial
intelligence (XAI) research. We perform a structured literature review to
examine the use of interpretability in the context of affective machine
learning. We focus on studies using audio, visual, or audiovisual data for
model training and identified 29 research articles. Our findings show an
emergence of the use of interpretability methods in the last five years.
However, their use is currently limited regarding the range of methods used,
the depth of evaluations, and the consideration of use-cases. We outline the
main gaps in the research and provide recommendations for researchers that aim
to implement interpretable methods for affective machine learning.


------------------------------------------------------------------------------

Title:
Overcoming the Limitations of Localization Uncertainty: Efficient &  Exact Non-Linear Post-Processing and Calibration

Abstract: Robustly and accurately localizing objects in real-world environments can be
challenging due to noisy data, hardware limitations, and the inherent
randomness of physical systems. To account for these factors, existing works
estimate the aleatoric uncertainty of object detectors by modeling their
localization output as a Gaussian distribution
$\mathcal{N}(\mu,\,\sigma^{2})\,$, and training with loss attenuation. We
identify three aspects that are unaddressed in the state of the art, but
warrant further exploration: (1) the efficient and mathematically sound
propagation of $\mathcal{N}(\mu,\,\sigma^{2})\,$ through non-linear
post-processing, (2) the calibration of the predicted uncertainty, and (3) its
interpretation. We overcome these limitations by: (1) implementing loss
attenuation in EfficientDet, and proposing two deterministic methods for the
exact and fast propagation of the output distribution, (2) demonstrating on the
KITTI and BDD100K datasets that the predicted uncertainty is miscalibrated, and
adapting two calibration methods to the localization task, and (3)
investigating the correlation between aleatoric uncertainty and task-relevant
error sources. Our contributions are: (1) up to five times faster propagation
while increasing localization performance by up to 1\%, (2) up to fifteen times
smaller expected calibration error, and (3) the predicted uncertainty is found
to correlate with occlusion, object distance, detection accuracy, and image
quality.


------------------------------------------------------------------------------

Title:
Improving Reading Comprehension Question Generation with Data  Augmentation and Overgenerate-and-rank

Abstract: Reading comprehension is a crucial skill in many aspects of education,
including language learning, cognitive development, and fostering early
literacy skills in children. Automated answer-aware reading comprehension
question generation has significant potential to scale up learner support in
educational activities. One key technical challenge in this setting is that
there can be multiple questions, sometimes very different from each other, with
the same answer; a trained question generation method may not necessarily know
which question human educators would prefer. To address this challenge, we
propose 1) a data augmentation method that enriches the training dataset with
diverse questions given the same context and answer and 2) an
overgenerate-and-rank method to select the best question from a pool of
candidates. We evaluate our method on the FairytaleQA dataset, showing a 5%
absolute improvement in ROUGE-L over the best existing method. We also
demonstrate the effectiveness of our method in generating harder, "implicit"
questions, where the answers are not contained in the context as text spans.


------------------------------------------------------------------------------

Title:
Multi-Temporal Relationship Inference in Urban Areas

Abstract: Finding multiple temporal relationships among locations can benefit a bunch
of urban applications, such as dynamic offline advertising and smart public
transport planning. While some efforts have been made on finding static
relationships among locations, little attention is focused on studying
time-aware location relationships. Indeed, abundant location-based human
activities are time-varying and the availability of these data enables a new
paradigm for understanding the dynamic relationships in a period among
connective locations. To this end, we propose to study a new problem, namely
multi-Temporal relationship inference among locations (Trial for short), where
the major challenge is how to integrate dynamic and geographical influence
under the relationship sparsity constraint. Specifically, we propose a solution
to Trial with a graph learning scheme, which includes a spatially evolving
graph neural network (SEENet) with two collaborative components: spatially
evolving graph convolution module (SEConv) and spatially evolving
self-supervised learning strategy (SE-SSL). SEConv performs the intra-time
aggregation and inter-time propagation to capture the multifaceted spatially
evolving contexts from the view of location message passing. In addition,
SE-SSL designs time-aware self-supervised learning tasks in a global-local
manner with additional evolving constraint to enhance the location
representation learning and further handle the relationship sparsity. Finally,
experiments on four real-world datasets demonstrate the superiority of our
method over several state-of-the-art approaches.


------------------------------------------------------------------------------

Title:
Pragmatic Inference with a CLIP Listener for Contrastive Captioning

Abstract: We propose a simple yet effective and robust method for contrastive
captioning: generating discriminative captions that distinguish target images
from very similar alternative distractor images. Our approach is built on a
pragmatic inference procedure that formulates captioning as a reference game
between a speaker, which produces possible captions describing the target, and
a listener, which selects the target given the caption. Unlike previous methods
that derive both speaker and listener distributions from a single captioning
model, we leverage an off-the-shelf CLIP model to parameterize the listener.
Compared with captioner-only pragmatic models, our method benefits from rich
vision language alignment representations from CLIP when reasoning over
distractors. Like previous methods for discriminative captioning, our method
uses a hyperparameter to control the tradeoff between the informativity (how
likely captions are to allow a human listener to discriminate the target image)
and the fluency of the captions. However, we find that our method is
substantially more robust to the value of this hyperparameter than past
methods, which allows us to automatically optimize the captions for
informativity - outperforming past methods for discriminative captioning by 11%
to 15% accuracy in human evaluations


------------------------------------------------------------------------------

Title:
Convert Monolithic Application to Microservice Application

Abstract: Microservice architecture is a trending topic in software design architecture
and many enterprises adopted microservice design due its benefits and the rapid
and wide deployment of cloud computing and as a result, many enterprises
transformed their existing monolithic application to microservice to achieve
business requirements as scaling up and agile development. In this paper we
will guide software developers how to convert their existing monolithic
application into microservice application without re-writing the whole
microservice application from scratch, and we will also discuss the common
issues that may face the software developer during the conversion processes. In
addition to converting the business logic to microservice, we mention steps for
converting the monolithic database into a database per service. Also, we
summarize how Netflix and Airbnb converted their monolithic application to
microservice application.


------------------------------------------------------------------------------

Title:
Tree Variational Autoencoders

Abstract: We propose a new generative hierarchical clustering model that learns a
flexible tree-based posterior distribution over latent variables. The proposed
Tree Variational Autoencoder (TreeVAE) hierarchically divides samples according
to their intrinsic characteristics, shedding light on hidden structure in the
data. It adapts its architecture to discover the optimal tree for encoding
dependencies between latent variables. The proposed tree-based generative
architecture permits lightweight conditional inference and improves generative
performance by utilizing specialized leaf decoders. We show that TreeVAE
uncovers underlying clusters in the data and finds meaningful hierarchical
relations between the different groups on a variety of datasets, including
real-world imaging data. We present empirically that TreeVAE provides a more
competitive log-likelihood lower bound than the sequential counterparts.
Finally, due to its generative nature, TreeVAE is able to generate new samples
from the discovered clusters via conditional sampling.


------------------------------------------------------------------------------

Title:
On the Effects and Optimal Design of Redundant Sensors in Sensor  Networks

Abstract: The existence of redundant sensors in sensor networks is a common occurrence,
yet their true significance remains elusive. This paper comprehensively
investigates the effects and optimal design of redundant sensors in sensor
networks that use Kalman filtering to estimate the state of a random process.
The paper presents two main results: a theoretical analysis of the effects of
redundant sensors and an engineering-oriented optimal design of redundant
sensors. In the theoretical analysis, the paper leverages Riccati equations and
Symplectic matrix theory to unveil the explicit role of redundant sensors in
cooperative state estimation. The results unequivocally demonstrate that the
addition of redundant sensors enhances the estimation performance of the sensor
network, aligning with the principle of ``more is better". Moreover, the paper
establishes a precise sufficient and necessary condition to assess whether the
inclusion of redundant sensors improves the estimation performance for all
state elements simultaneously. Moving towards engineering-oriented design
optimization, the paper proposes a novel algorithm to tackle the optimal design
problem of redundant sensors. By employing linear matrix inequalities and
iterative semi-definite programming techniques, the algorithm effectively
handles the nonlinear constraints inherent in the optimization problem.
Importantly, the convergence of the proposed algorithm is guaranteed. Numerical
simulations are provided to demonstrate and validate the results.


------------------------------------------------------------------------------

Title:
Document Entity Retrieval with Massive and Noisy Pre-training

Abstract: Visually-Rich Document Entity Retrieval (VDER) is a type of machine learning
task that aims at recovering text spans in the documents for each of the
entities in question. VDER has gained significant attention in recent years
thanks to its broad applications in enterprise AI. Unfortunately, as document
images often contain personally identifiable information (PII), publicly
available data have been scarce, not only because of privacy constraints but
also the costs of acquiring annotations. To make things worse, each dataset
would often define its own sets of entities, and the non-overlapping entity
spaces between datasets make it difficult to transfer knowledge between
documents. In this paper, we propose a method to collect massive-scale, noisy,
and weakly labeled data from the web to benefit the training of VDER models.
Such a method will generate a huge amount of document image data to compensate
for the lack of training data in many VDER settings. Moreover, the collected
dataset named DocuNet would not need to be dependent on specific document types
or entity sets, making it universally applicable to all VDER tasks. Empowered
by DocuNet, we present a lightweight multimodal architecture named UniFormer,
which can learn a unified representation from text, layout, and image crops
without needing extra visual pertaining. We experiment with our methods on
popular VDER models in various settings and show the improvements when this
massive dataset is incorporated with UniFormer on both classic entity retrieval
and few-shot learning settings.


------------------------------------------------------------------------------

Title:
Deep Learning for Energy Time-Series Analysis and Forecasting

Abstract: Energy time-series analysis describes the process of analyzing past energy
observations and possibly external factors so as to predict the future.
Different tasks are involved in the general field of energy time-series
analysis and forecasting, with electric load demand forecasting, personalized
energy consumption forecasting, as well as renewable energy generation
forecasting being among the most common ones. Following the exceptional
performance of Deep Learning (DL) in a broad area of vision tasks, DL models
have successfully been utilized in time-series forecasting tasks. This paper
aims to provide insight into various DL methods geared towards improving the
performance in energy time-series forecasting tasks, with special emphasis in
Greek Energy Market, and equip the reader with the necessary knowledge to apply
these methods in practice.


------------------------------------------------------------------------------

Title:
PEACE: Cross-Platform Hate Speech Detection- A Causality-guided  Framework

Abstract: Hate speech detection refers to the task of detecting hateful content that
aims at denigrating an individual or a group based on their religion, gender,
sexual orientation, or other characteristics. Due to the different policies of
the platforms, different groups of people express hate in different ways.
Furthermore, due to the lack of labeled data in some platforms it becomes
challenging to build hate speech detection models. To this end, we revisit if
we can learn a generalizable hate speech detection model for the cross platform
setting, where we train the model on the data from one (source) platform and
generalize the model across multiple (target) platforms. Existing
generalization models rely on linguistic cues or auxiliary information, making
them biased towards certain tags or certain kinds of words (e.g., abusive
words) on the source platform and thus not applicable to the target platforms.
Inspired by social and psychological theories, we endeavor to explore if there
exist inherent causal cues that can be leveraged to learn generalizable
representations for detecting hate speech across these distribution shifts. To
this end, we propose a causality-guided framework, PEACE, that identifies and
leverages two intrinsic causal cues omnipresent in hateful content: the overall
sentiment and the aggression in the text. We conduct extensive experiments
across multiple platforms (representing the distribution shift) showing if
causal cues can help cross-platform generalization.


------------------------------------------------------------------------------

Title:
Convergence of one-level and multilevel unsymmetric collocation for  second order elliptic boundary value problems

Abstract: Thepaperprovesconvergenceofone-levelandmultilevelunsymmetriccollocationforsecondorderelliptic
boundary value problems on the bounded domains. By using Schaback's linear
discretization theory,L2 errors are obtained based on the kernel-based trial
spaces generated by the compactly supported radial basis functions. For the
one-level unsymmetric collocation case, we obtain convergence when the testing
discretization is finer than the trial discretization. The convergence rates
depend on the regularity of the solution, the smoothness of the computing
domain, and the approximation of scaled kernel-based spaces. The multilevel
process is implemented by employing successive refinement scattered data sets
and scaled compactly supported radial basis functions with varying support
radii. Convergence of multilevel collocation is further proved based on the
theoretical results of one-level unsymmetric collocation. In addition to having
the same dependencies as the one-level collocation, the convergence rates of
multilevel unsymmetric collocation especially depends on the increasing rules
of scattered data and the selection of scaling parameters.


------------------------------------------------------------------------------

Title:
ScrollTimes: Tracing the Provenance of Paintings as a Window into  History

Abstract: Digital humanities research has flourished due to the diverse artifacts
available in cultural heritage databases. However, over-reliance on a single
artifact type can result in poor contextualization and a constrained
understanding of historical context. We collaborated with art historians to
examine handscrolls, a form of traditional Chinese painting which offers a
wealth of data for historical analysis and provides a unique opportunity for
understanding history through artwork. We propose ScrollTimes, a visual
analysis system for tracing handscroll historic context by linking multiple
data sources. Specifically, a unique layout is developed for efficiently
viewing long handscrolls. Using image processing techniques and language
models, we extract, verify, and supplement elements in handscrolls with
different cultural heritage databases. Furthermore, interactive biographies are
constructed for handscrolls to uncover their historical narratives, provenance
trajectories, and artistic legacies. Validated through case studies and expert
interviews, our approach offers a window into history, fostering a holistic
understanding of handscroll provenance and historical significance.


------------------------------------------------------------------------------

Title:
A Search for Nonlinear Balanced Boolean Functions by Leveraging  Phenotypic Properties

Abstract: In this paper, we consider the problem of finding perfectly balanced Boolean
functions with high non-linearity values. Such functions have extensive
applications in domains such as cryptography and error-correcting coding
theory. We provide an approach for finding such functions by a local search
method that exploits the structure of the underlying problem. Previous attempts
in this vein typically focused on using the properties of the fitness landscape
to guide the search. We opt for a different path in which we leverage the
phenotype landscape (the mapping from genotypes to phenotypes) instead. In the
context of the underlying problem, the phenotypes are represented by
Walsh-Hadamard spectra of the candidate solutions (Boolean functions). We
propose a novel selection criterion, under which the phenotypes are compared
directly, and test whether its use increases the convergence speed (measured by
the number of required spectra calculations) when compared to a competitive
fitness function used in the literature. The results reveal promising
convergence speed improvements for Boolean functions of sizes $N=6$ to $N=9$.


------------------------------------------------------------------------------

Title:
In Search of netUnicorn: A Data-Collection Platform to Develop  Generalizable ML Models for Network Security Problems

Abstract: The remarkable success of the use of machine learning-based solutions for
network security problems has been impeded by the developed ML models'
inability to maintain efficacy when used in different network environments
exhibiting different network behaviors. This issue is commonly referred to as
the generalizability problem of ML models. The community has recognized the
critical role that training datasets play in this context and has developed
various techniques to improve dataset curation to overcome this problem.
Unfortunately, these methods are generally ill-suited or even counterproductive
in the network security domain, where they often result in unrealistic or
poor-quality datasets.
To address this issue, we propose an augmented ML pipeline that leverages
explainable ML tools to guide the network data collection in an iterative
fashion. To ensure the data's realism and quality, we require that the new
datasets should be endogenously collected in this iterative process, thus
advocating for a gradual removal of data-related problems to improve model
generalizability. To realize this capability, we develop a data-collection
platform, netUnicorn, that takes inspiration from the classic "hourglass" model
and is implemented as its "thin waist" to simplify data collection for
different learning problems from diverse network environments. The proposed
system decouples data-collection intents from the deployment mechanisms and
disaggregates these high-level intents into smaller reusable, self-contained
tasks.
We demonstrate how netUnicorn simplifies collecting data for different
learning problems from multiple network environments and how the proposed
iterative data collection improves a model's generalizability.


------------------------------------------------------------------------------

Title:
Participatory Research as a Path to Community-Informed, Gender-Fair  Machine Translation

Abstract: Recent years have seen a strongly increased visibility of non-binary people
in public discourse. Accordingly, considerations of gender-fair language go
beyond a binary conception of male/female. However, language technology,
especially machine translation (MT), still suffers from binary gender bias.
Proposing a solution for gender-fair MT beyond the binary from a purely
technological perspective might fall short to accommodate different target user
groups and in the worst case might lead to misgendering. To address this
challenge, we propose a method and case study building on participatory action
research to include experiential experts, i.e., queer and non-binary people,
translators, and MT experts, in the MT design process. The case study focuses
on German, where central findings are the importance of context dependency to
avoid identity invalidation and a desire for customizable MT solutions.


------------------------------------------------------------------------------

Title:
Contrasting Intra-Modal and Ranking Cross-Modal Hard Negatives to  Enhance Visio-Linguistic Fine-grained Understanding

Abstract: Current Vision and Language Models (VLMs) demonstrate strong performance
across various vision-language tasks, yet they struggle with fine-grained
understanding. This issue stems from weak image-caption alignment in
pretraining datasets and a simplified contrastive objective that fails to
distinguish nuanced grounding elements such as relations, actions, and
attributes. As a result, the models tend to learn bag-of-words representations.
To mitigate these challenges, we introduce an intra-modal contrastive loss and
a unique cross-modal rank loss with an adaptive threshold that serves as
curriculum learning, utilizing our automatically generated hard negatives to
augment the model's capacity. Our strategy, which does not necessitate
additional annotations or parameters, can be incorporated into any VLM trained
with an image-text contrastive loss. Upon application to CLIP, our method leads
to significant improvements on three fine-grained benchmarks, and it also
enhances the performance of X-VLM, which is the state-of-art moodel on
fine-grained reasoning.


------------------------------------------------------------------------------

Title:
Mapping Researcher Activity based on Publication Data by means of  Transformers

Abstract: Modern performance on several natural language processing (NLP) tasks has
been enhanced thanks to the Transformer-based pre-trained language model BERT.
We employ this concept to investigate a local publication database. Research
papers are encoded and clustered to form a landscape view of the scientific
topics, in which research is active. Authors working on similar topics can be
identified by calculating the similarity between their papers. Based on this,
we define a similarity metric between authors. Additionally we introduce the
concept of self-similarity to indicate the topical variety of authors.


------------------------------------------------------------------------------

Title:
A study of concurrent multi-frontal solvers for modern massively  parallel architectures

Abstract: Leveraging Trace Theory, we investigate the efficient parallelization of
direct solvers for large linear equation systems. Our focus lies on a
multi-frontal algorithm, and we present a methodology for achieving
near-optimal scheduling on modern massively parallel machines. By employing
trace theory with Diekert Graphs and Foata Normal Form, we rigorously validate
the effectiveness of our proposed solution. To establish a strong link between
the mesh and elimination tree of the multi-frontal solver, we conduct extensive
testing on matrices derived from the Finite Element Method (FEM). Furthermore,
we assess the performance of computations on both GPU and CPU platforms,
employing practical implementation strategies.


------------------------------------------------------------------------------

Title:
SplatFlow: Learning Multi-frame Optical Flow via Splatting

Abstract: Occlusion problem remains a key challenge in Optical Flow Estimation (OFE)
despite the recent significant progress brought by deep learning in the field.
Most existing deep learning OFE methods, especially those based on two frames,
cannot properly handle occlusions, in part because there is no significant
feature similarity in occluded regions. The multi-frame settings have the
potential to mitigate the occlusion issue in OFE. However, the problem of
Multi-frame OFE (MOFE) remains underexplored, and the limited works are
specially designed for pyramid backbones and obtain the aligned temporal
information by time-consuming backward flow calculation or non-differentiable
forward warping transformation. To address these shortcomings, we propose an
efficient MOFE framework named SplatFlow, which is realized by introducing the
differentiable splatting transformation to align the temporal information,
designing a One-to-Many embedding method to densely guide the current frame's
estimation, and further remodelling the existing two-frame backbones. The
proposed SplatFlow is very efficient yet more accurate as it is able to handle
occlusions properly. Extensive experimental evaluations show that our SplatFlow
substantially outperforms all published methods on KITTI2015 and Sintel
benchmarks. Especially on Sintel benchmark, SplatFlow achieves errors of 1.12
(clean pass) and 2.07 (final pass), with surprisingly significant 19.4% and
16.2% error reductions from the previous best results submitted, respectively.
Code is available at this https URL


------------------------------------------------------------------------------

Title:
Motion Perceiver: Real-Time Occupancy Forecasting for Embedded Systems

Abstract: This work introduces a flexible architecture for real-time occupancy
forecasting. In contrast to existing, more computationally expensive
architectures, the proposed model exploits recursive latent state estimation,
using learned transformer-based prediction and update modules. This allows for
highly efficient real-time inference on an embedded system (profiled on an
Nvidia Xavier AGX), and the inclusion of a broad set of information from a
diverse set of sensors. The architecture is able to process sparse and occluded
observations of agent positions and scene context as this is made available,
and does not require motion tracklet inputs. \networkName{} accomplishes this
by encoding the scene into a latent state that evolves in time with
self-attention and is updated with contextual information such as traffic
signals, road topology or agent detections using cross-attention. Occupancy
predictions are made by sparsely querying positions of interest as opposed to
generating a fixed size raster image, which allows for variable resolution
occupancy prediction or local querying by downstream trajectory optimisation
algorithms, saving computational effort.


------------------------------------------------------------------------------

Title:
Improving the Lower Bound for the Union-closed Sets Conjecture via  Conditionally IID Coupling

Abstract: Recently, Gilmer proved the first constant lower bound for the union-closed
sets conjecture via an information-theoretic argument. The heart of the
argument is an entropic inequality involving the OR function of two i.i.d.\
binary vectors, and the best constant obtainable through the i.i.d.\ coupling
is $\frac{3-\sqrt{5}}{2}\approx0.38197$. Sawin demonstrated that the bound can
be strictly improved by considering a convex combination of the i.i.d.\
coupling and the max-entropy coupling, and the best constant obtainable through
this approach is around 0.38234, as evaluated by Yu and Cambie. In this work we
show analytically that the bound can be further strictly improved by
considering another class of coupling under which the two binary sequences are
i.i.d.\ conditioned on an auxiliary random variable. We also provide a new
class of bounds in terms of finite-dimensional optimization. For a basic
instance from this class, analysis assisted with numerically solved
9-dimensional optimization suggests that the optimizer assumes a certain
structure. Under numerically verified hypotheses, the lower bound for the
union-closed sets conjecture can be improved to approximately 0.38271, a number
that can be defined as the solution to an analytic equation.


------------------------------------------------------------------------------

Title:
Safeguarding Crowdsourcing Surveys from ChatGPT with Prompt Injection

Abstract: ChatGPT and other large language models (LLMs) have proven useful in
crowdsourcing tasks, where they can effectively annotate machine learning
training data. However, this means that they also have the potential for
misuse, specifically to automatically answer surveys. LLMs can potentially
circumvent quality assurance measures, thereby threatening the integrity of
methodologies that rely on crowdsourcing surveys. In this paper, we propose a
mechanism to detect LLM-generated responses to surveys. The mechanism uses
"prompt injection", such as directions that can mislead LLMs into giving
predictable responses. We evaluate our technique against a range of question
scenarios, types, and positions, and find that it can reliably detect
LLM-generated responses with more than 93% effectiveness. We also provide an
open-source software to help survey designers use our technique to detect LLM
responses. Our work is a step in ensuring that survey methodologies remain
rigorous vis-a-vis LLMs.


------------------------------------------------------------------------------

Title:
Neural Network Compression using Binarization and Few Full-Precision  Weights

Abstract: Quantization and pruning are known to be two effective Deep Neural Networks
model compression methods. In this paper, we propose Automatic Prune
Binarization (APB), a novel compression technique combining quantization with
pruning. APB enhances the representational capability of binary networks using
a few full-precision weights. Our technique jointly maximizes the accuracy of
the network while minimizing its memory impact by deciding whether each weight
should be binarized or kept in full precision. We show how to efficiently
perform a forward pass through layers compressed using APB by decomposing it
into a binary and a sparse-dense matrix multiplication. Moreover, we design two
novel efficient algorithms for extremely quantized matrix multiplication on
CPU, leveraging highly efficient bitwise operations. The proposed algorithms
are 6.9x and 1.5x faster than available state-of-the-art solutions. We perform
an extensive evaluation of APB on two widely adopted model compression
datasets, namely CIFAR10 and ImageNet. APB shows to deliver better
accuracy/memory trade-off compared to state-of-the-art methods based on i)
quantization, ii) pruning, and iii) combination of pruning and quantization.
APB outperforms quantization also in the accuracy/efficiency trade-off, being
up to 2x faster than the 2-bits quantized model with no loss in accuracy.


------------------------------------------------------------------------------

Title:
Energy Management for a DM-i Plug-in Hybrid Electric Vehicle via  Continuous-Discrete Reinforcement Learning

Abstract: Energy management strategy (EMS) is a key technology for plug-in hybrid
electric vehicles (PHEVs). The energy management of PHEVs needs to output
continuous variables such as engine torque, as well as discrete variables such
as clutch engagement or disengagement. This type of problem is a mixed-integer
programming problem. In addition, the hybrid powertrain system is highly
nonlinear and complex. Designing an efficient EMS is a challenging task. We
establish a control-oriented mathematical model for a BYD DM-i hybrid
powertrain system from the perspective of mixed-integer programming. Then, an
EMS based on continuous-discrete reinforcement learning is introduced, which
can output both continuous and discrete variables simultaneously. Finally, the
effectiveness of the proposed control strategy is verified by comparing EMS
based on charge-depleting charge-sustaining (CD-CS) and Dynamic Programming
(DP). The simulation results show that the reinforcement learning EMS can
improve energy efficiency by 10.08% compared to the CD-CS EMS, and the fuel
economy gap is about 6.4% compared with the benchmark global optimum based on
DP.


------------------------------------------------------------------------------

Title:
Exploring the Application of Large-scale Pre-trained Models on Adverse  Weather Removal

Abstract: Image restoration under adverse weather conditions (e.g., rain, snow and
haze) is a fundamental computer vision problem and has important indications
for various downstream applications. Different from early methods that are
specially designed for specific type of weather, most recent works tend to
remove various adverse weather effects simultaneously through either spatial
feature representation learning or semantic information embedding. Inspired by
the various successful applications of large-scale pre-trained models (e.g,
CLIP), in this paper, we explore the potential benefits of them for this task
through both spatial feature representation learning and semantic information
embedding aspects: 1) for spatial feature representation learning, we design a
Spatially-Adaptive Residual (\textbf{SAR}) Encoder to extract degraded areas
adaptively. To facilitate its training, we propose a Soft Residual Distillation
(\textbf{CLIP-SRD}) strategy to transfer the spatial knowledge from CLIP
between clean and adverse weather images; 2) for semantic information
embedding, we propose a CLIP Weather Prior (\textbf{CWP}) embedding module to
make the network handle different weather conditions adaptively. This module
integrates the sample specific weather prior extracted by CLIP image encoder
together with the distribution specific information learned by a set of
parameters, and embeds them through a cross attention mechanism. Extensive
experiments demonstrate that our proposed method can achieve state-of-the-art
performance under different and challenging adverse weather conditions. Code
will be made available.


------------------------------------------------------------------------------

Title:
On approximation of solutions of stochastic delay differential equations  via randomized Euler scheme

Abstract: We investigate existence, uniqueness and approximation of solutions to
stochastic delay differential equations (SDDEs) under Carath\'eodory-type drift
coefficients. Moreover, we also assume that both drift $f=f(t,x,z)$ and
diffusion $g=g(t,x,z)$ coefficient are Lipschitz continuous with respect to the
space variable $x$, but only H\"older continuous with respect to the delay
variable $z$. We provide a construction of randomized Euler scheme for
approximation of solutions of Carath\'eodory SDDEs, and investigate its upper
error bound. Finally, we report results of numerical experiments that confirm
our theoretical findings.


------------------------------------------------------------------------------

Title:
Scalable Resource Management for Dynamic MEC: An Unsupervised  Link-Output Graph Neural Network Approach

Abstract: Deep learning has been successfully adopted in mobile edge computing (MEC) to
optimize task offloading and resource allocation. However, the dynamics of edge
networks raise two challenges in neural network (NN)-based optimization
methods: low scalability and high training costs. Although conventional
node-output graph neural networks (GNN) can extract features of edge nodes when
the network scales, they fail to handle a new scalability issue whereas the
dimension of the decision space may change as the network scales. To address
the issue, in this paper, a novel link-output GNN (LOGNN)-based resource
management approach is proposed to flexibly optimize the resource allocation in
MEC for an arbitrary number of edge nodes with extremely low algorithm
inference delay. Moreover, a label-free unsupervised method is applied to train
the LOGNN efficiently, where the gradient of edge tasks processing delay with
respect to the LOGNN parameters is derived explicitly. In addition, a
theoretical analysis of the scalability of the node-output GNN and link-output
GNN is performed. Simulation results show that the proposed LOGNN can
efficiently optimize the MEC resource allocation problem in a scalable way,
with an arbitrary number of servers and users. In addition, the proposed
unsupervised training method has better convergence performance and speed than
supervised learning and reinforcement learning-based training methods. The code
is available at \url{this https URL}.


------------------------------------------------------------------------------

Title:
Fast Training of Diffusion Models with Masked Transformers

Abstract: We propose an efficient approach to train large diffusion models with masked
transformers. While masked transformers have been extensively explored for
representation learning, their application to generative learning is less
explored in the vision domain. Our work is the first to exploit masked training
to reduce the training cost of diffusion models significantly. Specifically, we
randomly mask out a high proportion (\emph{e.g.}, 50\%) of patches in diffused
input images during training. For masked training, we introduce an asymmetric
encoder-decoder architecture consisting of a transformer encoder that operates
only on unmasked patches and a lightweight transformer decoder on full patches.
To promote a long-range understanding of full patches, we add an auxiliary task
of reconstructing masked patches to the denoising score matching objective that
learns the score of unmasked patches. Experiments on ImageNet-256$\times$256
show that our approach achieves the same performance as the state-of-the-art
Diffusion Transformer (DiT) model, using only 31\% of its original training
time. Thus, our method allows for efficient training of diffusion models
without sacrificing the generative performance.


------------------------------------------------------------------------------

Title:
Awayvirus: A Playful and Tangible Approach to Improve Children's Hygiene  Habits in Family Education

Abstract: Despite various playful and educational tools have been developed to support
children's learning abilities, limited work focuses on tangible toys designed
to improve and maintain children's hygiene perception, habits and awareness, as
well as fostering their collaboration and social abilities in home education
contexts. We developed \textbf{Awayvirus} to address this research and design
gap, aiming to help children gain hygiene habits knowledge through tangible
blocks. Our findings indicate that a playful tangible interaction method can
effectively increase children's interest in learning and encourage parents to
become actively involved in their children's hygiene and health education.
Additionally, Awayvirus seeks to build a collaborative bridge between children
and parents, promoting communication strategies while mitigating the adverse
effects of the challenging the post-pandemic period.


------------------------------------------------------------------------------

Title:
Sim-on-Wheels: Physical World in the Loop Simulation for Self-Driving

Abstract: We present Sim-on-Wheels, a safe, realistic, and vehicle-in-loop framework to
test autonomous vehicles' performance in the real world under safety-critical
scenarios. Sim-on-wheels runs on a self-driving vehicle operating in the
physical world. It creates virtual traffic participants with risky behaviors
and seamlessly inserts the virtual events into images perceived from the
physical world in real-time. The manipulated images are fed into autonomy,
allowing the self-driving vehicle to react to such virtual events. The full
pipeline runs on the actual vehicle and interacts with the physical world, but
the safety-critical events it sees are virtual. Sim-on-Wheels is safe,
interactive, realistic, and easy to use. The experiments demonstrate the
potential of Sim-on-Wheels to facilitate the process of testing autonomous
driving in challenging real-world scenes with high fidelity and low risk.


------------------------------------------------------------------------------

Title:
Multi-objective path tracking control for car-like vehicles with  differentially bounded n-smooth output

Abstract: When designing path tracking controllers for car-like vehicles, two main
aspects are the tracking performance and the characteristics of the actuation
signal. Our work bases on an existing variable structure controller that was
designed with the geometrically optimal solution of a Dubins car, but with
chattering on the output. In this contribution, we extend this approach to
achieve an actuation signal that is n-smooth and differentially bounded. While
the global stability under matched disturbances is maintained, the finite time
reaching behavior is exchanged for asymptotic convergence. With the $n$-smooth
output, n new parameters are introduced, weighing the control characteristics
between the tracking performance and the magnitude of the steering angles
derivatives. The controller is also evaluated in simulation, demonstrating the
tuning capabilities, as well as the reaching and tracking behavior. The main
contribution of this work is a control law designed to produce a smooth
steering angle with implicit satisfaction of bounds on its derivatives.


------------------------------------------------------------------------------

Title:
Hands-on detection for steering wheels with neural networks

Abstract: In this paper the concept of a machine learning based hands-on detection
algorithm is proposed. The hand detection is implemented on the hardware side
using a capacitive method. A sensor mat in the steering wheel detects a change
in capacity as soon as the driver's hands come closer. The evaluation and final
decision about hands-on or hands-off situations is done using machine learning.
In order to find a suitable machine learning model, different models are
implemented and evaluated. Based on accuracy, memory consumption and
computational effort the most promising one is selected and ported on a micro
controller. The entire system is then evaluated in terms of reliability and
response time.


------------------------------------------------------------------------------

Title:
Who Needs to Know? Minimal Knowledge for Optimal Coordination

Abstract: To optimally coordinate with others in cooperative games, it is often crucial
to have information about one's collaborators: successful driving requires
understanding which side of the road to drive on. However, not every feature of
collaborators is strategically relevant: the fine-grained acceleration of
drivers may be ignored while maintaining optimal coordination. We show that
there is a well-defined dichotomy between strategically relevant and irrelevant
information. Moreover, we show that, in dynamic games, this dichotomy has a
compact representation that can be efficiently computed via a Bellman backup
operator. We apply this algorithm to analyze the strategically relevant
information for tasks in both a standard and a partially observable version of
the Overcooked environment. Theoretical and empirical results show that our
algorithms are significantly more efficient than baselines. Videos are
available at this https URL


------------------------------------------------------------------------------

Title:
Enhancing Neural Rendering Methods with Image Augmentations

Abstract: Faithfully reconstructing 3D geometry and generating novel views of scenes
are critical tasks in 3D computer vision. Despite the widespread use of image
augmentations across computer vision applications, their potential remains
underexplored when learning neural rendering methods (NRMs) for 3D scenes. This
paper presents a comprehensive analysis of the use of image augmentations in
NRMs, where we explore different augmentation strategies. We found that
introducing image augmentations during training presents challenges such as
geometric and photometric inconsistencies for learning NRMs from images.
Specifically, geometric inconsistencies arise from alterations in shapes,
positions, and orientations from the augmentations, disrupting spatial cues
necessary for accurate 3D reconstruction. On the other hand, photometric
inconsistencies arise from changes in pixel intensities introduced by the
augmentations, affecting the ability to capture the underlying 3D structures of
the scene. We alleviate these issues by focusing on color manipulations and
introducing learnable appearance embeddings that allow NRMs to explain away
photometric variations. Our experiments demonstrate the benefits of
incorporating augmentations when learning NRMs, including improved photometric
quality and surface reconstruction, as well as enhanced robustness against data
quality issues, such as reduced training data and image degradations.


------------------------------------------------------------------------------

Title:
Unsupervised speech intelligibility assessment with utterance level  alignment distance between teacher and learner Wav2Vec-2.0 representations

Abstract: Speech intelligibility is crucial in language learning for effective
communication. Thus, to develop computer-assisted language learning systems,
automatic speech intelligibility detection (SID) is necessary. Most of the
works have assessed the intelligibility in a supervised manner considering
manual annotations, which requires cost and time; hence scalability is limited.
To overcome these, this work proposes an unsupervised approach for SID. The
proposed approach considers alignment distance computed with dynamic-time
warping (DTW) between teacher and learner representation sequence as a measure
to separate intelligible versus non-intelligible speech. We obtain the feature
sequence using current state-of-the-art self-supervised representations from
Wav2Vec-2.0. We found the detection accuracies as 90.37\%, 92.57\% and 96.58\%,
respectively, with three alignment distance measures -- mean absolute error,
mean squared error and cosine distance (equal to one minus cosine similarity).


------------------------------------------------------------------------------

Title:
Ultra8T: A Sub-Threshold 8T SRAM with Leakage Detection

Abstract: In energy-constrained scenarios such as IoT applications, the primary
requirement for System-on-Chips (SoCs) is to increase battery life. However,
when performing sub/near-threshold operations, the relatively large leakage
current hinders Static Random Access Memory (SRAM) from normal read/write
functionalities at the lowest possible voltage (VDDMIN). In this work, we
propose an Ultra8T SRAM to aggressively reduce VDDMIN by using a leakage
detection strategy where the safety sensing time on bitlines is quantified
without any additional hardware overhead. We validate the proposed Ultra8T
using a 256x64 array in 28nm CMOS technology. Post-simulations show successful
read operation at 0.25V with 1.11{\mu}s read delay, and the minimum energy
required is 1.69pJ at 0.4V.


------------------------------------------------------------------------------

Title:
Searching for the Fakes: Efficient Neural Architecture Search for  General Face Forgery Detection

Abstract: As the saying goes, "seeing is believing". However, with the development of
digital face editing tools, we can no longer trust what we can see. Although
face forgery detection has made promising progress, most current methods are
designed manually by human experts, which is labor-consuming. In this paper, we
develop an end-to-end framework based on neural architecture search (NAS) for
deepfake detection, which can automatically design network architectures
without human intervention. First, a forgery-oriented search space is created
to choose appropriate operations for this task. Second, we propose a novel
performance estimation metric, which guides the search process to select more
general models. The cross-dataset search is also considered to develop more
general architectures. Eventually, we connect the cells in a cascaded pyramid
way for final forgery classification. Compared with state-of-the-art networks
artificially designed, our method achieves competitive performance in both
in-dataset and cross-dataset scenarios.


------------------------------------------------------------------------------

Title:
Yes, we CANN: Constrained Approximate Nearest Neighbors for local  feature-based visual localization

Abstract: Large-scale visual localization systems continue to rely on 3D point clouds
built from image collections using structure-from-motion. While the 3D points
in these models are represented using local image features, directly matching a
query image's local features against the point cloud is challenging due to the
scale of the nearest-neighbor search problem. Many recent approaches to visual
localization have thus proposed a hybrid method, where first a global (per
image) embedding is used to retrieve a small subset of database images, and
local features of the query are matched only against those. It seems to have
become common belief that global embeddings are critical for said
image-retrieval in visual localization, despite the significant downside of
having to compute two feature types for each query image. In this paper, we
take a step back from this assumption and propose Constrained Approximate
Nearest Neighbors (CANN), a joint solution of k-nearest-neighbors across both
the geometry and appearance space using only local features. We first derive
the theoretical foundation for k-nearest-neighbor retrieval across multiple
metrics and then showcase how CANN improves visual localization. Our
experiments on public localization benchmarks demonstrate that our method
significantly outperforms both state-of-the-art global feature-based retrieval
and approaches using local feature aggregation schemes. Moreover, it is an
order of magnitude faster in both index and query time than feature aggregation
schemes for these datasets. Code will be released.


------------------------------------------------------------------------------

Title:
Bridging the Gap between Decision and Logits in Decision-based Knowledge  Distillation for Pre-trained Language Models

Abstract: Conventional knowledge distillation (KD) methods require access to the
internal information of teachers, e.g., logits. However, such information may
not always be accessible for large pre-trained language models (PLMs). In this
work, we focus on decision-based KD for PLMs, where only teacher decisions
(i.e., top-1 labels) are accessible. Considering the information gap between
logits and decisions, we propose a novel method to estimate logits from the
decision distributions. Specifically, decision distributions can be both
derived as a function of logits theoretically and estimated with test-time data
augmentation empirically. By combining the theoretical and empirical
estimations of the decision distributions together, the estimation of logits
can be successfully reduced to a simple root-finding problem. Extensive
experiments show that our method significantly outperforms strong baselines on
both natural language understanding and machine reading comprehension datasets.


------------------------------------------------------------------------------

Title:
Logarithmic Bayes Regret Bounds

Abstract: We derive the first finite-time logarithmic regret bounds for Bayesian
bandits. For Gaussian bandits, we obtain a $O(c_h \log^2 n)$ bound, where $c_h$
is a prior-dependent constant. This matches the asymptotic lower bound of Lai
(1987). Our proofs mark a technical departure from prior works, and are simple
and general. To show generality, we apply our technique to linear bandits. Our
bounds shed light on the value of the prior in the Bayesian setting, both in
the objective and as a side information given to the learner. They
significantly improve the $\tilde{O}(\sqrt{n})$ bounds, that despite the
existing lower bounds, have become standard in the literature.


------------------------------------------------------------------------------

Title:
OMS-DPM: Optimizing the Model Schedule for Diffusion Probabilistic  Models

Abstract: Diffusion probabilistic models (DPMs) are a new class of generative models
that have achieved state-of-the-art generation quality in various domains.
Despite the promise, one major drawback of DPMs is the slow generation speed
due to the large number of neural network evaluations required in the
generation process. In this paper, we reveal an overlooked dimension -- model
schedule -- for optimizing the trade-off between generation quality and speed.
More specifically, we observe that small models, though having worse generation
quality when used alone, could outperform large models in certain generation
steps. Therefore, unlike the traditional way of using a single model, using
different models in different generation steps in a carefully designed
\emph{model schedule} could potentially improve generation quality and speed
\emph{simultaneously}. We design OMS-DPM, a predictor-based search algorithm,
to optimize the model schedule given an arbitrary generation time budget and a
set of pre-trained models. We demonstrate that OMS-DPM can find model schedules
that improve generation quality and speed than prior state-of-the-art methods
across CIFAR-10, CelebA, ImageNet, and LSUN datasets. When applied to the
public checkpoints of the Stable Diffusion model, we are able to accelerate the
sampling by 2$\times$ while maintaining the generation quality.


------------------------------------------------------------------------------

Title:
Fresh-Fi: Enhancing Information Freshness in Commodity WiFi Systems via  Customizing Lower Layers

Abstract: Enhancing information freshness in wireless networks has gained significant
attention in recent years. To optimize or analyze information freshness, which
is often characterized by the age of information (AoI) metric, extensive
theoretical studies have been conducted on various wireless networks. Early
research has demonstrated the significance of last-come-first-served (LCFS)
packet scheduling and controlled status sampling (i.e., packet generation) in
improving information freshness. These mechanisms have been widely adopted in
subsequent studies. However, the effective implementation of these mechanisms
in commercial off-the-shelf (COTS) wireless devices has not been thoroughly
investigated, which could limit the practical application of information
freshness-oriented protocols in real-world systems. Our work aims to address
the gap by exploring the effective implementation of the information
freshness-oriented mechanisms mentioned above in COTS WiFi devices that use the
Linux operating system. Our attempts reveal that implementing these mechanisms
in COTS systems is not a straightforward task. Specifically, we found that the
physical layer queue of WiFi devices operates on a first-come-first-served
(FCFS) basis, and the packet generation process cannot be precisely controlled
by default. To overcome these challenges, we develop Fresh-Fi, an information
freshness-oriented protocol stack that involves careful customization to the
lower layers of the Linux networking protocol stack. Fresh-Fi mainly
incorporates a mac80211 subsystem-based LCFS queue and a real-time kernel-based
cross-layer tunnel between the mac80211 subsystem and the application layer for
triggered packet generation.


------------------------------------------------------------------------------

Title:
LVLM-eHub: A Comprehensive Evaluation Benchmark for Large  Vision-Language Models

Abstract: Large Vision-Language Models (LVLMs) have recently played a dominant role in
multimodal vision-language learning. Despite the great success, it lacks a
holistic evaluation of their efficacy. This paper presents a comprehensive
evaluation of publicly available large multimodal models by building a LVLM
evaluation Hub (LVLM-eHub). Our LVLM-eHub consists of $8$ representative LVLMs
such as InstructBLIP and MiniGPT-4, which are thoroughly evaluated by a
quantitative capability evaluation and an online arena platform. The former
evaluates $6$ categories of multimodal capabilities of LVLMs such as visual
question answering and embodied artificial intelligence on $47$ standard
text-related visual benchmarks, while the latter provides the user-level
evaluation of LVLMs in an open-world question-answering scenario. The study
reveals several innovative findings. First, instruction-tuned LVLM with massive
in-domain data such as InstructBLIP heavily overfits many existing tasks,
generalizing poorly in the open-world scenario. Second, instruction-tuned LVLM
with moderate instruction-following data may result in object hallucination
issues (i.e., generate objects that are inconsistent with target images in the
descriptions). It either makes the current evaluation metric such as CIDEr for
image captioning ineffective or generates wrong answers. Third, employing a
multi-turn reasoning evaluation framework can mitigate the issue of object
hallucination, shedding light on developing an effective pipeline for LVLM
evaluation. The findings provide a foundational framework for the conception
and assessment of innovative strategies aimed at enhancing zero-shot multimodal
techniques. Our LVLM-eHub will be available at
this https URL


------------------------------------------------------------------------------

Title:
An Energy-Efficient Generic Accuracy Configurable Multiplier Based on  Block-Level Voltage Overscaling

Abstract: Voltage Overscaling (VOS) is one of the well-known techniques to increase the
energy efficiency of arithmetic units. Also, it can provide significant
lifetime improvements, while still meeting the accuracy requirements of
inherently error-resilient applications. This paper proposes a generic
accuracy-configurable multiplier that employs the VOS at a coarse-grained level
(block-level) to reduce the control logic required for applying VOS and its
associated overheads, thus enabling a high degree of trade-off between energy
consumption and output quality. The proposed configurable Block-Level VOS-based
(BL-VOS) multiplier relies on employing VOS in a multiplier composed of smaller
blocks, where applying VOS in different blocks results in structures with
various output accuracy levels. To evaluate the proposed concept, we implement
8-bit and 16-bit BL-VOS multipliers with various blocks width in a 15-nm FinFET
technology. The results show that the proposed multiplier achieves up to 15%
lower energy consumption and up to 21% higher output accuracy compared to the
state-of-the-art VOS-based multipliers. Also, the effects of Process Variation
(PV) and Bias Temperature Instability (BTI) induced delay on the proposed
multiplier are investigated. Finally, the effectiveness of the proposed
multiplier is studied for two different image processing applications, in terms
of quality and energy efficiency.


------------------------------------------------------------------------------

Title:
MetricPrompt: Prompting Model as a Relevance Metric for Few-shot Text  Classification

Abstract: Prompting methods have shown impressive performance in a variety of text
mining tasks and applications, especially few-shot ones. Despite the promising
prospects, the performance of prompting model largely depends on the design of
prompt template and verbalizer. In this work, we propose MetricPrompt, which
eases verbalizer design difficulty by reformulating few-shot text
classification task into text pair relevance estimation task. MetricPrompt
adopts prompting model as the relevance metric, further bridging the gap
between Pre-trained Language Model's (PLM) pre-training objective and text
classification task, making possible PLM's smooth adaption. Taking a training
sample and a query one simultaneously, MetricPrompt captures cross-sample
relevance information for accurate relevance estimation. We conduct experiments
on three widely used text classification datasets across four few-shot
settings. Results show that MetricPrompt outperforms manual verbalizer and
other automatic verbalizer design methods across all few-shot settings,
achieving new state-of-the-art (SOTA) performance.


------------------------------------------------------------------------------

Title:
PINNacle: A Comprehensive Benchmark of Physics-Informed Neural Networks  for Solving PDEs

Abstract: While significant progress has been made on Physics-Informed Neural Networks
(PINNs), a comprehensive comparison of these methods across a wide range of
Partial Differential Equations (PDEs) is still lacking. This study introduces
PINNacle, a benchmarking tool designed to fill this gap. PINNacle provides a
diverse dataset, comprising over 20 distinct PDEs from various domains
including heat conduction, fluid dynamics, biology, and electromagnetics. These
PDEs encapsulate key challenges inherent to real-world problems, such as
complex geometry, multi-scale phenomena, nonlinearity, and high dimensionality.
PINNacle also offers a user-friendly toolbox, incorporating about 10
state-of-the-art PINN methods for systematic evaluation and comparison. We have
conducted extensive experiments with these methods, offering insights into
their strengths and weaknesses. In addition to providing a standardized means
of assessing performance, PINNacle also offers an in-depth analysis to guide
future research, particularly in areas such as domain decomposition methods and
loss reweighting for handling multi-scale problems and complex geometry. While
PINNacle does not guarantee success in all real-world scenarios, it represents
a significant contribution to the field by offering a robust, diverse, and
comprehensive benchmark suite that will undoubtedly foster further research and
development in PINNs.


------------------------------------------------------------------------------

Title:
Neural Fields with Hard Constraints of Arbitrary Differential Order

Abstract: While deep learning techniques have become extremely popular for solving a
broad range of optimization problems, methods to enforce hard constraints
during optimization, particularly on deep neural networks, remain
underdeveloped. Inspired by the rich literature on meshless interpolation and
its extension to spectral collocation methods in scientific computing, we
develop a series of approaches for enforcing hard constraints on neural fields,
which we refer to as \emph{Constrained Neural Fields} (CNF). The constraints
can be specified as a linear operator applied to the neural field and its
derivatives. We also design specific model representations and training
strategies for problems where standard models may encounter difficulties, such
as conditioning of the system, memory consumption, and capacity of the network
when being constrained. Our approaches are demonstrated in a wide range of
real-world applications. Additionally, we develop a framework that enables
highly efficient model and constraint specification, which can be readily
applied to any downstream task where hard constraints need to be explicitly
satisfied during optimization.


------------------------------------------------------------------------------

Title:
One-Shot Learning of Visual Path Navigation for Autonomous Vehicles

Abstract: Autonomous driving presents many challenges due to the large number of
scenarios the autonomous vehicle (AV) may encounter. End-to-end deep learning
models are comparatively simplistic models that can handle a broad set of
scenarios. However, end-to-end models require large amounts of diverse data to
perform well. This paper presents a novel deep neural network that performs
image-to-steering path navigation that helps with the data problem by adding
one-shot learning to the system. Presented with a previously unseen path, the
vehicle can drive the path autonomously after being shown the path once and
without model retraining. In fact, the full path is not needed and images of
the road junctions is sufficient. In-vehicle testing and offline testing are
used to verify the performance of the proposed navigation and to compare
different candidate architectures.


------------------------------------------------------------------------------

Title:
Neural models for Factual Inconsistency Classification with Explanations

Abstract: Factual consistency is one of the most important requirements when editing
high quality documents. It is extremely important for automatic text generation
systems like summarization, question answering, dialog modeling, and language
modeling. Still, automated factual inconsistency detection is rather
under-studied. Existing work has focused on (a) finding fake news keeping a
knowledge base in context, or (b) detecting broad contradiction (as part of
natural language inference literature). However, there has been no work on
detecting and explaining types of factual inconsistencies in text, without any
knowledge base in context. In this paper, we leverage existing work in
linguistics to formally define five types of factual inconsistencies. Based on
this categorization, we contribute a novel dataset, FICLE (Factual
Inconsistency CLassification with Explanation), with ~8K samples where each
sample consists of two sentences (claim and context) annotated with type and
span of inconsistency. When the inconsistency relates to an entity type, it is
labeled as well at two levels (coarse and fine-grained). Further, we leverage
this dataset to train a pipeline of four neural models to predict inconsistency
type with explanations, given a (claim, context) sentence pair. Explanations
include inconsistent claim fact triple, inconsistent context span, inconsistent
claim component, coarse and fine-grained inconsistent entity types. The
proposed system first predicts inconsistent spans from claim and context; and
then uses them to predict inconsistency types and inconsistent entity types
(when inconsistency is due to entities). We experiment with multiple
Transformer-based natural language classification as well as generative models,
and find that DeBERTa performs the best. Our proposed methods provide a
weighted F1 of ~87% for inconsistency type classification across the five
classes.


------------------------------------------------------------------------------

Title:
Differentially Private Domain Adaptation with Theoretical Guarantees

Abstract: In many applications, the labeled data at the learner's disposal is subject
to privacy constraints and is relatively limited. To derive a more accurate
predictor for the target domain, it is often beneficial to leverage publicly
available labeled data from an alternative domain, somewhat close to the target
domain. This is the modern problem of supervised domain adaptation from a
public source to a private target domain. We present two $(\epsilon,
\delta)$-differentially private adaptation algorithms for supervised
adaptation, for which we make use of a general optimization problem, recently
shown to benefit from favorable theoretical learning guarantees. Our first
algorithm is designed for regression with linear predictors and shown to solve
a convex optimization problem. Our second algorithm is a more general solution
for loss functions that may be non-convex but Lipschitz and smooth. While our
main objective is a theoretical analysis, we also report the results of several
experiments first demonstrating that the non-private versions of our algorithms
outperform adaptation baselines and next showing that, for larger values of the
target sample size or $\epsilon$, the performance of our private algorithms
remains close to that of the non-private formulation.


------------------------------------------------------------------------------

Title:
Decentralized Social Navigation with Non-Cooperative Robots via Bi-Level  Optimization

Abstract: This paper presents a fully decentralized approach for realtime
non-cooperative multi-robot navigation in social mini-games, such as navigating
through a narrow doorway or negotiating right of way at a corridor
intersection. Our contribution is a new realtime bi-level optimization
algorithm, in which the top-level optimization consists of computing a fair and
collision-free ordering followed by the bottom-level optimization which plans
optimal trajectories conditioned on the ordering. We show that, given such a
priority order, we can impose simple kinodynamic constraints on each robot that
are sufficient for it to plan collision-free trajectories with minimal
deviation from their preferred velocities, similar to how humans navigate in
these scenarios.
We successfully deploy the proposed algorithm in the real world using F$1/10$
robots, a Clearpath Jackal, and a Boston Dynamics Spot as well as in simulation
using the SocialGym 2.0 multi-agent social navigation simulator, in the doorway
and corridor intersection scenarios. We compare with state-of-the-art social
navigation methods using multi-agent reinforcement learning, collision
avoidance algorithms, and crowd simulation models. We show that $(i)$ classical
navigation performs $44\%$ better than the state-of-the-art learning-based
social navigation algorithms, $(ii)$ without a scheduling protocol, our
approach results in collisions in social mini-games $(iii)$ our approach yields
$2\times$ and $5\times$ fewer velocity changes than CADRL in doorways and
intersections, and finally $(iv)$ bi-level navigation in doorways at a flow
rate of $2.8 - 3.3$ (ms)$^{-1}$ is comparable to flow rate in human navigation
at a flow rate of $4$ (ms)$^{-1}$.


------------------------------------------------------------------------------

Title:
A Gromov--Wasserstein Geometric View of Spectrum-Preserving Graph  Coarsening

Abstract: Graph coarsening is a technique for solving large-scale graph problems by
working on a smaller version of the original graph, and possibly interpolating
the results back to the original graph. It has a long history in scientific
computing and has recently gained popularity in machine learning, particularly
in methods that preserve the graph spectrum. This work studies graph coarsening
from a different perspective, developing a theory for preserving graph
distances and proposing a method to achieve this. The geometric approach is
useful when working with a collection of graphs, such as in graph
classification and regression. In this study, we consider a graph as an element
on a metric space equipped with the Gromov--Wasserstein (GW) distance, and
bound the difference between the distance of two graphs and their coarsened
versions. Minimizing this difference can be done using the popular weighted
kernel $K$-means method, which improves existing spectrum-preserving methods
with the proper choice of the kernel. The study includes a set of experiments
to support the theory and method, including approximating the GW distance,
preserving the graph spectrum, classifying graphs using spectral information,
and performing regression using graph convolutional networks. Code is available
at this https URL .


------------------------------------------------------------------------------

Title:
Functional Dependencies with Predicates: What Makes the $g_3$-error Easy  to Compute?

Abstract: The notion of functional dependencies (FDs) can be used by data scientists
and domain experts to confront background knowledge against data. To overcome
the classical, too restrictive, satisfaction of FDs, it is possible to replace
equality with more meaningful binary predicates, and use a coverage measure
such as the $g_3$-error to estimate the degree to which a FD matches the data.
It is known that the $g_3$-error can be computed in polynomial time if equality
is used, but unfortunately, the problem becomes NP-complete when relying on
more general predicates instead. However, there has been no analysis of which
class of predicates or which properties alter the complexity of the problem,
especially when going from equality to more general predicates.
In this work, we provide such an analysis. We focus on the properties of
commonly used predicates such as equality, similarity relations, and partial
orders. These properties are: reflexivity, transitivity, symmetry, and
antisymmetry. We show that symmetry and transitivity together are sufficient to
guarantee that the $g_3$-error can be computed in polynomial time. However,
dropping either of them makes the problem NP-complete.


------------------------------------------------------------------------------

Title:
Guided Sampling-Based Motion Planning with Dynamics in Unknown  Environments

Abstract: Despite recent progress improving the efficiency and quality of motion
planning, planning collision-free and dynamically-feasible trajectories in
partially-mapped environments remains challenging, since constantly replanning
as unseen obstacles are revealed during navigation both incurs significant
computational expense and can introduce problematic oscillatory behavior. To
improve the quality of motion planning in partial maps, this paper develops a
framework that augments sampling-based motion planning to leverage a high-level
discrete layer and prior solutions to guide motion-tree expansion during
replanning, affording both (i) faster planning and (ii) improved solution
coherence. Our framework shows significant improvements in runtime and solution
distance when compared with other sampling-based motion planners.


------------------------------------------------------------------------------

Title:
When Hyperspectral Image Classification Meets Diffusion Models: An  Unsupervised Feature Learning Framework

Abstract: Learning effective spectral-spatial features is important for the
hyperspectral image (HSI) classification task, but the majority of existing HSI
classification methods still suffer from modeling complex spectral-spatial
relations and characterizing low-level details and high-level semantics
comprehensively. As a new class of record-breaking generative models, diffusion
models are capable of modeling complex relations for understanding inputs well
as learning both high-level and low-level visual features. Meanwhile, diffusion
models can capture more abundant features by taking advantage of the extra and
unique dimension of timestep t. In view of these, we propose an unsupervised
spectral-spatial feature learning framework based on the diffusion model for
HSI classification for the first time, named Diff-HSI. Specifically, we first
pretrain the diffusion model with unlabeled HSI patches for unsupervised
feature learning, and then exploit intermediate hierarchical features from
different timesteps for classification. For better using the abundant
timestep-wise features, we design a timestep-wise feature bank and a dynamic
feature fusion module to construct timestep-wise features, adaptively learning
informative multi-timestep representations. Finally, an ensemble of linear
classifiers is applied to perform HSI classification. Extensive experiments are
conducted on three public HSI datasets, and our results demonstrate that
Diff-HSI outperforms state-of-the-art supervised and unsupervised methods for
HSI classification.


------------------------------------------------------------------------------

Title:
Reward-Free Curricula for Training Robust World Models

Abstract: There has been a recent surge of interest in developing generally-capable
agents that can adapt to new tasks without additional training in the
environment. Learning world models from reward-free exploration is a promising
approach, and enables policies to be trained using imagined experience for new
tasks. Achieving a general agent requires robustness across different
environments. However, different environments may require different amounts of
data to learn a suitable world model. In this work, we address the problem of
efficiently learning robust world models in the reward-free setting. As a
measure of robustness, we consider the minimax regret objective. We show that
the minimax regret objective can be connected to minimising the maximum error
in the world model across environments. This informs our algorithm, WAKER:
Weighted Acquisition of Knowledge across Environments for Robustness. WAKER
selects environments for data collection based on the estimated error of the
world model for each environment. Our experiments demonstrate that WAKER
outperforms naive domain randomisation, resulting in improved robustness,
efficiency, and generalisation.


------------------------------------------------------------------------------

Title:
Motion Capture Dataset for Practical Use of AI-based Motion Editing and  Stylization

Abstract: In this work, we proposed a new style-diverse dataset for the domain of
motion style transfer. The motion dataset uses an industrial-standard human
bone structure and thus is industry-ready to be plugged into 3D characters for
many projects. We claim the challenges in motion style transfer and encourage
future work in this domain by releasing the proposed motion dataset to the
public. We conduct a comprehensive study on motion style transfer in the
experiment using the state-of-the-art method, and the results show the proposed
dataset's validity for the motion style transfer task.


------------------------------------------------------------------------------

Title:
Optimal Best-Arm Identification in Bandits with Access to Offline Data

Abstract: Learning paradigms based purely on offline data as well as those based solely
on sequential online learning have been well-studied in the literature. In this
paper, we consider combining offline data with online learning, an area less
studied but of obvious practical importance. We consider the stochastic
$K$-armed bandit problem, where our goal is to identify the arm with the
highest mean in the presence of relevant offline data, with confidence
$1-\delta$. We conduct a lower bound analysis on policies that provide such
$1-\delta$ probabilistic correctness guarantees. We develop algorithms that
match the lower bound on sample complexity when $\delta$ is small. Our
algorithms are computationally efficient with an average per-sample acquisition
cost of $\tilde{O}(K)$, and rely on a careful characterization of the
optimality conditions of the lower bound problem.


------------------------------------------------------------------------------

Title:
Regression-Based Model Error Compensation for Hierarchical MPC Building  Energy Management System

Abstract: One of the major challenges in the development of energy management systems
(EMSs) for complex buildings is accurate modeling. To address this, we propose
an EMS, which combines a Model Predictive Control (MPC) approach with
data-driven model error compensation. The hierarchical MPC approach consists of
two layers: An aggregator controls the overall energy flows of the building in
an aggregated perspective, while a distributor distributes heating and cooling
powers to individual temperature zones. The controllers of both layers employ
regression-based error estimation to predict and incorporate the model error.
The proposed approach is evaluated in a software-in-the-loop simulation using a
physics-based digital twin model. Simulation results show the efficacy and
robustness of the proposed approach


------------------------------------------------------------------------------

Title:
Deep Generative Models for Decision-Making and Control

Abstract: Deep model-based reinforcement learning methods offer a conceptually simple
approach to the decision-making and control problem: use learning for the
purpose of estimating an approximate dynamics model, and offload the rest of
the work to classical trajectory optimization. However, this combination has a
number of empirical shortcomings, limiting the usefulness of model-based
methods in practice. The dual purpose of this thesis is to study the reasons
for these shortcomings and to propose solutions for the uncovered problems.
Along the way, we highlight how inference techniques from the contemporary
generative modeling toolbox, including beam search, classifier-guided sampling,
and image inpainting, can be reinterpreted as viable planning strategies for
reinforcement learning problems.


------------------------------------------------------------------------------

Title:
Revealing the Illusion of Joint Multimodal Understanding in VideoQA  Models

Abstract: While VideoQA Transformer models demonstrate competitive performance on
standard benchmarks, the reasons behind their success remain unclear. Do these
models jointly capture and leverage the rich multimodal structures and dynamics
from video and text? Or are they merely exploiting shortcuts to achieve high
scores? We analyze this with $\textit{QUAG}$ (QUadrant AveraGe), a lightweight
and non-parametric probe that systematically ablates the model's coupled
multimodal understanding during inference. Surprisingly, QUAG reveals that the
models manage to maintain high performance even when injected with multimodal
sub-optimality. Additionally, even after replacing self-attention in multimodal
fusion blocks with "QUAG-attention", a simplistic and less-expressive variant
of self-attention, the models maintain high performance. This means that
current VideoQA benchmarks and their metrics do not penalize shortcuts that
discount joint multimodal understanding. Motivated by this, we propose the
$\textit{CLAVI}$ (Counterfactual in LAnguage and VIdeo) benchmark, a diagnostic
dataset for benchmarking coupled multimodal understanding in VideoQA through
counterfactuals. CLAVI consists of temporal questions and videos that are
augmented to curate balanced counterfactuals in language and video domains.
Hence, it incentivizes, and identifies the reliability of learnt multimodal
representations. We evaluate CLAVI and find that models achieve high
performance on multimodal shortcut instances, but have very poor performance on
the counterfactuals. Hence, we position CLAVI as a litmus test to identify,
diagnose and improve the sub-optimality of learnt multimodal VideoQA
representations which the current benchmarks are unable to assess.


------------------------------------------------------------------------------

Title:
Hyperbolic Convolution via Kernel Point Aggregation

Abstract: Learning representations according to the underlying geometry is of vital
importance for non-Euclidean data. Studies have revealed that the hyperbolic
space can effectively embed hierarchical or tree-like data. In particular, the
few past years have witnessed a rapid development of hyperbolic neural
networks. However, it is challenging to learn good hyperbolic representations
since common Euclidean neural operations, such as convolution, do not extend to
the hyperbolic space. Most hyperbolic neural networks do not embrace the
convolution operation and ignore local patterns. Others either only use
non-hyperbolic convolution, or miss essential properties such as equivariance
to permutation. We propose HKConv, a novel trainable hyperbolic convolution
which first correlates trainable local hyperbolic features with fixed kernel
points placed in the hyperbolic space, then aggregates the output features
within a local neighborhood. HKConv not only expressively learns local features
according to the hyperbolic geometry, but also enjoys equivariance to
permutation of hyperbolic points and invariance to parallel transport of a
local neighborhood. We show that neural networks with HKConv layers advance
state-of-the-art in various tasks.


------------------------------------------------------------------------------

Title:
Path Generation for Wheeled Robots Autonomous Navigation on Vegetated  Terrain

Abstract: Wheeled robot navigation has been widely used in urban environments, but
little research has been conducted on its navigation in wild vegetation.
External sensors (LiDAR, camera etc.) are often used to construct point cloud
map of the surrounding environment, however, the supporting rigid ground used
for travelling cannot be detected due to the occlusion of vegetation. This
often causes unsafe or not smooth path during planning process. To address the
drawback, we propose the PE-RRT* algorithm, which effectively combines a novel
support plane estimation method and sampling algorithm to generate real-time
feasible and safe path in vegetation environments. In order to accurately
estimate the support plane, we combine external perception and proprioception,
and use Multivariate Gaussian Processe Regression (MV-GPR) to estimate the
terrain at the sampling nodes. We build a physical experimental platform and
conduct experiments in different outdoor environments. Experimental results
show that our method has high safety, robustness and generalization.


------------------------------------------------------------------------------

Title:
A comparative study for block chain applications in the MANET

Abstract: MANET- Mobile Ad-hoc Networks are famous for their infrastructure-less
arrangement for communication. In this network, nodes are self-organized and
can act as router. They are battery operated and self-organizing. Block chain
is a new concept from 2008 and researchers are trying the possible application
of Block chain in many sectors including MANETs. This paper surveys the
existing researches done in applying block chain in a MANET environment. Block
chain is mainly used in MANETs for improving security while routing packets
from one node to another. Some researchers have proposed trust models using
block chain. This paper reviews some of the existing approaches where block
chain is used in MANETs for routing the packets, creating trust models, and
dealing with network partitioning problem and scalability problem. This paper
acts as a review paper to study on block chain applications in MANET


------------------------------------------------------------------------------

Title:
Community Detection Attack against Collaborative Learning-based  Recommender Systems

Abstract: Collaborative-learning based recommender systems emerged following the
success of collaborative learning techniques such as Federated Learning (FL)
and Gossip Learning (GL). In these systems, users participate in the training
of a recommender system while keeping their history of consumed items on their
devices. While these solutions seemed appealing for preserving the privacy of
the participants at a first glance, recent studies have shown that
collaborative learning can be vulnerable to a variety of privacy attacks. In
this paper we propose a novel privacy attack called Community Detection Attack
(CDA), which allows an adversary to discover the members of a community based
on a set of items of her choice (e.g., discovering users interested in LGBT
content). Through experiments on three real recommendation datasets and by
using two state-of-the-art recommendation models, we assess the sensitivity of
an FL-based recommender system as well as two flavors of Gossip Learning-based
recommender systems to CDA. Results show that on all models and all datasets,
the FL setting is more vulnerable to CDA than Gossip settings. We further
evaluated two off-the-shelf mitigation strategies, namely differential privacy
(DP) and a share less policy, which consists in sharing a subset of model
parameters. Results show a better privacy-utility trade-off for the share less
policy compared to DP especially in the Gossip setting.


------------------------------------------------------------------------------

Title:
LOVM: Language-Only Vision Model Selection

Abstract: Pre-trained multi-modal vision-language models (VLMs) are becoming
increasingly popular due to their exceptional performance on downstream vision
applications, particularly in the few- and zero-shot settings. However,
selecting the best-performing VLM for some downstream applications is
non-trivial, as it is dataset and task-dependent. Meanwhile, the exhaustive
evaluation of all available VLMs on a novel application is not only time and
computationally demanding but also necessitates the collection of a labeled
dataset for evaluation. As the number of open-source VLM variants increases,
there is a need for an efficient model selection strategy that does not require
access to a curated evaluation dataset. This paper proposes a novel task and
benchmark for efficiently evaluating VLMs' zero-shot performance on downstream
applications without access to the downstream task dataset. Specifically, we
introduce a new task LOVM: Language-Only Vision Model Selection, where methods
are expected to perform both model selection and performance prediction based
solely on a text description of the desired downstream application. We then
introduced an extensive LOVM benchmark consisting of ground-truth evaluations
of 35 pre-trained VLMs and 23 datasets, where methods are expected to rank the
pre-trained VLMs and predict their zero-shot performance.


------------------------------------------------------------------------------

Title:
Advancing Volumetric Medical Image Segmentation via Global-Local Masked  Autoencoder

Abstract: Masked autoencoder (MAE) has emerged as a promising self-supervised
pretraining technique to enhance the representation learning of a neural
network without human intervention. To adapt MAE onto volumetric medical
images, existing methods exhibit two challenges: first, the global information
crucial for understanding the clinical context of the holistic data is lacked;
second, there was no guarantee of stabilizing the representations learned from
the randomly masked inputs. To tackle these limitations, we proposed
Global-Local Masked AutoEncoder (GL-MAE), a simple yet effective
self-supervised pre-training strategy. GL-MAE reconstructs both the masked
global and masked local volumes, which enables learning the essential local
details as well as the global context. We further introduced global-to-global
consistency and local-to-global correspondence via global-guided consistency
learning to enhance and stabilize the representation learning of the masked
volumes. Finetuning results on multiple datasets illustrate the superiority of
our method over other state-of-the-art self-supervised algorithms,
demonstrating its effectiveness on versatile volumetric medical image
segmentation tasks, even when annotations are scarce. Codes and models will be
released upon acceptance.


------------------------------------------------------------------------------

Title:
Knowledge Assembly: Semi-Supervised Multi-Task Learning from Multiple  Datasets with Disjoint Labels

Abstract: In real-world scenarios we often need to perform multiple tasks
simultaneously. Multi-Task Learning (MTL) is an adequate method to do so, but
usually requires datasets labeled for all tasks. We propose a method that can
leverage datasets labeled for only some of the tasks in the MTL framework. Our
work, Knowledge Assembly (KA), learns multiple tasks from disjoint datasets by
leveraging the unlabeled data in a semi-supervised manner, using model
augmentation for pseudo-supervision. Whilst KA can be implemented on any
existing MTL networks, we test our method on jointly learning person
re-identification (reID) and pedestrian attribute recognition (PAR). We surpass
the single task fully-supervised performance by $4.2\%$ points for reID and
$0.9\%$ points for PAR.


------------------------------------------------------------------------------

Title:
ViP: A Differentially Private Foundation Model for Computer Vision

Abstract: Artificial intelligence (AI) has seen a tremendous surge in capabilities
thanks to the use of foundation models trained on internet-scale data. On the
flip side, the uncurated nature of internet-scale data also poses significant
privacy and legal risks, as they often contain personal information or
copyrighted material that should not be trained on without permission. In this
work, we propose as a mitigation measure a recipe to train foundation vision
models with differential privacy (DP) guarantee. We identify masked
autoencoders as a suitable learning algorithm that aligns well with DP-SGD, and
train ViP -- a Vision transformer with differential Privacy -- under a strict
privacy budget of $\epsilon=8$ on the LAION400M dataset. We evaluate the
quality of representation learned by ViP using standard downstream vision
tasks; in particular, ViP achieves a (non-private) linear probing accuracy of
$55.7\%$ on ImageNet, comparable to that of end-to-end trained AlexNet (trained
and evaluated on ImageNet). Our result suggests that scaling to internet-scale
data can be practical for private learning.


------------------------------------------------------------------------------

Title:
Accelerating Dynamic Network Embedding with Billions of Parameter  Updates to Milliseconds

Abstract: Network embedding, a graph representation learning method illustrating
network topology by mapping nodes into lower-dimension vectors, is challenging
to accommodate the ever-changing dynamic graphs in practice. Existing research
is mainly based on node-by-node embedding modifications, which falls into the
dilemma of efficient calculation and accuracy. Observing that the embedding
dimensions are usually much smaller than the number of nodes, we break this
dilemma with a novel dynamic network embedding paradigm that rotates and scales
the axes of embedding space instead of a node-by-node update. Specifically, we
propose the Dynamic Adjacency Matrix Factorization (DAMF) algorithm, which
achieves an efficient and accurate dynamic network embedding by rotating and
scaling the coordinate system where the network embedding resides with no more
than the number of edge modifications changes of node embeddings. Moreover, a
dynamic Personalized PageRank is applied to the obtained network embeddings to
enhance node embeddings and capture higher-order neighbor information
dynamically. Experiments of node classification, link prediction, and graph
reconstruction on different-sized dynamic graphs suggest that DAMF advances
dynamic network embedding. Further, we unprecedentedly expand dynamic network
embedding experiments to billion-edge graphs, where DAMF updates billion-level
parameters in less than 10ms.


------------------------------------------------------------------------------

Title:
Partial-Label Regression

Abstract: Partial-label learning is a popular weakly supervised learning setting that
allows each training example to be annotated with a set of candidate labels.
Previous studies on partial-label learning only focused on the classification
setting where candidate labels are all discrete, which cannot handle continuous
labels with real values. In this paper, we provide the first attempt to
investigate partial-label regression, where each training example is annotated
with a set of real-valued candidate labels. To solve this problem, we first
propose a simple baseline method that takes the average loss incurred by
candidate labels as the predictive loss. The drawback of this method lies in
that the loss incurred by the true label may be overwhelmed by other false
labels. To overcome this drawback, we propose an identification method that
takes the least loss incurred by candidate labels as the predictive loss. We
further improve it by proposing a progressive identification method to
differentiate candidate labels using progressively updated weights for incurred
losses. We prove that the latter two methods are model-consistent and provide
convergence analyses. Our proposed methods are theoretically grounded and can
be compatible with any models, optimizers, and losses. Experiments validate the
effectiveness of our proposed methods.


------------------------------------------------------------------------------

Title:
A Framework for Learning from Demonstration with Minimal Human Effort

Abstract: We consider robot learning in the context of shared autonomy, where control
of the system can switch between a human teleoperator and autonomous control.
In this setting we address reinforcement learning, and learning from
demonstration, where there is a cost associated with human time. This cost
represents the human time required to teleoperate the robot, or recover the
robot from failures. For each episode, the agent must choose between requesting
human teleoperation, or using one of its autonomous controllers. In our
approach, we learn to predict the success probability for each controller,
given the initial state of an episode. This is used in a contextual multi-armed
bandit algorithm to choose the controller for the episode. A controller is
learnt online from demonstrations and reinforcement learning so that autonomous
performance improves, and the system becomes less reliant on the teleoperator
with more experience. We show that our approach to controller selection reduces
the human cost to perform two simulated tasks and a single real-world task.


------------------------------------------------------------------------------

Title:
On Strengthening and Defending Graph Reconstruction Attack with Markov  Chain Approximation

Abstract: Although powerful graph neural networks (GNNs) have boosted numerous
real-world applications, the potential privacy risk is still underexplored. To
close this gap, we perform the first comprehensive study of graph
reconstruction attack that aims to reconstruct the adjacency of nodes. We show
that a range of factors in GNNs can lead to the surprising leakage of private
links. Especially by taking GNNs as a Markov chain and attacking GNNs via a
flexible chain approximation, we systematically explore the underneath
principles of graph reconstruction attack, and propose two information
theory-guided mechanisms: (1) the chain-based attack method with adaptive
designs for extracting more private information; (2) the chain-based defense
method that sharply reduces the attack fidelity with moderate accuracy loss.
Such two objectives disclose a critical belief that to recover better in
attack, you must extract more multi-aspect knowledge from the trained GNN;
while to learn safer for defense, you must forget more link-sensitive
information in training GNNs. Empirically, we achieve state-of-the-art results
on six datasets and three common GNNs. The code is publicly available at:
this https URL


------------------------------------------------------------------------------

Title:
CAD-Estate: Large-scale CAD Model Annotation in RGB Videos

Abstract: We propose a method for annotating videos of complex multi-object scenes with
a globally-consistent 3D representation of the objects. We annotate each object
with a CAD model from a database, and place it in the 3D coordinate frame of
the scene with a 9-DoF pose transformation. Our method is semi-automatic and
works on commonly-available RGB videos, without requiring a depth sensor. Many
steps are performed automatically, and the tasks performed by humans are
simple, well-specified, and require only limited reasoning in 3D. This makes
them feasible for crowd-sourcing and has allowed us to construct a large-scale
dataset by annotating real-estate videos from YouTube. Our dataset CAD-Estate
offers 108K instances of 12K unique CAD models placed in the 3D representations
of 21K videos. In comparison to Scan2CAD, the largest existing dataset with CAD
model annotations on real scenes, CAD-Estate has 8x more instances and 4x more
unique CAD models. We showcase the benefits of pre-training a Mask2CAD model on
CAD-Estate for the task of automatic 3D object reconstruction and pose
estimation, demonstrating that it leads to improvements on the popular Scan2CAD
benchmark. We will release the data by mid July 2023.


------------------------------------------------------------------------------

Title:
Behavioral Cloning via Search in Embedded Demonstration Dataset

Abstract: Behavioural cloning uses a dataset of demonstrations to learn a behavioural
policy. To overcome various learning and policy adaptation problems, we propose
to use latent space to index a demonstration dataset, instantly access similar
relevant experiences, and copy behavior from these situations. Actions from a
selected similar situation can be performed by the agent until representations
of the agent's current situation and the selected experience diverge in the
latent space. Thus, we formulate our control problem as a search problem over a
dataset of experts' demonstrations. We test our approach on BASALT
MineRL-dataset in the latent representation of a Video PreTraining model. We
compare our model to state-of-the-art Minecraft agents. Our approach can
effectively recover meaningful demonstrations and show human-like behavior of
an agent in the Minecraft environment in a wide variety of scenarios.
Experimental results reveal that performance of our search-based approach is
comparable to trained models, while allowing zero-shot task adaptation by
changing the demonstration examples.


------------------------------------------------------------------------------

Title:
Prompt Performance Prediction for Generative IR

Abstract: The ability to predict the performance of a query in Information Retrieval
(IR) systems has been a longstanding challenge. In this paper, we introduce a
novel task called "Prompt Performance Prediction" that aims to predict the
performance of a query, referred to as a prompt, before obtaining the actual
search results. The context of our task leverages a generative model as an IR
engine to evaluate the prompts' performance on image retrieval tasks. We
demonstrate the plausibility of our task by measuring the correlation
coefficient between predicted and actual performance scores across three
datasets containing pairs of prompts and generated images. Our results show
promising performance prediction capabilities, suggesting potential
applications for optimizing generative IR systems.


------------------------------------------------------------------------------

Title:
Evaluation and Optimization of Gradient Compression for Distributed Deep  Learning

Abstract: To accelerate distributed training, many gradient compression methods have
been proposed to alleviate the communication bottleneck in synchronous
stochastic gradient descent (S-SGD), but their efficacy in real-world
applications still remains unclear. In this work, we first evaluate the
efficiency of three representative compression methods (quantization with
Sign-SGD, sparsification with Top-k SGD, and low-rank with Power-SGD) on a
32-GPU cluster. The results show that they cannot always outperform
well-optimized S-SGD or even worse due to their incompatibility with three key
system optimization techniques (all-reduce, pipelining, and tensor fusion) in
S-SGD. To this end, we propose a novel gradient compression method, called
alternate compressed Power-SGD (ACP-SGD), which alternately compresses and
communicates low-rank matrices. ACP-SGD not only significantly reduces the
communication volume, but also enjoys the three system optimizations like
S-SGD. Compared with Power-SGD, the optimized ACP-SGD can largely reduce the
compression and communication overheads, while achieving similar model
accuracy. In our experiments, ACP-SGD achieves an average of 4.06x and 1.43x
speedups over S-SGD and Power-SGD, respectively, and it consistently
outperforms other baselines across different setups (from 8 GPUs to 64 GPUs and
from 1Gb/s Ethernet to 100Gb/s InfiniBand).


------------------------------------------------------------------------------

Title:
ReLoop2: Building Self-Adaptive Recommendation Models via Responsive  Error Compensation Loop

Abstract: Industrial recommender systems face the challenge of operating in
non-stationary environments, where data distribution shifts arise from evolving
user behaviors over time. To tackle this challenge, a common approach is to
periodically re-train or incrementally update deployed deep models with newly
observed data, resulting in a continual training process. However, the
conventional learning paradigm of neural networks relies on iterative
gradient-based updates with a small learning rate, making it slow for large
recommendation models to adapt. In this paper, we introduce ReLoop2, a
self-correcting learning loop that facilitates fast model adaptation in online
recommender systems through responsive error compensation. Inspired by the
slow-fast complementary learning system observed in human brains, we propose an
error memory module that directly stores error samples from incoming data
streams. These stored samples are subsequently leveraged to compensate for
model prediction errors during testing, particularly under distribution shifts.
The error memory module is designed with fast access capabilities and undergoes
continual refreshing with newly observed data samples during the model serving
phase to support fast model adaptation. We evaluate the effectiveness of
ReLoop2 on three open benchmark datasets as well as a real-world production
dataset. The results demonstrate the potential of ReLoop2 in enhancing the
responsiveness and adaptiveness of recommender systems operating in
non-stationary environments.


------------------------------------------------------------------------------

Title:
MetaML: Automating Customizable Cross-Stage Design-Flow for Deep  Learning Acceleration

Abstract: This paper introduces a novel optimization framework for deep neural network
(DNN) hardware accelerators, enabling the rapid development of customized and
automated design flows. More specifically, our approach aims to automate the
selection and configuration of low-level optimization techniques, encompassing
DNN and FPGA low-level optimizations. We introduce novel optimization and
transformation tasks for building design-flow architectures, which are highly
customizable and flexible, thereby enhancing the performance and efficiency of
DNN accelerators. Our results demonstrate considerable reductions of up to 92\%
in DSP usage and 89\% in LUT usage for two networks, while maintaining accuracy
and eliminating the need for human effort or domain expertise. In comparison to
state-of-the-art approaches, our design achieves higher accuracy and utilizes
three times fewer DSP resources, underscoring the advantages of our proposed
framework.


------------------------------------------------------------------------------

Title:
Modules and PQ-trees in Robinson spaces

Abstract: A Robinson space is a dissimilarity space $(X,d)$ on $n$ points for which
there exists a compatible order, {\it i.e.} a total order $<$ on $X$ such that
$x<y<z$ implies that $d(x,y)\le d(x,z)$ and $d(y,z)\leq d(x,z)$. Recognizing if
a dissimilarity space is Robinson has numerous applications in seriation and
classification. A PQ-tree is a classical data structure introduced by Booth and
Lueker to compactly represent a set of related permutations on a set $X$. In
particular, the set of all compatible orders of a Robinson space are encoded by
a PQ-tree. An mmodule is a subset $M$ of $X$ which is not distinguishable from
the outside of $M$, {\it i.e.} the distances from any point of $X\setminus M$
to all points of $M$ are the same. Mmodules define the mmodule-tree of a
dissimilarity space $(X,d)$. Given $p\in X$, a $p$-copoint is a maximal mmodule
not containing $p$. The $p$-copoints form a partition of $X\setminus \{p\}$.
There exist two algorithms recognizing Robinson spaces in optimal $O(n^2)$
time. One uses PQ-trees and one uses a copoint partition of $(X, d)$.
In this paper, we establish correspondences between the PQ-trees and the
mmodule-trees of Robinson spaces. More precisely, we show how to construct the
mmodule-tree of a Robinson dissimilarity from its PQ-tree and how to construct
the PQ-tree from the odule-tree. To establish this translation, additionally to
the previous notions, we introduce the notions of $\delta$-graph $G_\delta$ of
a Robinson space and of $\delta$-mmodules, the connected components of
$G_\delta$. We also use the dendrogram of the subdominant ultrametric of $d$.
All these results also lead to optimal $O(n^2)$ time algorithms for
constructing the PQ-tree and the mmodule tree of Robinson spaces.


------------------------------------------------------------------------------

Title:
MPSA-DenseNet: A novel deep learning model for English accent  classification

Abstract: This paper presents three innovative deep learning models for English accent
classification: Multi-DenseNet, PSA-DenseNet, and MPSE-DenseNet, that combine
multi-task learning and the PSA module attention mechanism with DenseNet. We
applied these models to data collected from six dialects of English across
native English speaking regions (Britain, the United States, Scotland) and
nonnative English speaking regions (China, Germany, India). Our experimental
results show a significant improvement in classification accuracy, particularly
with MPSA-DenseNet, which outperforms all other models, including DenseNet and
EPSA models previously used for accent identification. Our findings indicate
that MPSA-DenseNet is a highly promising model for accurately identifying
English accents.


------------------------------------------------------------------------------

Title:
Continuous Learning Based Novelty Aware Emotion Recognition System

Abstract: Current works in human emotion recognition follow the traditional closed
learning approach governed by rigid rules without any consideration of novelty.
Classification models are trained on some collected datasets and expected to
have the same data distribution in the real-world deployment. Due to the fluid
and constantly changing nature of the world we live in, it is possible to have
unexpected and novel sample distribution which can lead the model to fail.
Hence, in this work, we propose a continuous learning based approach to deal
with novelty in the automatic emotion recognition task.


------------------------------------------------------------------------------

Title:
Towards vision-based dual arm robotic fruit harvesting

Abstract: Interest in agricultural robotics has increased considerably in recent years
due to benefits such as improvement in productivity and labor reduction.
However, current problems associated with unstructured environments make the
development of robotic harvesters challenging. Most research in agricultural
robotics focuses on single arm manipulation. Here, we propose a dual-arm
approach. We present a dual-arm fruit harvesting robot equipped with a RGB-D
camera, cutting and collecting tools. We exploit the cooperative task
description to maximize the capabilities of the dual-arm robot. We designed a
Hierarchical Quadratic Programming based control strategy to fulfill the set of
hard constrains related to the robot and environment: robot joint limits, robot
self-collisions, robot-fruit and robot-tree collisions. We combine deep
learning and standard image processing algorithms to detect and track fruits as
well as the tree trunk in the scene. We validate our perception methods on
real-world RGB-D images and our control method on simulated experiments.


------------------------------------------------------------------------------

Title:
Generalizable One-shot Neural Head Avatar

Abstract: We present a method that reconstructs and animates a 3D head avatar from a
single-view portrait image. Existing methods either involve time-consuming
optimization for a specific person with multiple images, or they struggle to
synthesize intricate appearance details beyond the facial region. To address
these limitations, we propose a framework that not only generalizes to unseen
identities based on a single-view image without requiring person-specific
optimization, but also captures characteristic details within and beyond the
face area (e.g. hairstyle, accessories, etc.). At the core of our method are
three branches that produce three tri-planes representing the coarse 3D
geometry, detailed appearance of a source image, as well as the expression of a
target image. By applying volumetric rendering to the combination of the three
tri-planes followed by a super-resolution module, our method yields a high
fidelity image of the desired identity, expression and pose. Once trained, our
model enables efficient 3D head avatar reconstruction and animation via a
single forward pass through a network. Experiments show that the proposed
approach generalizes well to unseen validation datasets, surpassing SOTA
baseline methods by a large margin on head avatar reconstruction and animation.


------------------------------------------------------------------------------

Title:
Deterministic and Work-Efficient Parallel Batch-Dynamic Trees in Low  Span

Abstract: Dynamic trees are a well-studied and fundamental building block of dynamic
graph algorithms dating back to the seminal work of Sleator and Tarjan
[STOC'81, (1981), pp. 114-122]. The problem is to maintain a tree subject to
online edge insertions and deletions while answering queries about the tree,
such as the heaviest weight on a path, etc. In the parallel batch-dynamic
setting, the goal is to process batches of edge updates work efficiently in low
($\text{polylog}\ n$) span. Two work-efficient algorithms are known,
batch-parallel Euler Tour Trees by Tseng et al. [ALENEX'19, (2019), pp. 92-106]
and parallel Rake-Compress (RC) Trees by Acar et al. [ESA'20, (2020), pp.
2:1-2:23]. Both however are randomized and work efficient in expectation.
Several downstream results that use these data structures (and indeed to the
best of our knowledge, all known work-efficient parallel batch-dynamic graph
algorithms) are therefore also randomized.
In this work, we give the first deterministic work-efficient solution to the
problem. Our algorithm maintains a dynamic parallel tree contraction subject to
batches of $k$ edge updates deterministically in worst-case $O(k \log(1 +
n/k))$ work and $O(\log n \log^{(c)} k)$ span for any constant $c$. This allows
us to implement parallel batch-dynamic RC-Trees with worst-case $O(k \log(1 +
n/k))$ work updates and queries deterministically. Our techniques that we use
to obtain the given span bound can also be applied to the state-of-the-art
randomized variant of the algorithm to improve its span from $O(\log n \log^*
n)$ to $O(\log n)$.


------------------------------------------------------------------------------

Title:
Theoretical Hardness and Tractability of POMDPs in RL with Partial  Hindsight State Information

Abstract: Partially observable Markov decision processes (POMDPs) have been widely
applied to capture many real-world applications. However, existing theoretical
results have shown that learning in general POMDPs could be intractable, where
the main challenge lies in the lack of latent state information. A key
fundamental question here is how much hindsight state information (HSI) is
sufficient to achieve tractability. In this paper, we establish a lower bound
that reveals a surprising hardness result: unless we have full HSI, we need an
exponentially scaling sample complexity to obtain an $\epsilon$-optimal policy
solution for POMDPs. Nonetheless, from the key insights in our lower-bound
construction, we find that there exist important tractable classes of POMDPs
even with partial HSI. In particular, for two novel classes of POMDPs with
partial HSI, we provide new algorithms that are shown to be near-optimal by
establishing new upper and lower bounds.


------------------------------------------------------------------------------

Title:
HOSSnet: an Efficient Physics-Guided Neural Network for Simulating Crack  Propagation

Abstract: Hybrid Optimization Software Suite (HOSS), which is a combined
finite-discrete element method (FDEM), is one of the advanced approaches to
simulating high-fidelity fracture and fragmentation processes but the
application of pure HOSS simulation is computationally expensive. At the same
time, machine learning methods, shown tremendous success in several scientific
problems, are increasingly being considered promising alternatives to
physics-based models in the scientific domains. Thus, our goal in this work is
to build a new data-driven methodology to reconstruct the crack fracture
accurately in the spatial and temporal fields. We leverage physical constraints
to regularize the fracture propagation in the long-term reconstruction. In
addition, we introduce perceptual loss and several extra pure machine learning
optimization approaches to improve the reconstruction performance of fracture
data further. We demonstrate the effectiveness of our proposed method through
both extrapolation and interpolation experiments. The results confirm that our
proposed method can reconstruct high-fidelity fracture data over space and time
in terms of pixel-wise reconstruction error and structural similarity. Visual
comparisons also show promising results in long-term


------------------------------------------------------------------------------

Title:
Uniform accuracy of implicit-explicit Runge-Kutta (IMEX-RK) schemes for  hyperbolic systems with relaxation

Abstract: Implicit-explicit Runge-Kutta (IMEX-RK) schemes are popular methods to treat
multiscale equations that contain a stiff part and a non-stiff part, where the
stiff part is characterized by a small parameter $\varepsilon$. In this work,
we prove rigorously the uniform stability and uniform accuracy of a class of
IMEX-RK schemes for a linear hyperbolic system with stiff relaxation. The
result we obtain is optimal in the sense that it holds regardless of the value
of $\varepsilon$ and the order of accuracy is the same as the design order of
the original scheme, i.e., there is no order reduction.


------------------------------------------------------------------------------

Title:
EPIC Fields: Marrying 3D Geometry and Video Understanding

Abstract: Neural rendering is fuelling a unification of learning, 3D geometry and video
understanding that has been waiting for more than two decades. Progress,
however, is still hampered by a lack of suitable datasets and benchmarks. To
address this gap, we introduce EPIC Fields, an augmentation of EPIC-KITCHENS
with 3D camera information. Like other datasets for neural rendering, EPIC
Fields removes the complex and expensive step of reconstructing cameras using
photogrammetry, and allows researchers to focus on modelling problems. We
illustrate the challenge of photogrammetry in egocentric videos of dynamic
actions and propose innovations to address them. Compared to other neural
rendering datasets, EPIC Fields is better tailored to video understanding
because it is paired with labelled action segments and the recent VISOR segment
annotations. To further motivate the community, we also evaluate two benchmark
tasks in neural rendering and segmenting dynamic objects, with strong baselines
that showcase what is not possible today. We also highlight the advantage of
geometry in semi-supervised video object segmentations on the VISOR
annotations. EPIC Fields reconstructs 96% of videos in EPICKITCHENS,
registering 19M frames in 99 hours recorded in 45 kitchens.


------------------------------------------------------------------------------

Title:
RAID Organizations for Improved Reliability and Performance: A Not  Entirely Unbiased Tutorial

Abstract: This is a followup to the 1994 tutorial by Berkeley RAID researchers whose
1988 RAID paper foresaw a revolutionary change in storage industry based on
advances in magnetic disk technology, i.e., replacement of large capacity
expensive disks with arrays of small capacity inexpensive disks. NAND flash
SSDs which use less power, incur very low latency, provide high bandwidth, and
are more reliable than HDDs are expected to replace HDDs as their prices drop.
Replication in the form of mirrored disks and erasure coding via parity and
Reed-Solomon codes are two methods to achieve higher reliability through
redundancy in disk arrays. RAID(4+k), k=1,2,... arrays utilizing k check strips
makes them k-disk-failure-tolerant with maximum distance separable coding with
minimum redundancy. Clustered RAID, local recovery codes, partial MDS, and
multilevel RAID are proposals to improve RAID reliability and performance. We
discuss RAID5 performance and reliability analysis in conjunction with HDDs w/o
and with latent sector errors - LSEs, which can be dealt with by intradisk
redundancy and disk scrubbing, the latter enhanced with machine learning
algorithms. Undetected disk errors causing silent data corruption are
propagated by rebuild. We utilize the M/G/1 queueing model for RAID5
performance evaluation, present approximations for fork/join response time in
degraded mode analysis, and the vacationing server model for rebuild analysis.
Methods and tools for reliability evaluation with Markov chain modeling and
simulation are discussed. Queueing and reliability analysis are based on
probability theory and stochastic processes so that the two topics can be
studied together. Their application is presented here in the context of RAID
arrays in a tutorial manner.


------------------------------------------------------------------------------

Title:
Density-Aware Reinforcement Learning to Optimise Energy Efficiency in  UAV-Assisted Networks

Abstract: Unmanned aerial vehicles (UAVs) serving as aerial base stations can be
deployed to provide wireless connectivity to mobile users, such as vehicles.
However, the density of vehicles on roads often varies spatially and temporally
primarily due to mobility and traffic situations in a geographical area, making
it difficult to provide ubiquitous service. Moreover, as energy-constrained
UAVs hover in the sky while serving mobile users, they may be faced with
interference from nearby UAV cells or other access points sharing the same
frequency band, thereby impacting the system's energy efficiency (EE). Recent
multi-agent reinforcement learning (MARL) approaches applied to optimise the
users' coverage worked well in reasonably even densities but might not perform
as well in uneven users' distribution, i.e., in urban road networks with uneven
concentration of vehicles. In this work, we propose a density-aware
communication-enabled multi-agent decentralised double deep Q-network
(DACEMAD-DDQN) approach that maximises the total system's EE by jointly
optimising the trajectory of each UAV, the number of connected users, and the
UAVs' energy consumption while keeping track of dense and uneven users'
distribution. Our result outperforms state-of-the-art MARL approaches in terms
of EE by as much as 65% - 85%.


------------------------------------------------------------------------------

Title:
On the Pulse Shaping for Delay-Doppler Communications

Abstract: In this paper, we study the pulse shaping for delay-Doppler (DD)
communications. We start with constructing a basis function in the DD domain
following the properties of the Zak transform. Particularly, we show that the
constructed basis functions are globally quasi-periodic while locally
twisted-shifted, and their significance in time and frequency domains are then
revealed. We further analyze the ambiguity function of the basis function, and
show that fully localized ambiguity function can be achieved by constructing
the basis function using periodic signals. More importantly, we prove that time
and frequency truncating such basis functions naturally leads to delay and
Doppler orthogonalities, if the truncating windows are orthogonal or periodic.
Motivated by this, we propose a DD Nyquist pulse shaping scheme considering
signals with periodicity. Finally, our conclusions are verified by using
various orthogonal and periodic pulses.


------------------------------------------------------------------------------

Title:
WavPool: A New Block for Deep Neural Networks

Abstract: Modern deep neural networks comprise many operational layers, such as dense
or convolutional layers, which are often collected into blocks. In this work,
we introduce a new, wavelet-transform-based network architecture that we call
the multi-resolution perceptron: by adding a pooling layer, we create a new
network block, the WavPool. The first step of the multi-resolution perceptron
is transforming the data into its multi-resolution decomposition form by
convolving the input data with filters of fixed coefficients but increasing
size. Following image processing techniques, we are able to make scale and
spatial information simultaneously accessible to the network without increasing
the size of the data vector. WavPool outperforms a similar multilayer
perceptron while using fewer parameters, and outperforms a comparable
convolutional neural network by ~ 10% on relative accuracy on CIFAR-10.


------------------------------------------------------------------------------

Title:
Towards trustworthy seizure onset detection using workflow notes

Abstract: A major barrier to deploying healthcare AI models is their trustworthiness.
One form of trustworthiness is a model's robustness across different subgroups:
while existing models may exhibit expert-level performance on aggregate
metrics, they often rely on non-causal features, leading to errors in hidden
subgroups. To take a step closer towards trustworthy seizure onset detection
from EEG, we propose to leverage annotations that are produced by healthcare
personnel in routine clinical workflows -- which we refer to as workflow notes
-- that include multiple event descriptions beyond seizures. Using workflow
notes, we first show that by scaling training data to an unprecedented level of
68,920 EEG hours, seizure onset detection performance significantly improves
(+12.3 AUROC points) compared to relying on smaller training sets with
expensive manual gold-standard labels. Second, we reveal that our binary
seizure onset detection model underperforms on clinically relevant subgroups
(e.g., up to a margin of 6.5 AUROC points between pediatrics and adults), while
having significantly higher false positives on EEG clips showing
non-epileptiform abnormalities compared to any EEG clip (+19 FPR points). To
improve model robustness to hidden subgroups, we train a multilabel model that
classifies 26 attributes other than seizures, such as spikes, slowing, and
movement artifacts. We find that our multilabel model significantly improves
overall seizure onset detection performance (+5.9 AUROC points) while greatly
improving performance among subgroups (up to +8.3 AUROC points), and decreases
false positives on non-epileptiform abnormalities by 8 FPR points. Finally, we
propose a clinical utility metric based on false positives per 24 EEG hours and
find that our multilabel model improves this clinical utility metric by a
factor of 2x across different clinical settings.


------------------------------------------------------------------------------

Title:
Multi-Object Manipulation via Object-Centric Neural Scattering Functions

Abstract: Learned visual dynamics models have proven effective for robotic manipulation
tasks. Yet, it remains unclear how best to represent scenes involving
multi-object interactions. Current methods decompose a scene into discrete
objects, but they struggle with precise modeling and manipulation amid
challenging lighting conditions as they only encode appearance tied with
specific illuminations. In this work, we propose using object-centric neural
scattering functions (OSFs) as object representations in a model-predictive
control framework. OSFs model per-object light transport, enabling
compositional scene re-rendering under object rearrangement and varying
lighting conditions. By combining this approach with inverse parameter
estimation and graph-based neural dynamics models, we demonstrate improved
model-predictive control performance and generalization in compositional
multi-object environments, even in previously unseen scenarios and harsh
lighting conditions.


------------------------------------------------------------------------------

Title:
Efficient Token-Guided Image-Text Retrieval with Consistent Multimodal  Contrastive Training

Abstract: Image-text retrieval is a central problem for understanding the semantic
relationship between vision and language, and serves as the basis for various
visual and language tasks. Most previous works either simply learn
coarse-grained representations of the overall image and text, or elaborately
establish the correspondence between image regions or pixels and text words.
However, the close relations between coarse- and fine-grained representations
for each modality are important for image-text retrieval but almost neglected.
As a result, such previous works inevitably suffer from low retrieval accuracy
or heavy computational cost. In this work, we address image-text retrieval from
a novel perspective by combining coarse- and fine-grained representation
learning into a unified framework. This framework is consistent with human
cognition, as humans simultaneously pay attention to the entire sample and
regional elements to understand the semantic content. To this end, a
Token-Guided Dual Transformer (TGDT) architecture which consists of two
homogeneous branches for image and text modalities, respectively, is proposed
for image-text retrieval. The TGDT incorporates both coarse- and fine-grained
retrievals into a unified framework and beneficially leverages the advantages
of both retrieval approaches. A novel training objective called Consistent
Multimodal Contrastive (CMC) loss is proposed accordingly to ensure the intra-
and inter-modal semantic consistencies between images and texts in the common
embedding space. Equipped with a two-stage inference method based on the mixed
global and local cross-modal similarity, the proposed method achieves
state-of-the-art retrieval performances with extremely low inference time when
compared with representative recent approaches.


------------------------------------------------------------------------------

Title:
A Two-Player Resource-Sharing Game with Asymmetric Information

Abstract: This paper considers a two-player game where each player chooses a resource
from a finite collection of options without knowing the opponent's choice in
the absence of any form of feedback. Each resource brings a random reward. Both
players have statistical information regarding the rewards of each resource.
Additionally, there exists an information asymmetry where each player has
knowledge of the reward realizations of different subsets of the resources. If
both players choose the same resource, the reward is divided equally between
them, whereas if they choose different resources, each player gains the full
reward of the resource. We first implement the iterative best response
algorithm to find an $\epsilon$-approximate Nash equilibrium for this game.
This method of finding a Nash equilibrium is impractical when players do not
trust each other and place no assumptions on the incentives of the opponent. To
handle this case, we solve the problem of maximizing the worst-case expected
utility of the first player. The solution leads to counter-intuitive insights
in certain special cases. To solve the general version of the problem, we
develop an efficient algorithmic solution that combines online-convex
optimization and the drift-plus penalty technique.


------------------------------------------------------------------------------

Title:
Graph Convolution Based Efficient Re-Ranking for Visual Retrieval

Abstract: Visual retrieval tasks such as image retrieval and person re-identification
(Re-ID) aim at effectively and thoroughly searching images with similar content
or the same identity. After obtaining retrieved examples, re-ranking is a
widely adopted post-processing step to reorder and improve the initial
retrieval results by making use of the contextual information from semantically
neighboring samples. Prevailing re-ranking approaches update distance metrics
and mostly rely on inefficient crosscheck set comparison operations while
computing expanded neighbors based distances. In this work, we present an
efficient re-ranking method which refines initial retrieval results by updating
features. Specifically, we reformulate re-ranking based on Graph Convolution
Networks (GCN) and propose a novel Graph Convolution based Re-ranking (GCR) for
visual retrieval tasks via feature propagation. To accelerate computation for
large-scale retrieval, a decentralized and synchronous feature propagation
algorithm which supports parallel or distributed computing is introduced. In
particular, the plain GCR is extended for cross-camera retrieval and an
improved feature propagation formulation is presented to leverage affinity
relationships across different cameras. It is also extended for video-based
retrieval, and Graph Convolution based Re-ranking for Video (GCRV) is proposed
by mathematically deriving a novel profile vector generation method for the
tracklet. Without bells and whistles, the proposed approaches achieve
state-of-the-art performances on seven benchmark datasets from three different
tasks, i.e., image retrieval, person Re-ID and video-based person Re-ID.


------------------------------------------------------------------------------

Title:
Explaining Explainability: Towards Deeper Actionable Insights into Deep  Learning through Second-order Explainability

Abstract: Explainability plays a crucial role in providing a more comprehensive
understanding of deep learning models' behaviour. This allows for thorough
validation of the model's performance, ensuring that its decisions are based on
relevant visual indicators and not biased toward irrelevant patterns existing
in training data. However, existing methods provide only instance-level
explainability, which requires manual analysis of each sample. Such manual
review is time-consuming and prone to human biases. To address this issue, the
concept of second-order explainable AI (SOXAI) was recently proposed to extend
explainable AI (XAI) from the instance level to the dataset level. SOXAI
automates the analysis of the connections between quantitative explanations and
dataset biases by identifying prevalent concepts. In this work, we explore the
use of this higher-level interpretation of a deep neural network's behaviour to
allows us to "explain the explainability" for actionable insights.
Specifically, we demonstrate for the first time, via example classification and
segmentation cases, that eliminating irrelevant concepts from the training set
based on actionable insights from SOXAI can enhance a model's performance.


------------------------------------------------------------------------------

Title:
Improving Selective Visual Question Answering by Learning from Your  Peers

Abstract: Despite advances in Visual Question Answering (VQA), the ability of models to
assess their own correctness remains underexplored. Recent work has shown that
VQA models, out-of-the-box, can have difficulties abstaining from answering
when they are wrong. The option to abstain, also called Selective Prediction,
is highly relevant when deploying systems to users who must trust the system's
output (e.g., VQA assistants for users with visual impairments). For such
scenarios, abstention can be especially important as users may provide
out-of-distribution (OOD) or adversarial inputs that make incorrect answers
more likely. In this work, we explore Selective VQA in both in-distribution
(ID) and OOD scenarios, where models are presented with mixtures of ID and OOD
data. The goal is to maximize the number of questions answered while minimizing
the risk of error on those questions. We propose a simple yet effective
Learning from Your Peers (LYP) approach for training multimodal selection
functions for making abstention decisions. Our approach uses predictions from
models trained on distinct subsets of the training data as targets for
optimizing a Selective VQA model. It does not require additional manual labels
or held-out data and provides a signal for identifying examples that are
easy/difficult to generalize to. In our extensive evaluations, we show this
benefits a number of models across different architectures and scales. Overall,
for ID, we reach 32.92% in the selective prediction metric coverage at 1% risk
of error (C@1%) which doubles the previous best coverage of 15.79% on this
task. For mixed ID/OOD, using models' softmax confidences for abstention
decisions performs very poorly, answering <5% of questions at 1% risk of error
even when faced with only 10% OOD examples, but a learned selection function
with LYP can increase that to 25.38% C@1%.


------------------------------------------------------------------------------

Title:
Resource Allocation and Performance Analysis of Hybrid RSMA-NOMA in the  Downlink

Abstract: Rate splitting multiple access (RSMA) and non-orthogonal multiple access
(NOMA) are the key enabling multiple access techniques to enable massive
connectivity. However, it is unclear whether RSMA would consistently outperform
NOMA from a system sum-rate perspective, users' fairness, as well as
convergence and feasibility of the resource allocation solutions. This paper
investigates the weighted sum-rate maximization problem to optimize power and
rate allocations in a hybrid RSMA-NOMA network. In the hybrid RSMA-NOMA, by
optimally allocating the maximum power budget to each scheme, the BS operates
on NOMA and RSMA in two orthogonal channels, allowing users to simultaneously
receive signals on both RSMA and NOMA. Based on the successive convex
approximation (SCA) approach, we jointly optimize the power allocation of users
in NOMA and RSMA, the rate allocation of users in RSMA, and the power budget
allocation for NOMA and RSMA considering successive interference cancellation
(SIC) constraints. Numerical results demonstrate the trade-offs that hybrid
RSMA-NOMA access offers in terms of system sum rate, fairness, convergence, and
feasibility of the solutions.


------------------------------------------------------------------------------

Title:
InfoDiffusion: Representation Learning Using Information Maximizing  Diffusion Models

Abstract: While diffusion models excel at generating high-quality samples, their latent
variables typically lack semantic meaning and are not suitable for
representation learning. Here, we propose InfoDiffusion, an algorithm that
augments diffusion models with low-dimensional latent variables that capture
high-level factors of variation in the data. InfoDiffusion relies on a learning
objective regularized with the mutual information between observed and hidden
variables, which improves latent space quality and prevents the latents from
being ignored by expressive diffusion-based decoders. Empirically, we find that
InfoDiffusion learns disentangled and human-interpretable latent
representations that are competitive with state-of-the-art generative and
contrastive methods, while retaining the high sample quality of diffusion
models. Our method enables manipulating the attributes of generated images and
has the potential to assist tasks that require exploring a learned latent space
to generate quality samples, e.g., generative design.


------------------------------------------------------------------------------

Title:
A Fluid-Solid-Growth Solver for Cardiovascular Modeling

Abstract: We implement full, three-dimensional constrained mixture theory for vascular
growth and remodeling into a finite element fluid-structure interaction (FSI)
solver. The resulting "fluid-solid-growth" (FSG) solver allows long term,
patient-specific predictions of changing hemodynamics, vessel wall morphology,
tissue composition, and material properties. This extension from short term
(FSI) to long term (FSG) simulations increases clinical relevance by enabling
mechanobioloigcally-dependent studies of disease progression in complex
domains.


------------------------------------------------------------------------------

Title:
Online Learning for Obstacle Avoidance

Abstract: We approach the fundamental problem of obstacle avoidance for robotic systems
via the lens of online learning. In contrast to prior work that either assumes
worst-case realizations of uncertainty in the environment or a stationary
stochastic model of uncertainty, we propose a method that is efficient to
implement and provably grants instance-optimality with respect to perturbations
of trajectories generated from an open-loop planner (in the sense of minimizing
worst-case regret). The resulting policy adapts online to realizations of
uncertainty and provably compares well with the best obstacle avoidance policy
in hindsight from a rich class of policies. The method is validated in
simulation on a dynamical system environment and compared to baseline open-loop
planning and robust Hamilton- Jacobi reachability techniques. Further, it is
implemented on a hardware example where a quadruped robot traverses a dense
obstacle field and encounters input disturbances due to time delays, model
uncertainty, and dynamics nonlinearities.


------------------------------------------------------------------------------

Title:
Utilizing Longitudinal Chest X-Rays and Reports to Pre-Fill Radiology  Reports

Abstract: Despite the reduction in turn-around times in radiology reports with the use
of speech recognition software, persistent communication errors can
significantly impact the interpretation of the radiology report. Pre-filling a
radiology report holds promise in mitigating reporting errors, and despite
efforts in the literature to generate medical reports, there exists a lack of
approaches that exploit the longitudinal nature of patient visit records in the
MIMIC-CXR dataset. To address this gap, we propose to use longitudinal
multi-modal data, i.e., previous patient visit CXR, current visit CXR, and
previous visit report, to pre-fill the 'findings' section of a current patient
visit report. We first gathered the longitudinal visit information for 26,625
patients from the MIMIC-CXR dataset and created a new dataset called
Longitudinal-MIMIC. With this new dataset, a transformer-based model was
trained to capture the information from longitudinal patient visit records
containing multi-modal data (CXR images + reports) via a cross-attention-based
multi-modal fusion module and a hierarchical memory-driven decoder. In contrast
to previous work that only uses current visit data as input to train a model,
our work exploits the longitudinal information available to pre-fill the
'findings' section of radiology reports. Experiments show that our approach
outperforms several recent approaches by >=3% on F1 score, and >=2% for BLEU-4,
METEOR and ROUGE-L respectively. The dataset and code will be made publicly
available.


------------------------------------------------------------------------------

Title:
Self-Supervised Polyp Re-Identification in Colonoscopy

Abstract: Computer-aided polyp detection (CADe) is becoming a standard, integral part
of any modern colonoscopy system. A typical colonoscopy CADe detects a polyp in
a single frame and does not track it through the video sequence. Yet, many
downstream tasks including polyp characterization (CADx), quality metrics,
automatic reporting, require aggregating polyp data from multiple frames. In
this work we propose a robust long term polyp tracking method based on
re-identification by visual appearance. Our solution uses an attention-based
self-supervised ML model, specifically designed to leverage the temporal nature
of video input. We quantitatively evaluate method's performance and demonstrate
its value for the CADx task.


------------------------------------------------------------------------------

Title:
Integrating machine learning paradigms and mixed-integer model  predictive control for irrigation scheduling

Abstract: The agricultural sector currently faces significant challenges in water
resource conservation and crop yield optimization, primarily due to concerns
over freshwater scarcity. Traditional irrigation scheduling methods often prove
inadequate in meeting the needs of large-scale irrigation systems. To address
this issue, this paper proposes a predictive irrigation scheduler that
leverages the three paradigms of machine learning to optimize irrigation
schedules. The proposed scheduler employs the k-means clustering approach to
divide the field into distinct irrigation management zones based on soil
hydraulic parameters and topology information. Furthermore, a long short-term
memory network is employed to develop dynamic models for each management zone,
enabling accurate predictions of soil moisture dynamics. Formulated as a
mixed-integer model predictive control problem, the scheduler aims to maximize
water uptake while minimizing overall water consumption and irrigation costs.
To tackle the mixed-integer optimization challenge, the proximal policy
optimization algorithm is utilized to train a reinforcement learning agent
responsible for making daily irrigation decisions. To evaluate the performance
of the proposed scheduler, a 26.4-hectare field in Lethbridge, Canada, was
chosen as a case study for the 2015 and 2022 growing seasons. The results
demonstrate the superiority of the proposed scheduler compared to a traditional
irrigation scheduling method in terms of water use efficiency and crop yield
improvement for both growing seasons. Notably, the proposed scheduler achieved
water savings ranging from 6.4% to 22.8%, along with yield increases ranging
from 2.3% to 4.3%.


------------------------------------------------------------------------------

Title:
Convergence properties of gradient methods for blind ptychography

Abstract: We consider blind ptychography, an imaging technique which aims to
reconstruct an object of interest from a set of its diffraction patterns, each
obtained by a local illumination. As the distribution of the light within the
illuminated region, called the window, is unknown, it also has to be estimated
as well. For the recovery, we consider gradient and stochastic gradient descent
methods for the minimization of amplitude-base squared loss. In particular,
this includes extended Ptychographic Iterative Engine as a special case of
stochastic gradient descent. We show that all methods converge to a critical
point at a sublinear rate with a proper choice of step sizes. We also discuss
possibilities for larger step sizes.


------------------------------------------------------------------------------

Title:
A Client-server Deep Federated Learning for Cross-domain Surgical Image  Segmentation

Abstract: This paper presents a solution to the cross-domain adaptation problem for 2D
surgical image segmentation, explicitly considering the privacy protection of
distributed datasets belonging to different centers. Deep learning
architectures in medical image analysis necessitate extensive training data for
better generalization. However, obtaining sufficient diagnostic and surgical
data is still challenging, mainly due to the inherent cost of data curation and
the need of experts for data annotation. Moreover, increased privacy and legal
compliance concerns can make data sharing across clinical sites or regions
difficult. Another ubiquitous challenge the medical datasets face is inevitable
domain shifts among the collected data at the different centers. To this end,
we propose a Client-server deep federated architecture for cross-domain
adaptation. A server hosts a set of immutable parameters common to both the
source and target domains. The clients consist of the respective
domain-specific parameters and make requests to the server while learning their
parameters and inferencing. We evaluate our framework in two benchmark
datasets, demonstrating applicability in computer-assisted interventions for
endoscopic polyp segmentation and diagnostic skin lesion detection and
analysis. Our extensive quantitative and qualitative experiments demonstrate
the superiority of the proposed method compared to competitive baseline and
state-of-the-art methods. Codes are available at:
this https URL


------------------------------------------------------------------------------

Title:
What can a cook in Italy teach a mechanic in India? Action Recognition  Generalisation Over Scenarios and Locations

Abstract: We propose and address a new generalisation problem: can a model trained for
action recognition successfully classify actions when they are performed within
a previously unseen scenario and in a previously unseen location? To answer
this question, we introduce the Action Recognition Generalisation Over
scenarios and locations dataset (ARGO1M), which contains 1.1M video clips from
the large-scale Ego4D dataset, across 10 scenarios and 13 locations. We
demonstrate recognition models struggle to generalise over 10 proposed test
splits, each of an unseen scenario in an unseen location. We thus propose CIR,
a method to represent each video as a Cross-Instance Reconstruction of videos
from other domains. Reconstructions are paired with text narrations to guide
the learning of a domain generalisable representation. We provide extensive
analysis and ablations on ARGO1M that show CIR outperforms prior domain
generalisation works on all test splits. Code and data:
this https URL


------------------------------------------------------------------------------

Title:
Explainable Software Defect Prediction from Cross Company Project  Metrics Using Machine Learning

Abstract: Predicting the number of defects in a project is critical for project test
managers to allocate budget, resources, and schedule for testing, support and
maintenance efforts. Software Defect Prediction models predict the number of
defects in given projects after training the model with historical defect
related information. The majority of defect prediction studies focused on
predicting defect-prone modules from methods, and class-level static
information, whereas this study predicts defects from project-level information
based on a cross-company project dataset. This study utilizes software sizing
metrics, effort metrics, and defect density information, and focuses on
developing defect prediction models that apply various machine learning
algorithms. One notable issue in existing defect prediction studies is the lack
of transparency in the developed models. Consequently, the explain-ability of
the developed model has been demonstrated using the state-of-the-art post-hoc
model-agnostic method called Shapley Additive exPlanations (SHAP). Finally,
important features for predicting defects from cross-company project
information were identified.


------------------------------------------------------------------------------

Title:
VIBR: Learning View-Invariant Value Functions for Robust Visual Control

Abstract: End-to-end reinforcement learning on images showed significant progress in
the recent years. Data-based approach leverage data augmentation and domain
randomization while representation learning methods use auxiliary losses to
learn task-relevant features. Yet, reinforcement still struggles in visually
diverse environments full of distractions and spurious noise. In this work, we
tackle the problem of robust visual control at its core and present VIBR
(View-Invariant Bellman Residuals), a method that combines multi-view training
and invariant prediction to reduce out-of-distribution (OOD) generalization gap
for RL based visuomotor control. Our model-free approach improve baselines
performances without the need of additional representation learning objectives
and with limited additional computational cost. We show that VIBR outperforms
existing methods on complex visuo-motor control environment with high visual
perturbation. Our approach achieves state-of the-art results on the Distracting
Control Suite benchmark, a challenging benchmark still not solved by current
methods, where we evaluate the robustness to a number of visual perturbators,
as well as OOD generalization and extrapolation capabilities.


------------------------------------------------------------------------------

Title:
Privacy-Preserving Password Cracking: How a Third Party Can Crack Our  Password Hash Without Learning the Hash Value or the Cleartext

Abstract: Using the computational resources of an untrusted third party to crack a
password hash can pose a high number of privacy and security risks. The act of
revealing the hash digest could in itself negatively impact both the data
subject who created the password, and the data controller who stores the hash
digest. This paper solves this currently open problem by presenting a
Privacy-Preserving Password Cracking protocol (3PC), that prevents the third
party cracking server from learning any useful information about the hash
digest, or the recovered cleartext. This is achieved by a tailored anonymity
set of decoy hashes, based on the concept of predicate encryption, where we
extend the definition of a predicate function, to evaluate the output of a one
way hash function. The protocol allows the client to maintain plausible
deniability where the real choice of hash digest cannot be proved, even by the
client itself. The probabilistic information the server obtains during the
cracking process can be calculated and minimized to a desired level. While in
theory cracking a larger set of hashes would decrease computational speed, the
3PC protocol provides constant-time lookup on an arbitrary list size, bounded
by the input/output operation per second (IOPS) capabilities of the third party
server, thereby allowing the protocol to scale efficiently. We demonstrate
these claims both theoretically and in practice, with a real-life use case
implemented on an FPGA architecture.


------------------------------------------------------------------------------

Title:
World-to-Words: Grounded Open Vocabulary Acquisition through Fast  Mapping in Vision-Language Models

Abstract: The ability to connect language units to their referents in the physical
world, referred to as grounding, is crucial to learning and understanding
grounded meanings of words. While humans demonstrate fast mapping in new word
learning, it remains unclear whether modern vision-language models can truly
represent language with their grounded meanings and how grounding may further
bootstrap new word learning. To this end, we introduce Grounded Open Vocabulary
Acquisition (GOVA) to examine grounding and bootstrapping in open-world
language learning. As an initial attempt, we propose object-oriented BERT
(OctoBERT), a novel visually-grounded language model by pre-training on
image-text pairs highlighting grounding as an objective. Through extensive
experiments and analysis, we demonstrate that OctoBERT is a more coherent and
fast grounded word learner, and that the grounding ability acquired during
pre-training helps the model to learn unseen words more rapidly and robustly.
Our code is available at this https URL


------------------------------------------------------------------------------

Title:
A statistical approach for finding property-access errors

Abstract: We study the problem of finding incorrect property accesses in JavaScript
where objects do not have a fixed layout, and properties (including methods)
can be added, overwritten, and deleted freely throughout the lifetime of an
object. Since referencing a non-existent property is not an error in
JavaScript, accidental accesses to non-existent properties (caused, perhaps, by
a typo or by a misunderstanding of API documentation) can go undetected without
thorough testing, and may manifest far from the source of the problem. We
propose a two-phase approach for detecting property access errors based on the
observation that, in practice, most property accesses will be correct. First a
large number of property access patterns is collected from an extensive corpus
of real-world JavaScript code, and a statistical analysis is performed to
identify anomalous usage patterns. Specific instances of these patterns may not
be bugs (due, e.g., dynamic type checks), so a local data-flow analysis filters
out instances of anomalous property accesses that are safe and leaves only
those likely to be actual bugs. We experimentally validate our approach,
showing that on a set of 100 concrete instances of anomalous property accesses,
the approach achieves a precision of 82% with a recall of 90%, making it
suitable for practical use. We also conducted an experiment to determine how
effective the popular VSCode code completion feature is at suggesting object
properties, and found that, while it never suggested an incorrect property
(precision of 100%), it failed to suggest the correct property in 62 out of 80
cases (recall of 22.5%). This shows that developers cannot rely on VSCode's
code completion alone to ensure that all property accesses are valid.


------------------------------------------------------------------------------

Title:
Contraction Rate Estimates of Stochastic Gradient Kinetic Langevin  Integrators

Abstract: In previous work, we introduced a method for determining convergence rates
for integration methods for the kinetic Langevin equation for
$M$-$\nabla$Lipschitz $m$-log-concave densities [arXiv:2302.10684, 2023]. In
this article, we exploit this method to treat several additional schemes
including the method of Brunger, Brooks and Karplus (BBK) and stochastic
position/velocity Verlet. We introduce a randomized midpoint scheme for kinetic
Langevin dynamics, inspired by the recent scheme of Bou-Rabee and Marsden
[arXiv:2211.11003, 2022]. We also extend our approach to stochastic gradient
variants of these schemes under minimal extra assumptions. We provide
convergence rates of $\mathcal{O}(m/M)$, with explicit stepsize restriction,
which are of the same order as the stability thresholds for Gaussian targets
and are valid for a large interval of the friction parameter. We compare the
contraction rate estimates of many kinetic Langevin integrators from molecular
dynamics and machine learning. Finally we present numerical experiments for a
Bayesian logistic regression example.


------------------------------------------------------------------------------

Title:
Babel-ImageNet: Massively Multilingual Evaluation of Vision-and-Language  Representations

Abstract: Vision-and-language (VL) models with separate encoders for each modality
(e.g., CLIP) have become the go-to models for zero-shot image classification
and image-text retrieval. The bulk of the evaluation of these models is,
however, performed with English text only: the costly creation of
language-specific image-caption datasets has limited multilingual VL benchmarks
to a handful of high-resource languages. In this work, we introduce
Babel-ImageNet, a massively multilingual benchmark that offers (partial)
translations of 1000 ImageNet labels to 92 languages, built without resorting
to machine translation (MT) or requiring manual annotation. We instead
automatically obtain reliable translations of ImageNext concepts by linking
them -- via shared WordNet synsets -- to BabelNet, a massively multilingual
lexico-semantic network. We evaluate 8 different publicly available
multilingual CLIP models on zero-shot image classification (ZS-IC) for each of
the 92 Babel-ImageNet languages, demonstrating a significant gap between
English ImageNet performance and that of high-resource languages (e.g., German
or Chinese), and an even bigger gap for low-resource languages (e.g., Sinhala
or Lao). Crucially, we show that the models' ZS-IC performance on
Babel-ImageNet highly correlates with their performance in image-text
retrieval, validating that Babel-ImageNet is suitable for estimating the
quality of the multilingual VL representation spaces for the vast majority of
languages that lack gold image-text data. Finally, we show that the performance
of multilingual CLIP for low-resource languages can be drastically improved via
cheap, parameter-efficient language-specific training. We make our code and
data publicly available: \url{this https URL}


------------------------------------------------------------------------------

Title:
Combining piano performance dimensions for score difficulty  classification

Abstract: Predicting the difficulty of playing a musical score is essential for
structuring and exploring score collections. Despite its importance for music
education, the automatic difficulty classification of piano scores is not yet
solved, mainly due to the lack of annotated data and the subjectiveness of the
annotations. This paper aims to advance the state-of-the-art in score
difficulty classification with two major contributions. To address the lack of
data, we present Can I Play It? (CIPI) dataset, a machine-readable piano score
dataset with difficulty annotations obtained from the renowned classical music
publisher Henle Verlag. The dataset is created by matching public domain scores
with difficulty labels from Henle Verlag, then reviewed and corrected by an
expert pianist. As a second contribution, we explore various input
representations from score information to pre-trained ML models for piano
fingering and expressiveness inspired by the musicology definition of
performance. We show that combining the outputs of multiple classifiers
performs better than the classifiers on their own, pointing to the fact that
the representations capture different aspects of difficulty. In addition, we
conduct numerous experiments that lay a foundation for score difficulty
classification and create a basis for future research. Our best-performing
model reports a 39.47% balanced accuracy and 1.13 median square error across
the nine difficulty levels proposed in this study. Code, dataset, and models
are made available for reproducibility.


------------------------------------------------------------------------------

Title:
On the Restricted $k$-Steiner Tree Problem

Abstract: Given a set $P$ of $n$ points in $\mathbb{R}^2$ and an input line $\gamma$ in
$\mathbb{R}^2$, we present an algorithm that runs in optimal $\Theta(n\log n)$
time and $\Theta(n)$ space to solve a restricted version of the $1$-Steiner
tree problem. Our algorithm returns a minimum-weight tree interconnecting $P$
using at most one Steiner point $s \in \gamma$, where edges are weighted by the
Euclidean distance between their endpoints. We then extend the result to $j$
input lines. Following this, we show how the algorithm of Brazil et al.
("Generalised k-Steiner Tree Problems in Normed Planes", arXiv:1111.1464) that
solves the $k$-Steiner tree problem in $\mathbb{R}^2$ in $O(n^{2k})$ time can
be adapted to our setting. For $k>1$, restricting the (at most) $k$ Steiner
points to lie on an input line, the runtime becomes $O(n^{k})$. Next we show
how the results of Brazil et al. ("Generalised k-Steiner Tree Problems in
Normed Planes", arXiv:1111.1464) allow us to maintain the same time and space
bounds while extending to some non-Euclidean norms and different tree cost
functions. Lastly, we extend the result to $j$ input curves.


------------------------------------------------------------------------------

Title:
Coordinating Distributed Energy Resources for Reliability can  Significantly Reduce Future Distribution Grid Upgrades and Peak Load

Abstract: Current DER coordination schemes, such as demand-response and VPPs, aim to
reduce electricity costs during peak demand events with no consideration of
distribution grid reliability. We show that coordinating DERs for grid
reliability can significantly reduce both the infrastructure upgrades needed to
support future increases in DER and electrification penetrations and peak load.
Specifically, using a power flow driven simulation-optimization methodology, we
compare the potential reliability improvements with a perfect-foresight
centralized DER controller that minimizes reliability violations to a local
controller that minimizes consumer electricity cost. We find, for example, that
by 2050 with local control, on average 81% of the transformers in a
distribution grid experience violations, compared to 28% with centralized
control, which simultaneously reduces peak load by 17%. These reductions are
achieved with only 5.1% increase in electricity cost. These findings suggest
that future incentives for DER adoption should include reliability
coordination.


------------------------------------------------------------------------------

Title:
Augment then Smooth: Reconciling Differential Privacy with Certified  Robustness

Abstract: Machine learning models are susceptible to a variety of attacks that can
erode trust in their deployment. These threats include attacks against the
privacy of training data and adversarial examples that jeopardize model
accuracy. Differential privacy and randomized smoothing are effective defenses
that provide certifiable guarantees for each of these threats, however, it is
not well understood how implementing either defense impacts the other. In this
work, we argue that it is possible to achieve both privacy guarantees and
certified robustness simultaneously. We provide a framework called DP-CERT for
integrating certified robustness through randomized smoothing into
differentially private model training. For instance, compared to differentially
private stochastic gradient descent on CIFAR10, DP-CERT leads to a 12-fold
increase in certified accuracy and a 10-fold increase in the average certified
radius at the expense of a drop in accuracy of 1.2%. Through in-depth
per-sample metric analysis, we show that the certified radius correlates with
the local Lipschitz constant and smoothness of the loss surface. This provides
a new way to diagnose when private models will fail to be robust.


------------------------------------------------------------------------------

Title:
Learning Cross-lingual Mappings for Data Augmentation to Improve  Low-Resource Speech Recognition

Abstract: Exploiting cross-lingual resources is an effective way to compensate for data
scarcity of low resource languages. Recently, a novel multilingual model fusion
technique has been proposed where a model is trained to learn cross-lingual
acoustic-phonetic similarities as a mapping function. However, handcrafted
lexicons have been used to train hybrid DNN-HMM ASR systems. To remove this
dependency, we extend the concept of learnable cross-lingual mappings for
end-to-end speech recognition. Furthermore, mapping models are employed to
transliterate the source languages to the target language without using
parallel data. Finally, the source audio and its transliteration is used for
data augmentation to retrain the target language ASR. The results show that any
source language ASR model can be used for a low-resource target language
recognition followed by proposed mapping model. Furthermore, data augmentation
results in a relative gain up to 5% over baseline monolingual model.


------------------------------------------------------------------------------

Title:
SQL2Circuits: Estimating Metrics for SQL Queries with A Quantum Natural  Language Processing Method

Abstract: Quantum computing has developed significantly in recent years. Developing
algorithms to estimate various metrics for SQL queries has been an important
research question in database research since the estimations affect query
optimization and database performance. This work represents a quantum natural
language processing (QNLP) -inspired approach for constructing a quantum
machine learning model which can classify SQL queries with respect to their
execution times and cardinalities. From the quantum machine learning
perspective, we compare our model and results to the previous research in QNLP
and conclude that our model reaches similar accuracy as the QNLP model in the
classification tasks. This indicates that the QNLP model is a promising method
even when applied to problems that are not in QNLP. We study the developed
quantum machine learning model by calculating its expressibility and entangling
capability histograms. The results show that the model has favorable properties
to be expressible but also not too complex to be executed on quantum hardware.


------------------------------------------------------------------------------

Title:
LoSh: Long-Short Text Joint Prediction Network for Referring Video  Object Segmentation

Abstract: Referring video object segmentation (RVOS) aims to segment the target
instance referred by a given text expression in a video clip. The text
expression normally contains sophisticated descriptions of the instance's
appearance, actions, and relations with others. It is therefore rather
difficult for an RVOS model to capture all these attributes correspondingly in
the video; in fact, the model often favours more on the action- and
relation-related visual attribute of the instance. This can end up with
incomplete or even incorrect mask prediction of the target instance. In this
paper, we tackle this problem by taking a subject-centric short text expression
from the original long text expression. The short one retains only the
appearance-related information of the target instance so that we can use it to
focus the model's attention on the instance's appearance. We let the model make
joint predictions using both long and short text expressions and introduce a
long-short predictions intersection loss to align the joint predictions.
Besides the improvement on the linguistic part, we also introduce a
forward-backward visual consistency loss, which utilizes optical flows to warp
visual features between the annotated frames and their temporal neighbors for
consistency. We build our method on top of two state of the art
transformer-based pipelines for end-to-end training. Extensive experiments on
A2D-Sentences and JHMDB-Sentences datasets show impressive improvements of our
method.


------------------------------------------------------------------------------

Title:
VidEdit: Zero-Shot and Spatially Aware Text-Driven Video Editing

Abstract: Recently, diffusion-based generative models have achieved remarkable success
for image generation and edition. However, their use for video editing still
faces important limitations. This paper introduces VidEdit, a novel method for
zero-shot text-based video editing ensuring strong temporal and spatial
consistency. Firstly, we propose to combine atlas-based and pre-trained
text-to-image diffusion models to provide a training-free and efficient editing
method, which by design fulfills temporal smoothness. Secondly, we leverage
off-the-shelf panoptic segmenters along with edge detectors and adapt their use
for conditioned diffusion-based atlas editing. This ensures a fine spatial
control on targeted regions while strictly preserving the structure of the
original video. Quantitative and qualitative experiments show that VidEdit
outperforms state-of-the-art methods on DAVIS dataset, regarding semantic
faithfulness, image preservation, and temporal consistency metrics. With this
framework, processing a single video only takes approximately one minute, and
it can generate multiple compatible edits based on a unique text prompt.
Project web-page at this https URL


------------------------------------------------------------------------------

Title:
PLAN: Variance-Aware Private Mean Estimation

Abstract: Differentially private mean estimation is an important building block in
privacy-preserving algorithms for data analysis and machine learning. Though
the trade-off between privacy and utility is well understood in the worst case,
many datasets exhibit structure that could potentially be exploited to yield
better algorithms. In this paper we present $\textit{Private Limit Adapted
Noise}$ (PLAN), a family of differentially private algorithms for mean
estimation in the setting where inputs are independently sampled from a
distribution $\mathcal{D}$ over $\mathbf{R}^d$, with coordinate-wise standard
deviations $\boldsymbol{\sigma} \in \mathbf{R}^d$. Similar to mean estimation
under Mahalanobis distance, PLAN tailors the shape of the noise to the shape of
the data, but unlike previous algorithms the privacy budget is spent
non-uniformly over the coordinates. Under a concentration assumption on
$\mathcal{D}$, we show how to exploit skew in the vector $\boldsymbol{\sigma}$,
obtaining a (zero-concentrated) differentially private mean estimate with
$\ell_2$ error proportional to $\|\boldsymbol{\sigma}\|_1$. Previous work has
either not taken $\boldsymbol{\sigma}$ into account, or measured error in
Mahalanobis distance $\unicode{x2013}$ in both cases resulting in $\ell_2$
error proportional to $\sqrt{d}\|\boldsymbol{\sigma}\|_2$, which can be up to a
factor $\sqrt{d}$ larger. To verify the effectiveness of PLAN, we empirically
evaluate accuracy on both synthetic and real world data.


------------------------------------------------------------------------------

Title:
Iterative self-transfer learning: A general methodology for response  time-history prediction based on small dataset

Abstract: There are numerous advantages of deep neural network surrogate modeling for
response time-history prediction. However, due to the high cost of refined
numerical simulations and actual experiments, the lack of data has become an
unavoidable bottleneck in practical applications. An iterative self-transfer
learningmethod for training neural networks based on small datasets is proposed
in this study. A new mapping-based transfer learning network, named as deep
adaptation network with three branches for regression (DAN-TR), is proposed. A
general iterative network training strategy is developed by coupling DAN-TR and
the pseudo-label strategy, and the establishment of corresponding datasets is
also discussed. Finally, a complex component is selected as a case study. The
results show that the proposed method can improve the model performance by near
an order of magnitude on small datasets without the need of external labeled
samples,well behaved pre-trainedmodels, additional artificial labeling, and
complex physical/mathematical analysis.


------------------------------------------------------------------------------

Title:
Fed-ZERO: Efficient Zero-shot Personalization with Federated Mixture of  Experts

Abstract: One of the goals in Federated Learning (FL) is to create personalized models
that can adapt to the context of each participating client, while utilizing
knowledge from a shared global model. Yet, often, personalization requires a
fine-tuning step using clients' labeled data in order to achieve good
performance. This may not be feasible in scenarios where incoming clients are
fresh and/or have privacy concerns. It, then, remains open how one can achieve
zero-shot personalization in these scenarios. We propose a novel solution by
using a Mixture-of-Experts (MoE) framework within a FL setup. Our method
leverages the diversity of the clients to train specialized experts on
different subsets of classes, and a gating function to route the input to the
most relevant expert(s). Our gating function harnesses the knowledge of a
pretrained model common expert to enhance its routing decisions on-the-fly. As
a highlight, our approach can improve accuracy up to 18\% in state of the art
FL settings, while maintaining competitive zero-shot performance. In practice,
our method can handle non-homogeneous data distributions, scale more
efficiently, and improve the state-of-the-art performance on common FL
benchmarks.


------------------------------------------------------------------------------

Title:
Recipes for Sequential Pre-training of Multilingual Encoder and Seq2Seq  Models

Abstract: Pre-trained encoder-only and sequence-to-sequence (seq2seq) models each have
advantages, however training both model types from scratch is computationally
expensive. We explore recipes to improve pre-training efficiency by
initializing one model from the other. (1) Extracting the encoder from a
seq2seq model, we show it under-performs a Masked Language Modeling (MLM)
encoder, particularly on sequence labeling tasks. Variations of masking during
seq2seq training, reducing the decoder size, and continuing with a small amount
of MLM training do not close the gap. (2) Conversely, using an encoder to
warm-start seq2seq training, we show that by unfreezing the encoder partway
through training, we can match task performance of a from-scratch seq2seq
model. Overall, this two-stage approach is an efficient recipe to obtain both a
multilingual encoder and a seq2seq model, matching the performance of training
each model from scratch while reducing the total compute cost by 27%.


------------------------------------------------------------------------------

Title:
Learning to Stabilize High-dimensional Unknown Systems Using  Lyapunov-guided Exploration

Abstract: Designing stabilizing controllers is a fundamental challenge in autonomous
systems, particularly for high-dimensional, nonlinear systems that cannot be
accurately modeled using differential equations. Lyapunov theory offers a
robust solution for stabilizing control systems, but current methods relying on
Lyapunov functions require access to complete model information or samples of
system executions throughout the entire state space. Consequently, these
methods are impractical for high-dimensional systems. In this paper, we
introduce a novel framework, LYGE, for learning stabilizing controllers
specifically tailored to high-dimensional, unknown systems. Our approach
employs Lyapunov theory to iteratively guide the search for samples during
exploration while simultaneously learning the local system dynamics, control
policy, and Lyapunov functions. We provide a theoretical analysis of our
framework and demonstrate its scalability on highly complex systems, including
a high-fidelity F-16 jet aircraft model from the Air Force featuring a
16-dimensional state space and a 4-dimensional input space. Experimental
results indicate that, compared to prior works in reinforcement learning,
imitation learning, and neural certificates, LYGE can reduce the distance to
the goal by approximately $50\%$ while requiring only $5\%$ to $32\%$ of the
samples. Furthermore, we demonstrate that our algorithm can be readily extended
to learn controllers guided by alternative control certificate functions for
unknown systems.


------------------------------------------------------------------------------

Title:
Gauss Newton method for solving variational problems of PDEs with neural  network discretizaitons

Abstract: The numerical solution of differential equations using machine learning-based
approaches has gained significant popularity. Neural network-based
discretization has emerged as a powerful tool for solving differential
equations by parameterizing a set of functions. Various approaches, such as the
deep Ritz method and physics-informed neural networks, have been developed for
numerical solutions. Training algorithms, including gradient descent and greedy
algorithms, have been proposed to solve the resulting optimization problems. In
this paper, we focus on the variational formulation of the problem and propose
a Gauss- Newton method for computing the numerical solution. We provide a
comprehensive analysis of the superlinear convergence properties of this
method, along with a discussion on semi-regular zeros of the vanishing
gradient. Numerical examples are presented to demonstrate the efficiency of the
proposed Gauss-Newton method.


------------------------------------------------------------------------------

Title:
Using Wikipedia Editor Information to Build High-performance Recommender  Systems

Abstract: Wikipedia has high-quality articles on a variety of topics and has been used
in diverse research areas. In this study, a method is presented for using
Wikipedia's editor information to build recommender systems in various domains
that outperform content-based systems.


------------------------------------------------------------------------------

Title:
Heterogeneous Continual Learning

Abstract: We propose a novel framework and a solution to tackle the continual learning
(CL) problem with changing network architectures. Most CL methods focus on
adapting a single architecture to a new task/class by modifying its weights.
However, with rapid progress in architecture design, the problem of adapting
existing solutions to novel architectures becomes relevant. To address this
limitation, we propose Heterogeneous Continual Learning (HCL), where a wide
range of evolving network architectures emerge continually together with novel
data/tasks. As a solution, we build on top of the distillation family of
techniques and modify it to a new setting where a weaker model takes the role
of a teacher; meanwhile, a new stronger architecture acts as a student.
Furthermore, we consider a setup of limited access to previous data and propose
Quick Deep Inversion (QDI) to recover prior task visual features to support
knowledge transfer. QDI significantly reduces computational costs compared to
previous solutions and improves overall performance. In summary, we propose a
new setup for CL with a modified knowledge distillation paradigm and design a
quick data inversion method to enhance distillation. Our evaluation of various
benchmarks shows a significant improvement on accuracy in comparison to
state-of-the-art methods over various networks architectures.


------------------------------------------------------------------------------

Title:
Decentralized Learning Dynamics in the Gossip Model

Abstract: We study a distributed multi-armed bandit setting among a population of $n$
memory-constrained nodes in the gossip model: at each round, every node locally
adopts one of $m$ arms, observes a reward drawn from the arm's (adversarially
chosen) distribution, and then communicates with a randomly sampled neighbor,
exchanging information to determine its policy in the next round. We introduce
and analyze several families of dynamics for this task that are decentralized:
each node's decision is entirely local and depends only on its most recently
obtained reward and that of the neighbor it sampled. We show a connection
between the global evolution of these decentralized dynamics with a certain
class of "zero-sum" multiplicative weight update algorithms, and we develop a
general framework for analyzing the population-level regret of these natural
protocols. Using this framework, we derive sublinear regret bounds under a wide
range of parameter regimes (i.e., the size of the population and number of
arms) for both the stationary reward setting (where the mean of each arm's
distribution is fixed over time) and the adversarial reward setting (where
means can vary over time). Further, we show that these protocols can
approximately optimize convex functions over the simplex when the reward
distributions are generated from a stochastic gradient oracle.


------------------------------------------------------------------------------

Title:
Label Noise Robust Image Representation Learning based on Supervised  Variational Autoencoders in Remote Sensing

Abstract: Due to the publicly available thematic maps and crowd-sourced data, remote
sensing (RS) image annotations can be gathered at zero cost for training deep
neural networks (DNNs). However, such annotation sources may increase the risk
of including noisy labels in training data, leading to inaccurate RS image
representation learning (IRL). To address this issue, in this paper we propose
a label noise robust IRL method that aims to prevent the interference of noisy
labels on IRL, independently from the learning task being considered in RS. To
this end, the proposed method combines a supervised variational autoencoder
(SVAE) with any kind of DNN. This is achieved by defining variational
generative process based on image features. This allows us to define the
importance of each training sample for IRL based on the loss values acquired
from the SVAE and the task head of the considered DNN. Then, the proposed
method imposes lower importance to images with noisy labels, while giving
higher importance to those with correct labels during IRL. Experimental results
show the effectiveness of the proposed method when compared to well-known label
noise robust IRL methods applied to RS images. The code of the proposed method
is publicly available at this https URL


------------------------------------------------------------------------------

Title:
AlbMoRe: A Corpus of Movie Reviews for Sentiment Analysis in Albanian

Abstract: Lack of available resources such as text corpora for low-resource languages
seriously hinders research on natural language processing and computational
linguistics. This paper presents AlbMoRe, a corpus of 800 sentiment annotated
movie reviews in Albanian. Each text is labeled as positive or negative and can
be used for sentiment analysis research. Preliminary results based on
traditional machine learning classifiers trained with the AlbMoRe samples are
also reported. They can serve as comparison baselines for future research
experiments.


------------------------------------------------------------------------------

Title:
ClimSim: An open large-scale dataset for training high-resolution  physics emulators in hybrid multi-scale climate simulators

Abstract: Modern climate projections lack adequate spatial and temporal resolution due
to computational constraints. A consequence is inaccurate and imprecise
prediction of critical processes such as storms. Hybrid methods that combine
physics with machine learning (ML) have introduced a new generation of higher
fidelity climate simulators that can sidestep Moore's Law by outsourcing
compute-hungry, short, high-resolution simulations to ML emulators. However,
this hybrid ML-physics simulation approach requires domain-specific treatment
and has been inaccessible to ML experts because of lack of training data and
relevant, easy-to-use workflows. We present ClimSim, the largest-ever dataset
designed for hybrid ML-physics research. It comprises multi-scale climate
simulations, developed by a consortium of climate scientists and ML
researchers. It consists of 5.7 billion pairs of multivariate input and output
vectors that isolate the influence of locally-nested, high-resolution,
high-fidelity physics on a host climate simulator's macro-scale physical state.
The dataset is global in coverage, spans multiple years at high sampling
frequency, and is designed such that resulting emulators are compatible with
downstream coupling into operational climate simulators. We implement a range
of deterministic and stochastic regression baselines to highlight the ML
challenges and their scoring. The data
(this https URL) and code
(this https URL) are released openly to support the
development of hybrid ML-physics and high-fidelity climate simulations for the
benefit of science and society.


------------------------------------------------------------------------------

Title:
Characterizing Bugs in Python and R Data Analytics Programs

Abstract: R and Python are among the most popular languages used in many critical data
analytics tasks. However, we still do not fully understand the capabilities of
these two languages w.r.t. bugs encountered in data analytics tasks. What type
of bugs are common? What are the main root causes? What is the relation between
bugs and root causes? How to mitigate these bugs? We present a comprehensive
study of 5,068 Stack Overflow posts, 1,800 bug fix commits from GitHub
repositories, and several GitHub issues of the most used libraries to
understand bugs in R and Python. Our key findings include: while both R and
Python have bugs due to inexperience with data analysis, Python see
significantly larger data preprocessing bugs compared to R. Developers
experience significantly more data flow bugs in R because intermediate results
are often implicit. We also found changes and bugs in packages and libraries
cause more bugs in R compared to Python while package or library misselection
and conflicts cause more bugs in Python than R. While R has a slightly higher
readability barrier for data analysts, the statistical power of R leads to a
less number of bad performance bugs. In terms of data visualization, R packages
have significantly more bugs than Python libraries. We also identified a strong
correlation between comparable packages in R and Python despite their
linguistic and methodological differences. Lastly, we contribute a large
dataset of manually verified R and Python bugs.


------------------------------------------------------------------------------

Title:
AiXpand AI OS -- Decentralized ubiquitous computing MLOps execution  engine

Abstract: Over the past few years, ubiquitous, or pervasive computing has gained
popularity as the primary approach for a wide range of applications, including
enterprise-grade systems, consumer applications, and gaming systems. Ubiquitous
computing refers to the integration of computing technologies into everyday
objects and environments, creating a network of interconnected devices that can
communicate with each other and with humans. By using ubiquitous computing
technologies, communities can become more connected and efficient, with members
able to communicate and collaborate more easily. This enabled
interconnectedness and collaboration can lead to a more successful and
sustainable community. The spread of ubiquitous computing, however, has
emphasized the importance of automated learning and smart applications in
general. Even though there have been significant strides in Artificial
Intelligence and Deep Learning, large scale adoption has been hesitant due to
mounting pressure on expensive and highly complex cloud numerical-compute
infrastructures. Adopting, and even developing, practical machine learning
systems can come with prohibitive costs, not only in terms of complex
infrastructures but also of solid expertise in Data Science and Machine
Learning. In this paper we present an innovative approach for low-code
development and deployment of end-to-end AI cooperative application pipelines.
We address infrastructure allocation, costs, and secure job distribution in a
fully decentralized global cooperative community based on tokenized economics.


------------------------------------------------------------------------------

Title:
A Mechanistic Transform Model for Synthesizing Eye Movement Data with  Improved Realism

Abstract: This manuscript demonstrates an improved model-based approach for synthetic
degradation of previously captured eye movement signals. Signals recorded on a
high-quality eye tracking sensor are transformed such that their resulting eye
tracking signal quality is similar to recordings captured on a low-quality
target device. The proposed model improves the realism of the degraded signals
versus prior approaches by introducing a mechanism for degrading spatial
accuracy and temporal precision. Moreover, a percentile-matching technique is
demonstrated for mimicking the relative distributional structure of the signal
quality characteristics of the target data set. The model is demonstrated to
improve realism on a per-feature and per-recording basis using data from an
EyeLink 1000 eye tracker and an SMI eye tracker embedded within a virtual
reality platform. The model improves the median classification accuracy
performance metric by 35.7% versus the benchmark model towards the ideal metric
of 50%. This paper also expands the literature by providing an
application-agnostic realism assessment workflow for synthetically generated
eye movement signals.


------------------------------------------------------------------------------

Title:
ICET Online Accuracy Characterization for Geometry-Based Laser Scan  Matching

Abstract: Distribution-to-Distribution (D2D) point cloud registration algorithms are
fast, interpretable, and perform well in unstructured environments.
Unfortunately, existing strategies for predicting solution error for these
methods are overly optimistic, particularly in regions containing large or
extended physical objects. In this paper we introduce the Iterative Closest
Ellipsoidal Transform (ICET), a novel 3D LIDAR scan-matching algorithm that
re-envisions NDT in order to provide robust accuracy prediction from first
principles. Like NDT, ICET subdivides a LIDAR scan into voxels in order to
analyze complex scenes by considering many smaller local point distributions,
however, ICET assesses the voxel distribution to distinguish random noise from
deterministic structure. ICET then uses a weighted least-squares formulation to
incorporate this noise/structure distinction into computing a localization
solution and predicting the solution-error covariance. In order to demonstrate
the reasonableness of our accuracy predictions, we verify 3D ICET in three
LIDAR tests involving real-world automotive data, high-fidelity simulated
trajectories, and simulated corner-case scenes. For each test, ICET
consistently performs scan matching with sub-centimeter accuracy. This level of
accuracy, combined with the fact that the algorithm is fully interpretable,
make it well suited for safety-critical transportation applications. Code is
available at this https URL


------------------------------------------------------------------------------

Title:
Training-free Diffusion Model Adaptation for Variable-Sized  Text-to-Image Synthesis

Abstract: Diffusion models (DMs) have recently gained attention with state-of-the-art
performance in text-to-image synthesis. Abiding by the tradition in deep
learning, DMs are trained and evaluated on the images with fixed sizes.
However, users are demanding for various images with specific sizes and various
aspect ratio. This paper focuses on adapting text-to-image diffusion models to
handle such variety while maintaining visual fidelity. First we observe that,
during the synthesis, lower resolution images suffer from incomplete object
portrayal, while higher resolution images exhibit repetitive presentation.
Next, we establish a statistical relationship indicating that attention entropy
changes with token quantity, suggesting that models aggregate spatial
information in proportion to image resolution. The subsequent interpretation on
our observations is that objects are incompletely depicted due to limited
spatial information for low resolutions, while repetitive presentation arises
from redundant spatial information for high resolutions. From this perspective,
we propose a scaling factor to alleviate the change of attention entropy and
mitigate the defective pattern observed. Extensive experimental results
validate the efficacy of the proposed scaling factor, which enables the model
to achieve better visual effects, image quality, and text alignment. Notably,
these improvements are achieved without additional training or fine-tuning
techniques.


------------------------------------------------------------------------------

Title:
Bayesian inversion for Electrical Impedance Tomography by sparse  interpolation

Abstract: We study the Electrical Impedance Tomography Bayesian inverse problem for
recovering the conductivity given noisy measurements of the voltage on some
boundary surface electrodes. The uncertain conductivity depends linearly on a
countable number of uniformly distributed random parameters in a compact
interval, with the coefficient functions in the linear expansion decaying at an
algebraic rate. We analyze the surrogate Markov Chain Monte Carlo (MCMC)
approach for sampling the posterior probability measure, where the multivariate
sparse adaptive interpolation, with interpolating points chosen according to a
lower index set, is used for approximating the forward map. The forward
equation is approximated once before running the MCMC for all the realizations,
using interpolation on the finite element (FE) approximation at the parametric
interpolating points. When evaluation of the solution is needed for a
realization, we only need to compute a polynomial, thus cutting drastically the
computation time. We contribute a rigorous error estimate for the MCMC
convergence. In particular, we show that there is a nested sequence of
interpolating lower index sets for which we can derive an interpolation error
estimate in terms of the cardinality of these sets, uniformly for all the
parameter realizations. An explicit convergence rate for the MCMC sampling of
the posterior expectation of the conductivity is rigorously derived, in terms
of the interpolating point number, the accuracy of the FE approximation of the
forward equation, and the MCMC sample number. We perform numerical experiments
using an adaptive greedy approach to construct the sets of interpolation
points. We show the benefits of this approach over the simple MCMC where the
forward equation is repeatedly solved for all the samples and the non-adaptive
surrogate MCMC with an isotropic index set treating all the random parameters
equally.


------------------------------------------------------------------------------

Title:
TomoSAM: a 3D Slicer extension using SAM for tomography segmentation

Abstract: TomoSAM has been developed to integrate the cutting-edge Segment Anything
Model (SAM) into 3D Slicer, a highly capable software platform used for 3D
image processing and visualization. SAM is a promptable deep learning model
that is able to identify objects and create image masks in a zero-shot manner,
based only on a few user clicks. The synergy between these tools aids in the
segmentation of complex 3D datasets from tomography or other imaging
techniques, which would otherwise require a laborious manual segmentation
process. The source code associated with this article can be found at
this https URL


------------------------------------------------------------------------------

Title:
Katakomba: Tools and Benchmarks for Data-Driven NetHack

Abstract: NetHack is known as the frontier of reinforcement learning research where
learning-based methods still need to catch up to rule-based solutions. One of
the promising directions for a breakthrough is using pre-collected datasets
similar to recent developments in robotics, recommender systems, and more under
the umbrella of offline reinforcement learning (ORL). Recently, a large-scale
NetHack dataset was released; while it was a necessary step forward, it has yet
to gain wide adoption in the ORL community. In this work, we argue that there
are three major obstacles for adoption: tool-wise, implementation-wise, and
benchmark-wise. To address them, we develop an open-source library that
provides workflow fundamentals familiar to the ORL community: pre-defined
D4RL-style tasks, uncluttered baseline implementations, and reliable evaluation
tools with accompanying configs and logs synced to the cloud.


------------------------------------------------------------------------------

Title:
The directed metric dimension of directed co-graphs

Abstract: A vertex $w$ resolves two vertices $u$ and $v$ in a directed graph $G$ if the
distance from $w$ to $u$ is different to the distance from $w$ to $v$. A set of
vertices $R$ is a resolving set for a directed graph $G$ if for every pair of
vertices $u, v$ which are not in $R$ there is at least one vertex in $R$ that
resolves $u$ and $v$ in $G$. The directed metric dimension of a directed graph
$G$ is the size of a minimum resolving set for $G$. The decision problem
Directed Metric Dimension for a given directed graph $G$ and a given number $k$
is the question whether $G$ has a resolving set of size at most $k$. In this
paper, we study directed co-graphs. We introduce a linear time algorithm for
computing a minimum resolving set for directed co-graphs and show that Directed
Metric Dimension already is NP-complete for directed acyclic graphs.


------------------------------------------------------------------------------

Title:
Language to Rewards for Robotic Skill Synthesis

Abstract: Large language models (LLMs) have demonstrated exciting progress in acquiring
diverse new capabilities through in-context learning, ranging from logical
reasoning to code-writing. Robotics researchers have also explored using LLMs
to advance the capabilities of robotic control. However, since low-level robot
actions are hardware-dependent and underrepresented in LLM training corpora,
existing efforts in applying LLMs to robotics have largely treated LLMs as
semantic planners or relied on human-engineered control primitives to interface
with the robot. On the other hand, reward functions are shown to be flexible
representations that can be optimized for control policies to achieve diverse
tasks, while their semantic richness makes them suitable to be specified by
LLMs. In this work, we introduce a new paradigm that harnesses this realization
by utilizing LLMs to define reward parameters that can be optimized and
accomplish variety of robotic tasks. Using reward as the intermediate interface
generated by LLMs, we can effectively bridge the gap between high-level
language instructions or corrections to low-level robot actions. Meanwhile,
combining this with a real-time optimizer, MuJoCo MPC, empowers an interactive
behavior creation experience where users can immediately observe the results
and provide feedback to the system. To systematically evaluate the performance
of our proposed method, we designed a total of 17 tasks for a simulated
quadruped robot and a dexterous manipulator robot. We demonstrate that our
proposed method reliably tackles 90% of the designed tasks, while a baseline
using primitive skills as the interface with Code-as-policies achieves 50% of
the tasks. We further validated our method on a real robot arm where complex
manipulation skills such as non-prehensile pushing emerge through our
interactive system.


------------------------------------------------------------------------------

Title:
Investigation of the Challenges of Underwater-Visual-Monocular-SLAM

Abstract: In this paper, we present a comprehensive investigation of the challenges of
Monocular Visual Simultaneous Localization and Mapping (vSLAM) methods for
underwater robots. While significant progress has been made in state estimation
methods that utilize visual data in the past decade, most evaluations have been
limited to controlled indoor and urban environments, where impressive
performance was demonstrated. However, these techniques have not been
extensively tested in extremely challenging conditions, such as underwater
scenarios where factors such as water and light conditions, robot path, and
depth can greatly impact algorithm performance. Hence, our evaluation is
conducted in real-world AUV scenarios as well as laboratory settings which
provide precise external reference. A focus is laid on understanding the impact
of environmental conditions, such as optical properties of the water and
illumination scenarios, on the performance of monocular vSLAM methods. To this
end, we first show that all methods perform very well in in-air settings and
subsequently show the degradation of their performance in challenging
underwater environments. The final goal of this study is to identify techniques
that can improve accuracy and robustness of SLAM methods in such conditions. To
achieve this goal, we investigate the potential of image enhancement techniques
to improve the quality of input images used by the SLAM methods, specifically
in low visibility and extreme lighting scenarios in scattering media. We
present a first evaluation on calibration maneuvers and simple image
restoration techniques to determine their ability to enable or enhance the
performance of monocular SLAM methods in underwater environments.


------------------------------------------------------------------------------

Title:
Flexible Krylov Methods for Group Sparsity Regularization

Abstract: This paper introduces new solvers for efficiently computing solutions to
large-scale inverse problems with group sparsity regularization, including both
non-overlapping and overlapping groups. Group sparsity regularization refers to
a type of structured sparsity regularization, where the goal is to impose
additional structure in the regularization process by assigning variables to
predefined groups that may represent graph or network structures. Special cases
of group sparsity regularization include $\ell_1$ and isotropic total variation
regularization. In this work, we develop hybrid projection methods based on
flexible Krylov subspaces, where we first recast the group sparsity
regularization term as a sequence of 2-norm penalization terms using adaptive
regularization matrices in an iterative reweighted norm fashion. Then we
exploit flexible preconditioning techniques to efficiently incorporate the
weight updates. The main advantages of these methods are that they are
computationally efficient (leveraging the advantages of flexible methods), they
are general (and therefore very easily adaptable to new regularization term
choices), and they are able to select the regularization parameters
automatically and adaptively (exploiting the advantages of hybrid methods).
Extensions to multiple regularization terms and solution decomposition
frameworks (e.g., for anomaly detection) are described, and a variety of
numerical examples demonstrate both the efficiency and accuracy of the proposed
approaches compared to existing solvers.


------------------------------------------------------------------------------

Title:
Temporally Extended Goal Recognition in Fully Observable  Non-Deterministic Domain Models

Abstract: Goal Recognition is the task of discerning the correct intended goal that an
agent aims to achieve, given a set of goal hypotheses, a domain model, and a
sequence of observations (i.e., a sample of the plan executed in the
environment). Existing approaches assume that goal hypotheses comprise a single
conjunctive formula over a single final state and that the environment dynamics
are deterministic, preventing the recognition of temporally extended goals in
more complex settings. In this paper, we expand goal recognition to temporally
extended goals in Fully Observable Non-Deterministic (FOND) planning domain
models, focusing on goals on finite traces expressed in Linear Temporal Logic
(LTLf) and Pure Past Linear Temporal Logic (PLTLf). We develop the first
approach capable of recognizing goals in such settings and evaluate it using
different LTLf and PLTLf goals over six FOND planning domain models. Empirical
results show that our approach is accurate in recognizing temporally extended
goals in different recognition settings.


------------------------------------------------------------------------------

Title:
Recognizing Unseen Objects via Multimodal Intensive Knowledge Graph  Propagation

Abstract: Zero-Shot Learning (ZSL), which aims at automatically recognizing unseen
objects, is a promising learning paradigm to understand new real-world
knowledge for machines continuously. Recently, the Knowledge Graph (KG) has
been proven as an effective scheme for handling the zero-shot task with
large-scale and non-attribute data. Prior studies always embed relationships of
seen and unseen objects into visual information from existing knowledge graphs
to promote the cognitive ability of the unseen data. Actually, real-world
knowledge is naturally formed by multimodal facts. Compared with ordinary
structural knowledge from a graph perspective, multimodal KG can provide
cognitive systems with fine-grained knowledge. For example, the text
description and visual content can depict more critical details of a fact than
only depending on knowledge triplets. Unfortunately, this multimodal
fine-grained knowledge is largely unexploited due to the bottleneck of feature
alignment between different modalities. To that end, we propose a multimodal
intensive ZSL framework that matches regions of images with corresponding
semantic embeddings via a designed dense attention module and self-calibration
loss. It makes the semantic transfer process of our ZSL framework learns more
differentiated knowledge between entities. Our model also gets rid of the
performance limitation of only using rough global features. We conduct
extensive experiments and evaluate our model on large-scale real-world data.
The experimental results clearly demonstrate the effectiveness of the proposed
model in standard zero-shot classification tasks.


------------------------------------------------------------------------------

Title:
Norm-guided latent space exploration for text-to-image generation

Abstract: Text-to-image diffusion models show great potential in synthesizing a large
variety of concepts in new compositions and scenarios. However, their latent
seed space is still not well understood and has been shown to have an impact in
generating new and rare concepts. Specifically, simple operations like
interpolation and centroid finding work poorly with the standard Euclidean and
spherical metrics in the latent space. This paper makes the observation that
current training procedures make diffusion models biased toward inputs with a
narrow range of norm values. This has strong implications for methods that rely
on seed manipulation for image generation that can be further applied to
few-shot and long-tail learning tasks. To address this issue, we propose a
novel method for interpolating between two seeds and demonstrate that it
defines a new non-Euclidean metric that takes into account a norm-based prior
on seeds. We describe a simple yet efficient algorithm for approximating this
metric and use it to further define centroids in the latent seed space. We show
that our new interpolation and centroid evaluation techniques significantly
enhance the generation of rare concept images. This further leads to
state-of-the-art performance on few-shot and long-tail benchmarks, improving
prior approach in terms of generation speed, image quality, and semantic
content.


------------------------------------------------------------------------------

Title:
RRSIS: Referring Remote Sensing Image Segmentation

Abstract: Localizing desired objects from remote sensing images is of great use in
practical applications. Referring image segmentation, which aims at segmenting
out the objects to which a given expression refers, has been extensively
studied in natural images. However, almost no research attention is given to
this task of remote sensing imagery. Considering its potential for real-world
applications, in this paper, we introduce referring remote sensing image
segmentation (RRSIS) to fill in this gap and make some insightful explorations.
Specifically, we create a new dataset, called RefSegRS, for this task, enabling
us to evaluate different methods. Afterward, we benchmark referring image
segmentation methods of natural images on the RefSegRS dataset and find that
these models show limited efficacy in detecting small and scattered objects. To
alleviate this issue, we propose a language-guided cross-scale enhancement
(LGCE) module that utilizes linguistic features to adaptively enhance
multi-scale visual features by integrating both deep and shallow features. The
proposed dataset, benchmarking results, and the designed LGCE module provide
insights into the design of a better RRSIS model. We will make our dataset and
code publicly available.


------------------------------------------------------------------------------

Title:
User Simulation for Evaluating Information Access Systems

Abstract: Information access systems, such as search engines, recommender systems, and
conversational assistants, have become integral to our daily lives as they help
us satisfy our information needs. However, evaluating the effectiveness of
these systems presents a long-standing and complex scientific challenge. This
challenge is rooted in the difficulty of assessing a system's overall
effectiveness in assisting users to complete tasks through interactive support,
and further exacerbated by the substantial variation in user behaviour and
preferences. To address this challenge, user simulation emerges as a promising
solution.
This book focuses on providing a thorough understanding of user simulation
techniques designed specifically for evaluation purposes. We begin with a
background of information access system evaluation and explore the diverse
applications of user simulation. Subsequently, we systematically review the
major research progress in user simulation, covering both general frameworks
for designing user simulators, utilizing user simulation for evaluation, and
specific models and algorithms for simulating user interactions with search
engines, recommender systems, and conversational assistants. Realizing that
user simulation is an interdisciplinary research topic, whenever possible, we
attempt to establish connections with related fields, including machine
learning, dialogue systems, user modeling, and economics. We end the book with
a detailed discussion of important future research directions, many of which
extend beyond the evaluation of information access systems and are expected to
have broader impact on how to evaluate interactive intelligent systems in
general.


------------------------------------------------------------------------------

Title:
Predicting Wireless Channel Quality by means of Moving Averages and  Regression Models

Abstract: The ability to reliably predict the future quality of a wireless channel, as
seen by the media access control layer, is a key enabler to improve performance
of future industrial networks that do not rely on wires. Knowing in advance how
much channel behavior may change can speed up procedures for adaptively
selecting the best channel, making the network more deterministic, reliable,
and less energy-hungry, possibly improving device roaming capabilities at the
same time.
To this aim, popular approaches based on moving averages and regression were
compared, using multiple key performance indicators, on data captured from a
real Wi-Fi setup. Moreover, a simple technique based on a linear combination of
outcomes from different techniques was presented and analyzed, to further
reduce the prediction error, and some considerations about lower bounds on
achievable errors have been reported. We found that the best model is the
exponential moving average, which managed to predict the frame delivery ratio
with a 2.10\% average error and, at the same time, has lower computational
complexity and memory consumption than the other models we analyzed.


------------------------------------------------------------------------------

Title:
WizardCoder: Empowering Code Large Language Models with Evol-Instruct

Abstract: Code Large Language Models (Code LLMs), such as StarCoder, have demonstrated
exceptional performance in code-related tasks. However, most existing models
are solely pre-trained on extensive raw code data without instruction
fine-tuning. In this paper, we introduce WizardCoder, which empowers Code LLMs
with complex instruction fine-tuning, by adapting the Evol-Instruct method to
the domain of code. Through comprehensive experiments on four prominent code
generation benchmarks, namely HumanEval, HumanEval+, MBPP, and DS-1000, we
unveil the exceptional capabilities of our model. It surpasses all other
open-source Code LLMs by a substantial margin. Moreover, our model even
outperforms the largest closed LLMs, Anthropic's Claude and Google's Bard, on
HumanEval and HumanEval+. Our code, model weights, and data are public at
this https URL


------------------------------------------------------------------------------

Title:
Does mBERT understand Romansh? Evaluating word embeddings using word  alignment

Abstract: We test similarity-based word alignment models (SimAlign and awesome-align)
in combination with word embeddings from mBERT and XLM-R on parallel sentences
in German and Romansh. Since Romansh is an unseen language, we are dealing with
a zero-shot setting. Using embeddings from mBERT, both models reach an
alignment error rate of 0.22, which outperforms fast_align, a statistical
model, and is on par with similarity-based word alignment for seen languages.
We interpret these results as evidence that mBERT contains information that can
be meaningful and applicable to Romansh.
To evaluate performance, we also present a new trilingual corpus, which we
call the DERMIT (DE-RM-IT) corpus, containing press releases made by the Canton
of Grisons in German, Romansh and Italian in the past 25 years. The corpus
contains 4 547 parallel documents and approximately 100 000 sentence pairs in
each language combination. We additionally present a gold standard for
German-Romansh word alignment. The data is available at
this https URL


------------------------------------------------------------------------------

Title:
Predict to Detect: Prediction-guided 3D Object Detection using  Sequential Images

Abstract: Recent camera-based 3D object detection methods have introduced sequential
frames to improve the detection performance hoping that multiple frames would
mitigate the large depth estimation error. Despite improved detection
performance, prior works rely on naive fusion methods (e.g., concatenation) or
are limited to static scenes (e.g., temporal stereo), neglecting the importance
of the motion cue of objects. These approaches do not fully exploit the
potential of sequential images and show limited performance improvements. To
address this limitation, we propose a novel 3D object detection model, P2D
(Predict to Detect), that integrates a prediction scheme into a detection
framework to explicitly extract and leverage motion features. P2D predicts
object information in the current frame using solely past frames to learn
temporal motion features. We then introduce a novel temporal feature
aggregation method that attentively exploits Bird's-Eye-View (BEV) features
based on predicted object information, resulting in accurate 3D object
detection. Experimental results demonstrate that P2D improves mAP and NDS by
3.0% and 3.7% compared to the sequential image-based baseline, illustrating
that incorporating a prediction scheme can significantly improve detection
accuracy.


------------------------------------------------------------------------------

Title:
Towards AGI in Computer Vision: Lessons Learned from GPT and Large  Language Models

Abstract: The AI community has been pursuing algorithms known as artificial general
intelligence (AGI) that apply to any kind of real-world problem. Recently, chat
systems powered by large language models (LLMs) emerge and rapidly become a
promising direction to achieve AGI in natural language processing (NLP), but
the path towards AGI in computer vision (CV) remains unclear. One may owe the
dilemma to the fact that visual signals are more complex than language signals,
yet we are interested in finding concrete reasons, as well as absorbing
experiences from GPT and LLMs to solve the problem. In this paper, we start
with a conceptual definition of AGI and briefly review how NLP solves a wide
range of tasks via a chat system. The analysis inspires us that unification is
the next important goal of CV. But, despite various efforts in this direction,
CV is still far from a system like GPT that naturally integrates all tasks. We
point out that the essential weakness of CV lies in lacking a paradigm to learn
from environments, yet NLP has accomplished the task in the text world. We then
imagine a pipeline that puts a CV algorithm (i.e., an agent) in world-scale,
interactable environments, pre-trains it to predict future frames with respect
to its action, and then fine-tunes it with instruction to accomplish various
tasks. We expect substantial research and engineering efforts to push the idea
forward and scale it up, for which we share our perspectives on future research
directions.


------------------------------------------------------------------------------

Title:
Are training trajectories of deep single-spike and deep ReLU network  equivalent?

Abstract: Communication by binary and sparse spikes is a key factor for the energy
efficiency of biological brains. However, training deep spiking neural networks
(SNNs) with backpropagation is harder than with artificial neural networks
(ANNs), which is puzzling given that recent theoretical results provide exact
mapping algorithms from ReLU to time-to-first-spike (TTFS) SNNs. Building upon
these results, we analyze in theory and in simulation the learning dynamics of
TTFS-SNNs. Our analysis highlights that even when an SNN can be mapped exactly
to a ReLU network, it cannot always be robustly trained by gradient descent.
The reason for that is the emergence of a specific instance of the
vanishing-or-exploding gradient problem leading to a bias in the gradient
descent trajectory in comparison with the equivalent ANN. After identifying
this issue we derive a generic solution for the network initialization and SNN
parameterization which guarantees that the SNN can be trained as robustly as
its ANN counterpart. Our theoretical findings are illustrated in practice on
image classification datasets. Our method achieves the same accuracy as deep
ConvNets on CIFAR10 and enables fine-tuning on the much larger PLACES365
dataset without loss of accuracy compared to the ANN. We argue that the
combined perspective of conversion and fine-tuning with robust gradient descent
in SNN will be decisive to optimize SNNs for hardware implementations needing
low latency and resilience to noise and quantization.


------------------------------------------------------------------------------

Title:
Toward Grounded Social Reasoning

Abstract: Consider a robot tasked with tidying a desk with a meticulously constructed
Lego sports car. A human may recognize that it is not socially appropriate to
disassemble the sports car and put it away as part of the "tidying". How can a
robot reach that conclusion? Although large language models (LLMs) have
recently been used to enable social reasoning, grounding this reasoning in the
real world has been challenging. To reason in the real world, robots must go
beyond passively querying LLMs and *actively gather information from the
environment* that is required to make the right decision. For instance, after
detecting that there is an occluded car, the robot may need to actively
perceive the car to know whether it is an advanced model car made out of Legos
or a toy car built by a toddler. We propose an approach that leverages an LLM
and vision language model (VLM) to help a robot actively perceive its
environment to perform grounded social reasoning. To evaluate our framework at
scale, we release the MessySurfaces dataset which contains images of 70
real-world surfaces that need to be cleaned. We additionally illustrate our
approach with a robot on 2 carefully designed surfaces. We find an average
12.9% improvement on the MessySurfaces benchmark and an average 15% improvement
on the robot experiments over baselines that do not use active perception. The
dataset, code, and videos of our approach can be found at
this https URL


------------------------------------------------------------------------------

Title:
Transpiling RTL Pseudo-code of the POWER Instruction Set Architecture to  C for Real-time Performance Analysis on Cavatools Simulator

Abstract: This paper presents a transpiler framework for converting RTL pseudo code of
the POWER Instruction Set Architecture (ISA) to C code, enabling its execution
on the Cavatools simulator. The transpiler consists of a lexer and parser,
which parse the RTL pseudo code and generate corresponding C code
representations. The lexer tokenizes the input code, while the parser applies
grammar rules to build an abstract syntax tree (AST). The transpiler ensures
compatibility with the Cavatools simulator by generating C code that adheres to
its requirements. The resulting C code can be executed on the Cavatools
simulator, allowing developers to analyze the instruction-level performance of
the Power ISA in real time. The proposed framework facilitates the seamless
integration of RTL pseudo code into the Cavatools ecosystem, enabling
comprehensive performance analysis and optimization of Power ISA-based code.


------------------------------------------------------------------------------

Title:
A Copernican Revolution in Data

Abstract: Half a century ago, Charles Bachman foresaw the significance and centrality
of data in the digital world. In this short paper, we delve into the evolution
of these ideas within the database community over the past decades. We believe
that this historical analysis helps deepen our comprehension of the fundamental
changes undergoing our discipline and provides insights into the future
trajectory of our field.


------------------------------------------------------------------------------

Title:
When to Use Efficient Self Attention? Profiling Text, Speech and Image  Transformer Variants

Abstract: We present the first unified study of the efficiency of self-attention-based
Transformer variants spanning text, speech and vision. We identify input length
thresholds (tipping points) at which efficient Transformer variants become more
efficient than vanilla models, using a variety of efficiency metrics (latency,
throughput, and memory). To conduct this analysis for speech, we introduce
L-HuBERT, a novel local-attention variant of a self-supervised speech model. We
observe that these thresholds are (a) much higher than typical dataset sequence
lengths and (b) dependent on the metric and modality, showing that choosing the
right model depends on modality, task type (long-form vs. typical context) and
resource constraints (time vs. memory). By visualising the breakdown of the
computational costs for transformer components, we also show that
non-self-attention components exhibit significant computational costs. We
release our profiling toolkit at
this https URL .


------------------------------------------------------------------------------

Title:
AssistGPT: A General Multi-modal Assistant that can Plan, Execute,  Inspect, and Learn

Abstract: Recent research on Large Language Models (LLMs) has led to remarkable
advancements in general NLP AI assistants. Some studies have further explored
the use of LLMs for planning and invoking models or APIs to address more
general multi-modal user queries. Despite this progress, complex visual-based
tasks still remain challenging due to the diverse nature of visual tasks. This
diversity is reflected in two aspects: 1) Reasoning paths. For many real-life
applications, it is hard to accurately decompose a query simply by examining
the query itself. Planning based on the specific visual content and the results
of each step is usually required. 2) Flexible inputs and intermediate results.
Input forms could be flexible for in-the-wild cases, and involves not only a
single image or video but a mixture of videos and images, e.g., a user-view
image with some reference videos. Besides, a complex reasoning process will
also generate diverse multimodal intermediate results, e.g., video narrations,
segmented video clips, etc. To address such general cases, we propose a
multi-modal AI assistant, AssistGPT, with an interleaved code and language
reasoning approach called Plan, Execute, Inspect, and Learn (PEIL) to integrate
LLMs with various tools. Specifically, the Planner is capable of using natural
language to plan which tool in Executor should do next based on the current
reasoning progress. Inspector is an efficient memory manager to assist the
Planner to feed proper visual information into a specific tool. Finally, since
the entire reasoning process is complex and flexible, a Learner is designed to
enable the model to autonomously explore and discover the optimal solution. We
conducted experiments on A-OKVQA and NExT-QA benchmarks, achieving
state-of-the-art results. Moreover, showcases demonstrate the ability of our
system to handle questions far more complex than those found in the benchmarks.


------------------------------------------------------------------------------

Title:
EMERSK -- Explainable Multimodal Emotion Recognition with Situational  Knowledge

Abstract: Automatic emotion recognition has recently gained significant attention due
to the growing popularity of deep learning algorithms. One of the primary
challenges in emotion recognition is effectively utilizing the various cues
(modalities) available in the data. Another challenge is providing a proper
explanation of the outcome of the learning.To address these challenges, we
present Explainable Multimodal Emotion Recognition with Situational Knowledge
(EMERSK), a generalized and modular system for human emotion recognition and
explanation using visual information. Our system can handle multiple
modalities, including facial expressions, posture, and gait, in a flexible and
modular manner. The network consists of different modules that can be added or
removed depending on the available data. We utilize a two-stream network
architecture with convolutional neural networks (CNNs) and encoder-decoder
style attention mechanisms to extract deep features from face images.
Similarly, CNNs and recurrent neural networks (RNNs) with Long Short-term
Memory (LSTM) are employed to extract features from posture and gait data. We
also incorporate deep features from the background as contextual information
for the learning process. The deep features from each module are fused using an
early fusion network. Furthermore, we leverage situational knowledge derived
from the location type and adjective-noun pair (ANP) extracted from the scene,
as well as the spatio-temporal average distribution of emotions, to generate
explanations. Ablation studies demonstrate that each sub-network can
independently perform emotion recognition, and combining them in a multimodal
approach significantly improves overall recognition performance. Extensive
experiments conducted on various benchmark datasets, including GroupWalk,
validate the superior performance of our approach compared to other
state-of-the-art methods.


------------------------------------------------------------------------------

Title:
TAPIR: Tracking Any Point with per-frame Initialization and temporal  Refinement

Abstract: We present a novel model for Tracking Any Point (TAP) that effectively tracks
any queried point on any physical surface throughout a video sequence. Our
approach employs two stages: (1) a matching stage, which independently locates
a suitable candidate point match for the query point on every other frame, and
(2) a refinement stage, which updates both the trajectory and query features
based on local correlations. The resulting model surpasses all baseline methods
by a significant margin on the TAP-Vid benchmark, as demonstrated by an
approximate 20% absolute average Jaccard (AJ) improvement on DAVIS. Our model
facilitates fast inference on long and high-resolution video sequences. On a
modern GPU, our implementation has the capacity to track points faster than
real-time. Visualizations, source code, and pretrained models can be found on
our project webpage.


------------------------------------------------------------------------------

Title:
Your Email Address Holds the Key: Understanding the Connection Between  Email and Password Security with Deep Learning

Abstract: In this work, we investigate the effectiveness of deep-learning-based
password guessing models for targeted attacks on human-chosen passwords. In
recent years, service providers have increased the level of security of
users'passwords. This is done by requiring more complex password generation
patterns and by using computationally expensive hash functions. For the
attackers this means a reduced number of available guessing attempts, which
introduces the necessity to target their guess by exploiting a victim's
publicly available information. In this work, we introduce a context-aware
password guessing model that better capture attackers'behavior. We demonstrate
that knowing a victim's email address is already critical in compromising the
associated password and provide an in-depth analysis of the relationship
between them. We also show the potential of such models to identify clusters of
users based on their password generation behaviour, which can spot fake
profiles and populations more vulnerable to context-aware guesses. The code is
publicly available at this https URL


------------------------------------------------------------------------------

Title:
Beyond Implicit Bias: The Insignificance of SGD Noise in Online Learning

Abstract: The success of SGD in deep learning has been ascribed by prior works to the
implicit bias induced by high learning rate or small batch size ("SGD noise").
While prior works that focused on offline learning (i.e., multiple-epoch
training), we study the impact of SGD noise on online (i.e., single epoch)
learning. Through an extensive empirical analysis of image and language data,
we demonstrate that large learning rate and small batch size do not confer any
implicit bias advantages in online learning. In contrast to offline learning,
the benefits of SGD noise in online learning are strictly computational,
facilitating larger or more cost-effective gradient steps. Our work suggests
that SGD in the online regime can be construed as taking noisy steps along the
"golden path" of the noiseless gradient flow algorithm. We provide evidence to
support this hypothesis by conducting experiments that reduce SGD noise during
training and by measuring the pointwise functional distance between models
trained with varying SGD noise levels, but at equivalent loss values. Our
findings challenge the prevailing understanding of SGD and offer novel insights
into its role in online learning.


------------------------------------------------------------------------------

Title:
Radiology-GPT: A Large Language Model for Radiology

Abstract: We introduce Radiology-GPT, a large language model for radiology. Using an
instruction tuning approach on an extensive dataset of radiology domain
knowledge, Radiology-GPT demonstrates superior performance compared to general
language models such as StableLM, Dolly and LLaMA. It exhibits significant
versatility in radiological diagnosis, research, and communication. This work
serves as a catalyst for future developments in clinical NLP. The successful
implementation of Radiology-GPT is indicative of the potential of localizing
generative large language models, specifically tailored for distinctive medical
specialties, while ensuring adherence to privacy standards such as HIPAA. The
prospect of developing individualized, large-scale language models that cater
to specific needs of various hospitals presents a promising direction. The
fusion of conversational competence and domain-specific knowledge in these
models is set to foster future development in healthcare AI. A demo of
Radiology-GPT is available at
this https URL


------------------------------------------------------------------------------

Title:
Tagged End-to-End Simultaneous Speech Translation Training using  Simultaneous Interpretation Data

Abstract: Simultaneous speech translation (SimulST) translates partial speech inputs
incrementally. Although the monotonic correspondence between input and output
is preferable for smaller latency, it is not the case for distant language
pairs such as English and Japanese. A prospective approach to this problem is
to mimic simultaneous interpretation (SI) using SI data to train a SimulST
model. However, the size of such SI data is limited, so the SI data should be
used together with ordinary bilingual data whose translations are given in
offline. In this paper, we propose an effective way to train a SimulST model
using mixed data of SI and offline. The proposed method trains a single model
using the mixed data with style tags that tell the model to generate SI- or
offline-style outputs. Experiment results show improvements of BLEURT in
different latency ranges, and our analyses revealed the proposed model
generates SI-style outputs more than the baseline.


------------------------------------------------------------------------------

Title:
OCAtari: Object-Centric Atari 2600 Reinforcement Learning Environments

Abstract: Cognitive science and psychology suggest that object-centric representations
of complex scenes are a promising step towards enabling efficient abstract
reasoning from low-level perceptual features. Yet, most deep reinforcement
learning approaches rely on only pixel-based representations that do not
capture the compositional properties of natural scenes. For this, we need
environments and datasets that allow us to work and evaluate object-centric
approaches. We present OCAtari, a set of environment that provides
object-centric state representations of Atari games, the most-used evaluation
framework for deep RL approaches. OCAtari also allows for RAM state
manipulations of the games to change and create specific or even novel
situations. The code base for this work is available at
github.com/k4ntz/OC_Atari.


------------------------------------------------------------------------------

Title:
Learning to Predict Scene-Level Implicit 3D from Posed RGBD Data

Abstract: We introduce a method that can learn to predict scene-level implicit
functions for 3D reconstruction from posed RGBD data. At test time, our system
maps a previously unseen RGB image to a 3D reconstruction of a scene via
implicit functions. While implicit functions for 3D reconstruction have often
been tied to meshes, we show that we can train one using only a set of posed
RGBD images. This setting may help 3D reconstruction unlock the sea of
accelerometer+RGBD data that is coming with new phones. Our system, D2-DRDF,
can match and sometimes outperform current methods that use mesh supervision
and shows better robustness to sparse data.


------------------------------------------------------------------------------

Title:
Vulnerability Assessment of Industrial Control System with an Improved  CVSS

Abstract: Cyberattacks on industrial control systems (ICS) have been drawing attention
in academia. However, this has not raised adequate concerns among some
industrial practitioners. Therefore, it is necessary to identify the vulnerable
locations and components in the ICS and investigate the attack scenarios and
techniques. This study proposes a method to assess the risk of cyberattacks on
ICS with an improved Common Vulnerability Scoring System (CVSS) and applies it
to a continuous stirred tank reactor (CSTR) model. The results show the
physical system levels of ICS have the highest severity once cyberattacked, and
controllers, workstations, and human-machine interface are the crucial
components in the cyberattack and defense.


------------------------------------------------------------------------------

Title:
Fast and Private Inference of Deep Neural Networks by Co-designing  Activation Functions

Abstract: Machine Learning as a Service (MLaaS) is an increasingly popular design where
a company with abundant computing resources trains a deep neural network and
offers query access for tasks like image classification. The challenge with
this design is that MLaaS requires the client to reveal their potentially
sensitive queries to the company hosting the model. Multi-party computation
(MPC) protects the client's data by allowing encrypted inferences. However,
current approaches suffer prohibitively large inference times. The inference
time bottleneck in MPC is the evaluation of non-linear layers such as ReLU
activation functions. Motivated by the success of previous work co-designing
machine learning and MPC aspects, we develop an activation function co-design.
We replace all ReLUs with a polynomial approximation and evaluate them with
single-round MPC protocols, which give state-of-the-art inference times in
wide-area networks. Furthermore, to address the accuracy issues previously
encountered with polynomial activations, we propose a novel training algorithm
that gives accuracy competitive with plaintext models. Our evaluation shows
between $4$ and $90\times$ speedups in inference time on large models with up
to $23$ million parameters while maintaining competitive inference accuracy.


------------------------------------------------------------------------------

Title:
Single-board Device Individual Authentication based on Hardware  Performance and Anomaly Detection for Crowdsensing Platforms

Abstract: The proliferation of the Internet of Things (IoT) has led to the emergence of
crowdsensing applications, where a multitude of interconnected devices
collaboratively collect and analyze data. Ensuring the authenticity and
integrity of the data collected by these devices is crucial for reliable
decision-making and maintaining trust in the system. Traditional authentication
methods are often vulnerable to attacks or can be easily duplicated, posing
challenges to securing crowdsensing applications. Besides, current solutions
leveraging device behavior are mostly focused on device identification, which
is a simpler task than authentication. To address these issues, an individual
IoT device authentication framework based on hardware behavior fingerprinting
and Transformer autoencoders is proposed in this work. This solution leverages
the inherent imperfections and variations in IoT device hardware to
differentiate between devices with identical specifications. By monitoring and
analyzing the behavior of key hardware components, such as the CPU, GPU, RAM,
and Storage on devices, unique fingerprints for each device are created. The
performance samples are considered as time series data and used to train
anomaly detection transformer models, one per device. Then, the framework is
validated within a spectrum crowdsensing system leveraging Raspberry Pi
devices. After a pool of experiments, the model from each device is able to
individually authenticate it between the 45 devices employed for validation. An
average True Positive Rate (TPR) of 0.74+-0.13 and an average maximum False
Positive Rate (FPR) of 0.06+-0.09 demonstrate the effectiveness of this
approach in enhancing authentication, security, and trust in crowdsensing
applications.


------------------------------------------------------------------------------

Title:
Graph-Based Matrix Completion Applied to Weather Data

Abstract: Low-rank matrix completion is the task of recovering unknown entries of a
matrix by assuming that the true matrix admits a good low-rank approximation.
Sometimes additional information about the variables is known, and
incorporating this information into a matrix completion model can lead to a
better completion quality. We consider the situation where information between
the column/row entities of the matrix is available as a weighted graph. In this
framework, we address the problem of completing missing entries in air
temperature data recorded by weather stations. We construct test sets by
holding back data at locations that mimic real-life gaps in weather data. On
such test sets, we show that adequate spatial and temporal graphs can
significantly improve the accuracy of the completion obtained by
graph-regularized low-rank matrix completion methods.


------------------------------------------------------------------------------

Title:
SimpleMapping: Real-Time Visual-Inertial Dense Mapping with Deep  Multi-View Stereo

Abstract: We present a real-time visual-inertial dense mapping method capable of
performing incremental 3D mesh reconstruction with high quality using only
sequential monocular images and inertial measurement unit (IMU) readings. 6-DoF
camera poses are estimated by a robust feature-based visual-inertial odometry
(VIO), which also generates noisy sparse 3D map points as a by-product. We
propose a sparse point aided multi-view stereo neural network (SPA-MVSNet) that
can effectively leverage the informative but noisy sparse points from the VIO
system. The sparse depth from VIO is firstly completed by a single-view depth
completion network. This dense depth map, although naturally limited in
accuracy, is then used as a prior to guide our MVS network in the cost volume
generation and regularization for accurate dense depth prediction. Predicted
depth maps of keyframe images by the MVS network are incrementally fused into a
global map using TSDF-Fusion. We extensively evaluate both the proposed
SPA-MVSNet and the entire visual-inertial dense mapping system on several
public datasets as well as our own dataset, demonstrating the system's
impressive generalization capabilities and its ability to deliver high-quality
3D mesh reconstruction online. Our proposed dense mapping system achieves a
39.7% improvement in F-score over existing systems when evaluated on the
challenging scenarios of the EuRoC dataset.


------------------------------------------------------------------------------

Title:
Noise Stability Optimization for Flat Minima with Optimal Convergence  Rates

Abstract: We consider finding flat, local minimizers by adding average weight
perturbations. Given a nonconvex function $f: \mathbb{R}^d \rightarrow
\mathbb{R}$ and a $d$-dimensional distribution $\mathcal{P}$ which is symmetric
at zero, we perturb the weight of $f$ and define $F(W) = \mathbb{E}[f({W +
U})]$, where $U$ is a random sample from $\mathcal{P}$. This injection induces
regularization through the Hessian trace of $f$ for small, isotropic Gaussian
perturbations. Thus, the weight-perturbed function biases to minimizers with
low Hessian trace. Several prior works have studied settings related to this
weight-perturbed function by designing algorithms to improve generalization.
Still, convergence rates are not known for finding minima under the average
perturbations of the function $F$. This paper considers an SGD-like algorithm
that injects random noise before computing gradients while leveraging the
symmetry of $\mathcal{P}$ to reduce variance. We then provide a rigorous
analysis, showing matching upper and lower bounds of our algorithm for finding
an approximate first-order stationary point of $F$ when the gradient of $f$ is
Lipschitz-continuous. We empirically validate our algorithm for several image
classification tasks with various architectures. Compared to sharpness-aware
minimization, we note a 12.6% and 7.8% drop in the Hessian trace and top
eigenvalue of the found minima, respectively, averaged over eight datasets.
Ablation studies validate the benefit of the design of our algorithm.


------------------------------------------------------------------------------

Title:
TensorKrowch: Smooth integration of tensor networks in machine learning

Abstract: Tensor networks are factorizations of high-dimensional tensors into networks
of smaller tensors. They have applications in physics and mathematics, and
recently have been proposed as promising machine learning architectures. To
ease the integration of tensor networks in machine learning pipelines, we
introduce TensorKrowch, an open source Python library built on top of PyTorch.
Providing a user-friendly interface, TensorKrowch allows users to construct any
tensor network, train it, and integrate it as a layer in more intricate deep
learning models. In this paper, we describe the main functionality and basic
usage of TensorKrowch, and provide technical details on its building blocks and
the optimizations performed to achieve efficient operation.


------------------------------------------------------------------------------

Title:
Parallel Algorithms for Hierarchical Nucleus Decomposition

Abstract: Nucleus decompositions have been shown to be a useful tool for finding dense
subgraphs. The coreness value of a clique represents its density based on the
number of other cliques it is adjacent to. One useful output of nucleus
decomposition is to generate a hierarchy among dense subgraphs at different
resolutions. However, existing parallel algorithms for nucleus decomposition do
not generate this hierarchy, and only compute the coreness values. This paper
presents a scalable parallel algorithm for hierarchy construction, with
practical optimizations, such as interleaving the coreness computation with
hierarchy construction and using a concurrent union-find data structure in an
innovative way to generate the hierarchy. We also introduce a parallel
approximation algorithm for nucleus decomposition, which achieves much lower
span in theory and better performance in practice. We prove strong theoretical
bounds on the work and span (parallel time) of our algorithms.
On a 30-core machine with two-way hyper-threading on real-world graphs, our
parallel hierarchy construction algorithm achieves up to a 58.84x speedup over
the state-of-the-art sequential hierarchy construction algorithm by Sariyuce et
al. and up to a 30.96x self-relative parallel speedup. On the same machine, our
approximation algorithm achieves a 3.3x speedup over our exact algorithm, while
generating coreness estimates with a multiplicative error of 1.33x on average.


------------------------------------------------------------------------------

Title:
From Driver to Supervisor: Comparing Cognitive Load and EEG-based  Attention Allocation across Automation Levels

Abstract: With increasing automation, drivers' role progressively transitions from
active operators to passive system supervisors, affecting their behaviour and
cognitive processes. This study aims to understand better attention allocation
and perceived cognitive load in manual, L2, and L3 driving in a realistic
environment. We conducted a test-track experiment with 30 participants. While
driving a prototype automated vehicle, participants were exposed to a passive
auditory oddball task and their EEG was recorded. We studied the P3a ERP
component elicited by novel environmental cues, an index of attention resources
used to process the stimuli. The self-reported cognitive load was assessed
using the NASA-TLX. Our findings revealed no significant difference in
perceived cognitive load between manual and L2 driving, with L3 driving
demonstrating a lower self-reported cognitive load. Despite this, P3a mean
amplitude was highest during manual driving, indicating greater attention
allocation towards processing environmental sounds compared to L2 and L3
driving. We argue that the need to integrate environmental information might be
attenuated in L2 and L3 driving. Further empirical evidence is necessary to
understand whether the decreased processing of environmental stimuli is due to
top-down attention control leading to attention withdrawal or a lack of
available resources due to high cognitive load. To the best of our knowledge,
this study is the first attempt to use the passive oddball paradigm outside the
laboratory. The insights of this study have significant implications for
automation safety and user interface design.


------------------------------------------------------------------------------

Title:
Learning to Rank when Grades Matter

Abstract: Graded labels are ubiquitous in real-world learning-to-rank applications,
especially in human rated relevance data. Traditional learning-to-rank
techniques aim to optimize the ranked order of documents. They typically,
however, ignore predicting actual grades. This prevents them from being adopted
in applications where grades matter, such as filtering out ``poor'' documents.
Achieving both good ranking performance and good grade prediction performance
is still an under-explored problem. Existing research either focuses only on
ranking performance by not calibrating model outputs, or treats grades as
numerical values, assuming labels are on a linear scale and failing to leverage
the ordinal grade information. In this paper, we conduct a rigorous study of
learning to rank with grades, where both ranking performance and grade
prediction performance are important. We provide a formal discussion on how to
perform ranking with non-scalar predictions for grades, and propose a
multiobjective formulation to jointly optimize both ranking and grade
predictions. In experiments, we verify on several public datasets that our
methods are able to push the Pareto frontier of the tradeoff between ranking
and grade prediction performance, showing the benefit of leveraging ordinal
grade information.


------------------------------------------------------------------------------

Title:
FROG: A new people detection dataset for knee-high 2D range finders

Abstract: Mobile robots require knowledge of the environment, especially of humans
located in its vicinity. While the most common approaches for detecting humans
involve computer vision, an often overlooked hardware feature of robots for
people detection are their 2D range finders. These were originally intended for
obstacle avoidance and mapping/SLAM tasks. In most robots, they are
conveniently located at a height approximately between the ankle and the knee,
so they can be used for detecting people too, and with a larger field of view
and depth resolution compared to cameras.
In this paper, we present a new dataset for people detection using knee-high
2D range finders called FROG. This dataset has greater laser resolution,
scanning frequency, and more complete annotation data compared to existing
datasets such as DROW. Particularly, the FROG dataset contains annotations for
100% of its laser scans (unlike DROW which only annotates 5%), 17x more
annotated scans, 100x more people annotations, and over twice the distance
traveled by the robot. We propose a benchmark based on the FROG dataset, and
analyze a collection of state-of-the-art people detectors based on 2D range
finder data.
We also propose and evaluate a new end-to-end deep learning approach for
people detection. Our solution works with the raw sensor data directly (not
needing hand-crafted input data features), thus avoiding CPU preprocessing and
releasing the developer of understanding specific domain heuristics.
Experimental results show how the proposed people detector attains results
comparable to the state of the art, while an optimized implementation for ROS
can operate at more than 500 Hz.


------------------------------------------------------------------------------

Title:
Improving Code-Switching and Named Entity Recognition in ASR with Speech  Editing based Data Augmentation

Abstract: Recently, end-to-end (E2E) automatic speech recognition (ASR) models have
made great strides and exhibit excellent performance in general speech
recognition. However, there remain several challenging scenarios that E2E
models are not competent in, such as code-switching and named entity
recognition (NER). Data augmentation is a common and effective practice for
these two scenarios. However, the current data augmentation methods mainly rely
on audio splicing and text-to-speech (TTS) models, which might result in
discontinuous, unrealistic, and less diversified speech. To mitigate these
potential issues, we propose a novel data augmentation method by applying the
text-based speech editing model. The augmented speech from speech editing
systems is more coherent and diversified, also more akin to real speech. The
experimental results on code-switching and NER tasks show that our proposed
method can significantly outperform the audio splicing and neural TTS based
data augmentation systems.


------------------------------------------------------------------------------

Title:
Explore In-Context Learning for 3D Point Cloud Understanding

Abstract: With the rise of large-scale models trained on broad data, in-context
learning has become a new learning paradigm that has demonstrated significant
potential in natural language processing and computer vision tasks. Meanwhile,
in-context learning is still largely unexplored in the 3D point cloud domain.
Although masked modeling has been successfully applied for in-context learning
in 2D vision, directly extending it to 3D point clouds remains a formidable
challenge. In the case of point clouds, the tokens themselves are the point
cloud positions (coordinates) that are masked during inference. Moreover,
position embedding in previous works may inadvertently introduce information
leakage. To address these challenges, we introduce a novel framework, named
Point-In-Context, designed especially for in-context learning in 3D point
clouds, where both inputs and outputs are modeled as coordinates for each task.
Additionally, we propose the Joint Sampling module, carefully designed to work
in tandem with the general point sampling operator, effectively resolving the
aforementioned technical issues. We conduct extensive experiments to validate
the versatility and adaptability of our proposed methods in handling a wide
range of tasks. Furthermore, with a more effective prompt selection strategy,
our framework surpasses the results of individually trained models.


------------------------------------------------------------------------------

Title:
Towards Automatic Identification of Violation Symptoms of Architecture  Erosion

Abstract: Architecture erosion has a detrimental effect on maintenance and evolution,
as the implementation drifts away from the intended architecture. To prevent
this, development teams need to understand early enough the symptoms of
erosion, and particularly violations of the intended architecture. One way to
achieve this, is through the automatic identification of architecture
violations from textual artifacts, and particularly code reviews. In this
paper, we developed 15 machine learning-based and 4 deep learning-based
classifiers with three pre-trained word embeddings to identify violation
symptoms of architecture erosion from developer discussions in code reviews.
Specifically, we looked at code review comments from four large open-source
projects from the OpenStack (Nova and Neutron) and Qt (Qt Base and Qt Creator)
communities. We then conducted a survey to acquire feedback from the involved
participants who discussed architecture violations in code reviews, to validate
the usefulness of our trained classifiers. The results show that the SVM
classifier based on word2vec pre-trained word embedding performs the best with
an F1-score of 0.779. In most cases, classifiers with the fastText pre-trained
word embedding model can achieve relatively good performance. Furthermore,
200-dimensional pre-trained word embedding models outperform classifiers that
use 100 and 300-dimensional models. In addition, an ensemble classifier based
on the majority voting strategy can further enhance the classifier and
outperforms the individual classifiers. Finally, an online survey of the
involved developers reveals that the violation symptoms identified by our
approaches have practical value and can provide early warnings for impending
architecture erosion.


------------------------------------------------------------------------------

Title:
Challenges of Indoor SLAM: A multi-modal multi-floor dataset for SLAM  evaluation

Abstract: Robustness in Simultaneous Localization and Mapping (SLAM) remains one of the
key challenges for the real-world deployment of autonomous systems. SLAM
research has seen significant progress in the last two and a half decades, yet
many state-of-the-art (SOTA) algorithms still struggle to perform reliably in
real-world environments. There is a general consensus in the research community
that we need challenging real-world scenarios which bring out different failure
modes in sensing modalities. In this paper, we present a novel multi-modal
indoor SLAM dataset covering challenging common scenarios that a robot will
encounter and should be robust to. Our data was collected with a mobile
robotics platform across multiple floors at Northeastern University's ISEC
building. Such a multi-floor sequence is typical of commercial office spaces
characterized by symmetry across floors and, thus, is prone to perceptual
aliasing due to similar floor layouts. The sensor suite comprises seven global
shutter cameras, a high-grade MEMS inertial measurement unit (IMU), a ZED
stereo camera, and a 128-channel high-resolution lidar. Along with the dataset,
we benchmark several SLAM algorithms and highlight the problems faced during
the runs, such as perceptual aliasing, visual degradation, and trajectory
drift. The benchmarking results indicate that parts of the dataset work well
with some algorithms, while other data sections are challenging for even the
best SOTA algorithms. The dataset is available at
this https URL


------------------------------------------------------------------------------

Title:
Multi-class Graph Clustering via Approximated Effective $p$-Resistance

Abstract: This paper develops an approximation to the (effective) $p$-resistance and
applies it to multi-class clustering. Spectral methods based on the graph
Laplacian and its generalization to the graph $p$-Laplacian have been a
backbone of non-euclidean clustering techniques. The advantage of the
$p$-Laplacian is that the parameter $p$ induces a controllable bias on cluster
structure. The drawback of $p$-Laplacian eigenvector based methods is that the
third and higher eigenvectors are difficult to compute. Thus, instead, we are
motivated to use the $p$-resistance induced by the $p$-Laplacian for
clustering. For $p$-resistance, small $p$ biases towards clusters with high
internal connectivity while large $p$ biases towards clusters of small
``extent,'' that is a preference for smaller shortest-path distances between
vertices in the cluster. However, the $p$-resistance is expensive to compute.
We overcome this by developing an approximation to the $p$-resistance. We prove
upper and lower bounds on this approximation and observe that it is exact when
the graph is a tree. We also provide theoretical justification for the use of
$p$-resistance for clustering. Finally, we provide experiments comparing our
approximated $p$-resistance clustering to other $p$-Laplacian based methods.


------------------------------------------------------------------------------

Title:
Probabilistic Regular Tree Priors for Scientific Symbolic Reasoning

Abstract: Symbolic Regression (SR) allows for the discovery of scientific equations
from data. To limit the large search space of possible equations, prior
knowledge has been expressed in terms of formal grammars that characterize
subsets of arbitrary strings. However, there is a mismatch between context-free
grammars required to express the set of syntactically correct equations,
missing closure properties of the former, and a tree structure of the latter.
Our contributions are to (i) compactly express experts' prior beliefs about
which equations are more likely to be expected by probabilistic Regular Tree
Expressions (pRTE), and (ii) adapt Bayesian inference to make such priors
efficiently available for symbolic regression encoded as finite state machines.
Our scientific case studies show its effectiveness in soil science to find
sorption isotherms and for modeling hyper-elastic materials.


------------------------------------------------------------------------------

Title:
Anticipatory Music Transformer

Abstract: We introduce anticipation: a method for constructing a controllable
generative model of a temporal point process (the event process) conditioned
asynchronously on realizations of a second, correlated process (the control
process). We achieve this by interleaving sequences of events and controls,
such that controls appear following stopping times in the event sequence. This
work is motivated by problems arising in the control of symbolic music
generation. We focus on infilling control tasks, whereby the controls are a
subset of the events themselves, and conditional generation completes a
sequence of events given the fixed control events. We train anticipatory
infilling models using the large and diverse Lakh MIDI music dataset. These
models match the performance of autoregressive models for prompted music
generation, with the additional capability to perform infilling control tasks,
including accompaniment. Human evaluators report that an anticipatory model
produces accompaniments with similar musicality to even music composed by
humans over a 20-second clip.


------------------------------------------------------------------------------

Title:
RISCLIP: Referring Image Segmentation Framework using CLIP

Abstract: Recent advances in computer vision and natural language processing have
naturally led to active research in multi-modal tasks, including Referring
Image Segmentation (RIS). Recent approaches have advanced the frontier of RIS
by impressive margins, but they require an additional pretraining stage on
external visual grounding datasets to achieve the state-of-the-art
performances. We attempt to break free from this requirement by effectively
adapting Contrastive Language-Image Pretraining (CLIP) to RIS. We propose a
novel framework that residually adapts frozen CLIP features to RIS with Fusion
Adapters and Backbone Adapters. Freezing CLIP preserves the backbone's rich,
general image-text alignment knowledge, whilst Fusion Adapters introduce
multi-modal communication and Backbone Adapters inject new knowledge useful in
solving RIS. Our method reaches a new state of the art on three major RIS
benchmarks. We attain such performance without additional pretraining and
thereby absolve the necessity of extra training and data preparation. Source
code and model weights will be available upon publication.


------------------------------------------------------------------------------

Title:
ITALIC: An Italian Intent Classification Dataset

Abstract: Recent large-scale Spoken Language Understanding datasets focus predominantly
on English and do not account for language-specific phenomena such as
particular phonemes or words in different lects. We introduce ITALIC, the first
large-scale speech dataset designed for intent classification in Italian. The
dataset comprises 16,521 crowdsourced audio samples recorded by 70 speakers
from various Italian regions and annotated with intent labels and additional
metadata. We explore the versatility of ITALIC by evaluating current
state-of-the-art speech and text models. Results on intent classification
suggest that increasing scale and running language adaptation yield better
speech models, monolingual text models outscore multilingual ones, and that
speech recognition on ITALIC is more challenging than on existing Italian
benchmarks. We release both the dataset and the annotation scheme to streamline
the development of new Italian SLU models and language-specific datasets.


------------------------------------------------------------------------------

Title:
A Relaxed Optimization Approach for Adversarial Attacks against Neural  Machine Translation Models

Abstract: In this paper, we propose an optimization-based adversarial attack against
Neural Machine Translation (NMT) models. First, we propose an optimization
problem to generate adversarial examples that are semantically similar to the
original sentences but destroy the translation generated by the target NMT
model. This optimization problem is discrete, and we propose a continuous
relaxation to solve it. With this relaxation, we find a probability
distribution for each token in the adversarial example, and then we can
generate multiple adversarial examples by sampling from these distributions.
Experimental results show that our attack significantly degrades the
translation quality of multiple NMT models while maintaining the semantic
similarity between the original and adversarial sentences. Furthermore, our
attack outperforms the baselines in terms of success rate, similarity
preservation, effect on translation quality, and token error rate. Finally, we
propose a black-box extension of our attack by sampling from an optimized
probability distribution for a reference model whose gradients are accessible.


------------------------------------------------------------------------------

Title:
A Networked Multi-Agent System for Mobile Wireless Infrastructure on  Demand

Abstract: Despite the prevalence of wireless connectivity in urban areas around the
globe, there remain numerous and diverse situations where connectivity is
insufficient or unavailable. To address this, we introduce mobile wireless
infrastructure on demand, a system of UAVs that can be rapidly deployed to
establish an ad-hoc wireless network. This network has the capability of
reconfiguring itself dynamically to satisfy and maintain the required quality
of communication. The system optimizes the positions of the UAVs and the
routing of data flows throughout the network to achieve this quality of service
(QoS). By these means, task agents using the network simply request a desired
QoS, and the system adapts accordingly while allowing them to move freely. We
have validated this system both in simulation and in real-world experiments.
The results demonstrate that our system effectively offers mobile wireless
infrastructure on demand, extending the operational range of task agents and
supporting complex mobility patterns, all while ensuring connectivity and being
resilient to agent failures.


------------------------------------------------------------------------------

Title:
A pose and shear-based tactile robotic system for object tracking,  surface following and object pushing

Abstract: Tactile perception is a crucial sensing modality in robotics, particularly in
scenarios that require precise manipulation and safe interaction with other
objects. Previous research in this area has focused extensively on tactile
perception of contact poses as this is an important capability needed for tasks
such as traversing an object's surface or edge, manipulating an object, or
pushing an object along a predetermined path. Another important capability
needed for tasks such as object tracking and manipulation is estimation of
post-contact shear but this has received much less attention. Indeed,
post-contact shear has often been considered a "nuisance variable" and is
removed if possible because it can have an adverse effect on other types of
tactile perception such as contact pose estimation. This paper proposes a
tactile robotic system that can simultaneously estimate both the contact pose
and post-contact shear, and use this information to control its interaction
with other objects. Moreover, our new system is capable of interacting with
other objects in a smooth and continuous manner, unlike the stepwise,
position-controlled systems we have used in the past. We demonstrate the
capabilities of our new system using several different controller
configurations, on tasks including object tracking, surface following,
single-arm object pushing, and dual-arm object pushing.


------------------------------------------------------------------------------

Title:
Federated Learning-based Vehicle Trajectory Prediction against  Cyberattacks

Abstract: With the development of the Internet of Vehicles (IoV), vehicle wireless
communication poses serious cybersecurity challenges. Faulty information, such
as fake vehicle positions and speeds sent by surrounding vehicles, could cause
vehicle collisions, traffic jams, and even casualties. Additionally, private
vehicle data leakages, such as vehicle trajectory and user account information,
may damage user property and security. Therefore, achieving a
cyberattack-defense scheme in the IoV system with faulty data saturation is
necessary. This paper proposes a Federated Learning-based Vehicle Trajectory
Prediction Algorithm against Cyberattacks (FL-TP) to address the above
problems. The FL-TP is intensively trained and tested using a publicly
available Vehicular Reference Misbehavior (VeReMi) dataset with five types of
cyberattacks: constant, constant offset, random, random offset, and eventual
stop. The results show that the proposed FL-TP algorithm can improve
cyberattack detection and trajectory prediction by up to 6.99% and 54.86%,
respectively, under the maximum cyberattack permeability scenarios compared
with benchmark methods.


------------------------------------------------------------------------------

Title:
Hybrids of Constraint-based and Noise-based Algorithms for Causal  Discovery from Time Series

Abstract: Constraint-based and noise-based methods have been proposed to discover
summary causal graphs from observational time series under strong assumptions
which can be violated or impossible to verify in real applications. Recently, a
hybrid method (Assaad et al, 2021) that combines these two approaches, proved
to be robust to assumption violation. However, this method assumes that the
summary causal graph is acyclic, but cycles are common in many applications.
For example, in ecological communities, there may be cyclic relationships
between predator and prey populations, creating feedback loops. Therefore, this
paper presents two new frameworks for hybrids of constraint-based and
noise-based methods that can discover summary causal graphs that may or may not
contain cycles. For each framework, we provide two hybrid algorithms which are
experimentally tested on simulated data, realistic ecological data, and real
data from various applications. Experiments show that our hybrid approaches are
robust and yield good results over most datasets.


------------------------------------------------------------------------------

Title:
Scalable Neural-Probabilistic Answer Set Programming

Abstract: The goal of combining the robustness of neural networks and the
expressiveness of symbolic methods has rekindled the interest in Neuro-Symbolic
AI. Deep Probabilistic Programming Languages (DPPLs) have been developed for
probabilistic logic programming to be carried out via the probability
estimations of deep neural networks. However, recent SOTA DPPL approaches allow
only for limited conditional probabilistic queries and do not offer the power
of true joint probability estimation. In our work, we propose an easy
integration of tractable probabilistic inference within a DPPL. To this end, we
introduce SLASH, a novel DPPL that consists of Neural-Probabilistic Predicates
(NPPs) and a logic program, united via answer set programming (ASP). NPPs are a
novel design principle allowing for combining all deep model types and
combinations thereof to be represented as a single probabilistic predicate. In
this context, we introduce a novel $+/-$ notation for answering various types
of probabilistic queries by adjusting the atom notations of a predicate. To
scale well, we show how to prune the stochastically insignificant parts of the
(ground) program, speeding up reasoning without sacrificing the predictive
performance. We evaluate SLASH on a variety of different tasks, including the
benchmark task of MNIST addition and Visual Question Answering (VQA).


------------------------------------------------------------------------------

Title:
The MacWilliams Identity for the Hermitian Rank Metric

Abstract: Error-correcting codes have an important role in data storage and
transmission and in cryptography, particularly in the post-quantum era.
Hermitian matrices over finite fields and equipped with the rank metric have
the potential to offer enhanced security with greater efficiency in encryption
and decryption. One crucial tool for evaluating the error-correcting
capabilities of a code is its weight distribution and the MacWilliams Theorem
has long been used to identify this structure of new codes from their known
duals. Earlier papers have developed the MacWilliams Theorem for certain
classes of matrices in the form of a functional transformation, developed using
$q$-algebra, character theory and Generalised Krawtchouk polynomials, which is
easy to apply and also allows for moments of the weight distribution to be
found. In this paper, recent work by Kai-Uwe Schmidt on the properties of codes
based on Hermitian matrices such as bounds on their size and the eigenvalues of
their association scheme is extended by introducing a negative-$q$ algebra to
establish a MacWilliams Theorem in this form together with some of its
associated moments. The similarities in this approach and in the paper for the
Skew-Rank metric by Friedlander et al. have been emphasised to facilitate
future generalisation to any translation scheme.


------------------------------------------------------------------------------

Title:
An Exploratory Study of Masked Face Recognition with Machine Learning  Algorithms

Abstract: Automated face recognition is a widely adopted machine learning technology
for contactless identification of people in various processes such as automated
border control, secure login to electronic devices, community surveillance,
tracking school attendance, workplace clock in and clock out. Using face masks
have become crucial in our daily life with the recent world-wide COVID-19
pandemic. The use of face masks causes the performance of conventional face
recognition technologies to degrade considerably. The effect of mask-wearing in
face recognition is yet an understudied issue. In this paper, we address this
issue by evaluating the performance of a number of face recognition models
which are tested by identifying masked and unmasked face images. We use six
conventional machine learning algorithms, which are SVC, KNN, LDA, DT, LR and
NB, to find out the ones which perform best, besides the ones which poorly
perform, in the presence of masked face images. Local Binary Pattern (LBP) is
utilized as the feature extraction operator. We generated and used synthesized
masked face images. We prepared unmasked, masked, and half-masked training
datasets and evaluated the face recognition performance against both masked and
unmasked images to present a broad view of this crucial problem. We believe
that our study is unique in elaborating the mask-aware facial recognition with
almost all possible scenarios including half_masked-to-masked and
half_masked-to-unmasked besides evaluating a larger number of conventional
machine learning algorithms compared the other studies in the literature.


------------------------------------------------------------------------------

Title:
Reliable Evaluation of Adversarial Transferability

Abstract: Adversarial examples (AEs) with small adversarial perturbations can mislead
deep neural networks (DNNs) into wrong predictions. The AEs created on one DNN
can also fool another DNN. Over the last few years, the transferability of AEs
has garnered significant attention as it is a crucial property for facilitating
black-box attacks. Many approaches have been proposed to improve adversarial
transferability. However, they are mainly verified across different
convolutional neural network (CNN) architectures, which is not a reliable
evaluation since all CNNs share some similar architectural biases. In this
work, we re-evaluate 12 representative transferability-enhancing attack methods
where we test on 18 popular models from 4 types of neural networks. Our
reevaluation revealed that the adversarial transferability is often
overestimated, and there is no single AE that can be transferred to all popular
models. The transferability rank of previous attacking methods changes when
under our comprehensive evaluation. Based on our analysis, we propose a
reliable benchmark including three evaluation protocols. Adversarial
transferability on our new benchmark is extremely low, which further confirms
the overestimation of adversarial transferability. We release our benchmark at
this https URL to facilitate future research, which includes
code, model checkpoints, and evaluation protocols.


------------------------------------------------------------------------------

Title:
Gesper: A Restoration-Enhancement Framework for General Speech  Reconstruction

Abstract: This paper describes a real-time General Speech Reconstruction (Gesper)
system submitted to the ICASSP 2023 Speech Signal Improvement (SSI) Challenge.
This novel proposed system is a two-stage architecture, in which the speech
restoration is performed, and then cascaded by speech enhancement. We propose a
complex spectral mapping-based generative adversarial network (CSM-GAN) as the
speech restoration module for the first time. For noise suppression and
dereverberation, the enhancement module is performed with fullband-wideband
parallel processing. On the blind test set of ICASSP 2023 SSI Challenge, the
proposed Gesper system, which satisfies the real-time condition, achieves 3.27
P.804 overall mean opinion score (MOS) and 3.35 P.835 overall MOS, ranked 1st
in both track 1 and track 2.


------------------------------------------------------------------------------

Title:
A Unified Framework of Graph Information Bottleneck for Robustness and  Membership Privacy

Abstract: Graph Neural Networks (GNNs) have achieved great success in modeling
graph-structured data. However, recent works show that GNNs are vulnerable to
adversarial attacks which can fool the GNN model to make desired predictions of
the attacker. In addition, training data of GNNs can be leaked under membership
inference attacks. This largely hinders the adoption of GNNs in high-stake
domains such as e-commerce, finance and bioinformatics. Though investigations
have been made in conducting robust predictions and protecting membership
privacy, they generally fail to simultaneously consider the robustness and
membership privacy. Therefore, in this work, we study a novel problem of
developing robust and membership privacy-preserving GNNs. Our analysis shows
that Information Bottleneck (IB) can help filter out noisy information and
regularize the predictions on labeled samples, which can benefit robustness and
membership privacy. However, structural noises and lack of labels in node
classification challenge the deployment of IB on graph-structured data. To
mitigate these issues, we propose a novel graph information bottleneck
framework that can alleviate structural noises with neighbor bottleneck. Pseudo
labels are also incorporated in the optimization to minimize the gap between
the predictions on the labeled set and unlabeled set for membership privacy.
Extensive experiments on real-world datasets demonstrate that our method can
give robust predictions and simultaneously preserve membership privacy.


------------------------------------------------------------------------------

Title:
Self-supervised Learning and Graph Classification under Heterophily

Abstract: Self-supervised learning has shown its promising capability in graph
representation learning in recent work. Most existing pre-training strategies
usually choose the popular Graph neural networks (GNNs), which can be seen as a
special form of low-pass filter, fail to effectively capture heterophily. In
this paper, we first present an experimental investigation exploring the
performance of low-pass and high-pass filters in heterophily graph
classification, where the results clearly show that high-frequency signal is
important for learning heterophily graph representation. On the other hand, it
is still unclear how to effectively capture the structural pattern of graphs
and how to measure the capability of the self-supervised pre-training strategy
in capturing graph structure. To address the problem, we first design a
quantitative metric to Measure Graph Structure (MGS), which analyzes
correlation between structural similarity and embedding similarity of graph
pairs. Then, to enhance the graph structural information captured by
self-supervised learning, we propose a novel self-supervised strategy for
Pre-training GNNs based on the Metric (PGM). Extensive experiments validate our
pre-training strategy achieves state-of-the-art performance for molecular
property prediction and protein function prediction. In addition, we find
choosing the suitable filter sometimes may be better than designing good
pre-training strategies for heterophily graph classification.


------------------------------------------------------------------------------

Title:
GenImage: A Million-Scale Benchmark for Detecting AI-Generated Image

Abstract: The extraordinary ability of generative models to generate photographic
images has intensified concerns about the spread of disinformation, thereby
leading to the demand for detectors capable of distinguishing between
AI-generated fake images and real images. However, the lack of large datasets
containing images from the most advanced image generators poses an obstacle to
the development of such detectors. In this paper, we introduce the GenImage
dataset, which has the following advantages: 1) Plenty of Images, including
over one million pairs of AI-generated fake images and collected real images.
2) Rich Image Content, encompassing a broad range of image classes. 3)
State-of-the-art Generators, synthesizing images with advanced diffusion models
and GANs. The aforementioned advantages allow the detectors trained on GenImage
to undergo a thorough evaluation and demonstrate strong applicability to
diverse images. We conduct a comprehensive analysis of the dataset and propose
two tasks for evaluating the detection method in resembling real-world
scenarios. The cross-generator image classification task measures the
performance of a detector trained on one generator when tested on the others.
The degraded image classification task assesses the capability of the detectors
in handling degraded images such as low-resolution, blurred, and compressed
images. With the GenImage dataset, researchers can effectively expedite the
development and evaluation of superior AI-generated image detectors in
comparison to prevailing methodologies.


------------------------------------------------------------------------------

Title:
FTIO: Detecting I/O Periodicity Using Frequency Techniques

Abstract: Characterizing the temporal I/O behavior of an HPC application is a
challenging task, but informing the system about it can be valuable for
techniques such as I/O scheduling, burst buffer management, and many more,
especially if provided online. In this work, we focus on the most commonly
discussed temporal I/O behavior aspect: the periodicity of I/O. Specifically,
we propose to examine the periodicity of the I/O phases using a signal
processing technique, namely the Discrete Fourier Transform (DFT). Our
approach, named FTIO, also provides metrics that quantify how far from being
periodic the signal is, and hence represent yield confidence in the
DFT-obtained period. We validate our approach with large-scale experiments on a
productive system and examine its limitations extensively.


------------------------------------------------------------------------------

Title:
Zero-Shot 3D Shape Sketch View Similarity and Retrieval

Abstract: We conduct a detailed study of the ability of pretrained on pretext tasks ViT
and ResNet feature layers to quantify the similarity between pairs of 2D sketch
views of individual 3D shapes. We assess the performance in terms of the
models' abilities to retrieve similar views and ground-truth 3D shapes. Going
beyond naive zero-shot performance study, we investigate alternative
fine-tuning strategies on one or several shape classes, and their
generalization to other shape classes. Leveraging progress in NPR (Non-Photo
Realistic) rendering, we generate synthetic sketch views in several styles
which we use to fine-tune pretrained foundation models using contrastive
learning. We study how the scale of an object in a sketch affects the
similarity of features at different network layers. We observe that depending
on the scale, different feature layers can be more indicative of shape
similarities in sketch views. However, we find that similar object scales
result in the best performance of ViT and ResNet. In summary, we show that
careful selection of a fine-tuning strategy allows us to obtain consistent
improvement in zero-shot shape retrieval accuracy. We believe that our work
will have a significant impact on research in the sketch domain, providing
insights and guidance on how to adopt large pretrained models as perceptual
losses.


------------------------------------------------------------------------------

Title:
Simple Embodied Language Learning as a Byproduct of Meta-Reinforcement  Learning

Abstract: Whereas machine learning models typically learn language by directly training
on language tasks (e.g., next-word prediction), language emerges in human
children as a byproduct of solving non-language tasks (e.g., acquiring food).
Motivated by this observation, we ask: can embodied reinforcement learning (RL)
agents also indirectly learn language from non-language tasks? Learning to
associate language with its meaning requires a dynamic environment with varied
language. Therefore, we investigate this question in a multi-task environment
with language that varies across the different tasks. Specifically, we design
an office navigation environment, where the agent's goal is to find a
particular office, and office locations differ in different buildings (i.e.,
tasks). Each building includes a floor plan with a simple language description
of the goal office's location, which can be visually read as an RGB image when
visited. We find RL agents indeed are able to indirectly learn language. Agents
trained with current meta-RL algorithms successfully generalize to reading
floor plans with held-out layouts and language phrases, and quickly navigate to
the correct office, despite receiving no direct language supervision.


------------------------------------------------------------------------------

Title:
Recovering Headerless Frames in LR-FHSS

Abstract: Long-Range Frequency Hopping Spread Spectrum (LR-FHSS) is a recent modulation
designed for communications from low-power ground end-devices to Low-Earth
Orbit (LEO) satellites. To decode a frame, an LR-FHSS gateway must receive at
least one header replica and a large proportion of payload fragments. However,
LR-FHSS headers will likely be lost when there are many concurrent
transmissions. In this paper, we motivate the header loss problem with an
analytical model, propose a linear programming model to extract headerless
frames and design a cost-effective sliding-window heuristic. Simulation results
show that our approach exhibits near-optimal headerless detection and
extraction results while ensuring a low computational cost. The proposed method
is, therefore, suitable for future LR-FHSS gateways located onboard
resource-constrained LEO satellites.


------------------------------------------------------------------------------

Title:
A new computational perceived risk model for automated vehicles based on  potential collision avoidance difficulty (PCAD)

Abstract: Perceived risk is crucial in designing trustworthy and acceptable vehicle
automation systems. However, our understanding of its dynamics is limited, and
models for perceived risk dynamics are scarce in the literature. This study
formulates a new computational perceived risk model based on potential
collision avoidance difficulty (PCAD) for drivers of SAE level 2 driving
automation. PCAD uses the 2D safe velocity gap as the potential collision
avoidance difficulty, and takes into account collision severity. The safe
velocity gap is defined as the 2D gap between the current velocity and the safe
velocity region, and represents the amount of braking and steering needed,
considering behavioural uncertainty of neighbouring vehicles and imprecise
control of the subject vehicle. The PCAD predicts perceived risk both in
continuous time and per event. We compare the PCAD model with three
state-of-the-art models and analyse the models both theoretically and
empirically with two unique datasets: Dataset Merging and Dataset Obstacle
Avoidance. The PCAD model generally outperforms the other models in terms of
model error, detection rate, and the ability to accurately capture the
tendencies of human drivers' perceived risk, albeit at a longer computation
time. Additionally, the study shows that the perceived risk is not static and
varies with the surrounding traffic conditions. This research advances our
understanding of perceived risk in automated driving and paves the way for
improved safety and acceptance of driving automation systems.


------------------------------------------------------------------------------

Title:
DiffuDetox: A Mixed Diffusion Model for Text Detoxification

Abstract: Text detoxification is a conditional text generation task aiming to remove
offensive content from toxic text. It is highly useful for online forums and
social media, where offensive content is frequently encountered. Intuitively,
there are diverse ways to detoxify sentences while preserving their meanings,
and we can select from detoxified sentences before displaying text to users.
Conditional diffusion models are particularly suitable for this task given
their demonstrated higher generative diversity than existing conditional text
generation models based on language models. Nonetheless, text fluency declines
when they are trained with insufficient data, which is the case for this task.
In this work, we propose DiffuDetox, a mixed conditional and unconditional
diffusion model for text detoxification. The conditional model takes toxic text
as the condition and reduces its toxicity, yielding a diverse set of detoxified
sentences. The unconditional model is trained to recover the input text, which
allows the introduction of additional fluent text for training and thus ensures
text fluency. Extensive experimental results and in-depth analysis demonstrate
the effectiveness of our proposed DiffuDetox.


------------------------------------------------------------------------------

Title:
Selective Concept Models: Permitting Stakeholder Customisation at  Test-Time

Abstract: Concept-based models perform prediction using a set of concepts that are
interpretable to stakeholders. However, such models often involve a fixed,
large number of concepts, which may place a substantial cognitive load on
stakeholders. We propose Selective COncept Models (SCOMs) which make
predictions using only a subset of concepts and can be customised by
stakeholders at test-time according to their preferences. We show that SCOMs
only require a fraction of the total concepts to achieve optimal accuracy on
multiple real-world datasets. Further, we collect and release a new dataset,
CUB-Sel, consisting of human concept set selections for 900 bird images from
the popular CUB dataset. Using CUB-Sel, we show that humans have unique
individual preferences for the choice of concepts they prefer to reason about,
and struggle to identify the most theoretically informative concepts. The
customisation and concept selection provided by SCOM improves the efficiency of
interpretation and intervention for stakeholders.


------------------------------------------------------------------------------

Title:
Knowledge Distillation of Large Language Models

Abstract: Knowledge Distillation (KD) is a promising technique for reducing the high
computational demand of large language models (LLMs). However, previous KD
methods are primarily applied to white-box classification models or training
small models to imitate black-box model APIs like ChatGPT. How to effectively
distill the knowledge from white-box generative LLMs is still under-explored,
which becomes more and more important with the prosperity of LLMs. In this
work, we propose MiniLLM that distills smaller language models from generative
larger language models. We first replace the forward Kullback-Leibler
divergence (KLD) objective in the standard KD approaches with reverse KLD,
which is more suitable for KD on generative language models, to prevent the
student model from overestimating the low-probability regions of the teacher
distribution. Then, we derive an effective optimization approach to learn this
objective. Extensive experiments in the instruction-following setting show that
the MiniLLM models generate more precise responses with the higher overall
quality, lower exposure bias, better calibration, and higher long-text
generation performance. Our method is also scalable for different model
families with 120M to 13B parameters. We will release our code and model
checkpoints at this https URL


------------------------------------------------------------------------------

Title:
Global-Local Processing in Convolutional Neural Networks

Abstract: Convolutional Neural Networks (CNNs) have achieved outstanding performance on
image processing challenges. Actually, CNNs imitate the typically developed
human brain structures at the micro-level (Artificial neurons). At the same
time, they distance themselves from imitating natural visual perception in
humans at the macro architectures (high-level cognition). Recently it has been
investigated that CNNs are highly biased toward local features and fail to
detect the global aspects of their input. Nevertheless, the literature offers
limited clues on this problem. To this end, we propose a simple yet effective
solution inspired by the unconscious behavior of the human pupil. We devise a
simple module called Global Advantage Stream (GAS) to learn and capture the
holistic features of input samples (i.e., the global features). Then, the GAS
features were combined with a CNN network as a plug-and-play component called
the Global/Local Processing (GLP) model. The experimental results confirm that
this stream improves the accuracy with an insignificant additional
computational/temporal load and makes the network more robust to adversarial
attacks. Furthermore, investigating the interpretation of the model shows that
it learns a more holistic representation similar to the perceptual system of
healthy humans


------------------------------------------------------------------------------

Title:
UIERL: Internal-External Representation Learning Network for Underwater  Image Enhancement

Abstract: Underwater image enhancement (UIE) is a meaningful but challenging task, and
many learning-based UIE methods have been proposed in recent years. Although
much progress has been made, these methods still exist two issues: (1) There
exists a significant region-wise quality difference in a single underwater
image due to the underwater imaging process, especially in regions with
different scene depths. However, existing methods neglect this internal
characteristic of underwater images, resulting in inferior performance; (2) Due
to the uniqueness of the acquisition approach, underwater image acquisition
tools usually capture multiple images in the same or similar scenes. Thus, the
underwater images to be enhanced in practical usage are highly correlated.
However, when processing a single image, existing methods do not consider the
rich external information provided by the related images. There is still room
for improvement in their performance. Motivated by these two aspects, we
propose a novel internal-external representation learning (UIERL) network to
better perform UIE tasks with internal and external information,
simultaneously. In the internal representation learning stage, a new
depth-based region feature guidance network is designed, including a region
segmentation based on scene depth to sense regions with different quality
levels, followed by a region-wise space encoder module. With performing
region-wise feature learning for regions with different quality separately, the
network provides an effective guidance for global features and thus guides
intra-image differentiated enhancement. In the external representation learning
stage, we first propose an external information extraction network to mine the
rich external information in the related images. Then, internal and external
features interact with each other via the proposed external-assist-internal
module and internal-assist-e


------------------------------------------------------------------------------

Title:
FRIGATE: Frugal Spatio-temporal Forecasting on Road Networks

Abstract: Modelling spatio-temporal processes on road networks is a task of growing
importance. While significant progress has been made on developing
spatio-temporal graph neural networks (Gnns), existing works are built upon
three assumptions that are not practical on real-world road networks. First,
they assume sensing on every node of a road network. In reality, due to
budget-constraints or sensor failures, all locations (nodes) may not be
equipped with sensors. Second, they assume that sensing history is available at
all installed sensors. This is unrealistic as well due to sensor failures, loss
of packets during communication, etc. Finally, there is an assumption of static
road networks. Connectivity within networks change due to road closures,
constructions of new roads, etc. In this work, we develop FRIGATE to address
all these shortcomings. FRIGATE is powered by a spatio-temporal Gnn that
integrates positional, topological, and temporal information into rich
inductive node representations. The joint fusion of this diverse information is
made feasible through a novel combination of gated Lipschitz embeddings with
Lstms. We prove that the proposed Gnn architecture is provably more expressive
than message-passing Gnns used in state-of-the-art algorithms. The higher
expressivity of FRIGATE naturally translates to superior empirical performance
conducted on real-world network-constrained traffic data. In addition, FRIGATE
is robust to frugal sensor deployment, changes in road network connectivity,
and temporal irregularity in sensing.


------------------------------------------------------------------------------

Title:
Deep Gaussian Markov Random Fields for Graph-Structured Dynamical  Systems

Abstract: Probabilistic inference in high-dimensional state-space models is
computationally challenging. For many spatiotemporal systems, however, prior
knowledge about the dependency structure of state variables is available. We
leverage this structure to develop a computationally efficient approach to
state estimation and learning in graph-structured state-space models with
(partially) unknown dynamics and limited historical data. Building on recent
methods that combine ideas from deep learning with principled inference in
Gaussian Markov random fields (GMRF), we reformulate graph-structured
state-space models as Deep GMRFs defined by simple spatial and temporal graph
layers. This results in a flexible spatiotemporal prior that can be learned
efficiently from a single time sequence via variational inference. Under linear
Gaussian assumptions, we retain a closed-form posterior, which can be sampled
efficiently using the conjugate gradient method, scaling favourably compared to
classical Kalman filter based approaches


------------------------------------------------------------------------------

Title:
On Waldron Interpolation on a Simplex in $\mathbb{R}^d$

Abstract: We introduce explicit families of good interpolation points for interpolation
on a triangle in $\mathbb{R}^2$ that may be used for either polynomial
interpolation or a certain rational interpolation for which we give explicit
formulas.


------------------------------------------------------------------------------

Title:
It Is Rude to Ask a Sensor Its Age-of-Information: Status Updates  Against an Eavesdropping Node

Abstract: We consider periodical status updates between a transmitter and a legitimate
receiver, in the presence of an eavesdropper that is sometimes able to capture
pieces of information. We assume that, in the absence of such a threat, the
connection between the transmitter and the receiver is controlled by the
transmitter with the aim to minimize the age of information at the receiver's
side. However, if the presence of an eavesdropper is known, the transmitter may
further tune the generation rate of status updates to trade off the age of
information values acquired by the eavesdropper and the receiver, respectively.
To analyze this problem, we first propose a metric that combines both
objectives according to a Bergson social welfare framework, and then we solve
the problem of finding the optimal generation rate as a function of the
probability of data capture by the eavesdropper. This enables us to derive
notable and sometimes counter-intuitive conclusions, and possibly establish an
extension of the age of information framework to security aspects from a
performance evaluation perspective.


------------------------------------------------------------------------------

Title:
Combinatorial generation via permutation languages. VI. Binary trees

Abstract: In this paper we propose a notion of pattern avoidance in binary trees that
generalizes contiguous tree patterns studied by Rowland and non-contiguous tree
patterns studied by Dairyko, Pudwell, Tyner, and Wynn. Specifically, we propose
algorithms for generating different classes of binary trees that are
characterized by avoiding one or more of these generalized patterns. This is
achieved by applying the recent Hartung-Hoang-M\"utze-Williams generation
framework, by encoding binary trees via permutations. In particular, we
establish a one-to-one correspondence between tree patterns and certain mesh
permutation patterns. We also conduct a systematic investigation of all tree
patterns on at most 5 vertices, and we establish bijections between
pattern-avoiding binary trees and other combinatorial objects, in particular
pattern-avoiding lattice paths and set partitions.


------------------------------------------------------------------------------

Title:
Robust Optimization, Structure/Control co-design, Distributed  Optimization, Monolithic Optimization, Robust Control, Parametric Uncertainty

Abstract: This paper presents an end-to-end framework for robust structure/control
optimization of an industrial benchmark. When dealing with space structures, a
reduction of the spacecraft mass is paramount to minimize the mission cost and
maximize the propellant availability. However, a lighter design comes with a
bigger structural flexibility and the resulting impact on control performance.
Two optimization architectures (distributed and monolithic) are proposed in
order to face this issue. In particular the Linear Fractional Transformation
(LFT) framework is exploited to formally set the two optimization problems by
including parametric uncertainties. Large sets of uncertainties have to be
indeed taken into account in spacecraft control design due to the impossibility
to completely validate structural models in micro-gravity conditions with
on-ground experiments and to the evolution of spacecraft dynamics during the
mission (structure degradation and fuel consumption). In particular the
Two-Input Two-Output Port (TITOP) multi-body approach is used to build the
flexible dynamics in a minimal LFT form. The two proposed optimization
algorithms are detailed and their performance are compared on an ESA future
exploration mission, the ENVISION benchmark. With both approaches, an important
reduction of the mass is obtained by coping with the mission's control
performance/stability requirements and a large set of uncertainties.


------------------------------------------------------------------------------

Title:
A Proxy-Free Strategy for Practically Improving the Poisoning Efficiency  in Backdoor Attacks

Abstract: Poisoning efficiency is a crucial factor in poisoning-based backdoor attacks.
Attackers prefer to use as few poisoned samples as possible to achieve the same
level of attack strength, in order to remain undetected. Efficient triggers
have significantly improved poisoning efficiency, but there is still room for
improvement. Recently, selecting efficient samples has shown promise, but it
requires a proxy backdoor injection task to find an efficient poisoned sample
set, which can lead to performance degradation if the proxy attack settings are
different from the actual settings used by the victims. In this paper, we
propose a novel Proxy-Free Strategy (PFS) that selects efficient poisoned
samples based on individual similarity and set diversity, effectively
addressing this issue. We evaluate the proposed strategy on several datasets,
triggers, poisoning ratios, architectures, and training hyperparameters. Our
experimental results demonstrate that PFS achieves higher backdoor attack
strength while x500 faster than previous proxy-based selection approaches.


------------------------------------------------------------------------------

Title:
A note on the complexity of addition

Abstract: We show that the sum of a sequence of integers can be computed in linear time
on a Turing machine. In particular, the most obvious algorithm for this
problem, which appears to require quadratic time due to carry propagation,
actually runs in linear time by amortized analysis.


------------------------------------------------------------------------------

Title:
Batches Stabilize the Minimum Norm Risk in High Dimensional  Overparameterized Linear Regression

Abstract: Learning algorithms that divide the data into batches are prevalent in many
machine-learning applications, typically offering useful trade-offs between
computational efficiency and performance. In this paper, we examine the
benefits of batch-partitioning through the lens of a minimum-norm
overparameterized linear regression model with isotropic Gaussian features. We
suggest a natural small-batch version of the minimum-norm estimator, and derive
an upper bound on its quadratic risk, showing it is inversely proportional to
the noise level as well as to the overparameterization ratio, for the optimal
choice of batch size. In contrast to minimum-norm, our estimator admits a
stable risk behavior that is monotonically increasing in the
overparameterization ratio, eliminating both the blowup at the interpolation
point and the double-descent phenomenon. Interestingly, we observe that this
implicit regularization offered by the batch partition is partially explained
by feature overlap between the batches.
Our bound is derived via a novel combination of techniques, in particular
normal approximation in the Wasserstein metric of noisy projections over random
subspaces.


------------------------------------------------------------------------------

Title:
SWAM: Revisiting Swap and OOMK for Improving Application Responsiveness  on Mobile Devices

Abstract: Existing memory reclamation policies on mobile devices may be no longer valid
because they have negative effects on the response time of running
applications. In this paper, we propose SWAM, a new integrated memory
management technique that complements the shortcomings of both the swapping and
killing mechanism in mobile devices and improves the application
responsiveness. SWAM consists of (1) Adaptive Swap that performs swapping
adaptively into memory or storage device while managing the swap space
dynamically, (2) OOM Cleaner that reclaims shared object pages in the swap
space to secure available memory and storage space, and (3) EOOM Killer that
terminates processes in the worst case while prioritizing the lowest
initialization cost applications as victim processes first. Experimental
results demonstrate that SWAM significantly reduces the number of applications
killed by OOMK (6.5x lower), and improves application launch time (36% faster)
and response time (41% faster), compared to the conventional schemes.


------------------------------------------------------------------------------

Title:
ARCOR2: Framework for Collaborative End-User Management of Industrial  Robotic Workplaces using Augmented Reality

Abstract: This paper presents a novel framework enabling end-users to perform the
management of complex robotic workplaces using a tablet and augmented reality.
The framework allows users to commission the workplace comprising different
types of robots, machines, or services irrespective of the vendor, set
task-important points in space, specify program steps, generate a code, and
control its execution. More users can collaborate simultaneously, for instance,
within a large-scale workplace. Spatially registered visualization and
programming enable a fast and easy understanding of workplace processes, while
high precision is achieved by combining kinesthetic teaching with specific
graphical tools for relative manipulation of poses. A visually defined program
is for execution translated into Python representation, allowing efficient
involvement of experts. The system was designed and developed in cooperation
with a system integrator based on an offline printed circuit board testing use
case, and its user interface was evaluated multiple times during the
development. The latest evaluation was performed by three experts and indicates
the high potential of the solution.


------------------------------------------------------------------------------

Title:
Multi-target Backdoor Attacks for Code Pre-trained Models

Abstract: Backdoor attacks for neural code models have gained considerable attention
due to the advancement of code intelligence. However, most existing works
insert triggers into task-specific data for code-related downstream tasks,
thereby limiting the scope of attacks. Moreover, the majority of attacks for
pre-trained models are designed for understanding tasks. In this paper, we
propose task-agnostic backdoor attacks for code pre-trained models. Our
backdoored model is pre-trained with two learning strategies (i.e., Poisoned
Seq2Seq learning and token representation learning) to support the multi-target
attack of downstream code understanding and generation tasks. During the
deployment phase, the implanted backdoors in the victim models can be activated
by the designed triggers to achieve the targeted attack. We evaluate our
approach on two code understanding tasks and three code generation tasks over
seven datasets. Extensive experiments demonstrate that our approach can
effectively and stealthily attack code-related downstream tasks.


------------------------------------------------------------------------------

Title:
Achievable Rate Analysis of the STAR-RIS Aided NOMA Uplink in the Face  of Imperfect CSI and Hardware Impairments

Abstract: Reconfigurable intelligent surfaces (RIS) are capable of beneficially
ameliorating the propagation environment by appropriately controlling the
passive reflecting elements. To extend the coverage area, the concept of
simultaneous transmitting and reflecting reconfigurable intelligent surfaces
(STAR-RIS) has been proposed, yielding supporting 360^circ coverage user
equipment (UE) located on both sides of the RIS. In this paper, we
theoretically formulate the ergodic sum-rate of the STAR-RIS assisted
non-orthogonal multiple access (NOMA) uplink in the face of channel estimation
errors and hardware impairments (HWI). Specifically, the STAR-RIS phase shift
is configured based on the statistical channel state information (CSI),
followed by linear minimum mean square error (LMMSE) channel estimation of the
equivalent channel spanning from the UEs to the access point (AP). Afterwards,
successive interference cancellation (SIC) is employed at the AP using the
estimated instantaneous CSI, and we derive the theoretical ergodic sum-rate
upper bound for both perfect and imperfect SIC decoding algorithm. The
theoretical analysis and the simulation results show that both the channel
estimation and the ergodic sum-rate have performance floor at high transmit
power region caused by transceiver hardware impairments.


------------------------------------------------------------------------------

Title:
3-Dimensional Sonic Phase-invariant Echo Localization

Abstract: Parallax and Time-of-Flight (ToF) are often regarded as complementary in
robotic vision where various light and weather conditions remain challenges for
advanced camera-based 3-Dimensional (3-D) reconstruction. To this end, this
paper establishes Parallax among Corresponding Echoes (PaCE) to triangulate
acoustic ToF pulses from arbitrary sensor positions in 3-D space for the first
time. This is achieved through a novel round-trip reflection model that
pinpoints targets at the intersection of ellipsoids, which are spanned by
sensor locations and detected arrival times. Inter-channel echo association
becomes a crucial prerequisite for target detection and is learned from feature
similarity obtained by a stack of Siamese Multi-Layer Perceptrons (MLPs). The
PaCE algorithm enables phase-invariant 3-D object localization from only 1
isotropic emitter and at least 3 ToF receivers with relaxed sensor position
constraints. Experiments are conducted with airborne ultrasound sensor hardware
and back this hypothesis with quantitative results.


------------------------------------------------------------------------------

Title:
Solving inverse scattering problems via reduced-order model embedding  procedures

Abstract: We present a reduced-order model (ROM) methodology for inverse scattering
problems in which the reduced-order models are data-driven, i.e. they are
constructed directly from data gathered by sensors. Moreover, the entries of
the ROM contain localised information about the coefficients of the wave
equation.
We solve the inverse problem by embedding the ROM in physical space. Such an
approach is also followed in the theory of ``optimal grids,'' where the ROMs
are interpreted as two-point finite-difference discretisations of an underlying
set of equations of a first-order continuous system on this special grid. Here,
we extend this line of work to wave equations and introduce a new embedding
technique, which we call Krein embedding, since it is inspired by Krein's
seminal work on vibrations of a string. In this embedding approach, an adaptive
grid and a set of medium parameters can be directly extracted from a ROM and we
show that several limitations of optimal grid embeddings can be avoided.
Furthermore, we show how Krein embedding is connected to classical optimal grid
embedding and that convergence results for optimal grids can be extended to
this novel embedding approach. Finally, we also briefly discuss Krein embedding
for open domains, that is, semi-infinite domains that extend to infinity in one
direction.


------------------------------------------------------------------------------

Title:
PoetryDiffusion: Towards Joint Semantic and Metrical Manipulation in  Poetry Generation

Abstract: Poetry generation is a typical and popular task in natural language
generation. While prior works have shown success in controlling either semantic
or metrical aspects of poetry generation, there are still challenges in
addressing both perspectives simultaneously. In this paper, we employ the
Diffusion model to generate poetry in Sonnet and SongCi in Chinese for the
first time to tackle such challenges. Different from autoregressive generation,
our PoetryDiffusion model, based on Diffusion model, generates the complete
sentence or poetry by taking into account the whole sentence information,
resulting in improved semantic expression. Additionally, we incorporate a novel
metrical controller to manipulate and evaluate metrics (format and rhythm). The
denoising process in PoetryDiffusion allows for gradual enhancement of
semantics and flexible integration of the metrical controller. Experimental
results on two datasets demonstrate that our model outperforms existing models
in terms of semantic, metrical and overall performance.


------------------------------------------------------------------------------

Title:
Understanding Privacy Over-collection in WeChat Sub-app Ecosystem

Abstract: Nowadays the app-in-app paradigm is becoming increasingly popular, and
sub-apps have become an important form of mobile applications. WeChat, the
leading app-in-app platform, provides millions of sub-apps that can be used for
online shopping, financing, social networking, etc. However, privacy issues in
this new ecosystem have not been well understood. This paper performs the first
systematic study of privacy over-collection in sub-apps (denoted as SPO), where
sub-apps actually collect more privacy data than they claim in their privacy
policies. We propose a taxonomy of privacy for this ecosystem and a framework
named SPOChecker to automatically detect SPO in real-world sub-apps. Based on
SPOChecker, we collect 5,521 popular and representative WeChat sub-apps and
conduct a measurement study to understand SPO from three aspects: its
landscape, accountability, and defense methods. The result is worrisome, that
more than half of all studied sub-apps do not provide users with privacy
policies. Among 2,511 sub-apps that provide privacy policies, 489 (19.47%) of
them contain SPO. We look into the detailed characteristics of SPO, figure out
possible reasons and the responsibilities of stakeholders in the ecosystem, and
rethink current defense methods. The measurement leads to several insightful
findings that can help the community to better understand SPO and protect
privacy in sub-apps.


------------------------------------------------------------------------------

Title:
A reinforcement learning strategy for p-adaptation in high order solvers

Abstract: Reinforcement learning (RL) has emerged as a promising approach to automating
decision processes. This paper explores the application of RL techniques to
optimise the polynomial order in the computational mesh when using high-order
solvers. Mesh adaptation plays a crucial role in improving the efficiency of
numerical simulations by improving accuracy while reducing the cost. Here,
actor-critic RL models based on Proximal Policy Optimization offer a
data-driven approach for agents to learn optimal mesh modifications based on
evolving conditions.
The paper provides a strategy for p-adaptation in high-order solvers and
includes insights into the main aspects of RL-based mesh adaptation, including
the formulation of appropriate reward structures and the interaction between
the RL agent and the simulation environment. We discuss the impact of RL-based
mesh p-adaptation on computational efficiency and accuracy. We test the RL
p-adaptation strategy on a 1D inviscid Burgers' equation to demonstrate the
effectiveness of the strategy. The RL strategy reduces the computational cost
and improves accuracy over uniform adaptation, while minimising human
intervention.


------------------------------------------------------------------------------

Title:
"Definition Modeling: To model definitions." Generating Definitions With  Little to No Semantics

Abstract: Definition Modeling, the task of generating definitions, was first proposed
as a means to evaluate the semantic quality of word embeddings-a coherent
lexical semantic representations of a word in context should contain all the
information necessary to generate its definition. The relative novelty of this
task entails that we do not know which factors are actually relied upon by a
Definition Modeling system. In this paper, we present evidence that the task
may not involve as much semantics as one might expect: we show how an earlier
model from the literature is both rather insensitive to semantic aspects such
as explicit polysemy, as well as reliant on formal similarities between
headwords and words occurring in its glosses, casting doubt on the validity of
the task as a means to evaluate embeddings.


------------------------------------------------------------------------------

Title:
A Novel Channel-Constrained Model for 6G Vehicular Networks with Traffic  Spikes

Abstract: Mobile Edge Computing (MEC) holds excellent potential in Congestion
Management (CM) of 6G vehicular networks. A reasonable schedule of MEC ensures
a more reliable and efficient CM system. Unfortunately, existing parallel and
sequential models cannot cope with scarce computing resources and constrained
channels, especially during traffic rush hour. In this paper, we propose a
channel-constrained multi-core sequential model (CCMSM) for task offloading and
resource allocation. The CCMSM incorporates a utility index that couples system
energy consumption and delay, applying Genetic Algorithm combining Sparrow
Search Algorithm (GA-SSA) in the branching optimization. Furthermore, we prove
that the system delay is the shortest with the FCFS computing strategy in the
MEC server. Simulation demonstrates that the proposed CCMSM achieves a higher
optimization level and exhibits better robustness and resilient scalability for
traffic spikes.


------------------------------------------------------------------------------

Title:
A Practical Early-Stopped Technique and Analysis for BCH Decoding  Algorithm

Abstract: In this paper, a practical technique for the conventional
Berlekamp-Massey(BM) algorithm is provided to reduce the latency of decoding
and save decoding power by early termination or early-stopped checking. We
investigate the consecutive zero discrepancies during the decoding iteration
and make a decision to early stop the decoding process. This technique is
subject to decoding failure in exchange for the decoding latency. We analyze
our proposed technique by considering the weight distribution of BCH code and
estimating the bounds of undetected error probability as the event of enormous
stop checking. The proposed method is effective in numerical results and the
probability of decoding failure is lower than $10^{-119}$ for decoding 16383
code length of BCH codes. Furthermore, the complexity compared the conventional
early termination method with the proposed approach for decoding the long BCH
code. The proposed approach reduces the complexity of the conventional approach
by up to 80\%. As a result, the FPGA testing on a USB device validates the
reliability of the proposed method.


------------------------------------------------------------------------------

Title:
Recoverable Robust Optimization with Commitment

Abstract: We propose a model for recoverable robust optimization with commitment. Given
a combinatorial optimization problem and uncertainty about elements that may
fail, we ask for a robust solution that, after the failing elements are
revealed, can be augmented in a limited way. However, we commit to preserve the
remaining elements of the initial solution. We consider different
polynomial-time solvable combinatorial optimization problems and settle the
computational complexity of their robust counterparts with commitment. We show
for the weighted matroid basis problem that an optimal solution to the nominal
problem is also optimal for its robust counterpart. Indeed, matroids are
provably the only structures with this strong property. Robust counterparts of
other problems are NP-hard such as the matching and the stable set problem,
even in bipartite graphs. However, we establish polynomial-time algorithms for
the robust counterparts of the unweighted stable set problem in bipartite
graphs and the weighted stable set problem in interval graphs, also known as
the interval scheduling problem.


------------------------------------------------------------------------------

Title:
Quantum Computing Enhanced Distance-Minimizing Data-Driven Computational  Mechanics

Abstract: The distance-minimizing data-driven computational mechanics has great
potential in engineering applications by eliminating material modeling error
and uncertainty. In this computational framework, the solution-seeking
procedure relies on minimizing the distance between the constitutive database
and the conservation law. However, the distance calculation is time-consuming
and often takes up most of the computational time in the case of a huge
database. In this paper, we show how to use quantum computing to enhance
data-driven computational mechanics by exponentially reducing the computational
complexity of distance calculation. The proposed method is not only validated
on the quantum computer simulator Qiskit, but also on the real quantum computer
from OriginQ. We believe that this work represents a promising step towards
integrating quantum computing into data-driven computational mechanics.


------------------------------------------------------------------------------

Title:
Complete Visibility Algorithm for Autonomous Mobile Luminous Robots  under an Asynchronous Scheduler on Grid Plane

Abstract: An autonomous mobile robot system is a distributed system consisting of
mobile computational entities (called robots) that autonomously and repeatedly
perform three operations: Look, Compute, and Move. Various problems related to
autonomous mobile robots, such as gathering, pattern formation, or flocking,
have been extensively studied to understand the relationship between each
robot's capabilities and the solvability of these problems. In this study, we
focus on the complete visibility problem, which involves relocating all the
robots on an infinite grid plane such that each robot is visible to every other
robot. We assume that each robot is a luminous robot (i.e., has a light with a
constant number of colors) and opaque (not transparent). In this paper, we
propose an algorithm to achieve complete visibility when a set of robots is
given. The algorithm ensures that complete visibility is achieved even when
robots operate asynchronously and have no knowledge of the total number of
robots on the grid plane using only two colors.


------------------------------------------------------------------------------

Title:
DistSim: A performance model of large-scale hybrid distributed DNN  training

Abstract: With the ever-increasing computational demand of DNN training workloads,
distributed training has been widely adopted. A combination of data, model and
pipeline parallelism strategy, called hybrid parallelism distributed training,
is imported to tackle the problem of deploying large-scale models. However, how
to evaluate the hybrid strategy and the utilization of each device remains a
challenge since existing works either profile on a real large-scale cluster
with high time and money costs or only analyze a specific type of parallelism
without considering the hybrid parallelism. In this work, we proposed DistSim,
an event-based performance model to accurately analyze each device's
computation and communication activities with low profiling costs. DistDim
breaks down the model into events according to the given distributed strategy,
which can be profiled on two nodes. Then DistSim leverages the hierarchy of
different parallel strategies to generate the computation and communication
event-flow from layer level to model level and finally the activity timeline of
each device participating in training. Experiment shows that DistSim can reach
\revise{<4\%} errors when predicting distributing training batch time and
\revise{<5\%} errors when predicting a single device's activity time in various
hybrid strategy settings. We also provide a use-case of DistSim, automatically
evaluate and search the best distributed training strategy, and find a hybrid
strategy with at most $7.37\times$ throughput improvement.


------------------------------------------------------------------------------

Title:
Adaptive Modeling of Satellite-Derived Nighttime Lights Time-Series for  Tracking Urban Change Processes Using Machine Learning

Abstract: Remotely sensed nighttime lights (NTL) uniquely capture urban change
processes that are important to human and ecological well-being, such as
urbanization, socio-political conflicts and displacement, impacts from
disasters, holidays, and changes in daily human patterns of movement. Though
several NTL products are global in extent, intrinsic city-specific factors that
affect lighting, such as development levels, and social, economic, and cultural
characteristics, are unique to each city, making the urban processes embedded
in NTL signatures difficult to characterize, and limiting the scalability of
urban change analyses. In this study, we propose a data-driven approach to
detect urban changes from daily satellite-derived NTL data records that is
adaptive across cities and effective at learning city-specific temporal
patterns. The proposed method learns to forecast NTL signatures from past data
records using neural networks and allows the use of large volumes of unlabeled
data, eliminating annotation effort. Urban changes are detected based on
deviations of observed NTL from model forecasts using an anomaly detection
approach. Comparing model forecasts with observed NTL also allows identifying
the direction of change (positive or negative) and monitoring change severity
for tracking recovery. In operationalizing the model, we consider ten urban
areas from diverse geographic regions with dynamic NTL time-series and
demonstrate the generalizability of the approach for detecting the change
processes with different drivers and rates occurring within these urban areas
based on NTL deviation. This scalable approach for monitoring changes from
daily remote sensing observations efficiently utilizes large data volumes to
support continuous monitoring and decision making.


------------------------------------------------------------------------------

Title:
Skill-Critic: Refining Learned Skills for Reinforcement Learning

Abstract: Hierarchical reinforcement learning (RL) can accelerate long-horizon
decision-making by temporally abstracting a policy into multiple levels.
Promising results in sparse reward environments have been seen with skills,
i.e. sequences of primitive actions. Typically, a skill latent space and policy
are discovered from offline data, but the resulting low-level policy can be
unreliable due to low-coverage demonstrations or distribution shifts. As a
solution, we propose fine-tuning the low-level policy in conjunction with
high-level skill selection. Our Skill-Critic algorithm optimizes both the low
and high-level policies; these policies are also initialized and regularized by
the latent space learned from offline demonstrations to guide the joint policy
optimization. We validate our approach in multiple sparse RL environments,
including a new sparse reward autonomous racing task in Gran Turismo Sport. The
experiments show that Skill-Critic's low-level policy fine-tuning and
demonstration-guided regularization are essential for optimal performance.
Images and videos are available at this https URL
We plan to open source the code with the final version.


------------------------------------------------------------------------------

Title:
Bandits with Replenishable Knapsacks: the Best of both Worlds

Abstract: The bandits with knapsack (BwK) framework models online decision-making
problems in which an agent makes a sequence of decisions subject to resource
consumption constraints. The traditional model assumes that each action
consumes a non-negative amount of resources and the process ends when the
initial budgets are fully depleted. We study a natural generalization of the
BwK framework which allows non-monotonic resource utilization, i.e., resources
can be replenished by a positive amount. We propose a best-of-both-worlds
primal-dual template that can handle any online learning problem with
replenishment for which a suitable primal regret minimizer exists. In
particular, we provide the first positive results for the case of adversarial
inputs by showing that our framework guarantees a constant competitive ratio
$\alpha$ when $B=\Omega(T)$ or when the possible per-round replenishment is a
positive constant. Moreover, under a stochastic input model, our algorithm
yields an instance-independent $\tilde{O}(T^{1/2})$ regret bound which
complements existing instance-dependent bounds for the same setting. Finally,
we provide applications of our framework to some economic problems of practical
relevance.


------------------------------------------------------------------------------

Title:
A Contemporary Survey on 6G Wireless Networks: Potentials, Recent  Advances, Technical Challenges and Future Trends

Abstract: Smart services based on Internet of everything (IoE) are prophesied to reap
notable attention by both academia and industry in the future. Although
fifth-generation (5G) is a promising communication technology, however it
cannot fulfill complete demands of novel applications. Sixth-generation (6G)
technology is envisaged to overcome limitations of 5G technology. The vision
and planning of future 6G network has been started with this aim to meet the
stringent requirements of mobile communication. Our aim is to explore recent
advances and potential challenges to enable 6G technology in this review. We
have devised a taxonomy based on computing technologies, networking
technologies, communication technologies, use cases, machine learning
algorithms and key enabler technologies. In this regard, we subsequently
highlight potential features and key areas of 6G. Key technological
breakthroughs which include quantum communication, tactile communication,
holographic communication, terahertz communication, visible light communication
(VLC) Internet of Bio Nano Things, which can put profound impact on wireless
communication, have been elaborated at length in this review. In this review,
our prime focus is to discuss potential enabling technologies which can develop
seamless and sustainable network, encompassing symbiotic radio, blockchain, new
communication paradigm, VLC and terahertz. In addition, we have investigated
open research challenges which can hamper the performance of 6G network.
Finally, we have outlined several practical considerations, 6G key projects and
future directions. We envision 6G undergoing unprecedented breakthroughs to
eliminate technical uncertainties and provide enlightening research directions
for subsequent future studies. Although it is impossible to envisage complete
details of 6G, we believe this study will pave the way for future research
work.


------------------------------------------------------------------------------

Title:
Towards Balanced Active Learning for Multimodal Classification

Abstract: Training multimodal networks requires a vast amount of data due to their
larger parameter space compared to unimodal networks. Active learning is a
widely used technique for reducing data annotation costs by selecting only
those samples that could contribute to improving model performance. However,
current active learning strategies are mostly designed for unimodal tasks, and
when applied to multimodal data, they often result in biased sample selection
from the dominant modality. This unfairness hinders balanced multimodal
learning, which is crucial for achieving optimal performance. To address this
issue, we propose three guidelines for designing a more balanced multimodal
active learning strategy. Following these guidelines, a novel approach is
proposed to achieve more fair data selection by modulating the gradient
embedding with the dominance degree among modalities. Our studies demonstrate
that the proposed method achieves more balanced multimodal learning by avoiding
greedy sample selection from the dominant modality. Our approach outperforms
existing active learning strategies on a variety of multimodal classification
tasks. Overall, our work highlights the importance of balancing sample
selection in multimodal active learning and provides a practical solution for
achieving more balanced active learning for multimodal classification.


------------------------------------------------------------------------------

Title:
Compatibility of Fairness Metrics with EU Non-Discrimination Laws:  Demographic Parity & Conditional Demographic Disparity

Abstract: Empirical evidence suggests that algorithmic decisions driven by Machine
Learning (ML) techniques threaten to discriminate against legally protected
groups or create new sources of unfairness. This work supports the contextual
approach to fairness in EU non-discrimination legal framework and aims at
assessing up to what point we can assure legal fairness through fairness
metrics and under fairness constraints. For that, we analyze the legal notion
of non-discrimination and differential treatment with the fairness definition
Demographic Parity (DP) through Conditional Demographic Disparity (CDD). We
train and compare different classifiers with fairness constraints to assess
whether it is possible to reduce bias in the prediction while enabling the
contextual approach to judicial interpretation practiced under EU
non-discrimination laws. Our experimental results on three scenarios show that
the in-processing bias mitigation algorithm leads to different performances in
each of them. Our experiments and analysis suggest that AI-assisted
decision-making can be fair from a legal perspective depending on the case at
hand and the legal justification. These preliminary results encourage future
work which will involve further case studies, metrics, and fairness notions.


------------------------------------------------------------------------------

Title:
Improving Generalization in Meta-Learning via Meta-Gradient Augmentation

Abstract: Meta-learning methods typically follow a two-loop framework, where each loop
potentially suffers from notorious overfitting, hindering rapid adaptation and
generalization to new tasks. Existing schemes solve it by enhancing the
mutual-exclusivity or diversity of training samples, but these data
manipulation strategies are data-dependent and insufficiently flexible. This
work alleviates overfitting in meta-learning from the perspective of gradient
regularization and proposes a data-independent \textbf{M}eta-\textbf{G}radient
\textbf{Aug}mentation (\textbf{MGAug}) method. The key idea is to first break
the rote memories by network pruning to address memorization overfitting in the
inner loop, and then the gradients of pruned sub-networks naturally form the
high-quality augmentation of the meta-gradient to alleviate learner overfitting
in the outer loop. Specifically, we explore three pruning strategies, including
\textit{random width pruning}, \textit{random parameter pruning}, and a newly
proposed \textit{catfish pruning} that measures a Meta-Memorization Carrying
Amount (MMCA) score for each parameter and prunes high-score ones to break rote
memories as much as possible. The proposed MGAug is theoretically guaranteed by
the generalization bound from the PAC-Bayes framework. In addition, we extend a
lightweight version, called MGAug-MaxUp, as a trade-off between performance
gains and resource overhead. Extensive experiments on multiple few-shot
learning benchmarks validate MGAug's effectiveness and significant improvement
over various meta-baselines. The code is publicly available at
\url{this https URL}.


------------------------------------------------------------------------------

Title:
Accelerating Machine Learning Queries with Linear Algebra Query  Processing

Abstract: The rapid growth of large-scale machine learning (ML) models has led numerous
commercial companies to utilize ML models for generating predictive results to
help business decision-making. As two primary components in traditional
predictive pipelines, data processing, and model predictions often operate in
separate execution environments, leading to redundant engineering and
computations. Additionally, the diverging mathematical foundations of data
processing and machine learning hinder cross-optimizations by combining these
two components, thereby overlooking potential opportunities to expedite
predictive pipelines.
In this paper, we propose an operator fusing method based on GPU-accelerated
linear algebraic evaluation of relational queries. Our method leverages linear
algebra computation properties to merge operators in machine learning
predictions and data processing, significantly accelerating predictive
pipelines by up to 317x. We perform a complexity analysis to deliver
quantitative insights into the advantages of operator fusion, considering
various data and model dimensions. Furthermore, we extensively evaluate matrix
multiplication query processing utilizing the widely-used Star Schema
Benchmark. Through comprehensive evaluations, we demonstrate the effectiveness
and potential of our approach in improving the efficiency of data processing
and machine learning workloads on modern hardware.


------------------------------------------------------------------------------

Title:
Identification of Energy Management Configuration Concepts from a Set of  Pareto-optimal Solutions

Abstract: Optimizing building configurations for an efficient use of energy is
increasingly receiving attention by current research and several methods have
been developed to address this task. Selecting a suitable configuration based
on multiple conflicting objectives, such as initial investment cost, recurring
cost, robustness with respect to uncertainty of grid operation is, however, a
difficult multi-criteria decision making problem. Concept identification can
facilitate a decision maker by sorting configuration options into semantically
meaningful groups (concepts), further introducing constraints to meet trade-off
expectations for a selection of objectives. In this study, for a set of 20000
Pareto-optimal building energy management configurations, resulting from a
many-objective evolutionary optimization, multiple concept identification
iterations are conducted to provide a basis for making an informed investment
decision. In a series of subsequent analysis steps, it is shown how the choice
of description spaces, i.e., the partitioning of the features into sets for
which consistent and non-overlapping concepts are required, impacts the type of
information that can be extracted and that different setups of description
spaces illuminate several different aspects of the configuration data - an
important aspect that has not been addressed in previous work.


------------------------------------------------------------------------------

Title:
Unifying Large Language Models and Knowledge Graphs: A Roadmap

Abstract: Large language models (LLMs), such as ChatGPT and GPT4, are making new waves
in the field of natural language processing and artificial intelligence, due to
their emergent ability and generalizability. However, LLMs are black-box
models, which often fall short of capturing and accessing factual knowledge. In
contrast, Knowledge Graphs (KGs), Wikipedia and Huapu for example, are
structured knowledge models that explicitly store rich factual knowledge. KGs
can enhance LLMs by providing external knowledge for inference and
interpretability. Meanwhile, KGs are difficult to construct and evolving by
nature, which challenges the existing methods in KGs to generate new facts and
represent unseen knowledge. Therefore, it is complementary to unify LLMs and
KGs together and simultaneously leverage their advantages. In this article, we
present a forward-looking roadmap for the unification of LLMs and KGs. Our
roadmap consists of three general frameworks, namely, 1) KG-enhanced LLMs,
which incorporate KGs during the pre-training and inference phases of LLMs, or
for the purpose of enhancing understanding of the knowledge learned by LLMs; 2)
LLM-augmented KGs, that leverage LLMs for different KG tasks such as embedding,
completion, construction, graph-to-text generation, and question answering; and
3) Synergized LLMs + KGs, in which LLMs and KGs play equal roles and work in a
mutually beneficial way to enhance both LLMs and KGs for bidirectional
reasoning driven by both data and knowledge. We review and summarize existing
efforts within these three frameworks in our roadmap and pinpoint their future
research directions.


------------------------------------------------------------------------------

Title:
Towards Rigorous Design of OoD Detectors

Abstract: Out-of-distribution (OoD) detection techniques are instrumental for
safety-related neural networks. We are arguing, however, that current
performance-oriented OoD detection techniques geared towards matching metrics
such as expected calibration error, are not sufficient for establishing safety
claims. What is missing is a rigorous design approach for developing,
verifying, and validating OoD detectors. These design principles need to be
aligned with the intended functionality and the operational domain. Here, we
formulate some of the key technical challenges, together with a possible way
forward, for developing a rigorous and safety-related design methodology for
OoD detectors.


------------------------------------------------------------------------------

Title:
Carbon emissions and sustainability of launching 5G mobile networks in  China

Abstract: Since 2021, China has deployed more than 2.1 million 5G base stations to
increase the network capacity and provide ubiquitous digital connectivity for
mobile terminals. However, the launch of 5G networks also exacerbates the
misalignment between cellular traffic and energy consumption, which reduces
carbon efficiency - the amount of network traffic that can be delivered for
each unit of carbon emission. In this study, we develop a large-scale
data-driven framework to estimate the carbon emissions induced by mobile
networks. We show that the decline in carbon efficiency leads to a carbon
efficiency trap, estimated to cause additional carbon emissions of 23.82 +-
1.07 megatons in China. To mitigate the misalignment and improve energy
efficiency, we propose DeepEnergy, an energy-saving method leveraging
collaborative deep reinforcement learning and graph neural networks. DeepEnergy
models complex collaboration among cells, making it possible to effectively
coordinate the working state of tens of thousands of cells, which could help
over 71% of Chinese provinces avoid carbon efficiency traps. In addition,
applying DeepEnergy is estimated to reduce 20.90 +- 0.98 megatons of carbon
emissions at the national level in 2023. We further assess the effects of
adopting renewable energy and discover that the mobile network could accomplish
more than 50% of its net-zero goal by integrating DeepEnergy and solar energy
systems. Our study provides insight into carbon emission mitigation in 5G
network infrastructure launching in China and overworld, paving the way towards
achieving sustainable development goals and future net-zero mobile networks.


------------------------------------------------------------------------------

Title:
Fairness and Privacy-Preserving in Federated Learning: A Survey

Abstract: Federated learning (FL) as distributed machine learning has gained popularity
as privacy-aware Machine Learning (ML) systems have emerged as a technique that
prevents privacy leakage by building a global model and by conducting
individualized training of decentralized edge clients on their own private
data. The existing works, however, employ privacy mechanisms such as Secure
Multiparty Computing (SMC), Differential Privacy (DP), etc. Which are immensely
susceptible to interference, massive computational overhead, low accuracy, etc.
With the increasingly broad deployment of FL systems, it is challenging to
ensure fairness and maintain active client participation in FL systems. Very
few works ensure reasonably satisfactory performances for the numerous diverse
clients and fail to prevent potential bias against particular demographics in
FL systems. The current efforts fail to strike a compromise between privacy,
fairness, and model performance in FL systems and are vulnerable to a number of
additional problems. In this paper, we provide a comprehensive survey stating
the basic concepts of FL, the existing privacy challenges, techniques, and
relevant works concerning privacy in FL. We also provide an extensive overview
of the increasing fairness challenges, existing fairness notions, and the
limited works that attempt both privacy and fairness in FL. By comprehensively
describing the existing FL systems, we present the potential future directions
pertaining to the challenges of privacy-preserving and fairness-aware FL
systems.


------------------------------------------------------------------------------

Title:
Strong regulatory graphs

Abstract: Logical modeling is a powerful tool in biology, offering a system-level
understanding of the complex interactions that govern biological processes. A
gap that hinders the scalability of logical models is the need to specify the
update function of every vertex in the network depending on the status of its
predecessors. To address this, we introduce in this paper the concept of strong
regulation, where a vertex is only updated to active/inactive if all its
predecessors agree in their influences; otherwise, it is set to ambiguous. We
explore the interplay between active, inactive, and ambiguous influences in a
network. We discuss the existence of phenotype attractors in such networks,
where the status of some of the variables is fixed to active/inactive, while
the others can have an arbitrary status, including ambiguous.


------------------------------------------------------------------------------

Title:
Measures and Optimization for Robustness and Vulnerability in  Disconnected Networks

Abstract: The function or performance of a network is strongly dependent on its
robustness, quantifying the ability of the network to continue functioning
under perturbations. While a wide variety of robustness metrics have been
proposed, they have their respective limitations. In this paper, we propose to
use the forest index as a measure of network robustness, which overcomes the
deficiencies of existing metrics. Using such a measure as an optimization
criterion, we propose and study the problem of breaking down a network by
attacking some key edges. We show that the objective function of the problem is
monotonic but not submodular, which impose more challenging on the problem. We
thus resort to greedy algorithms extended for non-submodular functions by
iteratively deleting the most promising edges. We first propose a simple greedy
algorithm with a proved bound for the approximation ratio and cubic-time
complexity. To confront the computation challenge for large networks, we
further propose an improved nearly-linear time greedy algorithm, which
significantly speeds up the process for edge selection but sacrifices little
accuracy. Extensive experimental results for a large set of real-world networks
verify the effectiveness and efficiency of our algorithms, demonstrating that
our algorithms outperform several baseline schemes.


------------------------------------------------------------------------------

Title:
C$^3$PS: Context-aware Conditional Cross Pseudo Supervision for  Semi-supervised Medical Image Segmentation

Abstract: Semi-supervised learning (SSL) methods, which can leverage a large amount of
unlabeled data for improved performance, has attracted increasing attention
recently. In this paper, we introduce a novel Context-aware Conditional Cross
Pseudo Supervision method (referred as C$^3$PS) for semi-supervised medical
image segmentation. Unlike previously published Cross Pseudo Supervision (CPS)
works, this paper introduces a novel Conditional Cross Pseudo Supervision
(CCPS) mechanism where the cross pseudo supervision is conditioned on a given
class label. Context-awareness is further introduced in the CCPS to improve the
quality of pseudo-labels for cross pseudo supervision. The proposed method has
the additional advantage that in the later training stage, it can focus on the
learning of hard organs. Validated on two typical yet challenging medical image
segmentation tasks, our method demonstrates superior performance over the
state-of-the-art methods.


------------------------------------------------------------------------------

Title:
SaDI: A Self-adaptive Decomposed Interpretable Framework for Electric  Load Forecasting under Extreme Events

Abstract: Accurate prediction of electric load is crucial in power grid planning and
management. In this paper, we solve the electric load forecasting problem under
extreme events such as scorching heats. One challenge for accurate forecasting
is the lack of training samples under extreme conditions. Also load usually
changes dramatically in these extreme conditions, which calls for interpretable
model to make better decisions. In this paper, we propose a novel forecasting
framework, named Self-adaptive Decomposed Interpretable framework~(SaDI), which
ensembles long-term trend, short-term trend, and period modelings to capture
temporal characteristics in different components. The external variable
triggered loss is proposed for the imbalanced learning under extreme events.
Furthermore, Generalized Additive Model (GAM) is employed in the framework for
desirable interpretability. The experiments on both Central China electric load
and public energy meters from buildings show that the proposed SaDI framework
achieves average 22.14% improvement compared with the current state-of-the-art
algorithms in forecasting under extreme events in terms of daily mean of
normalized RMSE. Code, Public datasets, and Appendix are available at:
this https URL .


------------------------------------------------------------------------------

Title:
Cryptography approach for Secure Outsourced Data Storage in Cloud  Environment

Abstract: A large amount of data and applications are migrated by researchers,
stakeholders, academia, and business organizations to the cloud environment due
to its large variety of services, which involve the least maintenance cost,
maximum flexibility, and on-demand service for storage, computation, and data
distribution intentions. Despite the various characteristics the cloud
environment supports, it also faces many challenges. However, data users may
not completely trust a cloud environment that is engaged by a third party.
Every cloud user always has a prime concern, i.e., security. Numerous methods
have been designed to solve the issue of data security during data storage,
calculation, and sharing across stakeholders and users. Nevertheless, there is
a lack of existing methods that tackle the issue of the security of data when
it is stored in a cloud environment. This article presents a precise security
method that has handled the security of data while it is being shared and
stored in the cloud. These methods have been utilized to lessen security
assaults and prevent unauthorized parties from accessing the actual data. The
article is concluded with some limitations and recommendations for the future
in terms of secure data retention and distribution.


------------------------------------------------------------------------------

Title:
Mediated Multi-Agent Reinforcement Learning

Abstract: The majority of Multi-Agent Reinforcement Learning (MARL) literature equates
the cooperation of self-interested agents in mixed environments to the problem
of social welfare maximization, allowing agents to arbitrarily share rewards
and private information. This results in agents that forgo their individual
goals in favour of social good, which can potentially be exploited by selfish
defectors. We argue that cooperation also requires agents' identities and
boundaries to be respected by making sure that the emergent behaviour is an
equilibrium, i.e., a convention that no agent can deviate from and receive
higher individual payoffs. Inspired by advances in mechanism design, we propose
to solve the problem of cooperation, defined as finding socially beneficial
equilibrium, by using mediators. A mediator is a benevolent entity that may act
on behalf of agents, but only for the agents that agree to it. We show how a
mediator can be trained alongside agents with policy gradient to maximize
social welfare subject to constraints that encourage agents to cooperate
through the mediator. Our experiments in matrix and iterative games highlight
the potential power of applying mediators in MARL.


------------------------------------------------------------------------------

Title:
TryOnDiffusion: A Tale of Two UNets

Abstract: Given two images depicting a person and a garment worn by another person, our
goal is to generate a visualization of how the garment might look on the input
person. A key challenge is to synthesize a photorealistic detail-preserving
visualization of the garment, while warping the garment to accommodate a
significant body pose and shape change across the subjects. Previous methods
either focus on garment detail preservation without effective pose and shape
variation, or allow try-on with the desired shape and pose but lack garment
details. In this paper, we propose a diffusion-based architecture that unifies
two UNets (referred to as Parallel-UNet), which allows us to preserve garment
details and warp the garment for significant pose and body change in a single
network. The key ideas behind Parallel-UNet include: 1) garment is warped
implicitly via a cross attention mechanism, 2) garment warp and person blend
happen as part of a unified process as opposed to a sequence of two separate
tasks. Experimental results indicate that TryOnDiffusion achieves
state-of-the-art performance both qualitatively and quantitatively.


------------------------------------------------------------------------------

Title:
Patterns of Patterns II

Abstract: We review how our earlier theorization of pattern methods fares in the wild.
The "wild" here included a graduate school classroom in New York, a workshop at
a transdisciplinary conference in Arizona, a nascent citizen science project in
Bristol, and a professional development day for a university in Oxford. We
encountered unexpected challenges such as working with students in a HyFlex
classroom, getting conference attendees to feel comfortable evaluating the
conference they were presently attending, and adapting our plans on the fly
when leading workshops with surprising attendee responses. We describe and
refine patterns specifications that will help other practitioners of patterns
in their own forays into the wild.


------------------------------------------------------------------------------

Title:
The Secretary Problem with Predictions

Abstract: The value maximization version of the secretary problem is the problem of
hiring a candidate with the largest value from a randomly ordered sequence of
candidates. In this work, we consider a setting where predictions of candidate
values are provided in advance. We propose an algorithm that achieves a nearly
optimal value if the predictions are accurate and results in a constant-factor
competitive ratio otherwise. We also show that the worst-case competitive ratio
of an algorithm cannot be higher than some constant $< 1/\mathrm{e}$, which is
the best possible competitive ratio when we ignore predictions, if the
algorithm performs nearly optimally when the predictions are accurate.
Additionally, for the multiple-choice secretary problem, we propose an
algorithm with a similar theoretical guarantee. We empirically illustrate that
if the predictions are accurate, the proposed algorithms perform well;
meanwhile, if the predictions are inaccurate, performance is comparable to
existing algorithms that do not use predictions.


------------------------------------------------------------------------------

Title:
Research on Named Entity Recognition in Improved transformer with R-Drop  structure

Abstract: To enhance the generalization ability of the model and improve the
effectiveness of the transformer for named entity recognition tasks, the
XLNet-Transformer-R model is proposed in this paper. The XLNet pre-trained
model and the Transformer encoder with relative positional encodings are
combined to enhance the model's ability to process long text and learn
contextual information to improve robustness. To prevent overfitting, the
R-Drop structure is used to improve the generalization capability and enhance
the accuracy of the model in named entity recognition tasks. The model in this
paper performs ablation experiments on the MSRA dataset and comparison
experiments with other models on four datasets with excellent performance,
demonstrating the strategic effectiveness of the XLNet-Transformer-R model.


------------------------------------------------------------------------------

Title:
Measuring and Controlling Divisiveness in Rank Aggregation

Abstract: In rank aggregation, members of a population rank issues to decide which are
collectively preferred. We focus instead on identifying divisive issues that
express disagreements among the preferences of individuals. We analyse the
properties of our divisiveness measures and their relation to existing notions
of polarisation. We also study their robustness under incomplete preferences
and algorithms for control and manipulation of divisiveness. Our results
advance our understanding of how to quantify disagreements in collective
decision-making.


------------------------------------------------------------------------------

Title:
System Information Decomposition

Abstract: In order to characterize complex higher-order interactions among variables in
a system, we introduce a new framework for decomposing the information entropy
of variables in a system, termed System Information Decomposition (SID).
Diverging from Partial Information Decomposition (PID) correlation methods,
which quantify the interaction between a single target variable and a
collection of source variables, SID extends those approaches by equally
examining the interactions among all system variables. Specifically, we
establish the robustness of the SID framework by proving all the information
atoms are symmetric, which detaches the unique, redundant, and synergistic
information from the specific target variable, empowering them to describe the
relationship among variables. Additionally, we analyze the relationship between
SID and existing information measures and propose several properties that SID
quantitative methods should follow. Furthermore, by employing an illustrative
example, we demonstrate that SID uncovers a higher-order interaction
relationships among variables that cannot be captured by current measures of
probability and information and provide two approximate calculation methods
verified by this case. This advance in higher-order measures enables SID to
explain why Holism posits that some systems cannot be decomposed without loss
of characteristics under existing measures, and offers a potential quantitative
framework for higher-order relationships across a broad spectrum of
disciplines.


------------------------------------------------------------------------------

Title:
Building a Corpus for Biomedical Relation Extraction of Species Mentions

Abstract: We present a manually annotated corpus, Species-Species Interaction, for
extracting meaningful binary relations between species, in biomedical texts, at
sentence level, with a focus on the gut microbiota. The corpus leverages
PubTator to annotate species in full-text articles after evaluating different
Named Entity Recognition species taggers. Our first results are promising for
extracting relations between species using BERT and its biomedical variants.


------------------------------------------------------------------------------

Title:
Object Detection in Hyperspectral Image via Unified Spectral-Spatial  Feature Aggregation

Abstract: Deep learning-based hyperspectral image (HSI) classification and object
detection techniques have gained significant attention due to their vital role
in image content analysis, interpretation, and wider HSI applications. However,
current hyperspectral object detection approaches predominantly emphasize
either spectral or spatial information, overlooking the valuable complementary
relationship between these two aspects. In this study, we present a novel
\textbf{S}pectral-\textbf{S}patial \textbf{A}ggregation (S2ADet) object
detector that effectively harnesses the rich spectral and spatial complementary
information inherent in hyperspectral images. S2ADet comprises a hyperspectral
information decoupling (HID) module, a two-stream feature extraction network,
and a one-stage detection head. The HID module processes hyperspectral images
by aggregating spectral and spatial information via band selection and
principal components analysis, consequently reducing redundancy. Based on the
acquired spatial and spectral aggregation information, we propose a feature
aggregation two-stream network for interacting spectral-spatial features.
Furthermore, to address the limitations of existing databases, we annotate an
extensive dataset, designated as HOD3K, containing 3,242 hyperspectral images
captured across diverse real-world scenes and encompassing three object
classes. These images possess a resolution of 512x256 pixels and cover 16 bands
ranging from 470 nm to 620 nm. Comprehensive experiments on two datasets
demonstrate that S2ADet surpasses existing state-of-the-art methods, achieving
robust and reliable results. The demo code and dataset of this work are
publicly available at \url{this https URL}.


------------------------------------------------------------------------------

Title:
Research on an improved Conformer end-to-end Speech Recognition Model  with R-Drop Structure

Abstract: To address the issue of poor generalization ability in end-to-end speech
recognition models within deep learning, this study proposes a new
Conformer-based speech recognition model called "Conformer-R" that incorporates
the R-drop structure. This model combines the Conformer model, which has shown
promising results in speech recognition, with the R-drop structure. By doing
so, the model is able to effectively model both local and global speech
information while also reducing overfitting through the use of the R-drop
structure. This enhances the model's ability to generalize and improves overall
recognition efficiency. The model was first pre-trained on the Aishell1 and
Wenetspeech datasets for general domain adaptation, and subsequently fine-tuned
on computer-related audio data. Comparison tests with classic models such as
LAS and Wenet were performed on the same test set, demonstrating the
Conformer-R model's ability to effectively improve generalization.


------------------------------------------------------------------------------

Title:
Distribution Shift Inversion for Out-of-Distribution Prediction

Abstract: Machine learning society has witnessed the emergence of a myriad of
Out-of-Distribution (OoD) algorithms, which address the distribution shift
between the training and the testing distribution by searching for a unified
predictor or invariant feature representation. However, the task of directly
mitigating the distribution shift in the unseen testing set is rarely
investigated, due to the unavailability of the testing distribution during the
training phase and thus the impossibility of training a distribution translator
mapping between the training and testing distribution. In this paper, we
explore how to bypass the requirement of testing distribution for distribution
translator training and make the distribution translation useful for OoD
prediction. We propose a portable Distribution Shift Inversion algorithm, in
which, before being fed into the prediction model, the OoD testing samples are
first linearly combined with additional Gaussian noise and then transferred
back towards the training distribution using a diffusion model trained only on
the source distribution. Theoretical analysis reveals the feasibility of our
method. Experimental results, on both multiple-domain generalization datasets
and single-domain generalization datasets, show that our method provides a
general performance gain when plugged into a wide range of commonly used OoD
algorithms.


------------------------------------------------------------------------------

Title:
Extracting Information from Twitter Screenshots

Abstract: Screenshots are prevalent on social media as a common approach for
information sharing. Users rarely verify before sharing a screenshot whether
the post it contains is fake or real. Information sharing through fake
screenshots can be highly responsible for misinformation and disinformation
spread on social media. Our ultimate goal is to develop a tool that could take
a screenshot of a tweet and provide a probability that the tweet is real, using
resources found on the live web and in web archives. This paper provides
methods for extracting the tweet text, timestamp, and Twitter handle from a
screenshot of a tweet.


------------------------------------------------------------------------------

Title:
Multimodal Optimal Transport-based Co-Attention Transformer with Global  Structure Consistency for Survival Prediction

Abstract: Survival prediction is a complicated ordinal regression task that aims to
predict the ranking risk of death, which generally benefits from the
integration of histology and genomic data. Despite the progress in joint
learning from pathology and genomics, existing methods still suffer from
challenging issues: 1) Due to the large size of pathological images, it is
difficult to effectively represent the gigapixel whole slide images (WSIs). 2)
Interactions within tumor microenvironment (TME) in histology are essential for
survival analysis. Although current approaches attempt to model these
interactions via co-attention between histology and genomic data, they focus on
only dense local similarity across modalities, which fails to capture global
consistency between potential structures, i.e. TME-related interactions of
histology and co-expression of genomic data. To address these challenges, we
propose a Multimodal Optimal Transport-based Co-Attention Transformer framework
with global structure consistency, in which optimal transport (OT) is applied
to match patches of a WSI and genes embeddings for selecting informative
patches to represent the gigapixel WSI. More importantly, OT-based co-attention
provides a global awareness to effectively capture structural interactions
within TME for survival prediction. To overcome high computational complexity
of OT, we propose a robust and efficient implementation over micro-batch of WSI
patches by approximating the original OT with unbalanced mini-batch OT.
Extensive experiments show the superiority of our method on five benchmark
datasets compared to the state-of-the-art methods. The code is released.


------------------------------------------------------------------------------

Title:
LiveChat: A Large-Scale Personalized Dialogue Dataset Automatically  Constructed from Live Streaming

Abstract: Open-domain dialogue systems have made promising progress in recent years.
While the state-of-the-art dialogue agents are built upon large-scale
text-based social media data and large pre-trained models, there is no
guarantee these agents could also perform well in fast-growing scenarios, such
as live streaming, due to the bounded transferability of pre-trained models and
biased distributions of public datasets from Reddit and Weibo, etc. To improve
the essential capability of responding and establish a benchmark in the live
open-domain scenario, we introduce the LiveChat dataset, composed of 1.33
million real-life Chinese dialogues with almost 3800 average sessions across
351 personas and fine-grained profiles for each persona. LiveChat is
automatically constructed by processing numerous live videos on the Internet
and naturally falls within the scope of multi-party conversations, where the
issues of Who says What to Whom should be considered. Therefore, we target two
critical tasks of response modeling and addressee recognition and propose
retrieval-based baselines grounded on advanced techniques. Experimental results
have validated the positive effects of leveraging persona profiles and larger
average sessions per persona. In addition, we also benchmark the
transferability of advanced generation-based models on LiveChat and pose some
future directions for current challenges.


------------------------------------------------------------------------------

Title:
Chart2Vec: A Universal Embedding of Context-Aware Visualizations

Abstract: The advances in AI-enabled techniques have accelerated the creation and
automation of visualizations in the past decade. However, presenting
visualizations in a descriptive and generative format is still challenging.
Moreover, current visualization embedding methods focus on standalone
visualizations, neglecting the importance of contextual information for
multi-view visualizations. To address this issue, we propose a new
representation model, Chart2Vec, to learn a universal embedding of
visualizations with context-aware information. Chart2Vec aims to support a wide
range of downstream visualization tasks such as recommendation and
storytelling. Our model considers both structural and semantic information of
visualizations in declarative specifications. To enhance the context-aware
capability, Chart2Vec employs multi-task learning on both supervised and
unsupervised tasks concerning the co-occurrence of visualizations. We evaluate
our method through an ablation study, a user study, and a quantitative
comparison. The results verified the consistency of our embedding method with
human cognition and showed its advantages over existing methods.


------------------------------------------------------------------------------

Title:
Deblurring Masked Autoencoder is Better Recipe for Ultrasound Image  Recognition

Abstract: Masked autoencoder (MAE) has attracted unprecedented attention and achieves
remarkable performance in many vision tasks. It reconstructs random masked
image patches (known as proxy task) during pretraining and learns meaningful
semantic representations that can be transferred to downstream tasks. However,
MAE has not been thoroughly explored in ultrasound imaging. In this work, we
investigate the potential of MAE for ultrasound image recognition. Motivated by
the unique property of ultrasound imaging in high noise-to-signal ratio, we
propose a novel deblurring MAE approach that incorporates deblurring into the
proxy task during pretraining. The addition of deblurring facilitates the
pretraining to better recover the subtle details presented in the ultrasound
images, thus improving the performance of the downstream classification task.
Our experimental results demonstrate the effectiveness of our deblurring MAE,
achieving state-of-the-art performance in ultrasound image classification.
Overall, our work highlights the potential of MAE for ultrasound image
recognition and presents a novel approach that incorporates deblurring to
further improve its effectiveness.


------------------------------------------------------------------------------

Title:
Semi-supervised Cell Recognition under Point Supervision

Abstract: Cell recognition is a fundamental task in digital histopathology image
analysis. Point-based cell recognition (PCR) methods normally require a vast
number of annotations, which is extremely costly, time-consuming and
labor-intensive. Semi-supervised learning (SSL) can provide a shortcut to make
full use of cell information in gigapixel whole slide images without exhaustive
labeling. However, research into semi-supervised point-based cell recognition
(SSPCR) remains largely overlooked. Previous SSPCR works are all built on
density map-based PCR models, which suffer from unsatisfactory accuracy, slow
inference speed and high sensitivity to hyper-parameters. To address these
issues, end-to-end PCR models are proposed recently. In this paper, we develop
a SSPCR framework suitable for the end-to-end PCR models for the first time.
Overall, we use the current models to generate pseudo labels for unlabeled
images, which are in turn utilized to supervise the models training. Besides,
we introduce a co-teaching strategy to overcome the confirmation bias problem
that generally exists in self-training. A distribution alignment technique is
also incorporated to produce high-quality, unbiased pseudo labels for unlabeled
data. Experimental results on four histopathology datasets concerning different
types of staining styles show the effectiveness and versatility of the proposed
framework. Code is available at
\textcolor{magenta}{\url{this https URL}


------------------------------------------------------------------------------

Title:
Kalman Filter for Online Classification of Non-Stationary Data

Abstract: In Online Continual Learning (OCL) a learning system receives a stream of
data and sequentially performs prediction and training steps. Important
challenges in OCL are concerned with automatic adaptation to the particular
non-stationary structure of the data, and with quantification of predictive
uncertainty. Motivated by these challenges we introduce a probabilistic
Bayesian online learning model by using a (possibly pretrained) neural
representation and a state space model over the linear predictor weights.
Non-stationarity over the linear predictor weights is modelled using a
parameter drift transition density, parametrized by a coefficient that
quantifies forgetting. Inference in the model is implemented with efficient
Kalman filter recursions which track the posterior distribution over the linear
weights, while online SGD updates over the transition dynamics coefficient
allows to adapt to the non-stationarity seen in data. While the framework is
developed assuming a linear Gaussian model, we also extend it to deal with
classification problems and for fine-tuning the deep learning representation.
In a set of experiments in multi-class classification using data sets such as
CIFAR-100 and CLOC we demonstrate the predictive ability of the model and its
flexibility to capture non-stationarity.


------------------------------------------------------------------------------

Title:
A semantically enhanced dual encoder for aspect sentiment triplet  extraction

Abstract: Aspect sentiment triplet extraction (ASTE) is a crucial subtask of
aspect-based sentiment analysis (ABSA) that aims to comprehensively identify
sentiment triplets. Previous research has focused on enhancing ASTE through
innovative table-filling strategies. However, these approaches often overlook
the multi-perspective nature of language expressions, resulting in a loss of
valuable interaction information between aspects and opinions. To address this
limitation, we propose a framework that leverages both a basic encoder,
primarily based on BERT, and a particular encoder comprising a Bi-LSTM network
and graph convolutional network (GCN ). The basic encoder captures the
surface-level semantics of linguistic expressions, while the particular encoder
extracts deeper semantics, including syntactic and lexical information. By
modeling the dependency tree of comments and considering the part-of-speech and
positional information of words, we aim to capture semantics that are more
relevant to the underlying intentions of the sentences. An interaction strategy
combines the semantics learned by the two encoders, enabling the fusion of
multiple perspectives and facilitating a more comprehensive understanding of
aspect--opinion relationships. Experiments conducted on benchmark datasets
demonstrate the state-of-the-art performance of our proposed framework.


------------------------------------------------------------------------------

Title:
Higher-order link prediction via local information

Abstract: Link prediction has been widely studied as an important research direction.
Higher-order link prediction has gained especially significant attention since
higher-order networks provide a more accurate description of real-world complex
systems. However, higher-order networks contain more complex information than
traditional pairwise networks, making the prediction of higher-order links a
formidable challenging task. Recently, researchers have discovered that local
features have advantages over long-range features in higher-order link
prediction. Therefore, it is necessary to develop more efficient and concise
higher-order link prediction algorithms based on local features. In this paper,
we proposed two similarity metrics via local information, simplicial
decomposition weight (SDW) and closed ratio weight (CRW), to predict possible
future higher-order interactions (simplices) in simplicial networks. These two
algorithms capture local higher-order information at two aspects: simplex
decomposition and cliques' state (closed or open). We tested their performance
in eight empirical simplicial networks, and the results show that our proposed
metrics outperform other benchmarks in predicting third-order and fourth-order
interactions (simplices) in most cases. In addition, we explore the robustness
of the proposed algorithms, and the results suggest that the performance of
these novel algorithms is advanced under different sizes of training sets.


------------------------------------------------------------------------------

Title:
Diffusion in Diffusion: Cyclic One-Way Diffusion for  Text-Vision-Conditioned Generation

Abstract: Text-to-Image (T2I) generation with diffusion models allows users to control
the semantic content in the synthesized images given text conditions. As a
further step toward a more customized image creation application, we introduce
a new multi-modality generation setting that synthesizes images based on not
only the semantic-level textual input but also on the pixel-level visual
conditions. Existing literature first converts the given visual information to
semantic-level representation by connecting it to languages, and then
incorporates it into the original denoising process. Seemingly intuitive, such
methodological design loses the pixel values during the semantic transition,
thus failing to fulfill the task scenario where the preservation of low-level
vision is desired (e.g., ID of a given face image). To this end, we propose
Cyclic One-Way Diffusion (COW), a training-free framework for creating
customized images with respect to semantic text and pixel-visual conditioning.
Notably, we observe that sub-regions of an image impose mutual interference,
just like physical diffusion, to achieve ultimate harmony along the denoising
trajectory. Thus we propose to repetitively utilize the given visual condition
in a cyclic way, by planting the visual condition as a high-concentration
"seed" at the initialization step of the denoising process, and "diffuse" it
into a harmonious picture by controlling a one-way information flow from the
visual condition. We repeat the destroy-and-construct process multiple times to
gradually but steadily impose the internal diffusion process within the image.
Experiments on the challenging one-shot face and text-conditioned image
synthesis task demonstrate our superiority in terms of speed, image quality,
and conditional fidelity compared to learning-based text-vision conditional
methods. Project page is available at: this https URL


------------------------------------------------------------------------------

Title:
LargeST: A Benchmark Dataset for Large-Scale Traffic Forecasting

Abstract: Traffic forecasting plays a critical role in smart city initiatives and has
experienced significant advancements thanks to the power of deep learning in
capturing non-linear patterns of traffic data. However, the promising results
achieved on current public datasets may not be applicable to practical
scenarios due to limitations within these datasets. First, the limited sizes of
them may not reflect the real-world scale of traffic networks. Second, the
temporal coverage of these datasets is typically short, posing hurdles in
studying long-term patterns and acquiring sufficient samples for training deep
models. Third, these datasets often lack adequate metadata for sensors, which
compromises the reliability and interpretability of the data. To mitigate these
limitations, we introduce the LargeST benchmark dataset. It encompasses a total
number of 8,600 sensors with a 5-year time coverage and includes comprehensive
metadata. Using LargeST, we perform in-depth data analysis to extract data
insights, benchmark well-known baselines in terms of their performance and
efficiency, and identify challenges as well as opportunities for future
research. We release the datasets and baseline implementations at:
this https URL


------------------------------------------------------------------------------

Title:
Characterizing First Arrival Position Channels: Noise Distribution and  Capacity Analysis

Abstract: This paper addresses two fundamental problems in diffusive molecular
communication: characterizing the first arrival position (FAP) density and
bounding the information transmission capacity of FAP channels. Previous
studies on FAP channel models, mostly captured by the density function of
noise, have been limited to specific spatial dimensions, drift directions, and
receiver geometries. In response, we propose a unified solution for identifying
the FAP density in molecular communication systems with fully-absorbing
receivers. Leveraging stochastic analysis tools, we derive a concise expression
with universal applicability, covering any spatial dimension, drift direction,
and receiver shape. We demonstrate that several existing FAP density formulas
are special cases of this innovative expression. Concurrently, we establish
explicit upper and lower bounds on the capacity of three-dimensional,
vertically-drifted FAP channels, drawing inspiration from vector Gaussian
interference channels. In the course of deriving these bounds, we unravel an
explicit analytical expression for the characteristic function of
vertically-drifted FAP noise distributions, providing a more compact
characterization compared to the density function. Notably, this expression
sheds light on a previously undiscovered weak stability property intrinsic to
vertically-drifted FAP noise distributions.


------------------------------------------------------------------------------

Title:
Team Composition in Software Engineering Education

Abstract: One of the objectives of software engineering education is to make students
to learn essential teamwork skills. This is done by having the students work in
groups for course assignments. Student team composition plays a vital role in
this, as it significantly affects learning outcomes, what is learned, and how.
The study presented in this paper aims to better understand the student team
composition in software engineering education and investigate the factors
affecting it in the international software engineering education context. Those
factors should be taken into consideration by software engineering teachers
when they design group work assignments in their courses. In this paper, the
initial findings of the ongoing Action research study are presented. The
results give some identified principles that should be considered when
designing student team composition in software engineering courses.


------------------------------------------------------------------------------

Title:
GBSD: Generative Bokeh with Stage Diffusion

Abstract: The bokeh effect is an artistic technique that blurs out-of-focus areas in a
photograph and has gained interest due to recent developments in text-to-image
synthesis and the ubiquity of smart-phone cameras and photo-sharing apps. Prior
work on rendering bokeh effects have focused on post hoc image manipulation to
produce similar blurring effects in existing photographs using classical
computer graphics or neural rendering techniques, but have either depth
discontinuity artifacts or are restricted to reproducing bokeh effects that are
present in the training data. More recent diffusion based models can synthesize
images with an artistic style, but either require the generation of
high-dimensional masks, expensive fine-tuning, or affect global image
characteristics. In this paper, we present GBSD, the first generative
text-to-image model that synthesizes photorealistic images with a bokeh style.
Motivated by how image synthesis occurs progressively in diffusion models, our
approach combines latent diffusion models with a 2-stage conditioning algorithm
to render bokeh effects on semantically defined objects. Since we can focus the
effect on objects, this semantic bokeh effect is more versatile than classical
rendering techniques. We evaluate GBSD both quantitatively and qualitatively
and demonstrate its ability to be applied in both text-to-image and
image-to-image settings.


------------------------------------------------------------------------------

Title:
Expanding Versatility of Agile Locomotion through Policy Transitions  Using Latent State Representation

Abstract: This paper proposes the transition-net, a robust transition strategy that
expands the versatility of robot locomotion in the real-world setting. To this
end, we start by distributing the complexity of different gaits into dedicated
locomotion policies applicable to real-world robots. Next, we expand the
versatility of the robot by unifying the policies with robust transitions into
a single coherent meta-controller by examining the latent state
representations. Our approach enables the robot to iteratively expand its skill
repertoire and robustly transition between any policy pair in a library. In our
framework, adding new skills does not introduce any process that alters the
previously learned skills. Moreover, training of a locomotion policy takes less
than an hour with a single consumer GPU. Our approach is effective in the
real-world and achieves a 19% higher average success rate for the most
challenging transition pairs in our experiments compared to existing
approaches.


------------------------------------------------------------------------------

Title:
Provably Personalized and Robust Federated Learning

Abstract: Clustering clients with similar objectives and learning a model per cluster
is an intuitive and interpretable approach to personalization in federated
learning. However, doing so with provable and optimal guarantees has remained
an open challenge. In this work, we formalize personalized federated learning
as a stochastic optimization problem where the stochastic gradients on a client
may correspond to one of $K$ distributions. In such a setting, we show that
using i) a simple thresholding-based clustering algorithm, and ii) local client
gradients obtains optimal convergence guarantees. In fact, our rates
asymptotically match those obtained if we knew the true underlying clustering
of the clients. Furthermore, our algorithms are provably robust in the
Byzantine setting where some fraction of the gradients are corrupted.


------------------------------------------------------------------------------

Title:
Curricular Subgoals for Inverse Reinforcement Learning

Abstract: Inverse Reinforcement Learning (IRL) aims to reconstruct the reward function
from expert demonstrations to facilitate policy learning, and has demonstrated
its remarkable success in imitation learning. To promote expert-like behavior,
existing IRL methods mainly focus on learning global reward functions to
minimize the trajectory difference between the imitator and the expert.
However, these global designs are still limited by the redundant noise and
error propagation problems, leading to the unsuitable reward assignment and
thus downgrading the agent capability in complex multi-stage tasks. In this
paper, we propose a novel Curricular Subgoal-based Inverse Reinforcement
Learning (CSIRL) framework, that explicitly disentangles one task with several
local subgoals to guide agent imitation. Specifically, CSIRL firstly introduces
decision uncertainty of the trained agent over expert trajectories to
dynamically select subgoals, which directly determines the exploration boundary
of different task stages. To further acquire local reward functions for each
stage, we customize a meta-imitation objective based on these curricular
subgoals to train an intrinsic reward generator. Experiments on the D4RL and
autonomous driving benchmarks demonstrate that the proposed methods yields
results superior to the state-of-the-art counterparts, as well as better
interpretability. Our code is available at this https URL


------------------------------------------------------------------------------

Title:
Unbiased Learning of Deep Generative Models with Structured Discrete  Representations

Abstract: By composing graphical models with deep learning architectures, we learn
generative models with the strengths of both frameworks. The structured
variational autoencoder (SVAE) inherits structure and interpretability from
graphical models, and flexible likelihoods for high-dimensional data from deep
learning, but poses substantial optimization challenges. We propose novel
algorithms for learning SVAEs, and are the first to demonstrate the SVAE's
ability to handle multimodal uncertainty when data is missing by incorporating
discrete latent variables. Our memory-efficient implicit differentiation scheme
makes the SVAE tractable to learn via gradient descent, while demonstrating
robustness to incomplete optimization. To more rapidly learn accurate graphical
model parameters, we derive a method for computing natural gradients without
manual derivations, which avoids biases found in prior work. These optimization
innovations enable the first comparisons of the SVAE to state-of-the-art time
series models, where the SVAE performs competitively while learning
interpretable and structured discrete data representations.


------------------------------------------------------------------------------

Title:
How to estimate carbon footprint when training deep learning models? A  guide and review

Abstract: Machine learning and deep learning models have become essential in the recent
fast development of artificial intelligence in many sectors of the society. It
is now widely acknowledge that the development of these models has an
environmental cost that has been analyzed in many studies. Several online and
software tools have been developed to track energy consumption while training
machine learning models. In this paper, we propose a comprehensive introduction
and comparison of these tools for AI practitioners wishing to start estimating
the environmental impact of their work. We review the specific vocabulary, the
technical requirements for each tool, and provide some advice on how and when
to use these tools.


------------------------------------------------------------------------------

Title:
Hierarchical Task Network Planning for Facilitating Cooperative  Multi-Agent Reinforcement Learning

Abstract: Exploring sparse reward multi-agent reinforcement learning (MARL)
environments with traps in a collaborative manner is a complex task. Agents
typically fail to reach the goal state and fall into traps, which affects the
overall performance of the system. To overcome this issue, we present SOMARL, a
framework that uses prior knowledge to reduce the exploration space and assist
learning. In SOMARL, agents are treated as part of the MARL environment, and
symbolic knowledge is embedded using a tree structure to build a knowledge
hierarchy. The framework has a two-layer hierarchical structure, comprising a
hybrid module with a Hierarchical Task Network (HTN) planning and
meta-controller at the higher level, and a MARL-based interactive module at the
lower level. The HTN module and meta-controller use Hierarchical Domain
Definition Language (HDDL) and the option framework to formalize symbolic
knowledge and obtain domain knowledge and a symbolic option set, respectively.
Moreover, the HTN module leverages domain knowledge to guide low-level agent
exploration by assisting the meta-controller in selecting symbolic options. The
meta-controller further computes intrinsic rewards of symbolic options to limit
exploration behavior and adjust HTN planning solutions as needed. We evaluate
SOMARL on two benchmarks, FindTreasure and MoveBox, and report superior
performance over state-of-the-art MARL and subgoal-based baselines for MARL
environments significantly.


------------------------------------------------------------------------------

Title:
Verification of NP-hardness Reduction Functions for Exact Lattice  Problems

Abstract: This paper describes the formal verification of NP-hardness reduction
functions of two key problems relevant in algebraic lattice theory: the closest
vector problem and the shortest vector problem, both in the infinity norm. The
formalization uncovered a number of problems with the existing proofs in the
literature. The paper describes how these problems were corrected in the
formalization. The work was carried out in the proof assistant Isabelle.


------------------------------------------------------------------------------

Title:
Perceptions and Realities of Text-to-Image Generation

Abstract: Generative artificial intelligence (AI) is a widely popular technology that
will have a profound impact on society and individuals. Less than a decade ago,
it was thought that creative work would be among the last to be automated - yet
today, we see AI encroaching on many creative domains. In this paper, we
present the findings of a survey study on people's perceptions of text-to-image
generation. We touch on participants' technical understanding of the emerging
technology, their fears and concerns, and thoughts about risks and dangers of
text-to-image generation to the individual and society. We find that while
participants were aware of the risks and dangers associated with the
technology, only few participants considered the technology to be a personal
risk. The risks for others were more easy to recognize for participants.
Artists were particularly seen at risk. Participants who had tried the
technology rated its future importance lower than those who had not tried it.
This result shows that many people are still oblivious of the potential
personal risks of generative artificial intelligence and the impending societal
changes associated with this technology.


------------------------------------------------------------------------------

Title:
Multiclass Confidence and Localization Calibration for Object Detection

Abstract: Albeit achieving high predictive accuracy across many challenging computer
vision problems, recent studies suggest that deep neural networks (DNNs) tend
to make overconfident predictions, rendering them poorly calibrated. Most of
the existing attempts for improving DNN calibration are limited to
classification tasks and restricted to calibrating in-domain predictions.
Surprisingly, very little to no attempts have been made in studying the
calibration of object detection methods, which occupy a pivotal space in
vision-based security-sensitive, and safety-critical applications. In this
paper, we propose a new train-time technique for calibrating modern object
detection methods. It is capable of jointly calibrating multiclass confidence
and box localization by leveraging their predictive uncertainties. We perform
extensive experiments on several in-domain and out-of-domain detection
benchmarks. Results demonstrate that our proposed train-time calibration method
consistently outperforms several baselines in reducing calibration error for
both in-domain and out-of-domain predictions. Our code and models are available
at this https URL


------------------------------------------------------------------------------

Title:
Subjective-objective policy making approach: Coupling of resident-values  multiple regression analysis with value-indices, multi-agent-based simulation

Abstract: Given the concerns around the existing subjective and objective policy
evaluation approaches, this study proposes a new combined subjective-objective
policy evaluation approach to choose better policy that reflects the will of
citizens and is backed up by objective facts. Subjective approaches, such as
the Life Satisfaction Approach and the Contingent Valuation Method, convert
subjectivity into economic value, raising the question whether a higher
economic value really accords with what citizens want. Objective policy
evaluation approaches, such as Evidence Based Policy Making and
Multi-Agent-Based Simulation, do not take subjectivity into account, making it
difficult to choose from diverse and pluralistic candidate policies. The
proposed approach establishes a subjective target function based on a multiple
regression analysis of the results of a residents questionnaire survey, and
uses MABS to calculate the objective evaluation indices for a number of
candidate policies. Next, a new subjective-objective coupling target function,
combining the explanatory variables of the subjective target function with
objective evaluation indices, is set up, optimized to select the preferred
policies from numerous candidates. To evaluate this approach, we conducted a
verification of renewable energy introduction policies at Takaharu Town in
Miyazaki Prefecture, Japan. The results show a good potential for using a new
subjective-objective coupling target function to select policies consistent
with the residents values for well-being from 20,000 policy candidates for
social, ecological, and economic values obtained in MABS. Using the new
approach to compare several policies enables concrete expression of the will of
stakeholders with diverse values, and contributes to constructive discussions
and consensus-building.


------------------------------------------------------------------------------

Title:
$\textbf{A}^2\textbf{CiD}^2$: Accelerating Asynchronous Communication in  Decentralized Deep Learning

Abstract: Distributed training of Deep Learning models has been critical to many recent
successes in the field. Current standard methods primarily rely on synchronous
centralized algorithms which induce major communication bottlenecks and limit
their usability to High-Performance Computing (HPC) environments with strong
connectivity. Decentralized asynchronous algorithms are emerging as a potential
alternative but their practical applicability still lags. In this work, we
focus on peerto-peer asynchronous methods due to their flexibility and
parallelization potentials. In order to mitigate the increase in bandwidth they
require at large scale and in poorly connected contexts, we introduce a
principled asynchronous, randomized, gossip-based algorithm which works thanks
to a continuous momentum named $\textbf{A}^2\textbf{CiD}^2$. In addition to
inducing a significant communication acceleration at no cost other than
doubling the parameters, minimal adaptation is required to incorporate
$\textbf{A}^2\textbf{CiD}^2$ to other asynchronous approaches. We demonstrate
its efficiency theoretically and numerically. Empirically on the ring graph,
adding $\textbf{A}^2\textbf{CiD}^2$ has the same effect as doubling the
communication rate. In particular, we show consistent improvement on the
ImageNet dataset using up to 64 asynchronous workers (A100 GPUs) and various
communication network topologies.


------------------------------------------------------------------------------

Title:
Automated Speaker Independent Visual Speech Recognition: A Comprehensive  Survey

Abstract: Speaker-independent VSR is a complex task that involves identifying spoken
words or phrases from video recordings of a speaker's facial movements. Over
the years, there has been a considerable amount of research in the field of VSR
involving different algorithms and datasets to evaluate system performance.
These efforts have resulted in significant progress in developing effective VSR
models, creating new opportunities for further research in this area. This
survey provides a detailed examination of the progression of VSR over the past
three decades, with a particular emphasis on the transition from
speaker-dependent to speaker-independent systems. We also provide a
comprehensive overview of the various datasets used in VSR research and the
preprocessing techniques employed to achieve speaker independence. The survey
covers the works published from 1990 to 2023, thoroughly analyzing each work
and comparing them on various parameters. This survey provides an in-depth
analysis of speaker-independent VSR systems evolution from 1990 to 2023. It
outlines the development of VSR systems over time and highlights the need to
develop end-to-end pipelines for speaker-independent VSR. The pictorial
representation offers a clear and concise overview of the techniques used in
speaker-independent VSR, thereby aiding in the comprehension and analysis of
the various methodologies. The survey also highlights the strengths and
limitations of each technique and provides insights into developing novel
approaches for analyzing visual speech cues. Overall, This comprehensive review
provides insights into the current state-of-the-art speaker-independent VSR and
highlights potential areas for future research.


------------------------------------------------------------------------------

Title:
A Graph Data Structure to Optimize Dynamic Graph Processing on GPUs

Abstract: Graph processing on GPUs is gaining momentum due to the high throughputs
observed compared to traditional CPUs, attributed to the vast number of
processing cores on GPUs that can exploit parallelism in graph analytics. This
paper discusses a graph data structure for dynamic graph processing on GPUs.
Unlike static graphs, dynamic graphs mutate over their lifetime through vertex
and/or edge batch updates. The proposed work aims to provide fast batch updates
and graph querying without consuming too much GPU memory. Experimental results
show improved initialization timings by 1968-1269024%, improved batch edge
insert timings by 30-30047%, and improved batch edge delete timings by
50-25262% while consuming less memory when the batch size is large.


------------------------------------------------------------------------------

Title:
Maestro: A Gamified Platform for Teaching AI Robustness

Abstract: Although the prevention of AI vulnerabilities is critical to preserve the
safety and privacy of users and businesses, educational tools for robust AI are
still underdeveloped worldwide. We present the design, implementation, and
assessment of Maestro. Maestro is an effective open-source game-based platform
that contributes to the advancement of robust AI education. Maestro provides
goal-based scenarios where college students are exposed to challenging
life-inspired assignments in a competitive programming environment. We assessed
Maestro's influence on students' engagement, motivation, and learning success
in robust AI. This work also provides insights into the design features of
online learning tools that promote active learning opportunities in the robust
AI domain. We analyzed the reflection responses (measured with Likert scales)
of 147 undergraduate students using Maestro in two quarterly college courses in
AI. According to the results, students who felt the acquisition of new skills
in robust AI tended to appreciate highly Maestro and scored highly on material
consolidation, curiosity, and mastery in robust AI. Moreover, the leaderboard,
our key gamification element in Maestro, has effectively contributed to
students' engagement and learning. Results also indicate that Maestro can be
effectively adapted to any course length and depth without losing its
educational quality.


------------------------------------------------------------------------------

Title:
Sequential Deep Learning Operator Network (S-DeepONet) for  Time-Dependent Loads

Abstract: Deep Operator Network (DeepONet), a recently introduced deep learning
operator network, approximates linear and nonlinear solution operators by
taking parametric functions (infinite-dimensional objects) as inputs and
mapping them to solution functions in contrast to classical neural networks
(NNs) that need re-training for every new set of parametric inputs. In this
work, we have extended the classical formulation of DeepONets by introducing
recurrent neural networks (RNNs) in its branch in so-called sequential
DeepONets (S-DeepONets) thus allowing accurate solution predictions in the
entire domain for parametric and time-dependent loading histories. We have
demonstrated this novel formulation's generality and exceptional accuracy with
thermal and mechanical random loading histories applied to highly nonlinear
thermal solidification and plastic deformation use cases. We show that once
S-DeepONet is properly trained, it can accurately predict the final solutions
in the entire domain and is several orders of magnitude more computationally
efficient than the finite element method for arbitrary loading histories
without additional training.


------------------------------------------------------------------------------

Title:
MMASD: A Multimodal Dataset for Autism Intervention Analysis

Abstract: Autism spectrum disorder (ASD) is a developmental disorder characterized by
significant social communication impairments and difficulties perceiving and
presenting communication cues. Machine learning techniques have been broadly
adopted to facilitate autism studies and assessments. However, computational
models are primarily concentrated on specific analysis and validated on private
datasets in the autism community, which limits comparisons across models due to
privacy-preserving data sharing complications. This work presents a novel
privacy-preserving open-source dataset, MMASD as a MultiModal ASD benchmark
dataset, collected from play therapy interventions of children with Autism.
MMASD includes data from 32 children with ASD, and 1,315 data samples segmented
from over 100 hours of intervention recordings. To promote public access, each
data sample consists of four privacy-preserving modalities of data: (1) optical
flow, (2) 2D skeleton, (3) 3D skeleton, and (4) clinician ASD evaluation scores
of children, e.g., ADOS scores. MMASD aims to assist researchers and therapists
in understanding children's cognitive status, monitoring their progress during
therapy, and customizing the treatment plan accordingly. It also has
inspiration for downstream tasks such as action quality assessment and
interpersonal synchrony estimation. MMASD dataset can be easily accessed at
this https URL


------------------------------------------------------------------------------

Title:
Uncertainty-Aware Robust Learning on Noisy Graphs

Abstract: Graph neural networks have shown impressive capabilities in solving various
graph learning tasks, particularly excelling in node classification. However,
their effectiveness can be hindered by the challenges arising from the
widespread existence of noisy measurements associated with the topological or
nodal information present in real-world graphs. These inaccuracies in
observations can corrupt the crucial patterns within the graph data, ultimately
resulting in undesirable performance in practical applications. To address
these issues, this paper proposes a novel uncertainty-aware graph learning
framework motivated by distributionally robust optimization. Specifically, we
use a graph neural network-based encoder to embed the node features and find
the optimal node embeddings by minimizing the worst-case risk through a minimax
formulation. Such an uncertainty-aware learning process leads to improved node
representations and a more robust graph predictive model that effectively
mitigates the impact of uncertainty arising from data noise. Our experimental
result shows that the proposed framework achieves superior predictive
performance compared to the state-of-the-art baselines under various noisy
settings.


------------------------------------------------------------------------------

Title:
Protecting User Privacy in Remote Conversational Systems: A  Privacy-Preserving framework based on text sanitization

Abstract: Large Language Models (LLMs) are gaining increasing attention due to their
exceptional performance across numerous tasks. As a result, the general public
utilize them as an influential tool for boosting their productivity while
natural language processing researchers endeavor to employ them in solving
existing or new research problems. Unfortunately, individuals can only access
such powerful AIs through APIs, which ultimately leads to the transmission of
raw data to the models' providers and increases the possibility of privacy data
leakage. Current privacy-preserving methods for cloud-deployed language models
aim to protect privacy information in the pre-training dataset or during the
model training phase. However, they do not meet the specific challenges
presented by the remote access approach of new large-scale language models.
This paper introduces a novel task, "User Privacy Protection for Dialogue
Models," which aims to safeguard sensitive user information from any possible
disclosure while conversing with chatbots. We also present an evaluation scheme
for this task, which covers evaluation metrics for privacy protection, data
availability, and resistance to simulation attacks. Moreover, we propose the
first framework for this task, namely privacy protection through text
sanitization. Before sending the input to remote large models, it filters out
the sensitive information, using several rounds of text sanitization based on
privacy types that users define. Upon receiving responses from the larger
model, our framework automatically restores privacy to ensure that the
conversation goes smoothly, without intervention from the privacy filter.
Experiments based on real-world datasets demonstrate the efficacy of our
privacy-preserving approach against eavesdropping from potential attackers.


------------------------------------------------------------------------------

Title:
Efficient Backdoor Attacks for Deep Neural Networks in Real-world  Scenarios

Abstract: Recent deep neural networks (DNNs) have come to rely on vast amounts of
training data, providing an opportunity for malicious attackers to exploit and
contaminate the data to carry out backdoor attacks. These attacks significantly
undermine the reliability of DNNs. However, existing backdoor attack methods
make unrealistic assumptions, assuming that all training data comes from a
single source and that attackers have full access to the training data. In this
paper, we address this limitation by introducing a more realistic attack
scenario where victims collect data from multiple sources, and attackers cannot
access the complete training data. We refer to this scenario as
data-constrained backdoor attacks. In such cases, previous attack methods
suffer from severe efficiency degradation due to the entanglement between
benign and poisoning features during the backdoor injection process.
To tackle this problem, we propose a novel approach that leverages the
pre-trained Contrastive Language-Image Pre-Training (CLIP) model. We introduce
three CLIP-based technologies from two distinct streams: Clean Feature
Suppression, which aims to suppress the influence of clean features to enhance
the prominence of poisoning features, and Poisoning Feature Augmentation, which
focuses on augmenting the presence and impact of poisoning features to
effectively manipulate the model's behavior.
To evaluate the effectiveness, harmlessness to benign accuracy, and
stealthiness of our method, we conduct extensive experiments on 3 target
models, 3 datasets, and over 15 different settings. The results demonstrate
remarkable improvements, with some settings achieving over 100% improvement
compared to existing attacks in data-constrained scenarios. Our research
contributes to addressing the limitations of existing methods and provides a
practical and effective solution for data-constrained backdoor attacks.


------------------------------------------------------------------------------

Title:
Uplink Performance of RIS-aided Cell-Free Massive MIMO System with  Electromagnetic Interference

Abstract: Cell-free (CF) massive multiple-input multiple-output (MIMO) and
reconfigurable intelligent surface (RIS) are two promising technologies for
realizing future beyond-fifth generation (B5G) networks. In this paper, we
consider a practical spatially correlated RIS-aided CF massive MIMO system with
multi-antenna access points (APs) over spatially correlated fading channels.
Different from previous work, the electromagnetic interference (EMI) at RIS is
considered to further characterize the system performance of the actual
environment. Then, we derive the closed-form expression for the system spectral
efficiency (SE) with the maximum ratio (MR) combining at the APs and the
large-scale fading decoding (LSFD) at the central processing unit (CPU).
Moreover, to counteract the near-far effect and EMI, we propose practical
fractional power control (FPC) and max-min power control algorithms to further
improve the system performance. We unveil the impact of EMI, channel
correlations, and different signal processing methods on the uplink SE of user
equipments (UEs). The accuracy of our derived analytical results is verified by
extensive Monte-Carlo simulations. Our results show that the EMI can
substantially degrade the SE, especially for those UEs with unsatisfactory
channel conditions. Besides, increasing the number of RIS elements is always
beneficial in terms of the SE, but with diminishing returns when the number of
RIS elements is sufficiently large. Furthermore, the existence of spatial
correlations among RIS elements can deteriorate the system performance when RIS
is impaired by EMI.


------------------------------------------------------------------------------

Title:
Nearly Optimal Algorithms with Sublinear Computational Complexity for  Online Kernel Regression

Abstract: The trade-off between regret and computational cost is a fundamental problem
for online kernel regression, and previous algorithms worked on the trade-off
can not keep optimal regret bounds at a sublinear computational complexity. In
this paper, we propose two new algorithms, AOGD-ALD and NONS-ALD, which can
keep nearly optimal regret bounds at a sublinear computational complexity, and
give sufficient conditions under which our algorithms work. Both algorithms
dynamically maintain a group of nearly orthogonal basis used to approximate the
kernel mapping, and keep nearly optimal regret bounds by controlling the
approximate error. The number of basis depends on the approximate error and the
decay rate of eigenvalues of the kernel matrix. If the eigenvalues decay
exponentially, then AOGD-ALD and NONS-ALD separately achieves a regret of
$O(\sqrt{L(f)})$ and $O(\mathrm{d}_{\mathrm{eff}}(\mu)\ln{T})$ at a
computational complexity in $O(\ln^2{T})$. If the eigenvalues decay
polynomially with degree $p\geq 1$, then our algorithms keep the same regret
bounds at a computational complexity in $o(T)$ in the case of $p>4$ and $p\geq
10$, respectively. $L(f)$ is the cumulative losses of $f$ and
$\mathrm{d}_{\mathrm{eff}}(\mu)$ is the effective dimension of the problem. The
two regret bounds are nearly optimal and are not comparable.


------------------------------------------------------------------------------

Title:
SaliencyCut: Augmenting Plausible Anomalies for Open-set Fine-Grained  Anomaly Detection

Abstract: Open-set fine-grained anomaly detection is a challenging task that requires
learning discriminative fine-grained features to detect anomalies that were
even unseen during training. As a cheap yet effective approach, data
augmentation has been widely used to create pseudo anomalies for better
training of such models. Recent wisdom of augmentation methods focuses on
generating random pseudo instances that may lead to a mixture of augmented
instances with seen anomalies, or out of the typical range of anomalies. To
address this issue, we propose a novel saliency-guided data augmentation
method, SaliencyCut, to produce pseudo but more common anomalies which tend to
stay in the plausible range of anomalies. Furthermore, we deploy a two-head
learning strategy consisting of normal and anomaly learning heads, to learn the
anomaly score of each sample. Theoretical analyses show that this mechanism
offers a more tractable and tighter lower bound of the data log-likelihood. We
then design a novel patch-wise residual module in the anomaly learning head to
extract and assess the fine-grained anomaly features from each sample,
facilitating the learning of discriminative representations of anomaly
instances. Extensive experiments conducted on six real-world anomaly detection
datasets demonstrate the superiority of our method to the baseline and other
state-of-the-art methods under various settings.


------------------------------------------------------------------------------

Title:
Investigating the dynamics of hand and lips in French Cued Speech using  attention mechanisms and CTC-based decoding

Abstract: Hard of hearing or profoundly deaf people make use of cued speech (CS) as a
communication tool to understand spoken language. By delivering cues that are
relevant to the phonetic information, CS offers a way to enhance lipreading. In
literature, there have been several studies on the dynamics between the hand
and the lips in the context of human production. This article proposes a way to
investigate how a neural network learns this relation for a single speaker
while performing a recognition task using attention mechanisms. Further, an
analysis of the learnt dynamics is utilized to establish the relationship
between the two modalities and extract automatic segments. For the purpose of
this study, a new dataset has been recorded for French CS. Along with the
release of this dataset, a benchmark will be reported for word-level
recognition, a novelty in the automatic recognition of French CS.


------------------------------------------------------------------------------

Title:
SpeechGLUE: How Well Can Self-Supervised Speech Models Capture  Linguistic Knowledge?

Abstract: Self-supervised learning (SSL) for speech representation has been
successfully applied in various downstream tasks, such as speech and speaker
recognition. More recently, speech SSL models have also been shown to be
beneficial in advancing spoken language understanding tasks, implying that the
SSL models have the potential to learn not only acoustic but also linguistic
information. In this paper, we aim to clarify if speech SSL techniques can well
capture linguistic knowledge. For this purpose, we introduce SpeechGLUE, a
speech version of the General Language Understanding Evaluation (GLUE)
benchmark. Since GLUE comprises a variety of natural language understanding
tasks, SpeechGLUE can elucidate the degree of linguistic ability of speech SSL
models. Experiments demonstrate that speech SSL models, although inferior to
text-based SSL models, perform better than baselines, suggesting that they can
acquire a certain amount of general linguistic knowledge from just unlabeled
speech data.


------------------------------------------------------------------------------

Title:
Transmission and Distribution Coordination for DER-rich Energy Markets:  A Parametric Programming Approach

Abstract: In this paper, a framework is proposed to coordinate the operation of the
independent system operator (ISO) and distribution system operator (DSO). The
framework is compatible with current practice of the U.S. wholesale market to
enable massive distributed energy resources (DERs) to participate in the
wholesale market. The DSO builds a bid-in cost function to be submitted to the
ISO market through parametric programming. Once the ISO clears the wholesale
market, the dispatch and payment of the DSO are determined by ISO. Then, the
DSO determines the dispatch and payment of the DER aggregators. To compare the
proposed framework, an ideal case is defined in which DER aggregators can
participate in the wholesale market directly and ISO overseas operation of both
transmission and distribution systems. We proved 1) the dispatches of the
proposed ISO-DSO coordination framework are identical to those of the ideal
case; 2) the payments to each DER aggregator are identical in the proposed
framework and in the ideal case. Case studies are performed on a small
illustrative example as well as a large test system which includes IEEE 118 bus
transmission system and two distribution systems - the IEEE 33 node and IEEE
240 node test systems.


------------------------------------------------------------------------------

Title:
Ball Trajectory Inference from Multi-Agent Sports Contexts Using Set  Transformer and Hierarchical Bi-LSTM

Abstract: As artificial intelligence spreads out to numerous fields, the application of
AI to sports analytics is also in the spotlight. However, one of the major
challenges is the difficulty of automated acquisition of continuous movement
data during sports matches. In particular, it is a conundrum to reliably track
a tiny ball on a wide soccer pitch with obstacles such as occlusion and
imitations. Tackling the problem, this paper proposes an inference framework of
ball trajectory from player trajectories as a cost-efficient alternative to
ball tracking. We combine Set Transformers to get permutation-invariant and
equivariant representations of the multi-agent contexts with a hierarchical
architecture that intermediately predicts the player ball possession to support
the final trajectory inference. Also, we introduce the reality loss term and
postprocessing to secure the estimated trajectories to be physically realistic.
The experimental results show that our model provides natural and accurate
trajectories as well as admissible player ball possession at the same time.
Lastly, we suggest several practical applications of our framework including
missing trajectory imputation, semi-automated pass annotation, automated
zoom-in for match broadcasting, and calculating possession-wise running
performance metrics.


------------------------------------------------------------------------------

Title:
Differentially Private Wireless Federated Learning Using Orthogonal  Sequences

Abstract: We propose a novel privacy-preserving uplink over-the-air computation
(AirComp) method, termed FLORAS, for single-input single-output (SISO) wireless
federated learning (FL) systems. From the communication design perspective,
FLORAS eliminates the requirement of channel state information at the
transmitters (CSIT) by leveraging the properties of orthogonal sequences. From
the privacy perspective, we prove that FLORAS can offer both item-level and
client-level differential privacy (DP) guarantees. Moreover, by adjusting the
system parameters, FLORAS can flexibly achieve different DP levels at no
additional cost. A novel FL convergence bound is derived which, combined with
the privacy guarantees, allows for a smooth tradeoff between convergence rate
and differential privacy levels. Numerical results demonstrate the advantages
of FLORAS compared with the baseline AirComp method, and validate that our
analytical results can guide the design of privacy-preserving FL with different
tradeoff requirements on the model convergence and privacy levels.


------------------------------------------------------------------------------

Title:
Efficient Training of Physics-Informed Neural Networks with Direct Grid  Refinement Algorithm

Abstract: This research presents the development of an innovative algorithm tailored
for the adaptive sampling of residual points within the framework of
Physics-Informed Neural Networks (PINNs). By addressing the limitations
inherent in existing adaptive sampling techniques, our proposed methodology
introduces a direct mesh refinement approach that effectively ensures both
computational efficiency and adaptive point placement. Verification studies
were conducted to evaluate the performance of our algorithm, showcasing
reasonable agreement between the model based on our novel approach and
benchmark model results. Comparative analyses with established adaptive
resampling techniques demonstrated the superior performance of our approach,
particularly when implemented with higher refinement factor. Overall, our
findings highlight the enhancement of simulation accuracy achievable through
the application of our adaptive sampling algorithm for Physics-Informed Neural
Networks.


------------------------------------------------------------------------------

Title:
Analyzing Robustness of Angluin's L$^*$ Algorithm in Presence of Noise

Abstract: Angluin's L$^*$ algorithm learns the minimal deterministic finite automaton
(DFA) of a regular language using membership and equivalence queries. Its
probabilistic approximatively correct (PAC) version substitutes an equivalence
query by numerous random membership queries to get a high level confidence to
the answer. Thus it can be applied to any kind of device and may be viewed as
an algorithm for synthesizing an automaton abstracting the behavior of the
device based on observations. Here we are interested on how Angluin's PAC
learning algorithm behaves for devices which are obtained from a DFA by
introducing some noise. More precisely we study whether Angluin's algorithm
reduces the noise and produces a DFA closer to the original one than the noisy
device. We propose several ways to introduce the noise: (1) the noisy device
inverts the classification of words w.r.t. the DFA with a small probability,
(2) the noisy device modifies with a small probability the letters of the word
before asking its classification w.r.t. the DFA, (3) the noisy device combines
the classification of a word w.r.t. the DFA and its classification w.r.t. a
counter automaton, and (4) the noisy DFA is obtained by a random process from
two DFA such that the language of the first one is included in the second one.
Then when a word is accepted (resp. rejected) by the first (resp. second) one,
it is also accepted (resp. rejected) and in the remaining cases, it is accepted
with probability 0.5. Our main experimental contributions consist in showing
that: (1) Angluin's algorithm behaves well whenever the noisy device is
produced by a random process, (2) but poorly with a structured noise, and, that
(3) is able to eliminate pathological behaviours specified in a regular way.
Theoretically, we show that randomness almost surely yields systems with
non-recursively enumerable languages.


------------------------------------------------------------------------------

Title:
The Devil is in the Details: Analyzing the Lucrative Ad Fraud Patterns  of the Online Ad Ecosystem

Abstract: The online advertising market has recently reached the 500 billion dollar
mark, and to accommodate the need to match a user with the highest bidder at a
fraction of a second, it has moved towards a complex automated model involving
numerous agents and middle men. Stimulated by potential revenue and the lack of
transparency, bad actors have found ways to abuse it, circumvent restrictions,
and generate substantial revenue from objectionable and even illegal content.
To make matters worse, they often receive advertisements from respectable
companies which have nothing to do with these illegal activities. Altogether,
advertiser money is funneled towards unknown entities, supporting their
objectionable operations and maintaining their existence.
In this project, we work towards understanding the extent of the problem and
shed light on how shady agents take advantage of gaps in the ad ecosystem to
monetize their operations. We study over 7 million websites and examine how
state-of-the-art standards associated with online advertising are applied. We
discover and present actual practices observed in the wild and show that
publishers are able to monetize objectionable and illegal content and generate
thousands of dollars of revenue on a monthly basis.


------------------------------------------------------------------------------

Title:
Early Detection of Late Blight Tomato Disease using Histogram Oriented  Gradient based Support Vector Machine

Abstract: The tomato is one of the most important fruits on earth. It plays an
important and useful role in the agricultural production of any country. This
research propose a novel smart technique for early detection of late blight
diseases in tomatoes. This work improve the dataset with an increase in images
from the field (the Plant Village dataset) and proposed a hybrid algorithm
composed of support vector machines (SVM) and histogram-oriented gradients
(HOG) for real-time detection of late blight tomato disease. To propose a
HOG-based SVM model for early detection of late blight tomato leaf disease. To
check the performance of the proposed model in terms of MSE, accuracy,
precision, and recall as compared to Decision Tree and KNN. The integration of
advanced technology in agriculture has the potential to revolutionize the
industry, making it more efficient, sustainable, and profitable. This research
work on the early detection of tomato diseases contributes to the growing
importance of smart farming, the need for climate-smart agriculture, the rising
need to more efficiently utilize natural resources, and the demand for higher
crop yields. The proposed hybrid algorithm of SVM and HOG has significant
potential for the early detection of late blight disease in tomato plants. The
performance of the proposed model against decision tree and KNN algorithms and
the results may assist in selecting the best algorithm for future applications.
The research work can help farmers make data-driven decisions to optimize crop
yield and quality while also reducing the environmental impact of farming
practices.


------------------------------------------------------------------------------

Title:
CLIPXPlore: Coupled CLIP and Shape Spaces for 3D Shape Exploration

Abstract: This paper presents CLIPXPlore, a new framework that leverages a
vision-language model to guide the exploration of the 3D shape space. Many
recent methods have been developed to encode 3D shapes into a learned latent
shape space to enable generative design and modeling. Yet, existing methods
lack effective exploration mechanisms, despite the rich information. To this
end, we propose to leverage CLIP, a powerful pre-trained vision-language model,
to aid the shape-space exploration. Our idea is threefold. First, we couple the
CLIP and shape spaces by generating paired CLIP and shape codes through sketch
images and training a mapper network to connect the two spaces. Second, to
explore the space around a given shape, we formulate a co-optimization strategy
to search for the CLIP code that better matches the geometry of the shape.
Third, we design three exploration modes, binary-attribute-guided, text-guided,
and sketch-guided, to locate suitable exploration trajectories in shape space
and induce meaningful changes to the shape. We perform a series of experiments
to quantitatively and visually compare CLIPXPlore with different baselines in
each of the three exploration modes, showing that CLIPXPlore can produce many
meaningful exploration results that cannot be achieved by the existing
solutions.


------------------------------------------------------------------------------

Title:
X-Detect: Explainable Adversarial Patch Detection for Object Detectors  in Retail

Abstract: Object detection models, which are widely used in various domains (such as
retail), have been shown to be vulnerable to adversarial attacks. Existing
methods for detecting adversarial attacks on object detectors have had
difficulty detecting new real-life attacks. We present X-Detect, a novel
adversarial patch detector that can: i) detect adversarial samples in real
time, allowing the defender to take preventive action; ii) provide explanations
for the alerts raised to support the defender's decision-making process, and
iii) handle unfamiliar threats in the form of new attacks. Given a new scene,
X-Detect uses an ensemble of explainable-by-design detectors that utilize
object extraction, scene manipulation, and feature transformation techniques to
determine whether an alert needs to be raised. X-Detect was evaluated in both
the physical and digital space using five different attack scenarios (including
adaptive attacks) and the COCO dataset and our new Superstore dataset. The
physical evaluation was performed using a smart shopping cart setup in
real-world settings and included 17 adversarial patch attacks recorded in 1,700
adversarial videos. The results showed that X-Detect outperforms the
state-of-the-art methods in distinguishing between benign and adversarial
scenes for all attack scenarios while maintaining a 0% FPR (no false alarms)
and providing actionable explanations for the alerts raised. A demo is
available.


------------------------------------------------------------------------------

Title:
On the Robustness of Latent Diffusion Models

Abstract: Latent diffusion models achieve state-of-the-art performance on a variety of
generative tasks, such as image synthesis and image editing. However, the
robustness of latent diffusion models is not well studied. Previous works only
focus on the adversarial attacks against the encoder or the output image under
white-box settings, regardless of the denoising process. Therefore, in this
paper, we aim to analyze the robustness of latent diffusion models more
thoroughly. We first study the influence of the components inside latent
diffusion models on their white-box robustness. In addition to white-box
scenarios, we evaluate the black-box robustness of latent diffusion models via
transfer attacks, where we consider both prompt-transfer and model-transfer
settings and possible defense mechanisms. However, all these explorations need
a comprehensive benchmark dataset, which is missing in the literature.
Therefore, to facilitate the research of the robustness of latent diffusion
models, we propose two automatic dataset construction pipelines for two kinds
of image editing models and release the whole dataset. Our code and dataset are
available at \url{this https URL}.


------------------------------------------------------------------------------

Title:
T5-SR: A Unified Seq-to-Seq Decoding Strategy for Semantic Parsing

Abstract: Translating natural language queries into SQLs in a seq2seq manner has
attracted much attention recently. However, compared with
abstract-syntactic-tree-based SQL generation, seq2seq semantic parsers face
much more challenges, including poor quality on schematical information
prediction and poor semantic coherence between natural language queries and
SQLs. This paper analyses the above difficulties and proposes a
seq2seq-oriented decoding strategy called SR, which includes a new intermediate
representation SSQL and a reranking method with score re-estimator to solve the
above obstacles respectively. Experimental results demonstrate the
effectiveness of our proposed techniques and T5-SR-3b achieves new
state-of-the-art results on the Spider dataset.


------------------------------------------------------------------------------

Title:
Towards Building Voice-based Conversational Recommender Systems:  Datasets, Potential Solutions, and Prospects

Abstract: Conversational recommender systems (CRSs) have become crucial emerging
research topics in the field of RSs, thanks to their natural advantages of
explicitly acquiring user preferences via interactive conversations and
revealing the reasons behind recommendations. However, the majority of current
CRSs are text-based, which is less user-friendly and may pose challenges for
certain users, such as those with visual impairments or limited writing and
reading abilities. Therefore, for the first time, this paper investigates the
potential of voice-based CRS (VCRSs) to revolutionize the way users interact
with RSs in a natural, intuitive, convenient, and accessible fashion. To
support such studies, we create two VCRSs benchmark datasets in the e-commerce
and movie domains, after realizing the lack of such datasets through an
exhaustive literature review. Specifically, we first empirically verify the
benefits and necessity of creating such datasets. Thereafter, we convert the
user-item interactions to text-based conversations through the ChatGPT-driven
prompts for generating diverse and natural templates, and then synthesize the
corresponding audios via the text-to-speech model. Meanwhile, a number of
strategies are delicately designed to ensure the naturalness and high quality
of voice conversations. On this basis, we further explore the potential
solutions and point out possible directions to build end-to-end VCRSs by
seamlessly extracting and integrating voice-based inputs, thus delivering
performance-enhanced, self-explainable, and user-friendly VCRSs. Our study aims
to establish the foundation and motivate further pioneering research in the
emerging field of VCRSs. This aligns with the principles of explainable AI and
AI for social good, viz., utilizing technology's potential to create a fair,
sustainable, and just world.


------------------------------------------------------------------------------

Title:
Why Using Either Aggregated Features or Adjacency Lists in Directed or  Undirected Graph? Empirical Study and Simple Classification Method

Abstract: Node classification is one of the hottest tasks in graph analysis. In this
paper, we focus on the choices of node representations (aggregated features vs.
adjacency lists) and the edge direction of an input graph (directed vs.
undirected), which have a large influence on classification results. We address
the first empirical study to benchmark the performance of various GNNs that use
either combination of node representations and edge directions. Our experiments
demonstrate that no single combination stably achieves state-of-the-art results
across datasets, which indicates that we need to select appropriate
combinations depending on the characteristics of datasets. In response, we
propose a simple yet holistic classification method A2DUG which leverages all
combinations of node representation variants in directed and undirected graphs.
We demonstrate that A2DUG stably performs well on various datasets.
Surprisingly, it largely outperforms the current state-of-the-art methods in
several datasets. This result validates the importance of the adaptive effect
control on the combinations of node representations and edge directions.


------------------------------------------------------------------------------

Title:
SMC-UDA: Structure-Modal Constraint for Unsupervised Cross-Domain Renal  Segmentation

Abstract: Medical image segmentation based on deep learning often fails when deployed
on images from a different domain. The domain adaptation methods aim to solve
domain-shift challenges, but still face some problems. The transfer learning
methods require annotation on the target domain, and the generative
unsupervised domain adaptation (UDA) models ignore domain-specific
representations, whose generated quality highly restricts segmentation
performance. In this study, we propose a novel Structure-Modal Constrained
(SMC) UDA framework based on a discriminative paradigm and introduce edge
structure as a bridge between domains. The proposed multi-modal learning
backbone distills structure information from image texture to distinguish
domain-invariant edge structure. With the structure-constrained self-learning
and progressive ROI, our methods segment the kidney by locating the 3D spatial
structure of the edge. We evaluated SMC-UDA on public renal segmentation
datasets, adapting from the labeled source domain (CT) to the unlabeled target
domain (CT/MRI). The experiments show that our proposed SMC-UDA has a strong
generalization and outperforms generative UDA methods.


------------------------------------------------------------------------------

Title:
GCformer: An Efficient Framework for Accurate and Scalable Long-Term  Multivariate Time Series Forecasting

Abstract: Transformer-based models have emerged as promising tools for time series
forecasting.
However, these model cannot make accurate prediction for long input time
series. On the one hand, they failed to capture global dependencies within time
series data. On the other hand, the long input sequence usually leads to large
model size and high time complexity.
To address these limitations, we present GCformer, which combines a
structured global convolutional branch for processing long input sequences with
a local Transformer-based branch for capturing short, recent signals. A
cohesive framework for a global convolution kernel has been introduced,
utilizing three distinct parameterization methods. The selected structured
convolutional kernel in the global branch has been specifically crafted with
sublinear complexity, thereby allowing for the efficient and effective
processing of lengthy and noisy input signals. Empirical studies on six
benchmark datasets demonstrate that GCformer outperforms state-of-the-art
methods, reducing MSE error in multivariate time series benchmarks by 4.38% and
model parameters by 61.92%. In particular, the global convolutional branch can
serve as a plug-in block to enhance the performance of other models, with an
average improvement of 31.93\%, including various recently published
Transformer-based models. Our code is publicly available at
this https URL


------------------------------------------------------------------------------

Title:
Ground-VIO: Monocular Visual-Inertial Odometry with Online Calibration  of Camera-Ground Geometric Parameters

Abstract: Monocular visual-inertial odometry (VIO) is a low-cost solution to provide
high-accuracy, low-drifting pose estimation. However, it has been meeting
challenges in vehicular scenarios due to limited dynamics and lack of stable
features. In this paper, we propose Ground-VIO, which utilizes ground features
and the specific camera-ground geometry to enhance monocular VIO performance in
realistic road environments. In the method, the camera-ground geometry is
modeled with vehicle-centered parameters and integrated into an
optimization-based VIO framework. These parameters could be calibrated online
and simultaneously improve the odometry accuracy by providing stable
scale-awareness. Besides, a specially designed visual front-end is developed to
stably extract and track ground features via the inverse perspective mapping
(IPM) technique. Both simulation tests and real-world experiments are conducted
to verify the effectiveness of the proposed method. The results show that our
implementation could dramatically improve monocular VIO accuracy in vehicular
scenarios, achieving comparable or even better performance than state-of-art
stereo VIO solutions. The system could also be used for the auto-calibration of
IPM which is widely used in vehicle perception. A toolkit for ground feature
processing, together with the experimental datasets, would be made open-source
(this https URL).


------------------------------------------------------------------------------

Title:
OT-Net: A Reusable Neural Optimal Transport Solver

Abstract: With the widespread application of optimal transport (OT), its calculation
becomes essential, and various algorithms have emerged. However, the existing
methods either have low efficiency or cannot represent discontinuous maps. A
novel reusable neural OT solver OT-Net is thus presented, which first learns
Brenier's height representation via the neural network to obtain its potential,
and then gained the OT map by computing the gradient of the potential. The
algorithm has two merits, 1) it can easily represent discontinuous maps, which
allows it to match any target distribution with discontinuous supports and
achieve sharp boundaries. This can well eliminate mode collapse in the
generated models. 2) The OT map can be calculated straightly by the proposed
algorithm when new target samples are added, which greatly improves the
efficiency and reusability of the map. Moreover, the theoretical error bound of
the algorithm is analyzed, and we have demonstrated the empirical success of
our approach in image generation, color transfer, and domain adaptation.


------------------------------------------------------------------------------

Title:
NodeFormer: A Scalable Graph Structure Learning Transformer for Node  Classification

Abstract: Graph neural networks have been extensively studied for learning with
inter-connected data. Despite this, recent evidence has revealed GNNs'
deficiencies related to over-squashing, heterophily, handling long-range
dependencies, edge incompleteness and particularly, the absence of graphs
altogether. While a plausible solution is to learn new adaptive topology for
message passing, issues concerning quadratic complexity hinder simultaneous
guarantees for scalability and precision in large networks. In this paper, we
introduce a novel all-pair message passing scheme for efficiently propagating
node signals between arbitrary nodes, as an important building block for a
pioneering Transformer-style network for node classification on large graphs,
dubbed as \textsc{NodeFormer}. Specifically, the efficient computation is
enabled by a kernerlized Gumbel-Softmax operator that reduces the algorithmic
complexity to linearity w.r.t. node numbers for learning latent graph
structures from large, potentially fully-connected graphs in a differentiable
manner. We also provide accompanying theory as justification for our design.
Extensive experiments demonstrate the promising efficacy of the method in
various tasks including node classification on graphs (with up to 2M nodes) and
graph-enhanced applications (e.g., image classification) where input graphs are
missing.


------------------------------------------------------------------------------

Title:
Optimisation of Nonlinear Spring and Damper Characteristics for Vehicle  Ride and Handling Improvement

Abstract: In this paper, the optimum linear/nonlinear spring and linear/nonlinear
damper force versus displacement and force versus velocity characteristic
functions, respectively, are determined using simple lumped parameter models of
a quarter car front independent suspension and a half car rear solid axle
suspension of a light commercial vehicle. The complexity of a nonlinear
function optimisation problem is reduced by determining the shape a priori
based on typical shapes supplied by the car manufacturer and then scaling it up
or down in the optimisation process. The vehicle ride and handling responses
are investigated considering models of increased complexity. The linear and
nonlinear optimised spring characteristics are first obtained using lower
complexity lumped parameter models. The commercial vehicle dynamics software
Carmaker is then used in the optimisation as the higher complexity, more
realistic model. The performance of the optimised suspension units are also
verified using this more realistic Carmaker model.


------------------------------------------------------------------------------

Title:
Contrastive Loss is All You Need to Recover Analogies as Parallel Lines

Abstract: While static word embedding models are known to represent linguistic
analogies as parallel lines in high-dimensional space, the underlying mechanism
as to why they result in such geometric structures remains obscure. We find
that an elementary contrastive-style method employed over distributional
information performs competitively with popular word embedding models on
analogy recovery tasks, while achieving dramatic speedups in training time.
Further, we demonstrate that a contrastive loss is sufficient to create these
parallel structures in word embeddings, and establish a precise relationship
between the co-occurrence statistics and the geometric structure of the
resulting word embeddings.


------------------------------------------------------------------------------

Title:
Operationalising Representation in Natural Language Processing

Abstract: Despite its centrality in the philosophy of cognitive science, there has been
little prior philosophical work engaging with the notion of representation in
contemporary NLP practice. This paper attempts to fill that lacuna: drawing on
ideas from cognitive science, I introduce a framework for evaluating the
representational claims made about components of neural NLP models, proposing
three criteria with which to evaluate whether a component of a model represents
a property and operationalising these criteria using probing classifiers, a
popular analysis technique in NLP (and deep learning more broadly).
The project of operationalising a philosophically-informed notion of
representation should be of interest to both philosophers of science and NLP
practitioners. It affords philosophers a novel testing-ground for claims about
the nature of representation, and helps NLPers organise the large literature on
probing experiments, suggesting novel avenues for empirical research.


------------------------------------------------------------------------------

Title:
POP: Prompt Of Prompts for Continual Learning

Abstract: Continual learning (CL) has attracted increasing attention in the recent
past. It aims to mimic the human ability to learn new concepts without
catastrophic forgetting. While existing CL methods accomplish this to some
extent, they are still prone to semantic drift of the learned feature space.
Foundation models, which are endowed with a robust feature representation,
learned from very large datasets, provide an interesting substrate for the
solution of the CL problem. Recent work has also shown that they can be adapted
to specific tasks by prompt tuning techniques that leave the generality of the
representation mostly unscathed. An open question is, however, how to learn
both prompts that are task specific and prompts that are global, i.e. capture
cross-task information. In this work, we propose the Prompt Of Prompts (POP)
model, which addresses this goal by progressively learning a group of
task-specified prompts and a group of global prompts, denoted as POP, to
integrate information from the former. We show that a foundation model equipped
with POP learning is able to outperform classic CL methods by a significant
margin. Moreover, as prompt tuning only requires a small set of training
samples, POP is able to perform CL in the few-shot setting, while still
outperforming competing methods trained on the entire dataset.


------------------------------------------------------------------------------

Title:
Agile Catching with Whole-Body MPC and Blackbox Policy Learning

Abstract: We address a benchmark task in agile robotics: catching objects thrown at
high-speed. This is a challenging task that involves tracking, intercepting,
and cradling a thrown object with access only to visual observations of the
object and the proprioceptive state of the robot, all within a fraction of a
second. We present the relative merits of two fundamentally different solution
strategies: (i) Model Predictive Control using accelerated constrained
trajectory optimization, and (ii) Reinforcement Learning using zeroth-order
optimization. We provide insights into various performance trade-offs including
sample efficiency, sim-to-real transfer, robustness to distribution shifts, and
whole-body multimodality via extensive on-hardware experiments. We conclude
with proposals on fusing "classical" and "learning-based" techniques for agile
robot control. Videos of our experiments may be found at
this https URL


------------------------------------------------------------------------------

Title:
Inductive Linear Probing for Few-shot Node Classification

Abstract: Meta-learning has emerged as a powerful training strategy for few-shot node
classification, demonstrating its effectiveness in the transductive setting.
However, the existing literature predominantly focuses on transductive few-shot
node classification, neglecting the widely studied inductive setting in the
broader few-shot learning community. This oversight limits our comprehensive
understanding of the performance of meta-learning based methods on graph data.
In this work, we conduct an empirical study to highlight the limitations of
current frameworks in the inductive few-shot node classification setting.
Additionally, we propose a simple yet competitive baseline approach
specifically tailored for inductive few-shot node classification tasks. We hope
our work can provide a new path forward to better understand how the
meta-learning paradigm works in the graph domain.


------------------------------------------------------------------------------

Title:
EmoStim: A Database of Emotional Film Clips with Discrete and  Componential Assessment

Abstract: Emotion elicitation using emotional film clips is one of the most common and
ecologically valid methods in Affective Computing. However, selecting and
validating appropriate materials that evoke a range of emotions is challenging.
Here we present EmoStim: A Database of Emotional Film Clips as a film library
with a rich and varied content. EmoStim is designed for researchers interested
in studying emotions in relation to either discrete or componential models of
emotion. To create the database, 139 film clips were selected from literature
and then annotated by 638 participants through the CrowdFlower platform. We
selected 99 film clips based on the distribution of subjective ratings that
effectively distinguished between emotions defined by the discrete model. We
show that the selected film clips reliably induce a range of specific emotions
according to the discrete model. Further, we describe relationships between
emotions, emotion organization in the componential space, and underlying
dimensions representing emotional experience. The EmoStim database and
participant annotations are freely available for research purposes. The
database can be used to enrich our understanding of emotions further and serve
as a guide to select or create additional materials.


------------------------------------------------------------------------------

Title:
Safeguarding Data in Multimodal AI: A Differentially Private Approach to  CLIP Training

Abstract: The surge in multimodal AI's success has sparked concerns over data privacy
in vision-and-language tasks. While CLIP has revolutionized multimodal learning
through joint training on images and text, its potential to unintentionally
disclose sensitive information necessitates the integration of
privacy-preserving mechanisms. We introduce a differentially private adaptation
of the Contrastive Language-Image Pretraining (CLIP) model that effectively
addresses privacy concerns while retaining accuracy. Our proposed method,
Dp-CLIP, is rigorously evaluated on benchmark datasets encompassing diverse
vision-and-language tasks such as image classification and visual question
answering. We demonstrate that our approach retains performance on par with the
standard non-private CLIP model. Furthermore, we analyze our proposed algorithm
under linear representation settings. We derive the convergence rate of our
algorithm and show a trade-off between utility and privacy when gradients are
clipped per-batch and the loss function does not satisfy smoothness conditions
assumed in the literature for the analysis of DP-SGD.


------------------------------------------------------------------------------

Title:
TopP&R: Robust Support Estimation Approach for Evaluating Fidelity and  Diversity in Generative Models

Abstract: We propose a robust and reliable evaluation metric for generative models by
introducing topological and statistical treatments for rigorous support
estimation. Existing metrics, such as Inception Score (IS), Frechet Inception
Distance (FID), and the variants of Precision and Recall (P&R), heavily rely on
supports that are estimated from sample features. However, the reliability of
their estimation has not been seriously discussed (and overlooked) even though
the quality of the evaluation entirely depends on it. In this paper, we propose
Topological Precision and Recall (TopP&R, pronounced 'topper'), which provides
a systematic approach to estimating supports, retaining only topologically and
statistically important features with a certain level of confidence. This not
only makes TopP&R strong for noisy features, but also provides statistical
consistency. Our theoretical and experimental results show that TopP&R is
robust to outliers and non-independent and identically distributed (Non-IID)
perturbations, while accurately capturing the true trend of change in samples.
To the best of our knowledge, this is the first evaluation metric focused on
the robust estimation of the support and provides its statistical consistency
under noise.


------------------------------------------------------------------------------

Title:
Language models are not naysayers: An analysis of language models on  negation benchmarks

Abstract: Negation has been shown to be a major bottleneck for masked language models,
such as BERT. However, whether this finding still holds for larger-sized
auto-regressive language models (``LLMs'') has not been studied
comprehensively. With the ever-increasing volume of research and applications
of LLMs, we take a step back to evaluate the ability of current-generation LLMs
to handle negation, a fundamental linguistic phenomenon that is central to
language understanding. We evaluate different LLMs -- including the open-source
GPT-neo, GPT-3, and InstructGPT -- against a wide range of negation benchmarks.
Through systematic experimentation with varying model sizes and prompts, we
show that LLMs have several limitations including insensitivity to the presence
of negation, an inability to capture the lexical semantics of negation, and a
failure to reason under negation.


------------------------------------------------------------------------------

Title:
Unraveling the ARC Puzzle: Mimicking Human Solutions with Object-Centric  Decision Transformer

Abstract: In the pursuit of artificial general intelligence (AGI), we tackle
Abstraction and Reasoning Corpus (ARC) tasks using a novel two-pronged
approach. We employ the Decision Transformer in an imitation learning paradigm
to model human problem-solving, and introduce an object detection algorithm,
the Push and Pull clustering method. This dual strategy enhances AI's ARC
problem-solving skills and provides insights for AGI progression. Yet, our work
reveals the need for advanced data collection tools, robust training datasets,
and refined model structures. This study highlights potential improvements for
Decision Transformers and propels future AGI research.


------------------------------------------------------------------------------

Title:
ZeroForge: Feedforward Text-to-Shape Without 3D Supervision

Abstract: Current state-of-the-art methods for text-to-shape generation either require
supervised training using a labeled dataset of pre-defined 3D shapes, or
perform expensive inference-time optimization of implicit neural
representations. In this work, we present ZeroForge, an approach for zero-shot
text-to-shape generation that avoids both pitfalls. To achieve open-vocabulary
shape generation, we require careful architectural adaptation of existing
feed-forward approaches, as well as a combination of data-free CLIP-loss and
contrastive losses to avoid mode collapse. Using these techniques, we are able
to considerably expand the generative ability of existing feed-forward
text-to-shape models such as CLIP-Forge. We support our method via extensive
qualitative and quantitative evaluations


------------------------------------------------------------------------------

Title:
Graph Laplacian Learning with Exponential Family Noise

Abstract: A common challenge in applying graph machine learning methods is that the
underlying graph of a system is often unknown. Although different graph
inference methods have been proposed for continuous graph signals, inferring
the graph structure underlying other types of data, such as discrete counts, is
under-explored. In this paper, we generalize a graph signal processing (GSP)
framework for learning a graph from smooth graph signals to the exponential
family noise distribution to model various data types. We propose an
alternating algorithm that estimates the graph Laplacian as well as the
unobserved smooth representation from the noisy signals. We demonstrate in
synthetic and real-world data that our new algorithm outperforms competing
Laplacian estimation methods under noise model mismatch.


------------------------------------------------------------------------------

Title:
Contextual Font Recommendations based on User Intent

Abstract: Adobe Fonts has a rich library of over 20,000 unique fonts that Adobe users
utilize for creating graphics, posters, composites etc. Due to the nature of
the large library, knowing what font to select can be a daunting task that
requires a lot of experience. For most users in Adobe products, especially
casual users of Adobe Express, this often means choosing the default font
instead of utilizing the rich and diverse fonts available. In this work, we
create an intent-driven system to provide contextual font recommendations to
users to aid in their creative journey. Our system takes in multilingual text
input and recommends suitable fonts based on the user's intent. Based on user
entitlements, the mix of free and paid fonts is adjusted. The feature is
currently used by millions of Adobe Express users with a CTR of >25%.


------------------------------------------------------------------------------

Title:
Solving Large-scale Spatial Problems with Convolutional Neural Networks

Abstract: Over the past decade, deep learning research has been accelerated by
increasingly powerful hardware, which facilitated rapid growth in the model
complexity and the amount of data ingested. This is becoming unsustainable and
therefore refocusing on efficiency is necessary. In this paper, we employ
transfer learning to improve training efficiency for large-scale spatial
problems. We propose that a convolutional neural network (CNN) can be trained
on small windows of signals, but evaluated on arbitrarily large signals with
little to no performance degradation, and provide a theoretical bound on the
resulting generalization error. Our proof leverages shift-equivariance of CNNs,
a property that is underexploited in transfer learning. The theoretical results
are experimentally supported in the context of mobile infrastructure on demand
(MID). The proposed approach is able to tackle MID at large scales with
hundreds of agents, which was computationally intractable prior to this work.


------------------------------------------------------------------------------

Title:
Assessing the Effectiveness of GPT-3 in Detecting False Political  Statements: A Case Study on the LIAR Dataset

Abstract: The detection of political fake statements is crucial for maintaining
information integrity and preventing the spread of misinformation in society.
Historically, state-of-the-art machine learning models employed various methods
for detecting deceptive statements. These methods include the use of metadata
(W. Wang et al., 2018), n-grams analysis (Singh et al., 2021), and linguistic
(Wu et al., 2022) and stylometric (Islam et al., 2020) features. Recent
advancements in large language models, such as GPT-3 (Brown et al., 2020) have
achieved state-of-the-art performance on a wide range of tasks. In this study,
we conducted experiments with GPT-3 on the LIAR dataset (W. Wang et al., 2018)
and achieved higher accuracy than state-of-the-art models without using any
additional meta or linguistic features. Additionally, we experimented with
zero-shot learning using a carefully designed prompt and achieved near
state-of-the-art performance. An advantage of this approach is that the model
provided evidence for its decision, which adds transparency to the model's
decision-making and offers a chance for users to verify the validity of the
evidence provided.


------------------------------------------------------------------------------

Title:
Inertial randomized Kaczmarz algorithms for solving coherent linear  systems

Abstract: In this paper, by regarding the two-subspace Kaczmarz method [20] as an
alternated inertial randomized Kaczmarz algorithm we present a new convergence
rate estimate which is shown to be better than that in [20] under a mild
condition. Furthermore, we accelerate the alternated inertial randomized
Kaczmarz algorithm and introduce a multi-step inertial randomized Kaczmarz
algorithm which is proved to have a faster convergence rate. Numerical
experiments support the theory results and illustrate that the multi-inertial
randomized Kaczmarz algorithm significantly outperform the two-subspace
Kaczmarz method in solving coherent linear systems.


------------------------------------------------------------------------------

Title:
Is Your Wallet Snitching On You? An Analysis on the Privacy Implications  of Web3

Abstract: With the recent hype around the Metaverse and NFTs, Web3 is getting more and
more popular. The goal of Web3 is to decentralize the web via decentralized
applications. Wallets play a crucial role as they act as an interface between
these applications and the user. Wallets such as MetaMask are being used by
millions of users nowadays. Unfortunately, Web3 is often advertised as more
secure and private. However, decentralized applications as well as wallets are
based on traditional technologies, which are not designed with privacy of users
in mind. In this paper, we analyze the privacy implications that Web3
technologies such as decentralized applications and wallets have on users. To
this end, we build a framework that measures exposure of wallet information.
First, we study whether information about installed wallets is being used to
track users online. We analyze the top 100K websites and find evidence of 1,325
websites running scripts that probe whether users have wallets installed in
their browser. Second, we measure whether decentralized applications and
wallets leak the user's unique wallet address to third-parties. We intercept
the traffic of 616 decentralized applications and 100 wallets and find over
2000 leaks across 211 applications and more than 300 leaks across 13 wallets.
Our study shows that Web3 poses a threat to users' privacy and requires new
designs towards more privacy-aware wallet architectures.


------------------------------------------------------------------------------

Title:
Decentralizing Custodial Wallets with MFKDF

Abstract: The average cryptocurrency user today faces a difficult choice between
centralized custodial wallets, which are notoriously prone to spontaneous
collapse, or cumbersome self-custody solutions, which if not managed properly
can cause a total loss of funds. In this paper, we present a "best of both
worlds" cryptocurrency wallet design that looks like, and inherits the user
experience of, a centralized custodial solution, while in fact being entirely
decentralized in design and implementation. In our design, private keys are not
stored on any device, but are instead derived directly from a user's
authentication factors, such as passwords, soft tokens (e.g., Google
Authenticator), hard tokens (e.g., YubiKey), or out-of-band authentication
(e.g., SMS). Public parameters (salts, one-time pads, etc.) needed to access
the wallet can be safely stored in public view, such as on a public blockchain,
thereby providing strong availability guarantees. Users can then simply "log
in" to their decentralized wallet on any device using standard credentials and
even recover from lost credentials, thereby providing the usability of a
custodial wallet with the trust and security of a decentralized approach.


------------------------------------------------------------------------------

Title:
Beyond Black Box AI-Generated Plagiarism Detection: From Sentence to  Document Level

Abstract: The increasing reliance on large language models (LLMs) in academic writing
has led to a rise in plagiarism. Existing AI-generated text classifiers have
limited accuracy and often produce false positives. We propose a novel approach
using natural language processing (NLP) techniques, offering quantifiable
metrics at both sentence and document levels for easier interpretation by human
evaluators. Our method employs a multi-faceted approach, generating multiple
paraphrased versions of a given question and inputting them into the LLM to
generate answers. By using a contrastive loss function based on cosine
similarity, we match generated sentences with those from the student's
response. Our approach achieves up to 94% accuracy in classifying human and AI
text, providing a robust and adaptable solution for plagiarism detection in
academic settings. This method improves with LLM advancements, reducing the
need for new model training or reconfiguration, and offers a more transparent
way of evaluating and detecting AI-generated text.


------------------------------------------------------------------------------

Title:
Don't Leak Your Keys: Understanding, Measuring, and Exploiting the  AppSecret Leaks in Mini-Programs

Abstract: Mobile mini-programs in WeChat have gained significant popularity since their
debut in 2017, reaching a scale similar to that of Android apps in the Play
Store. Like Google, Tencent, the provider of WeChat, offers APIs to support the
development of mini-programs and also maintains a mini-program market within
the WeChat app. However, mini-program APIs often manage sensitive user data
within the social network platform, both on the WeChat client app and in the
cloud. As a result, cryptographic protocols have been implemented to secure
data access. In this paper, we demonstrate that WeChat should have required the
use of the "appsecret" master key, which is used to authenticate a
mini-program, to be used only in the mini-program back-end. If this key is
leaked in the front-end of the mini-programs, it can lead to catastrophic
attacks on both mini-program developers and users. Using a mini-program crawler
and a master key leakage inspector, we measured 3,450,586 crawled mini-programs
and found that 40,880 of them had leaked their master keys, allowing attackers
to carry out various attacks such as account hijacking, promotion abuse, and
service theft. Similar issues were confirmed through testing and measuring of
Baidu mini-programs too. We have reported these vulnerabilities and the list of
vulnerable mini-programs to Tencent and Baidu, which awarded us with bug
bounties, and also Tencent recently released a new API to defend against these
attacks based on our findings.


------------------------------------------------------------------------------

Title:
Fast-Grasp'D: Dexterous Multi-finger Grasp Generation Through  Differentiable Simulation

Abstract: Multi-finger grasping relies on high quality training data, which is hard to
obtain: human data is hard to transfer and synthetic data relies on simplifying
assumptions that reduce grasp quality. By making grasp simulation
differentiable, and contact dynamics amenable to gradient-based optimization,
we accelerate the search for high-quality grasps with fewer limiting
assumptions. We present Grasp'D-1M: a large-scale dataset for multi-finger
robotic grasping, synthesized with Fast- Grasp'D, a novel differentiable
grasping simulator. Grasp'D- 1M contains one million training examples for
three robotic hands (three, four and five-fingered), each with multimodal
visual inputs (RGB+depth+segmentation, available in mono and stereo). Grasp
synthesis with Fast-Grasp'D is 10x faster than GraspIt! and 20x faster than the
prior Grasp'D differentiable simulator. Generated grasps are more stable and
contact-rich than GraspIt! grasps, regardless of the distance threshold used
for contact generation. We validate the usefulness of our dataset by retraining
an existing vision-based grasping pipeline on Grasp'D-1M, and showing a
dramatic increase in model performance, predicting grasps with 30% more
contact, a 33% higher epsilon metric, and 35% lower simulated displacement.
Additional details at this https URL


------------------------------------------------------------------------------

Title:
The aesthetics of cyber security: How do users perceive them?

Abstract: While specific aesthetic philosophies may differ across cultures, all human
societies have used aesthetics to support communication and learning. Within
the fields of usability and usable security, aesthetics have been deployed for
such diverse purposes as enhancing students' e-learning experiences and
optimising user interface design. In this paper, we seek to understand how
individual users perceive the visual assets that accompany cyber security
information, and how these visual assets and user perceptions underwrite a
distinct \emph{cyber security aesthetic}. We ask, (1) What constitutes cyber
security aesthetics, from the perspective of an individual user? and (2) How
might these aesthetics affect users' perceived self-efficacy as they informally
learn cyber security precepts? To begin answering these questions, we compile
an image-set from cyber security web articles and analyse the distinct visual
properties and sentiments of these images.


------------------------------------------------------------------------------

Title:
AutoML in the Age of Large Language Models: Current Challenges, Future  Opportunities and Risks

Abstract: The fields of both Natural Language Processing (NLP) and Automated Machine
Learning (AutoML) have achieved remarkable results over the past years. In NLP,
especially Large Language Models (LLMs) have experienced a rapid series of
breakthroughs very recently. We envision that the two fields can radically push
the boundaries of each other through tight integration. To showcase this
vision, we explore the potential of a symbiotic relationship between AutoML and
LLMs, shedding light on how they can benefit each other. In particular, we
investigate both the opportunities to enhance AutoML approaches with LLMs from
different perspectives and the challenges of leveraging AutoML to further
improve LLMs. To this end, we survey existing work, and we critically assess
risks. We strongly believe that the integration of the two fields has the
potential to disrupt both fields, NLP and AutoML. By highlighting conceivable
synergies, but also risks, we aim to foster further exploration at the
intersection of AutoML and LLMs.


------------------------------------------------------------------------------

Title:
Ergonomic-Centric Holography: Optimizing Realism,Immersion, and Comfort  for Holographic Display

Abstract: We introduce ergonomic-centric holography, an algorithmic framework that
simultaneously optimizes for realistic incoherent defocus, unrestricted pupil
movements in the eye box, and high-order diffractions for filtering-free
holography. The proposed method outperforms prior algorithms on holographic
display prototypes operating in unfiltered and pupil-mimicking modes, offering
the potential to enhance next-generation virtual and augmented reality
experiences.


------------------------------------------------------------------------------

Title:
Can ChatGPT Enable ITS? The Case of Mixed Traffic Control via  Reinforcement Learning

Abstract: The surge in Reinforcement Learning (RL) applications in Intelligent
Transportation Systems (ITS) has contributed to its growth as well as
highlighted key challenges. However, defining objectives of RL agents in
traffic control and management tasks, as well as aligning policies with these
goals through an effective formulation of Markov Decision Process (MDP), can be
challenging and often require domain experts in both RL and ITS. Recent
advancements in Large Language Models (LLMs) such as GPT-4 highlight their
broad general knowledge, reasoning capabilities, and commonsense priors across
various domains. In this work, we conduct a large-scale user study involving 70
participants to investigate whether novices can leverage ChatGPT to solve
complex mixed traffic control problems. Three environments are tested,
including ring road, bottleneck, and intersection. We find ChatGPT has mixed
results. For intersection and bottleneck, ChatGPT increases number of
successful policies by 150% and 136% compared to solely beginner capabilities,
with some of them even outperforming experts. However, ChatGPT does not provide
consistent improvements across all scenarios.


------------------------------------------------------------------------------

Title:
Better Generalization with Semantic IDs: A case study in Ranking for  Recommendations

Abstract: Training good representations for items is critical in recommender models.
Typically, an item is assigned a unique randomly generated ID, and is commonly
represented by learning an embedding corresponding to the value of the random
ID. Although widely used, this approach have limitations when the number of
items are large and items are power-law distributed -- typical characteristics
of real-world recommendation systems. This leads to the item cold-start
problem, where the model is unable to make reliable inferences for tail and
previously unseen items. Removing these ID features and their learned
embeddings altogether to combat cold-start issue severely degrades the
recommendation quality. Content-based item embeddings are more reliable, but
they are expensive to store and use, particularly for users' past item
interaction sequence. In this paper, we use Semantic IDs, a compact discrete
item representations learned from content embeddings using RQ-VAE that captures
hierarchy of concepts in items. We showcase how we use them as a replacement of
item IDs in a resource-constrained ranking model used in an industrial-scale
video sharing platform. Moreover, we show how Semantic IDs improves the
generalization ability of our system, without sacrificing top-level metrics.


------------------------------------------------------------------------------

Title:
Cooperative Adaptive Cruise Control Design and Implementation

Abstract: In this manuscript a design and implementation of CACC on an autonomous
vehicle platform (2017 Ford Fusion) is presented. The developed CACC controls
the intervehicle distance between the target vehicle and ego vehicle using a
feedforward PD controller. In this design the feedforward information is the
acceleration of the target vehicle which is communicated through Dedicated
Short-Range Communication (DSRC) modem. The manuscript explains the detailed
architecture of the designed CACC with used hardware and methods for the both
simulation and experiments. Also, an approach to overcome detection failures at
the curved roads is presented to improve overall quality of the designed CACC
system. As a result, the initial simulation and experimental results with the
designed CACC system is presented in the paper. The presented results indicate
that CACC improves the car following performance of the ego vehicle as compared
to the classical Adaptive Cruise Controller.


------------------------------------------------------------------------------

Title:
Machine Learning Approach on Multiclass Classification of Internet  Firewall Log Files

Abstract: Firewalls are critical components in securing communication networks by
screening all incoming (and occasionally exiting) data packets. Filtering is
carried out by comparing incoming data packets to a set of rules designed to
prevent malicious code from entering the network. To regulate the flow of data
packets entering and leaving a network, an Internet firewall keeps a track of
all activity. While the primary function of log files is to aid in
troubleshooting and diagnostics, the information they contain is also very
relevant to system audits and forensics. Firewalls primary function is to
prevent malicious data packets from being sent. In order to better defend
against cyberattacks and understand when and how malicious actions are
influencing the internet, it is necessary to examine log files. As a result,
the firewall decides whether to 'allow,' 'deny,' 'drop,' or 'reset-both' the
incoming and outgoing packets. In this research, we apply various
categorization algorithms to make sense of data logged by a firewall device.
Harmonic mean F1 score, recall, and sensitivity measurement data with a 99%
accuracy score in the random forest technique are used to compare the
classifier's performance. To be sure, the proposed characteristics did
significantly contribute to enhancing the firewall classification rate, as seen
by the high accuracy rates generated by the other methods.


------------------------------------------------------------------------------

Title:
Where Does My Model Underperform? A Human Evaluation of Slice Discovery  Algorithms

Abstract: Machine learning (ML) models that achieve high average accuracy can still
underperform on semantically coherent subsets (i.e. "slices") of data. This
behavior can have significant societal consequences for the safety or bias of
the model in deployment, but identifying these underperforming slices can be
difficult in practice, especially in domains where practitioners lack access to
group annotations to define coherent subsets of their data. Motivated by these
challenges, ML researchers have developed new slice discovery algorithms that
aim to group together coherent and high-error subsets of data. However, there
has been little evaluation focused on whether these tools help humans form
correct hypotheses about where (for which groups) their model underperforms. We
conduct a controlled user study (N = 15) where we show 40 slices output by two
state-of-the-art slice discovery algorithms to users, and ask them to form
hypotheses about where an object detection model underperforms. Our results
provide positive evidence that these tools provide some benefit over a naive
baseline, and also shed light on challenges faced by users during the
hypothesis formation step. We conclude by discussing design opportunities for
ML and HCI researchers. Our findings point to the importance of centering users
when designing and evaluating new tools for slice discovery.


------------------------------------------------------------------------------

Title:
Privacy Inference-Empowered Stealthy Backdoor Attack on Federated  Learning under Non-IID Scenarios

Abstract: Federated learning (FL) naturally faces the problem of data heterogeneity in
real-world scenarios, but this is often overlooked by studies on FL security
and privacy. On the one hand, the effectiveness of backdoor attacks on FL may
drop significantly under non-IID scenarios. On the other hand, malicious
clients may steal private data through privacy inference attacks. Therefore, it
is necessary to have a comprehensive perspective of data heterogeneity,
backdoor, and privacy inference. In this paper, we propose a novel privacy
inference-empowered stealthy backdoor attack (PI-SBA) scheme for FL under
non-IID scenarios. Firstly, a diverse data reconstruction mechanism based on
generative adversarial networks (GANs) is proposed to produce a supplementary
dataset, which can improve the attacker's local data distribution and support
more sophisticated strategies for backdoor attacks. Based on this, we design a
source-specified backdoor learning (SSBL) strategy as a demonstration, allowing
the adversary to arbitrarily specify which classes are susceptible to the
backdoor trigger. Since the PI-SBA has an independent poisoned data synthesis
process, it can be integrated into existing backdoor attacks to improve their
effectiveness and stealthiness in non-IID scenarios. Extensive experiments
based on MNIST, CIFAR10 and Youtube Aligned Face datasets demonstrate that the
proposed PI-SBA scheme is effective in non-IID FL and stealthy against
state-of-the-art defense methods.


------------------------------------------------------------------------------

Title:
Analogue and Physical Reservoir Computing Using Water Waves

Abstract: More than 3.5 billion people live in rural areas, where water and water
energy resources play an important role in ensuring sustainable and productive
rural economies. This article reviews and critically analyses the recent
advances in the field of analogue and reservoir computing that have been driven
by unique physical properties and energy of water waves. It also demonstrates
that analogue and reservoir computing hold the potential to bring artificial
intelligence closer to people living outside large cities, thus enabling them
to enjoy the benefits of novel technologies that already work in large cities
but are not readily available and suitable for regional communities.


------------------------------------------------------------------------------

Title:
Uncovering and Exploiting Hidden APIs in Mobile Super Apps

Abstract: Mobile applications, particularly those from social media platforms such as
WeChat and TikTok, are evolving into "super apps" that offer a wide range of
services such as instant messaging and media sharing, e-commerce, e-learning,
and e-government. These super apps often provide APIs for developers to create
"miniapps" that run within the super app. These APIs should have been
thoroughly scrutinized for security. Unfortunately, we find that many of them
are undocumented and unsecured, potentially allowing miniapps to bypass
restrictions and gain higher privileged access. To systematically identify
these hidden APIs before they are exploited by attackers, we developed a tool
APIScope with both static analysis and dynamic analysis, where static analysis
is used to recognize hidden undocumented APIs, and dynamic analysis is used to
confirm whether the identified APIs can be invoked by an unprivileged 3rdparty
miniapps. We have applied APIScope to five popular super apps (i.e., WeChat,
WeCom, Baidu, QQ, and Tiktok) and found that all of them contain hidden APIs,
many of which can be exploited due to missing security checks. We have also
quantified the hidden APIs that may have security implications by verifying if
they have access to resources protected by Android permissions. Furthermore, we
demonstrate the potential security hazards by presenting various attack
scenarios, including unauthorized access to any web pages, downloading and
installing malicious software, and stealing sensitive information. We have
reported our findings to the relevant vendors, some of whom have patched the
vulnerabilities and rewarded us with bug bounties.


------------------------------------------------------------------------------

Title:
PersonaPKT: Building Personalized Dialogue Agents via  Parameter-efficient Knowledge Transfer

Abstract: Personalized dialogue agents (DAs) powered by large pre-trained language
models (PLMs) often rely on explicit persona descriptions to maintain
personality consistency. However, such descriptions may not always be available
or may pose privacy concerns. To tackle this bottleneck, we introduce
PersonaPKT, a lightweight transfer learning approach that can build
persona-consistent dialogue models without explicit persona descriptions. By
representing each persona as a continuous vector, PersonaPKT learns implicit
persona-specific features directly from a small number of dialogue samples
produced by the same persona, adding less than 0.1% trainable parameters for
each persona on top of the PLM backbone. Empirical results demonstrate that
PersonaPKT effectively builds personalized DAs with high storage efficiency,
outperforming various baselines in terms of persona consistency while
maintaining good response generation quality. In addition, it enhances privacy
protection by avoiding explicit persona descriptions. Overall, PersonaPKT is an
effective solution for creating personalized DAs that respect user privacy.


------------------------------------------------------------------------------

Title:
Enhanced Sampling with Machine Learning: A Review

Abstract: Molecular dynamics (MD) enables the study of physical systems with excellent
spatiotemporal resolution but suffers from severe time-scale limitations. To
address this, enhanced sampling methods have been developed to improve
exploration of configurational space. However, implementing these is
challenging and requires domain expertise. In recent years, integration of
machine learning (ML) techniques in different domains has shown promise,
prompting their adoption in enhanced sampling as well. Although ML is often
employed in various fields primarily due to its data-driven nature, its
integration with enhanced sampling is more natural with many common underlying
synergies. This review explores the merging of ML and enhanced MD by presenting
different shared viewpoints. It offers a comprehensive overview of this rapidly
evolving field, which can be difficult to stay updated on. We highlight
successful strategies like dimensionality reduction, reinforcement learning,
and flow-based methods. Finally, we discuss open problems at the exciting
ML-enhanced MD interface.


------------------------------------------------------------------------------

Title:
Learning on Graphs under Label Noise

Abstract: Node classification on graphs is a significant task with a wide range of
applications, including social analysis and anomaly detection. Even though
graph neural networks (GNNs) have produced promising results on this task,
current techniques often presume that label information of nodes is accurate,
which may not be the case in real-world applications. To tackle this issue, we
investigate the problem of learning on graphs with label noise and develop a
novel approach dubbed Consistent Graph Neural Network (CGNN) to solve it.
Specifically, we employ graph contrastive learning as a regularization term,
which promotes two views of augmented nodes to have consistent representations.
Since this regularization term cannot utilize label information, it can enhance
the robustness of node representations to label noise. Moreover, to detect
noisy labels on the graph, we present a sample selection technique based on the
homophily assumption, which identifies noisy nodes by measuring the consistency
between the labels with their neighbors. Finally, we purify these confident
noisy labels to permit efficient semantic graph learning. Extensive experiments
on three well-known benchmark datasets demonstrate the superiority of our CGNN
over competing approaches.


------------------------------------------------------------------------------

Title:
Unpacking Privacy Labels: A Measurement and Developer Perspective on  Google's Data Safety Section

Abstract: Google has mandated developers to use Data Safety Sections (DSS) to increase
transparency in data collection and sharing practices. In this paper, we
present a comprehensive analysis of Google's Data Safety Section (DSS) using
both quantitative and qualitative methods. We conduct the first large-scale
measurement study of DSS using apps from Android Play store (n=1.1M). We find
that there are internal inconsistencies within the reported practices. We also
find trends of both over and under-reporting practices in the DSSs.
Next, we conduct a longitudinal study of DSS to explore how the reported
practices evolve over time, and find that the developers are still adjusting
their practices. To contextualize these findings, we conduct a developer study,
uncovering the process that app developers undergo when working with DSS. We
highlight the challenges faced and strategies employed by developers for DSS
submission, and the factors contributing to changes in the DSS. Our research
contributes valuable insights into the complexities of implementing and
maintaining privacy labels, underlining the need for better resources, tools,
and guidelines to aid developers. This understanding is crucial as the accuracy
and reliability of privacy labels directly impact their effectiveness.


------------------------------------------------------------------------------

Title:
Modification in Elliptic Curve Cryptography based Mutual authentication  scheme for smart grid communication using biometric approach

Abstract: Smart grid is a self-sufficient system. That tracks how the energy is used
from its source to its final destination. The smart grid can increase the
service quality while reducing the consumption of electricity. However, the
safety and confidentiality of information data is the major challenge in smart
grid environment. To overcome this there are numerous authentication procedures
that have been documented. The mutual authentication system for the smart grid
that is based on elliptic curve cryptography and biometrics was thus introduced
by A.A. Khan et al.s. This protocol is secure from various attacks. But we
found an inability of password and biometric updating phase. Therefore we
provided the password and biometric updating phase in this protocol.


------------------------------------------------------------------------------

Title:
Accelerated Convergence of Nesterov's Momentum for Deep Neural Networks  under Partial Strong Convexity

Abstract: Current state-of-the-art analyses on the convergence of gradient descent for
training neural networks focus on characterizing properties of the loss
landscape, such as the Polyak-Lojaciewicz (PL) condition and the restricted
strong convexity. While gradient descent converges linearly under such
conditions, it remains an open question whether Nesterov's momentum enjoys
accelerated convergence under similar settings and assumptions. In this work,
we consider a new class of objective functions, where only a subset of the
parameters satisfies strong convexity, and show Nesterov's momentum achieves
acceleration in theory for this objective class. We provide two realizations of
the problem class, one of which is deep ReLU networks, which --to the best of
our knowledge--constitutes this work the first that proves accelerated
convergence rate for non-trivial neural network architectures.


------------------------------------------------------------------------------

Title:
Correct-by-Construction Design of Contextual Robotic Missions Using  Contracts

Abstract: Effectively specifying and implementing robotic missions pose a set of
challenges to software engineering for robotic systems, since they require
formalizing and executing a robot's high-level tasks while considering various
application scenarios and conditions, also known as contexts, in real-world
operational environments.
Writing correct mission specifications that explicitly account for multiple
contexts can be a tedious and error-prone task. Moreover, as the number of
context, hence the specification, becomes more complex, generating a
correct-by-construction implementation, e.g., by using synthesis methods, can
become intractable. A viable approach to address these issues is to decompose
the mission specification into smaller sub-missions, with each sub-mission
corresponding to a specific context. However, such a compositional approach
would still pose challenges in ensuring the overall mission correctness.
In this paper, we propose a new, compositional framework for the
specification and implementation of contextual robotic missions using
assume-guarantee contracts. The mission specification is captured in a
hierarchical and modular way and each sub-mission is synthesized as a robot
controller. We address the problem of dynamically switching between sub-mission
controllers while ensuring correctness under certain conditions.


------------------------------------------------------------------------------

Title:
Multi-Factor Credential Hashing for Asymmetric Brute-Force Attack  Resistance

Abstract: Since the introduction of bcrypt in 1999, adaptive password hashing
functions, whereby brute-force resistance increases symmetrically with
computational difficulty for legitimate users, have been our most powerful
post-breach countermeasure against credential disclosure. Unfortunately, the
relatively low tolerance of users to added latency places an upper bound on the
deployment of this technique in most applications. In this paper, we present a
multi-factor credential hashing function (MFCHF) that incorporates the
additional entropy of multi-factor authentication into password hashes to
provide asymmetric resistance to brute-force attacks. MFCHF provides full
backward compatibility with existing authentication software (e.g., Google
Authenticator) and hardware (e.g., YubiKeys), with support for common usability
features like factor recovery. The result is a 10^6 to 10^48 times increase in
the difficulty of cracking hashed credentials, with little added latency or
usability impact.


------------------------------------------------------------------------------

Title:
ChatGPT vs. Lightweight Security: First Work Implementing the NIST  Cryptographic Standard ASCON

Abstract: This study, to the best of our knowledge, is the first to explore the
intersection between lightweight cryptography (LWC) and advanced artificial
intelligence (AI) language models. LWC, in particular the ASCON algorithm which
has been selected as the LWC standard by the National Institute of Standards
and Technology (NIST) in Feb. 2023, has become increasingly significant for
preserving data security given the quick expansion and resource limitations of
Internet of Things (IoT) devices. On the other hand, OpenAI's large language
model (LLM) ChatGPT has demonstrated significant potential in producing
complex, human-like text. This paper offers a novel method for implementing the
NIST LWC standard, ASCON, using the GPT-4 model. Moreover, this paper details
the design and functionality of ASCON, the procedures and actual Python
implementation of ASCON using ChatGPT, and a discussion of the outcomes. The
results contribute valuable insights into the efficient application of advanced
AI language models in cryptography, particularly in constrained environments.
Source code can be found at: this https URL


------------------------------------------------------------------------------

Title:
Dynamic Clustering Transformer Network for Point Cloud Segmentation

Abstract: Point cloud segmentation is one of the most important tasks in computer
vision with widespread scientific, industrial, and commercial applications. The
research thereof has resulted in many breakthroughs in 3D object and scene
understanding. Previous methods typically utilized hierarchical architectures
for feature representation. However, the commonly used sampling and grouping
methods in hierarchical networks are only based on point-wise three-dimensional
coordinates, ignoring local semantic homogeneity of point clusters.
Additionally, the prevalent Farthest Point Sampling (FPS) method is often a
computational bottleneck. To address these issues, we propose a novel 3D point
cloud representation network, called Dynamic Clustering Transformer Network
(DCTNet). It has an encoder-decoder architecture, allowing for both local and
global feature learning. Specifically, we propose novel semantic feature-based
dynamic sampling and clustering methods in the encoder, which enables the model
to be aware of local semantic homogeneity for local feature aggregation.
Furthermore, in the decoder, we propose an efficient semantic feature-guided
upsampling method. Our method was evaluated on an object-based dataset
(ShapeNet), an urban navigation dataset (Toronto-3D), and a multispectral LiDAR
dataset, verifying the performance of DCTNet across a wide variety of practical
engineering applications. The inference speed of DCTNet is 3.8-16.8$\times$
faster than existing State-of-the-Art (SOTA) models on the ShapeNet dataset,
while achieving an instance-wise mIoU of $86.6\%$, the current top score. Our
method similarly outperforms previous methods on the other datasets, verifying
it as the new State-of-the-Art in point cloud segmentation.


------------------------------------------------------------------------------

Title:
BPKD: Boundary Privileged Knowledge Distillation For Semantic  Segmentation

Abstract: Current approaches for knowledge distillation in semantic segmentation tend
to adopt a holistic approach that treats all spatial locations equally.
However, for dense prediction tasks, it is crucial to consider the knowledge
representation for different spatial locations in a different manner.
Furthermore, edge regions between adjacent categories are highly uncertain due
to context information leakage, which is particularly pronounced for compact
networks. To address this challenge, this paper proposes a novel approach
called boundary-privileged knowledge distillation (BPKD). BPKD distills the
knowledge of the teacher model's body and edges separately from the compact
student model. Specifically, we employ two distinct loss functions: 1) Edge
Loss, which aims to distinguish between ambiguous classes at the pixel level in
edge regions. 2) Body Loss, which utilizes shape constraints and selectively
attends to the inner-semantic regions. Our experiments demonstrate that the
proposed BPKD method provides extensive refinements and aggregation for edge
and body regions. Additionally, the method achieves state-of-the-art
distillation performance for semantic segmentation on three popular benchmark
datasets, highlighting its effectiveness and generalization ability. BPKD shows
consistent improvements over various lightweight semantic segmentation
structures. The code is available at \url{this https URL}.


------------------------------------------------------------------------------

Title:
Realising Synthetic Active Inference Agents, Part I: Epistemic  Objectives and Graphical Specification Language

Abstract: The Free Energy Principle (FEP) is a theoretical framework for describing how
(intelligent) systems self-organise into coherent, stable structures by
minimising a free energy functional. Active Inference (AIF) is a corollary of
the FEP that specifically details how systems that are able to plan for the
future (agents) function by minimising particular free energy functionals that
incorporate information seeking components. This paper is the first in a series
of two where we derive a synthetic version of AIF on free form factor graphs.
The present paper focuses on deriving a local version of the free energy
functionals used for AIF. This enables us to construct a version of AIF which
applies to arbitrary graphical models and interfaces with prior work on message
passing algorithms. The resulting messages are derived in our companion paper.
We also identify a gap in the graphical notation used for factor graphs. While
factor graphs are great at expressing a generative model, they have so far been
unable to specify the full optimisation problem including constraints. To solve
this problem we develop Constrained Forney-style Factor Graph (CFFG) notation
which permits a fully graphical description of variational inference
objectives. We then proceed to show how CFFG's can be used to reconstruct prior
algorithms for AIF as well as derive new ones. The latter is demonstrated by
deriving an algorithm that permits direct policy inference for AIF agents,
circumventing a long standing scaling issue that has so far hindered the
application of AIF in industrial settings. We demonstrate our algorithm on the
classic T-maze task and show that it reproduces the information seeking
behaviour that is a hallmark feature of AIF.


------------------------------------------------------------------------------

Title:
Multi-market Energy Optimization with Renewables via Reinforcement  Learning

Abstract: This paper introduces a deep reinforcement learning (RL) framework for
optimizing the operations of power plants pairing renewable energy with
storage. The objective is to maximize revenue from energy markets while
minimizing storage degradation costs and renewable curtailment. The framework
handles complexities such as time coupling by storage devices, uncertainty in
renewable generation and energy prices, and non-linear storage models. The
study treats the problem as a hierarchical Markov Decision Process (MDP) and
uses component-level simulators for storage. It utilizes RL to incorporate
complex storage models, overcoming restrictions of optimization-based methods
that require convex and differentiable component models. A significant aspect
of this approach is ensuring policy actions respect system constraints,
achieved via a novel method of projecting potentially infeasible actions onto a
safe state-action set. The paper demonstrates the efficacy of this approach
through extensive experiments using data from US and Indian electricity
markets, comparing the learned RL policies with a baseline control policy and a
retrospective optimal control policy. It validates the adaptability of the
learning framework with various storage models and shows the effectiveness of
RL in a complex energy optimization setting, in the context of multi-market
bidding, probabilistic forecasts, and accurate storage component models.


------------------------------------------------------------------------------

Title:
DORSal: Diffusion for Object-centric Representations of Scenes  $\textit{et al.}$

Abstract: Recent progress in 3D scene understanding enables scalable learning of
representations across large datasets of diverse scenes. As a consequence,
generalization to unseen scenes and objects, rendering novel views from just a
single or a handful of input images, and controllable scene generation that
supports editing, is now possible. However, training jointly on a large number
of scenes typically compromises rendering quality when compared to single-scene
optimized models such as NeRFs. In this paper, we leverage recent progress in
diffusion models to equip 3D scene representation learning models with the
ability to render high-fidelity novel views, while retaining benefits such as
object-level scene editing to a large degree. In particular, we propose DORSal,
which adapts a video diffusion architecture for 3D scene generation conditioned
on object-centric slot-based representations of scenes. On both complex
synthetic multi-object scenes and on the real-world large-scale Street View
dataset, we show that DORSal enables scalable neural rendering of 3D scenes
with object-level editing and improves upon existing approaches.


------------------------------------------------------------------------------

Title:
The interplay of fatigue dynamics and task achievement using optimal  control predictive simulation

Abstract: Predictive simulation of human motion could provide insight into optimal
techniques. In repetitive or long-duration tasks, these simulations must
predict fatigue-induced adaptation. However, most studies minimize cost
function terms related to actuator activations, assuming it minimizes fatigue.
An additional modeling layer is needed to consider the previous use of muscles
to reveal adaptive strategies to the decreased force production capability.
Here, we propose interfacing Xia's three-compartment fatigue dynamics model
with rigid-body dynamics. A stabilization invariant was added to Xia's model.
We simulated the maximum repetition of dumbbell biceps curls as an optimal
control problem (OCP) using direct multiple shooting. We explored three cost
functions (minimizing minimum torque, fatigue, or both) and two OCP
formulations (full-horizon and sliding-horizon approaches). We found that Xia's
model modified with the stabilization invariant (10 or 5) was adapted to direct
multiple shooting. Sliding-horizon OCPs achieved 20 to 21 repetitions. The
kinematic strategy slowly deviated from a plausible dumbbell lifting task to a
swinging strategy as fatigue onset increasingly compromised the ability to keep
the arm vertical. In full-horizon OCPs, the latter kinematic strategy was used
over the whole motion, resulting in 32 repetitions. We showed that
sliding-horizon OCPs revealed a reactive strategy to fatigue when only torque
was included in the cost function, whereas an anticipatory strategy was
revealed when the fatigue term was included in the cost function. Overall, the
proposed approach has the potential to be a valuable tool in optimizing
performance and helping reduce fatigue-related injuries in a variety of fields.


------------------------------------------------------------------------------

Title:
Pose-aware Attention Network for Flexible Motion Retargeting by Body  Part

Abstract: Motion retargeting is a fundamental problem in computer graphics and computer
vision. Existing approaches usually have many strict requirements, such as the
source-target skeletons needing to have the same number of joints or share the
same topology. To tackle this problem, we note that skeletons with different
structure may have some common body parts despite the differences in joint
numbers. Following this observation, we propose a novel, flexible motion
retargeting framework. The key idea of our method is to regard the body part as
the basic retargeting unit rather than directly retargeting the whole body
motion. To enhance the spatial modeling capability of the motion encoder, we
introduce a pose-aware attention network (PAN) in the motion encoding phase.
The PAN is pose-aware since it can dynamically predict the joint weights within
each body part based on the input pose, and then construct a shared latent
space for each body part by feature pooling. Extensive experiments show that
our approach can generate better motion retargeting results both qualitatively
and quantitatively than state-of-the-art methods. Moreover, we also show that
our framework can generate reasonable results even for a more challenging
retargeting scenario, like retargeting between bipedal and quadrupedal
skeletons because of the body part retargeting strategy and PAN. Our code is
publicly available.


------------------------------------------------------------------------------

Title:
RETINA: Distributed and Secure Trust Management for Smart Grid  Applications and Energy Trading

Abstract: The rapid adoption of smart grids demands robust security and efficiency
measures due to their critical role in delivering electricity and their
potential for customer-oriented benefits. This paper presents an innovative
framework, named RETINA, which provides a resilient and secure energy trading
mechanism within smart grid systems. RETINA tackles the inherent security and
infrastructure challenges in smart grids by establishing a trust-based security
layer and facilitating energy transactions through blockchain technology. Our
proposed solution integrates Public Key Infrastructure (PKI) and the Web of
Trust (WoT) concepts, promoting decentralized communication channels and robust
key management. We further introduce a smart contract-based energy trading
mechanism that factors in trust, distance, and energy type (green or non-green)
in cost calculation. The utility and robustness of RETINA have been validated
in a virtualized testbed environment with 500 nodes, demonstrating superior
performance in terms of scalability and resilience compared to the existing WoT
scheme. Furthermore, RETINA successfully enables a secure and efficient energy
trading scheme, promoting the use of renewable energy sources. Future
enhancements will include application to a realistic smart grid deployment and
the integration of additional functionalities. This groundbreaking solution has
the potential to revolutionize the smart grid ecosystem, addressing its current
limitations and propelling the industry towards a future of advanced and secure
energy exchange.


------------------------------------------------------------------------------

Title:
Detection and classification of faults aimed at preventive maintenance  of PV systems

Abstract: Diagnosis in PV systems aims to detect, locate and identify faults.
Diagnosing these faults is vital to guarantee energy production and extend the
useful life of PV power plants. In the literature, multiple machine learning
approaches have been proposed for this purpose. However, few of these works
have paid special attention to the detection of fine faults and the specialized
process of extraction and selection of features for their classification. A
fine fault is one whose characteristic signature is difficult to distinguish to
that of a healthy panel. As a contribution to the detection of fine faults
(especially of the snail trail type), this article proposes an innovative
approach based on the Random Forest (RF) algorithm. This approach uses a
complex feature extraction and selection method that improves the computational
time of fault classification while maintaining high accuracy.


------------------------------------------------------------------------------

Title:
A Novel Scheme to classify Read and Spontaneous Speech

Abstract: The COVID-19 pandemic has led to an increased use of remote telephonic
interviews, making it important to distinguish between scripted and spontaneous
speech in audio recordings. In this paper, we propose a novel scheme for
identifying read and spontaneous speech. Our approach uses a pre-trained
DeepSpeech audio-to-alphabet recognition engine to generate a sequence of
alphabets from the audio. From these alphabets, we derive features that allow
us to discriminate between read and spontaneous speech. Our experimental
results show that even a small set of self-explanatory features can effectively
classify the two types of speech very effectively.


------------------------------------------------------------------------------

Title:
Causal Feature Engineering of Price Directions of Cryptocurrencies using  Dynamic Bayesian Networks

Abstract: Cryptocurrencies have gained popularity across various sectors, especially in
finance and investment. The popularity is partly due to their unique
specifications originating from blockchain-related characteristics such as
privacy, decentralisation, and untraceability. Despite their growing
popularity, cryptocurrencies remain a high-risk investment due to their price
volatility and uncertainty. The inherent volatility in cryptocurrency prices,
coupled with internal cryptocurrency-related factors and external influential
global economic factors makes predicting their prices and price movement
directions challenging. Nevertheless, the knowledge obtained from predicting
the direction of cryptocurrency prices can provide valuable guidance for
investors in making informed investment decisions. To address this issue, this
paper proposes a dynamic Bayesian network (DBN) approach, which can model
complex systems in multivariate settings, to predict the price movement
direction of five popular altcoins (cryptocurrencies other than Bitcoin) in the
next trading day. The efficacy of the proposed model in predicting
cryptocurrency price directions is evaluated from two perspectives. Firstly,
our proposed approach is compared to two baseline models, namely an
auto-regressive integrated moving average and support vector regression.
Secondly, from a feature engineering point of view, the impact of twenty-three
different features, grouped into four categories, on the DBN's prediction
performance is investigated. The experimental results demonstrate that the DBN
significantly outperforms the baseline models. In addition, among the groups of
features, technical indicators are found to be the most effective predictors of
cryptocurrency price directions.


------------------------------------------------------------------------------

Title:
Securing Visually-Aware Recommender Systems: An Adversarial Image  Reconstruction and Detection Framework

Abstract: With rich visual data, such as images, becoming readily associated with
items, visually-aware recommendation systems (VARS) have been widely used in
different applications. Recent studies have shown that VARS are vulnerable to
item-image adversarial attacks, which add human-imperceptible perturbations to
the clean images associated with those items. Attacks on VARS pose new security
challenges to a wide range of applications such as e-Commerce and social
networks where VARS are widely used. How to secure VARS from such adversarial
attacks becomes a critical problem. Currently, there is still a lack of
systematic study on how to design secure defense strategies against visual
attacks on VARS. In this paper, we attempt to fill this gap by proposing an
adversarial image reconstruction and detection framework to secure VARS. Our
proposed method can simultaneously (1) secure VARS from adversarial attacks
characterized by local perturbations by image reconstruction based on global
vision transformers; and (2) accurately detect adversarial examples using a
novel contrastive learning approach. Meanwhile, our framework is designed to be
used as both a filter and a detector so that they can be jointly trained to
improve the flexibility of our defense strategy to a variety of attacks and
VARS models. We have conducted extensive experimental studies with two popular
attack methods (FGSM and PGD). Our experimental results on two real-world
datasets show that our defense strategy against visual attacks is effective and
outperforms existing methods on different attacks. Moreover, our method can
detect adversarial examples with high accuracy.


------------------------------------------------------------------------------

Title:
Curatr: A Platform for Semantic Analysis and Curation of Historical  Literary Texts

Abstract: The increasing availability of digital collections of historical and
contemporary literature presents a wealth of possibilities for new research in
the humanities. The scale and diversity of such collections however, presents
particular challenges in identifying and extracting relevant content. This
paper presents Curatr, an online platform for the exploration and curation of
literature with machine learning-supported semantic search, designed within the
context of digital humanities scholarship. The platform provides a text mining
workflow that combines neural word embeddings with expert domain knowledge to
enable the generation of thematic lexicons, allowing researches to curate
relevant sub-corpora from a large corpus of 18th and 19th century digitised
texts.


------------------------------------------------------------------------------

Title:
INT2.1: Towards Fine-Tunable Quantized Large Language Models with Error  Correction through Low-Rank Adaptation

Abstract: We introduce a method that dramatically reduces fine-tuning VRAM requirements
and rectifies quantization errors in quantized Large Language Models. First, we
develop an extremely memory-efficient fine-tuning (EMEF) method for quantized
models using Low-Rank Adaptation (LoRA), and drawing upon it, we construct an
error-correcting algorithm designed to minimize errors induced by the
quantization process. Our method reduces the memory requirements by up to 5.6
times, which enables fine-tuning a 7 billion parameter Large Language Model
(LLM) on consumer laptops. At the same time, we propose a Low-Rank Error
Correction (LREC) method that exploits the added LoRA layers to ameliorate the
gap between the quantized model and its float point counterpart. Our error
correction framework leads to a fully functional INT2 quantized LLM with the
capacity to generate coherent English text. To the best of our knowledge, this
is the first INT2 Large Language Model that has been able to reach such a
performance. The overhead of our method is merely a 1.05 times increase in
model size, which translates to an effective precision of INT2.1. Also, our
method readily generalizes to other quantization standards, such as INT3, INT4,
and INT8, restoring their lost performance, which marks a significant milestone
in the field of model quantization. The strategies delineated in this paper
hold promising implications for the future development and optimization of
quantized models, marking a pivotal shift in the landscape of low-resource
machine learning computations.


------------------------------------------------------------------------------

Title:
Symbolic Regression via Control Variable Genetic Programming

Abstract: Learning symbolic expressions directly from experiment data is a vital step
in AI-driven scientific discovery. Nevertheless, state-of-the-art approaches
are limited to learning simple expressions. Regressing expressions involving
many independent variables still remain out of reach. Motivated by the control
variable experiments widely utilized in science, we propose Control Variable
Genetic Programming (CVGP) for symbolic regression over many independent
variables. CVGP expedites symbolic expression discovery via customized
experiment design, rather than learning from a fixed dataset collected a
priori. CVGP starts by fitting simple expressions involving a small set of
independent variables using genetic programming, under controlled experiments
where other variables are held as constants. It then extends expressions
learned in previous generations by adding new independent variables, using new
control variable experiments in which these variables are allowed to vary.
Theoretically, we show CVGP as an incremental building approach can yield an
exponential reduction in the search space when learning a class of expressions.
Experimentally, CVGP outperforms several baselines in learning symbolic
expressions involving multiple independent variables.


------------------------------------------------------------------------------

Title:
An Interval Arithmetic Approach to Input-Output Reachability

Abstract: The safety region of operation of a system is the subset of allowed outputs
for which no undesirable outcome would occur. Knowing if a system would ever
leave its safety regions of operation is important information for the planning
and control of systems. The computation of the set of outputs also known as the
reachable set of the system is, in many cases, intensive and a simple
overestimation of it is preferred, instead. There are several perspectives to
address this problem including set-based approaches, Mixed-Monotonicity,
Hamilton-Jacobi reachability, neural networks, and recently reachability via
Chen-Fliess series. In the present work, a Chen-Fliess series representing the
input-output behavior of a dynamical system along with interval arithmetic is
used to overestimate the reachable set of a system. The advantage of this
combination of techniques is that it provides a closed-form of the
overestimating set. Examples are presented to illustrate the results.


------------------------------------------------------------------------------

Title:
CipherSniffer: Classifying Cipher Types

Abstract: Ciphers are a powerful tool for encrypting communication. There are many
different cipher types, which makes it computationally expensive to solve a
cipher using brute force. In this paper, we frame the decryption task as a
classification problem. We first create a dataset of transpositions,
substitutions, text reversals, word reversals, sentence shifts, and unencrypted
text. Then, we evaluate the performance of various tokenizer-model combinations
on this task.


------------------------------------------------------------------------------

Title:
The Use of Public Data and Free Tools in National CSIRTs' Operational  Practices: A Systematic Literature Review

Abstract: Many CSIRTs, including national CSIRTs, routinely use public data, including
open-source intelligence (OSINT) and free tools, which include open-source
tools in their work. However, we observed a lack of public information and
systematic discussions regarding how national CSIRTs use and perceive public
data and free tools in their operational practices. Therefore, this paper
provides a systematic literature review (SLR) to comprehensively understand how
national CSIRTs use and perceive public data and free tools in facilitating
incident responses in operations. Our SLR method followed a three-stage
approach: 1) a systematic search to identify relevant publications from
websites of pertinent CSIRT organisations, 2) a conventional SLR into the
research literature, and 3) synthesise data from stages one and two to answer
the research questions. In the first stage, we searched the websites of 100
national CSIRTs and 11 cross-CSIRT organisations to identify relevant
information about national CSIRTs. In the second stage, we searched a
scientific database (Scopus) to identify relevant research papers. Our primary
finding from the SLR is that most discussions concerning public data and free
tools by national CSIRTs are incomplete, ad hoc, or fragmented. We discovered a
lack of discussions on how the staff of national CSIRTs perceive the usefulness
of public data and free tools to facilitate incident responses. Such gaps can
prevent us from understanding how national CSIRTs can benefit from public data
and free tools and how other organisations, individuals and researchers can
help by providing such data and tools to improve national CSIRTs' operation.
These findings call for more empirical research on how national CSIRTs use and
perceive public data and free tools to improve the overall incident responses
at national CSIRTs and other incident response organisations.


------------------------------------------------------------------------------

Title:
Few-shot learning for sentence pair classification and its applications  in software engineering

Abstract: Few-shot learning-the ability to train models with access to limited data-has
become increasingly popular in the natural language processing (NLP) domain, as
large language models such as GPT and T0 have been empirically shown to achieve
high performance in numerous tasks with access to just a handful of labeled
examples. Smaller language models such as BERT and its variants have also been
shown to achieve strong performance with just a handful of labeled examples
when combined with few-shot learning algorithms like pattern-exploiting
training (PET) and SetFit. The focus of this work is to investigate the
performance of alternative few-shot learning approaches with BERT-based models.
Specifically, vanilla fine-tuning, PET and SetFit are compared for numerous
BERT-based checkpoints over an array of training set sizes. To facilitate this
investigation, applications of few-shot learning are considered in software
engineering. For each task, high-performance techniques and their associated
model checkpoints are identified through detailed empirical analysis. Our
results establish PET as a strong few-shot learning approach, and our analysis
shows that with just a few hundred labeled examples it can achieve performance
near that of fine-tuning on full-sized data sets.


------------------------------------------------------------------------------

Title:
360TripleView: 360-Degree Video View Management System Driven by  Convergence Value of Viewing Preferences

Abstract: 360-degree video has become increasingly popular in content consumption.
However, finding the viewing direction for important content within each frame
poses a significant challenge. Existing approaches rely on either viewer input
or algorithmic determination to select the viewing direction, but neither mode
consistently outperforms the other in terms of content-importance. In this
paper, we propose 360TripleView, the first view management system for
360-degree video that automatically infers and utilizes the better view mode
for each frame, ultimately providing viewers with higher content-importance
views. Through extensive experiments and a user study, we demonstrate that
360TripleView achieves over 90\% accuracy in inferring the better mode and
significantly enhances content-importance compared to existing methods.


------------------------------------------------------------------------------

Title:
Class-Conditional Conformal Prediction With Many Classes

Abstract: Standard conformal prediction methods provide a marginal coverage guarantee,
which means that for a random test point, the conformal prediction set contains
the true label with a user-chosen probability. In many classification problems,
we would like to obtain a stronger guarantee -- that for test points of a
specific class, the prediction set contains the true label with the same
user-chosen probability. Existing conformal prediction methods do not work well
when there is a limited amount of labeled data per class, as is often the case
in real applications where the number of classes is large. We propose a method
called clustered conformal prediction, which clusters together classes that
have "similar" conformal scores and then performs conformal prediction at the
cluster level. Based on empirical evaluation across four image data sets with
many (up to 1000) classes, we find that clustered conformal typically
outperforms existing methods in terms of class-conditional coverage and set
size metrics.


------------------------------------------------------------------------------

Title:
Semantic-Based Neural Network Repair

Abstract: Recently, neural networks have spread into numerous fields including many
safety-critical systems. Neural networks are built (and trained) by programming
in frameworks such as TensorFlow and PyTorch. Developers apply a rich set of
pre-defined layers to manually program neural networks or to automatically
generate them (e.g., through AutoML). Composing neural networks with different
layers is error-prone due to the non-trivial constraints that must be satisfied
in order to use those layers. In this work, we propose an approach to
automatically repair erroneous neural networks. The challenge is in identifying
a minimal modification to the network so that it becomes valid. Modifying a
layer might have cascading effects on subsequent layers and thus our approach
must search recursively to identify a "globally" minimal modification. Our
approach is based on an executable semantics of deep learning layers and
focuses on four kinds of errors which are common in practice. We evaluate our
approach for two usage scenarios, i.e., repairing automatically generated
neural networks and manually written ones suffering from common model bugs. The
results show that we are able to repair 100% of a set of randomly generated
neural networks (which are produced with an existing AI framework testing
approach) effectively and efficiently (with an average repair time of 21.08s)
and 93.75% of a collection of real neural network bugs (with an average time of
3min 40s).


------------------------------------------------------------------------------

Title:
Flexible Channel Dimensions for Differentiable Architecture Search

Abstract: Finding optimal channel dimensions (i.e., the number of filters in DNN
layers) is essential to design DNNs that perform well under computational
resource constraints. Recent work in neural architecture search aims at
automating the optimization of the DNN model implementation. However, existing
neural architecture search methods for channel dimensions rely on fixed search
spaces, which prevents achieving an efficient and fully automated solution. In
this work, we propose a novel differentiable neural architecture search method
with an efficient dynamic channel allocation algorithm to enable a flexible
search space for channel dimensions. We show that the proposed framework is
able to find DNN architectures that are equivalent to previous methods in task
accuracy and inference latency for the CIFAR-10 dataset with an improvement of
$1.3-1.7\times$ in GPU-hours and $1.5-1.7\times$ in the memory requirements
during the architecture search stage. Moreover, the proposed frameworks do not
require a well-engineered search space a priori, which is an important step
towards fully automated design of DNN architectures.


------------------------------------------------------------------------------

Title:
Graph Structure and Feature Extrapolation for Out-of-Distribution  Generalization

Abstract: Out-of-distribution (OOD) generalization deals with the prevalent learning
scenario where test distribution shifts from training distribution. With rising
application demands and inherent complexity, graph OOD problems call for
specialized solutions. While data-centric methods exhibit performance
enhancements on many generic machine learning tasks, there is a notable absence
of data augmentation methods tailored for graph OOD generalization. In this
work, we propose to achieve graph OOD generalization with the novel design of
non-Euclidean-space linear extrapolation. The proposed augmentation strategy
extrapolates both structure and feature spaces to generate OOD graph data. Our
design tailors OOD samples for specific shifts without corrupting underlying
causal mechanisms. Theoretical analysis and empirical results evidence the
effectiveness of our method in solving target shifts, showing substantial and
constant improvements across various graph OOD tasks.


------------------------------------------------------------------------------

Title:
Neural Mixed Effects for Nonlinear Personalized Predictions

Abstract: Personalized prediction is a machine learning approach that predicts a
person's future observations based on their past labeled observations and is
typically used for sequential tasks, e.g., to predict daily mood ratings. When
making personalized predictions, a model can combine two types of trends: (a)
trends shared across people, i.e., person-generic trends, such as being happier
on weekends, and (b) unique trends for each person, i.e., person-specific
trends, such as a stressful weekly meeting. Mixed effect models are popular
statistical models to study both trends by combining person-generic and
person-specific parameters. Though linear mixed effect models are gaining
popularity in machine learning by integrating them with neural networks, these
integrations are currently limited to linear person-specific parameters: ruling
out nonlinear person-specific trends. In this paper, we propose Neural Mixed
Effect (NME) models to optimize nonlinear person-specific parameters anywhere
in a neural network in a scalable manner. NME combines the efficiency of neural
network optimization with nonlinear mixed effects modeling. Empirically, we
observe that NME improves performance across six unimodal and multimodal
datasets, including a smartphone dataset to predict daily mood and a
mother-adolescent dataset to predict affective state sequences where half the
mothers experience at least moderate symptoms of depression. Furthermore, we
evaluate NME for two model architectures, including for neural conditional
random fields (CRF) to predict affective state sequences where the CRF learns
nonlinear person-specific temporal transitions between affective states.
Analysis of these person-specific transitions on the mother-adolescent dataset
shows interpretable trends related to the mother's depression symptoms.


------------------------------------------------------------------------------

Title:
Cross Chain Bribery Contracts: Majority vs Mighty Minority

Abstract: Bribery is a perilous issue in the real world, especially in an economical
aspect. This fraudulence is unavoidable, and more importantly, it is more
difficult to trace in case smart contracts are utilized for bribing on a
distributed public blockchain. In our paper, we propose a new threat to the
security of a blockchain system, cross-chain bribery using smart contracts. An
arbitrary wealthy briber can utilize cross-chain smart contracts to manipulate
a consensus mechanism on a victim's blockchain or to disgrace a victim's
blockchain. To better understand this threat, our paper proposes a framework to
analyze bribery using cross-chain smart contracts. We analyze the amount of
incentive to bribe rational miners in a victim's blockchain and also a full
cost of conducting a cross-chain bribery attack. The result is that such
attacks can be carried out with a reasonable amount of money or
cryptocurrencies.


------------------------------------------------------------------------------

Title:
Leveraging dendritic properties to advance machine learning and  neuro-inspired computing

Abstract: The brain is a remarkably capable and efficient system. It can process and
store huge amounts of noisy and unstructured information using minimal energy.
In contrast, current artificial intelligence (AI) systems require vast
resources for training while still struggling to compete in tasks that are
trivial for biological agents. Thus, brain-inspired engineering has emerged as
a promising new avenue for designing sustainable, next-generation AI systems.
Here, we describe how dendritic mechanisms of biological neurons have inspired
innovative solutions for significant AI problems, including credit assignment
in multilayer networks, catastrophic forgetting, and high energy consumption.
These findings provide exciting alternatives to existing architectures, showing
how dendritic research can pave the way for building more powerful and
energy-efficient artificial learning systems.


------------------------------------------------------------------------------

Title:
Reinforcement Learning-Driven Linker Design via Fast Attention-based  Point Cloud Alignment

Abstract: Proteolysis-Targeting Chimeras (PROTACs) represent a novel class of small
molecules which are designed to act as a bridge between an E3 ligase and a
disease-relevant protein, thereby promoting its subsequent degradation. PROTACs
are composed of two protein binding "active" domains, linked by a "linker"
domain. The design of the linker domain is challenging due to geometric and
chemical constraints given by its interactions, and the need to maximize
drug-likeness. To tackle these challenges, we introduce ShapeLinker, a method
for de novo design of linkers. It performs fragment-linking using reinforcement
learning on an autoregressive SMILES generator. The method optimizes for a
composite score combining relevant physicochemical properties and a novel,
attention-based point cloud alignment score. This new method successfully
generates linkers that satisfy both relevant 2D and 3D requirements, and
achieves state-of-the-art results in producing novel linkers assuming a target
linker conformation. This allows for more rational and efficient PROTAC design
and optimization. Code and data are available at
this https URL


------------------------------------------------------------------------------

Title:
Software Supply Chain Vulnerabilities Detection in Source Code:  Performance Comparison between Traditional and Quantum Machine Learning  Algorithms

Abstract: The software supply chain (SSC) attack has become one of the crucial issues
that are being increased rapidly with the advancement of the software
development domain. In general, SSC attacks execute during the software
development processes lead to vulnerabilities in software products targeting
downstream customers and even involved stakeholders. Machine Learning
approaches are proven in detecting and preventing software security
vulnerabilities. Besides, emerging quantum machine learning can be promising in
addressing SSC attacks. Considering the distinction between traditional and
quantum machine learning, performance could be varies based on the proportions
of the experimenting dataset. In this paper, we conduct a comparative analysis
between quantum neural networks (QNN) and conventional neural networks (NN)
with a software supply chain attack dataset known as ClaMP. Our goal is to
distinguish the performance between QNN and NN and to conduct the experiment,
we develop two different models for QNN and NN by utilizing Pennylane for
quantum and TensorFlow and Keras for traditional respectively. We evaluated the
performance of both models with different proportions of the ClaMP dataset to
identify the f1 score, recall, precision, and accuracy. We also measure the
execution time to check the efficiency of both models. The demonstration result
indicates that execution time for QNN is slower than NN with a higher
percentage of datasets. Due to recent advancements in QNN, a large level of
experiments shall be carried out to understand both models accurately in our
future research.


------------------------------------------------------------------------------

Title:
Adding 3D Geometry Control to Diffusion Models

Abstract: Diffusion models have emerged as a powerful method of generative modeling
across a range of fields, capable of producing stunning photo-realistic images
from natural language descriptions. However, these models lack explicit control
over the 3D structure of the objects in the generated images. In this paper, we
propose a novel method that incorporates 3D geometry control into diffusion
models, making them generate even more realistic and diverse images. To achieve
this, our method exploits ControlNet, which extends diffusion models by using
visual prompts in addition to text prompts. We generate images of 3D objects
taken from a 3D shape repository (e.g., ShapeNet and Objaverse), render them
from a variety of poses and viewing directions, compute the edge maps of the
rendered images, and use these edge maps as visual prompts to generate
realistic images. With explicit 3D geometry control, we can easily change the
3D structures of the objects in the generated images and obtain ground-truth 3D
annotations automatically. This allows us to use the generated images to
improve a lot of vision tasks, e.g., classification and 3D pose estimation, in
both in-distribution (ID) and out-of-distribution (OOD) settings. We
demonstrate the effectiveness of our method through extensive experiments on
ImageNet-50, ImageNet-R, PASCAL3D+, ObjectNet3D, and OOD-CV datasets. The
results show that our method significantly outperforms existing methods across
multiple benchmarks (e.g., 4.6 percentage points on ImageNet-50 using ViT and
3.5 percentage points on PASCAL3D+ and ObjectNet3D using NeMo).


------------------------------------------------------------------------------

Title:
Trustworthy Artificial Intelligence Framework for Proactive Detection  and Risk Explanation of Cyber Attacks in Smart Grid

Abstract: The rapid growth of distributed energy resources (DERs), such as renewable
energy sources, generators, consumers, and prosumers in the smart grid
infrastructure, poses significant cybersecurity and trust challenges to the
grid controller. Consequently, it is crucial to identify adversarial tactics
and measure the strength of the attacker's DER. To enable a trustworthy smart
grid controller, this work investigates a trustworthy artificial intelligence
(AI) mechanism for proactive identification and explanation of the cyber risk
caused by the control/status message of DERs. Thus, proposing and developing a
trustworthy AI framework to facilitate the deployment of any AI algorithms for
detecting potential cyber threats and analyzing root causes based on Shapley
value interpretation while dynamically quantifying the risk of an attack based
on Ward's minimum variance formula. The experiment with a state-of-the-art
dataset establishes the proposed framework as a trustworthy AI by fulfilling
the capabilities of reliability, fairness, explainability, transparency,
reproducibility, and accountability.


------------------------------------------------------------------------------

Title:
Survey on Sociodemographic Bias in Natural Language Processing

Abstract: Deep neural networks often learn unintended biases during training, which
might have harmful effects when deployed in real-world settings. This paper
surveys 209 papers on bias in NLP models, most of which address
sociodemographic bias. To better understand the distinction between bias and
real-world harm, we turn to ideas from psychology and behavioral economics to
propose a definition for sociodemographic bias. We identify three main
categories of NLP bias research: types of bias, quantifying bias, and
debiasing. We conclude that current approaches on quantifying bias face
reliability issues, that many of the bias metrics do not relate to real-world
biases, and that current debiasing techniques are superficial and hide bias
rather than removing it. Finally, we provide recommendations for future work.


------------------------------------------------------------------------------

Title:
Pruning the Way to Reliable Policies: A Multi-Objective Deep Q-Learning  Approach to Critical Care

Abstract: Most medical treatment decisions are sequential in nature. Hence, there is
substantial hope that reinforcement learning may make it possible to formulate
precise data-driven treatment plans. However, a key challenge for most
applications in this field is the sparse nature of primarily mortality-based
reward functions, leading to decreased stability of offline estimates. In this
work, we introduce a deep Q-learning approach able to obtain more reliable
critical care policies. This method integrates relevant but noisy intermediate
biomarker signals into the reward specification, without compromising the
optimization of the main outcome of interest (e.g. patient survival). We
achieve this by first pruning the action set based on all available rewards,
and second training a final model based on the sparse main reward but with a
restricted action set. By disentangling accurate and approximated rewards
through action pruning, potential distortions of the main objective are
minimized, all while enabling the extraction of valuable information from
intermediate signals that can guide the learning process. We evaluate our
method in both off-policy and offline settings using simulated environments and
real health records of patients in intensive care units. Our empirical results
indicate that pruning significantly reduces the size of the action space while
staying mostly consistent with the actions taken by physicians, outperforming
the current state-of-the-art offline reinforcement learning method conservative
Q-learning. Our work is a step towards developing reliable policies by
effectively harnessing the wealth of available information in data-intensive
critical care environments.


------------------------------------------------------------------------------

Title:
ArtWhisperer: A Dataset for Characterizing Human-AI Interactions in  Artistic Creations

Abstract: As generative AI becomes more prevalent, it is important to study how human
users interact with such models. In this work, we investigate how people use
text-to-image models to generate desired target images. To study this
interaction, we created ArtWhisperer, an online game where users are given a
target image and are tasked with iteratively finding a prompt that creates a
similar-looking image as the target. Through this game, we recorded over 50,000
human-AI interactions; each interaction corresponds to one text prompt created
by a user and the corresponding generated image. The majority of these are
repeated interactions where a user iterates to find the best prompt for their
target image, making this a unique sequential dataset for studying human-AI
collaborations. In an initial analysis of this dataset, we identify several
characteristics of prompt interactions and user strategies. People submit
diverse prompts and are able to discover a variety of text descriptions that
generate similar images. Interestingly, prompt diversity does not decrease as
users find better prompts. We further propose to a new metric the study the
steerability of AI using our dataset. We define steerability as the expected
number of interactions required to adequately complete a task. We estimate this
value by fitting a Markov chain for each target task and calculating the
expected time to reach an adequate score in the Markov chain. We quantify and
compare AI steerability across different types of target images and two
different models, finding that images of cities and natural world images are
more steerable than artistic and fantasy images. These findings provide
insights into human-AI interaction behavior, present a concrete method of
assessing AI steerability, and demonstrate the general utility of the
ArtWhisperer dataset.


------------------------------------------------------------------------------

Title:
Soft Soil Gait Planning and Control for Biped Robot using Deep  Deterministic Policy Gradient Approach

Abstract: Biped robots have plenty of benefits over wheeled, quadruped, or hexapod
robots due to their ability to behave like human beings in tough and non-flat
environments. Deformable terrain is another challenge for biped robots as it
has to deal with sinkage and maintain stability without falling. In this study,
we are proposing a Deep Deterministic Policy Gradient (DDPG) approach for
motion control of a flat-foot biped robot walking on deformable terrain. We
have considered a 7-link biped robot for our proposed approach. For soft soil
terrain modeling, we have considered triangular Mesh to describe its geometry,
where mesh parameters determine the softness of soil. All simulations have been
performed on PyChrono, which can handle soft soil environments.


------------------------------------------------------------------------------

Title:
AVIS: Autonomous Visual Information Seeking with Large Language Models

Abstract: In this paper, we propose an autonomous information seeking visual question
answering framework, AVIS. Our method leverages a Large Language Model (LLM) to
dynamically strategize the utilization of external tools and to investigate
their outputs, thereby acquiring the indispensable knowledge needed to provide
answers to the posed questions. Responding to visual questions that necessitate
external knowledge, such as "What event is commemorated by the building
depicted in this image?", is a complex task. This task presents a combinatorial
search space that demands a sequence of actions, including invoking APIs,
analyzing their responses, and making informed decisions. We conduct a user
study to collect a variety of instances of human decision-making when faced
with this task. This data is then used to design a system comprised of three
components: an LLM-powered planner that dynamically determines which tool to
use next, an LLM-powered reasoner that analyzes and extracts key information
from the tool outputs, and a working memory component that retains the acquired
information throughout the process. The collected user behavior serves as a
guide for our system in two key ways. First, we create a transition graph by
analyzing the sequence of decisions made by users. This graph delineates
distinct states and confines the set of actions available at each state.
Second, we use examples of user decision-making to provide our LLM-powered
planner and reasoner with relevant contextual instances, enhancing their
capacity to make informed decisions. We show that AVIS achieves
state-of-the-art results on knowledge-intensive visual question answering
benchmarks such as Infoseek and OK-VQA.


------------------------------------------------------------------------------

Title:
Chainlet Orbits: Topological Address Embedding for the Bitcoin  Blockchain

Abstract: The rise of cryptocurrencies like Bitcoin, which enable transactions with a
degree of pseudonymity, has led to a surge in various illicit activities,
including ransomware payments and transactions on darknet markets. These
illegal activities often utilize Bitcoin as the preferred payment method.
However, current tools for detecting illicit behavior either rely on a few
heuristics and laborious data collection processes or employ computationally
inefficient graph neural network (GNN) models that are challenging to
interpret.
To overcome the computational and interpretability limitations of existing
techniques, we introduce an effective solution called Chainlet Orbits. This
approach embeds Bitcoin addresses by leveraging their topological
characteristics in transactions. By employing our innovative address embedding,
we investigate e-crime in Bitcoin networks by focusing on distinctive
substructures that arise from illicit behavior.
The results of our node classification experiments demonstrate superior
performance compared to state-of-the-art methods, including both topological
and GNN-based approaches. Moreover, our approach enables the use of
interpretable and explainable machine learning models in as little as 15
minutes for most days on the Bitcoin transaction network.


------------------------------------------------------------------------------

Title:
Efficient 3D Semantic Segmentation with Superpoint Transformer

Abstract: We introduce a novel superpoint-based transformer architecture for efficient
semantic segmentation of large-scale 3D scenes. Our method incorporates a fast
algorithm to partition point clouds into a hierarchical superpoint structure,
which makes our preprocessing 7 times times faster than existing
superpoint-based approaches. Additionally, we leverage a self-attention
mechanism to capture the relationships between superpoints at multiple scales,
leading to state-of-the-art performance on three challenging benchmark
datasets: S3DIS (76.0% mIoU 6-fold validation), KITTI-360 (63.5% on Val), and
DALES (79.6%). With only 212k parameters, our approach is up to 200 times more
compact than other state-of-the-art models while maintaining similar
performance. Furthermore, our model can be trained on a single GPU in 3 hours
for a fold of the S3DIS dataset, which is 7x to 70x fewer GPU-hours than the
best-performing methods. Our code and models are accessible at
github.com/drprojects/superpoint_transformer.


------------------------------------------------------------------------------

Title:
DTW k-means clustering for fault detection in photovoltaic modules

Abstract: The increase in the use of photovoltaic (PV) energy in the world has shown
that the useful life and maintenance of a PV plant directly depend on
theability to quickly detect severe faults on a PV plant. To solve this problem
of detection, data based approaches have been proposed in the
literature.However, these previous solutions consider only specific behavior of
one or few faults. Most of these approaches can be qualified as supervised,
requiring an enormous labelling effort (fault types clearly identified in each
technology). In addition, most of them are validated in PV cells or one PV
module. That is hardly applicable in large-scale PV plants considering their
complexity. Alternatively, some unsupervised well-known approaches based on
data try to detect anomalies but are not able to identify precisely the type of
fault. The most performant of these methods do manage to efficiently group
healthy panels and separate them from faulty panels. In that way, this article
presents an unsupervised approach called DTW K-means. This approach takes
advantages of both the dynamic time warping (DWT) metric and the Kmeans
clustering algorithm as a data-driven approach. The results of this mixed
method in a PV string are compared to diagnostic labels established by visual
inspection of the panels.


------------------------------------------------------------------------------

Title:
Adaptive physics-informed neural networks for dynamic thermo-mechanical  coupling problems in large-size-ratio functionally graded materials

Abstract: In this paper, we present the adaptive physics-informed neural networks
(PINNs) for resolving three dimensional (3D) dynamic thermo-mechanical coupling
problems in large-size-ratio functionally graded materials (FGMs). The physical
laws described by coupled governing equations and the constraints imposed by
the initial and boundary conditions are leveraged to form the loss function of
PINNs by means of the automatic differentiation algorithm, and an adaptive loss
balancing scheme is introduced to improve the performance of PINNs. The
adaptive PINNs are meshfree and trained on batches of randomly sampled
collocation points, which is the key feature and superiority of the approach,
since mesh-based methods will encounter difficulties in solving problems with
large size ratios. The developed methodology is tested for several 3D
thermo-mechanical coupling problems in large-size-ratio FGMs, and the numerical
results demonstrate that the adaptive PINNs are effective and reliable for
dealing with coupled problems in coating structures with large size ratios up
to 109, as well as complex large-size-ratio geometries such as the
electrostatic comb, the airplane and the submarine.


------------------------------------------------------------------------------

Title:
Optimal control of port-Hamiltonian systems: energy, entropy, and exergy

Abstract: We consider irreversible and coupled reversible-irreversible nonlinear
port-Hamiltonian systems and the respective sets of thermodynamic equilibria.
In particular, we are concerned with optimal state transitions and output
stabilization on finite-time horizons. We analyze a class of optimal control
problems, where the performance functional can be interpreted as a linear
combination of energy supply, entropy generation, or exergy supply. Our results
establish the integral turnpike property towards the set of thermodynamic
equilibria providing a rigorous connection of optimal system trajectories to
optimal steady states. Throughout the paper, we illustrate our findings by
means of two examples: a network of heat exchangers and a gas-piston system.


------------------------------------------------------------------------------

Title:
Lexical Speaker Error Correction: Leveraging Language Models for Speaker  Diarization Error Correction

Abstract: Speaker diarization (SD) is typically used with an automatic speech
recognition (ASR) system to ascribe speaker labels to recognized words. The
conventional approach reconciles outputs from independently optimized ASR and
SD systems, where the SD system typically uses only acoustic information to
identify the speakers in the audio stream. This approach can lead to speaker
errors especially around speaker turns and regions of speaker overlap. In this
paper, we propose a novel second-pass speaker error correction system using
lexical information, leveraging the power of modern language models (LMs). Our
experiments across multiple telephony datasets show that our approach is both
effective and robust. Training and tuning only on the Fisher dataset, this
error correction approach leads to relative word-level diarization error rate
(WDER) reductions of 15-30% on three telephony datasets: RT03-CTS, Callhome
American English and held-out portions of Fisher.


------------------------------------------------------------------------------

Title:
Self-Knowledge Distillation for Surgical Phase Recognition

Abstract: Purpose: Advances in surgical phase recognition are generally led by training
deeper networks. Rather than going further with a more complex solution, we
believe that current models can be exploited better. We propose a
self-knowledge distillation framework that can be integrated into current
state-of-the-art (SOTA) models without requiring any extra complexity to the
models or annotations.
Methods: Knowledge distillation is a framework for network regularization
where knowledge is distilled from a teacher network to a student network. In
self-knowledge distillation, the student model becomes the teacher such that
the network learns from itself. Most phase recognition models follow an
encoder-decoder framework. Our framework utilizes self-knowledge distillation
in both stages. The teacher model guides the training process of the student
model to extract enhanced feature representations from the encoder and build a
more robust temporal decoder to tackle the over-segmentation problem.
Results: We validate our proposed framework on the public dataset Cholec80.
Our framework is embedded on top of four popular SOTA approaches and
consistently improves their performance. Specifically, our best GRU model
boosts performance by +3.33% accuracy and +3.95% F1-score over the same
baseline model.
Conclusion: We embed a self-knowledge distillation framework for the first
time in the surgical phase recognition training pipeline. Experimental results
demonstrate that our simple yet powerful framework can improve performance of
existing phase recognition models. Moreover, our extensive experiments show
that even with 75% of the training set we still achieve performance on par with
the same baseline model trained on the full set.


------------------------------------------------------------------------------

Title:
Exploring Resolution Fields for Scalable Image Compression with  Uncertainty Guidance

Abstract: Recently, there are significant advancements in learning-based image
compression methods surpassing traditional coding standards. Most of them
prioritize achieving the best rate-distortion performance for a particular
compression rate, which limits their flexibility and adaptability in various
applications with complex and varying constraints. In this work, we explore the
potential of resolution fields in scalable image compression and propose the
reciprocal pyramid network (RPN) that fulfills the need for more adaptable and
versatile compression. Specifically, RPN first builds a compression pyramid and
generates the resolution fields at different levels in a top-down manner. The
key design lies in the cross-resolution context mining module between adjacent
levels, which performs feature enriching and distillation to mine meaningful
contextualized information and remove unnecessary redundancy, producing
informative resolution fields as residual priors. The scalability is achieved
by progressive bitstream reusing and resolution field incorporation varying at
different levels. Furthermore, between adjacent compression levels, we
explicitly quantify the aleatoric uncertainty from the bottom decoded
representations and develop an uncertainty-guided loss to update the
upper-level compression parameters, forming a reverse pyramid process that
enforces the network to focus on the textured pixels with high variance for
more reliable and accurate reconstruction. Combining resolution field
exploration and uncertainty guidance in a pyramid manner, RPN can effectively
achieve spatial and quality scalable image compression. Experiments show the
superiority of RPN against existing classical and deep learning-based scalable
codecs. Code will be available at this https URL


------------------------------------------------------------------------------

Title:
Time-Domain Wideband ISM for Spherical Microphone Arrays

Abstract: This paper presents the time-domain wideband spherical microphone array
impulse response generator (TDW-SMIR generator), which is a time-domain
wideband image source method (ISM) for generating the room impulse responses
captured by an open spherical microphone array. To incorporate loudspeaker
directivity, the TDW-SMIR generator considers a source that emits a sequence of
spherical wave fronts whose amplitudes are related to the loudspeaker
directional impulse responses measured in the far-field. The TDW-SMIR generator
uses geometric models to derive the time-domain signals recorded by the
spherical microphone array. Comparisons are made with frequency-domain single
band ISMs. Simulation results prove the results of the TDW-SMIR generator are
similar to those of frequency-domain single band ISMs.


------------------------------------------------------------------------------

Title:
A Survey on Cross-Architectural IoT Malware Threat Hunting

Abstract: In recent years, the increase in non-Windows malware threats had turned the
focus of the cybersecurity community. Research works on hunting Windows
PE-based malwares are maturing, whereas the developments on Linux malware
threat hunting are relatively scarce. With the advent of the Internet of Things
(IoT) era, smart devices that are getting integrated into human life have
become a hackers highway for their malicious activities. The IoT devices employ
various Unix-based architectures that follow ELF (Executable and Linkable
Format) as their standard binary file specification. This study aims at
providing a comprehensive survey on the latest developments in
cross-architectural IoT malware detection and classification approaches. Aided
by a modern taxonomy, we discuss the feature representations, feature
extraction techniques, and machine learning models employed in the surveyed
works. We further provide more insights on the practical challenges involved in
cross-architectural IoT malware threat hunting and discuss various avenues to
instill potential future research.


------------------------------------------------------------------------------

Title:
Feature Engineering-Based Detection of Buffer Overflow Vulnerability in  Source Code Using Neural Networks

Abstract: One of the most significant challenges in the field of software code auditing
is the presence of vulnerabilities in software source code. Every year, more
and more software flaws are discovered, either internally in proprietary code
or publicly disclosed. These flaws are highly likely to be exploited and can
lead to system compromise, data leakage, or denial of service. To create a
large-scale machine learning system for function level vulnerability
identification, we utilized a sizable dataset of C and C++ open-source code
containing millions of functions with potential buffer overflow exploits. We
have developed an efficient and scalable vulnerability detection method based
on neural network models that learn features extracted from the source codes.
The source code is first converted into an intermediate representation to
remove unnecessary components and shorten dependencies. We maintain the
semantic and syntactic information using state of the art word embedding
algorithms such as GloVe and fastText. The embedded vectors are subsequently
fed into neural networks such as LSTM, BiLSTM, LSTM Autoencoder, word2vec,
BERT, and GPT2 to classify the possible vulnerabilities. We maintain the
semantic and syntactic information using state of the art word embedding
algorithms such as GloVe and fastText. The embedded vectors are subsequently
fed into neural networks such as LSTM, BiLSTM, LSTM Autoencoder, word2vec,
BERT, and GPT2 to classify the possible vulnerabilities. Furthermore, we have
proposed a neural network model that can overcome issues associated with
traditional neural networks. We have used evaluation metrics such as F1 score,
precision, recall, accuracy, and total execution time to measure the
performance. We have conducted a comparative analysis between results derived
from features containing a minimal text representation and semantic and
syntactic information.


------------------------------------------------------------------------------

Title:
FLamE: Few-shot Learning from Natural Language Explanations

Abstract: Natural language explanations have the potential to provide rich information
that in principle guides model reasoning. Yet, recent work by Lampinen et al.
(2022) has shown limited utility of natural language explanations in improving
classification. To effectively learn from explanations, we present FLamE, a
two-stage few-shot learning framework that first generates explanations using
GPT-3, and then finetunes a smaller model (e.g., RoBERTa) with generated
explanations. Our experiments on natural language inference demonstrate
effectiveness over strong baselines, increasing accuracy by 17.6% over GPT-3
Babbage and 5.7% over GPT-3 Davinci in e-SNLI. Despite improving classification
performance, human evaluation surprisingly reveals that the majority of
generated explanations does not adequately justify classification decisions.
Additional analyses point to the important role of label-specific cues (e.g.,
"not know" for the neutral label) in generated explanations.


------------------------------------------------------------------------------

Title:
h2oGPT: Democratizing Large Language Models

Abstract: Applications built on top of Large Language Models (LLMs) such as GPT-4
represent a revolution in AI due to their human-level capabilities in natural
language processing. However, they also pose many significant risks such as the
presence of biased, private, or harmful text, and the unauthorized inclusion of
copyrighted material.
We introduce h2oGPT, a suite of open-source code repositories for the
creation and use of LLMs based on Generative Pretrained Transformers (GPTs).
The goal of this project is to create the world's best truly open-source
alternative to closed-source approaches. In collaboration with and as part of
the incredible and unstoppable open-source community, we open-source several
fine-tuned h2oGPT models from 7 to 40 Billion parameters, ready for commercial
use under fully permissive Apache 2.0 licenses. Included in our release is
100\% private document search using natural language.
Open-source language models help boost AI development and make it more
accessible and trustworthy. They lower entry hurdles, allowing people and
groups to tailor these models to their needs. This openness increases
innovation, transparency, and fairness. An open-source strategy is needed to
share AI benefits fairly, and H2O.ai will continue to democratize AI and LLMs.


------------------------------------------------------------------------------

Title:
Towards Faster Non-Asymptotic Convergence for Diffusion-Based Generative  Models

Abstract: Diffusion models, which convert noise into new data instances by learning to
reverse a Markov diffusion process, have become a cornerstone in contemporary
generative modeling. While their practical power has now been widely
recognized, the theoretical underpinnings remain far from mature. In this work,
we develop a suite of non-asymptotic theory towards understanding the data
generation process of diffusion models in discrete time, assuming access to
reliable estimates of the (Stein) score functions. For a popular deterministic
sampler (based on the probability flow ODE), we establish a convergence rate
proportional to $1/T$ (with $T$ the total number of steps), improving upon past
results; for another mainstream stochastic sampler (i.e., a type of the
denoising diffusion probabilistic model (DDPM)), we derive a convergence rate
proportional to $1/\sqrt{T}$, matching the state-of-the-art theory. Our theory
imposes only minimal assumptions on the target data distribution (e.g., no
smoothness assumption is imposed), and is developed based on an elementary yet
versatile non-asymptotic approach without resorting to toolboxes for SDEs and
ODEs. Further, we design two accelerated variants, improving the convergence to
$1/T^2$ for the ODE-based sampler and $1/T$ for the DDPM-type sampler, which
might be of independent theoretical and empirical interest.


------------------------------------------------------------------------------

Title:
MSSRNet: Manipulating Sequential Style Representation for Unsupervised  Text Style Transfer

Abstract: Unsupervised text style transfer task aims to rewrite a text into target
style while preserving its main content. Traditional methods rely on the use of
a fixed-sized vector to regulate text style, which is difficult to accurately
convey the style strength for each individual token. In fact, each token of a
text contains different style intensity and makes different contribution to the
overall style. Our proposed method addresses this issue by assigning individual
style vector to each token in a text, allowing for fine-grained control and
manipulation of the style strength. Additionally, an adversarial training
framework integrated with teacher-student learning is introduced to enhance
training stability and reduce the complexity of high-dimensional optimization.
The results of our experiments demonstrate the efficacy of our method in terms
of clearly improved style transfer accuracy and content preservation in both
two-style transfer and multi-style transfer settings.


------------------------------------------------------------------------------

Title:
Stabilizer Testing and Magic Entropy

Abstract: We introduce systematic protocols to perform stabilizer testing for quantum
states and gates. These protocols are based on quantum convolutions and
swap-tests, realized by quantum circuits that implement the quantum convolution
for both qubit and qudit systems. We also introduce ''magic entropy'' to
quantify magic in quantum states and gates, in a way which may be measurable
experimentally.


------------------------------------------------------------------------------

Title:
MolCAP: Molecular Chemical reActivity pretraining and  prompted-finetuning enhanced molecular representation learning

Abstract: Molecular representation learning (MRL) is a fundamental task for drug
discovery. However, previous deep-learning (DL) methods focus excessively on
learning robust inner-molecular representations by mask-dominated pretraining
framework, neglecting abundant chemical reactivity molecular relationships that
have been demonstrated as the determining factor for various molecular property
prediction tasks. Here, we present MolCAP to promote MRL, a graph pretraining
Transformer based on chemical reactivity (IMR) knowledge with prompted
finetuning. Results show that MolCAP outperforms comparative methods based on
traditional molecular pretraining framework, in 13 publicly available molecular
datasets across a diversity of biomedical tasks. Prompted by MolCAP, even basic
graph neural networks are capable of achieving surprising performance that
outperforms previous models, indicating the promising prospect of applying
reactivity information for MRL. In addition, manual designed molecular templets
are potential to uncover the dataset bias. All in all, we expect our MolCAP to
gain more chemical meaningful insights for the entire process of drug
discovery.


------------------------------------------------------------------------------

Title:
Probabilistic-based Feature Embedding of 4-D Light Fields for  Compressive Imaging and Denoising

Abstract: The high-dimensional nature of the 4-D light field (LF) poses great
challenges in efficient and effective feature embedding that severely impact
the performance of downstream tasks. To tackle this crucial issue, in contrast
to existing methods with empirically-designed architectures, we propose
probabilistic-based feature embedding (PFE), which learns a feature embedding
architecture by assembling various low-dimensional convolution patterns in a
probability space for fully capturing spatial-angular information. Building
upon the proposed PFE, we then leverage the intrinsic linear imaging model of
the coded aperture camera to construct a cycle-consistent 4-D LF reconstruction
network from coded measurements. Moreover, we incorporate PFE into an iterative
optimization framework for 4-D LF denoising. Our extensive experiments
demonstrate the significant superiority of our methods on both real-world and
synthetic 4-D LF images, both quantitatively and qualitatively, when compared
with state-of-the-art methods. The source code will be publicly available at
this https URL


------------------------------------------------------------------------------

Title:
A Markovian Formalism for Active Querying

Abstract: Active learning algorithms have been an integral part of recent advances in
artificial intelligence. However, the research in the field is widely varying
and lacks an overall organizing leans. We outline a Markovian formalism for the
field of active learning and survey the literature to demonstrate the
organizing capability of our proposed formalism. Our formalism takes a
partially observable Markovian system approach to the active learning process
as a whole. We specifically outline how querying, dataset augmentation, reward
updates, and other aspects of active learning can be viewed as a transition
between meta-states in a Markovian system, and give direction into how other
aspects of active learning can fit into our formalism.


------------------------------------------------------------------------------

Title:
Learnable Weight Initialization for Volumetric Medical Image  Segmentation

Abstract: Hybrid volumetric medical image segmentation models, combining the advantages
of local convolution and global attention, have recently received considerable
attention. While mainly focusing on architectural modifications, most existing
hybrid approaches still use conventional data-independent weight initialization
schemes which restrict their performance due to ignoring the inherent
volumetric nature of the medical data. To address this issue, we propose a
learnable weight initialization approach that utilizes the available medical
training data to effectively learn the contextual and structural cues via the
proposed self-supervised objectives. Our approach is easy to integrate into any
hybrid model and requires no external training data. Experiments on multi-organ
and lung cancer segmentation tasks demonstrate the effectiveness of our
approach, leading to state-of-the-art segmentation performance. Our source code
and models are available at: this https URL


------------------------------------------------------------------------------

Title:
On Faking a Nash Equilibrium

Abstract: We characterize offline data poisoning attacks on Multi-Agent Reinforcement
Learning (MARL), where an attacker may change a data set in an attempt to
install a (potentially fictitious) unique Markov-perfect Nash equilibrium. We
propose the unique Nash set, namely the set of games, specified by their Q
functions, with a specific joint policy being the unique Nash equilibrium. The
unique Nash set is central to poisoning attacks because the attack is
successful if and only if data poisoning pushes all plausible games inside it.
The unique Nash set generalizes the reward polytope commonly used in inverse
reinforcement learning to MARL. For zero-sum Markov games, both the inverse
Nash set and the set of plausible games induced by data are polytopes in the Q
function space. We exhibit a linear program to efficiently compute the optimal
poisoning attack. Our work sheds light on the structure of data poisoning
attacks on offline MARL, a necessary step before one can design more robust
MARL algorithms.


------------------------------------------------------------------------------

Title:
Algorithmic Cluster Expansions for Quantum Problems

Abstract: We establish a general framework for developing approximation algorithms for
a class of counting problems. Our framework is based on the cluster expansion
of abstract polymer models formalism of Koteck\'y and Preiss. We apply our
framework to obtain efficient algorithms for (1) approximating probability
amplitudes of a class of quantum circuits close to the identity, (2)
approximating expectation values of a class of quantum circuits with operators
close to the identity, (3) approximating partition functions of a class of
quantum spin systems at high temperature, and (4) approximating thermal
expectation values of a class of quantum spin systems at high temperature with
positive-semidefinite operators. Further, we obtain hardness of approximation
results for approximating probability amplitudes of quantum circuits and
partition functions of quantum spin systems. This establishes a computational
complexity transition for these problems and shows that our algorithmic
conditions are optimal under complexity-theoretic assumptions. Finally, we show
that our algorithmic condition is almost optimal for expectation values and
optimal for thermal expectation values in the sense of zero freeness.


------------------------------------------------------------------------------

Title:
Distributed Trust Through the Lens of Software Architecture

Abstract: Distributed trust is a nebulous concept that has evolved from different
perspectives in recent years. While one can attribute its current prominence to
blockchain and cryptocurrency, the distributed trust concept has been
cultivating progress in federated learning, trustworthy and responsible AI in
an ecosystem setting, data sharing, privacy issues across organizational
boundaries, and zero trust cybersecurity. This paper will survey the concept of
distributed trust in multiple disciplines. It will take a system/software
architecture point of view to look at trust redistribution/shift and the
associated tradeoffs in systems and applications enabled by distributed trust
technologies.


------------------------------------------------------------------------------

Title:
Kinetic based optimization enhanced by genetic dynamics

Abstract: We propose and analyse a variant of the recently introduced kinetic based
optimization method that incorporates ideas like survival-of-the-fittest and
mutation strategies well-known from genetic algorithms. Thus, we provide a
first attempt to reach out from the class of consensus/kinetic-based algorithms
towards genetic metaheuristics. Different generations of genetic algorithms are
represented via two species identified with different labels, binary
interactions are prescribed on the particle level and then we derive a
mean-field approximation in order to analyse the method in terms of
convergence. Numerical results underline the feasibility of the approach and
show in particular that the genetic dynamics allows to improve the efficiency,
of this class of global optimization methods in terms of computational cost.


------------------------------------------------------------------------------

Title:
(Amplified) Banded Matrix Factorization: A unified approach to private  training

Abstract: Matrix factorization (MF) mechanisms for differential privacy (DP) have
substantially improved the state-of-the-art in privacy-utility-computation
tradeoffs for ML applications in a variety of scenarios, but in both the
centralized and federated settings there remain instances where either MF
cannot be easily applied, or other algorithms provide better tradeoffs
(typically, as $\epsilon$ becomes small).
In this work, we show how MF can subsume prior state-of-the-art algorithms in
both federated and centralized training settings, across all privacy budgets.
The key technique throughout is the construction of MF mechanisms with banded
matrices. For cross-device federated learning (FL), this enables
multiple-participations with a relaxed device participation schema compatible
with practical FL infrastructure (as demonstrated by a production deployment).
In the centralized setting, we prove that banded matrices enjoy the same
privacy amplification results as for the ubiquitous DP-SGD algorithm, but can
provide strictly better performance in most scenarios -- this lets us always at
least match DP-SGD, and often outperform it even at $\epsilon\ll2$. Finally,
$\hat{b}$-banded matrices substantially reduce the memory and time complexity
of per-step noise generation from $\mathcal{O}(n)$, $n$ the total number of
iterations, to a constant $\mathcal{O}(\hat{b})$, compared to general MF
mechanisms.


------------------------------------------------------------------------------

Title:
A Heavy-Tailed Algebra for Probabilistic Programming

Abstract: Despite the successes of probabilistic models based on passing noise through
neural networks, recent work has identified that such methods often fail to
capture tail behavior accurately, unless the tails of the base distribution are
appropriately calibrated. To overcome this deficiency, we propose a systematic
approach for analyzing the tails of random variables, and we illustrate how
this approach can be used during the static analysis (before drawing samples)
pass of a probabilistic programming language compiler. To characterize how the
tails change under various operations, we develop an algebra which acts on a
three-parameter family of tail asymptotics and which is based on the
generalized Gamma distribution. Our algebraic operations are closed under
addition and multiplication; they are capable of distinguishing sub-Gaussians
with differing scales; and they handle ratios sufficiently well to reproduce
the tails of most important statistical distributions directly from their
definitions. Our empirical results confirm that inference algorithms that
leverage our heavy-tailed algebra attain superior performance across a number
of density modeling and variational inference tasks.


------------------------------------------------------------------------------

Title:
Friend or Foe Inside? Exploring In-Process Isolation to Maintain Memory  Safety for Unsafe Rust

Abstract: Rust is a popular memory-safe systems programming language. In order to
interact with hardware or call into non-Rust libraries, Rust provides
\emph{unsafe} language features that shift responsibility for ensuring memory
safety to the developer. Failing to do so, may lead to memory safety violations
in unsafe code which can violate safety of the entire application. In this work
we explore in-process isolation with Memory Protection Keys as a mechanism to
shield safe program sections from safety violations that may happen in unsafe
sections. Our approach is easy to use and comprehensive as it prevents heap and
stack-based violations. We further compare process-based and in-process
isolation mechanisms and the necessary requirements for data serialization,
communication, and context switching. Our results show that in-process
isolation can be effective and efficient, permits for a high degree of
automation, and also enables a notion of application rewinding where the safe
program section may detect and safely handle violations in unsafe code.


------------------------------------------------------------------------------

Title:
Domain Information Control at Inference Time for Acoustic Scene  Classification

Abstract: Domain shift is considered a challenge in machine learning as it causes
significant degradation of model performance. In the Acoustic Scene
Classification task (ASC), domain shift is mainly caused by different recording
devices. Several studies have already targeted domain generalization to improve
the performance of ASC models on unseen domains, such as new devices. Recently,
the Controllable Gate Adapter ConGater has been proposed in Natural Language
Processing to address the biased training data problem. ConGater allows
controlling the debiasing process at inference time. ConGater's main advantage
is the continuous and selective debiasing of a trained model, during inference.
In this work, we adapt ConGater to the audio spectrogram transformer for an
acoustic scene classification task. We show that ConGater can be used to
selectively adapt the learned representations to be invariant to device domain
shifts such as recording devices. Our analysis shows that ConGater can
progressively remove device information from the learned representations and
improve the model generalization, especially under domain shift conditions
(e.g. unseen devices). We show that information removal can be extended to both
device and location domain. Finally, we demonstrate ConGater's ability to
enhance specific device performance without further training.


------------------------------------------------------------------------------

Title:
Synchronizing random automata through repeated 'a' inputs

Abstract: In a recent article by Chapuy and Perarnau, it was shown that a uniformly
chosen automaton on $n$ states with a $2$-letter alphabet has a synchronizing
word of length $O(\sqrt{n}\log n)$ with high probability. In this note, we give
a new simplified proof of a slightly weaker version of this statement. Our
proof is based on two properties of random automata. First, by repeating a
fixed character from the alphabet sufficiently many times in a row, the number
of possible states reduces to, in expectation, $O(\sqrt{n})$. Second, with high
probability, each pair of states can be synchronized by a word of length
$O(\log n)$.


------------------------------------------------------------------------------

Title:
Predicting Real-time Crash Risks during Hurricane Evacuation Using  Connected Vehicle Data

Abstract: Hurricane evacuation, ordered to save lives of people of coastal regions,
generates high traffic demand with increased crash risk. To mitigate such risk,
transportation agencies need to anticipate highway locations with high crash
risks to deploy appropriate countermeasures. With ubiquitous sensors and
communication technologies, it is now possible to retrieve micro-level
vehicular data containing individual vehicle trajectory and speed information.
Such high-resolution vehicle data, potentially available in real time, can be
used to assess prevailing traffic safety conditions. Using vehicle speed and
acceleration profiles, potential crash risks can be predicted in real time.
Previous studies on real-time crash risk prediction mainly used data from
infrastructure-based sensors which may not cover many road segments. In this
paper, we present methods to determine potential crash risks during hurricane
evacuation from an emerging alternative data source known as connected vehicle
data. Such data contain vehicle location, speed, and acceleration information
collected at a very high frequency (less than 30 seconds). To predict potential
crash risks, we utilized a dataset collected during the evacuation period of
Hurricane Ida on Interstate-10 (I-10) in the state of Louisiana. Multiple
machine learning models were trained considering weather features and different
traffic characteristics extracted from the connected vehicle data in 5-minute
intervals. The results indicate that the Gaussian Process Boosting (GPBoost)
and Extreme Gradient Boosting (XGBoost) models perform better (recall = 0.91)
than other models. The real-time connected vehicle data for crash risks
assessment will allow traffic managers to efficiently utilize resources to
proactively take safety measures.


------------------------------------------------------------------------------

Title:
Fundamental connections between utility theories of wealth and  information theory

Abstract: We establish fundamental connections between utility theories of wealth from
the economic sciences and information-theoretic quantities. In particular, we
introduce operational tasks based on betting where both gambler and bookmaker
have access to side information, or betting tasks with double side information
for short. In order to characterise these operational tasks we introduce new
conditional R\'enyi divergences, and explore some of their properties.
Furthermore, we introduce an utility theory of wealth ratios, and operationally
interpret there the two-parameter $(q,r)$ generalised mutual information
measure recently introduced by V. M. Ili\'c and I. V. Djordjevi\'c; it
quantifies the advantage provided by side information in betting tasks for
utility theories of wealth ratios. Moreover, we show that the
Ili\'c-Djordjevi\'c conditional entropy satisfies a type of generalised chain
rule, which generalises that of Arimoto-R\'enyi. Finally, we address the
implications of these results on the quantum resource theories of informative
measurements and non-constant channels. Altogether, these results further help
strengthening the bridge between the theory of expected utility from the
economic sciences and Shannon's theory of information.


------------------------------------------------------------------------------

Title:
Utilizing Social Media Attributes for Enhanced Keyword Detection: An  IDF-LDA Model Applied to Sina Weibo

Abstract: With the rapid development of social media such as Twitter and Weibo,
detecting keywords from a huge volume of text data streams in real-time has
become a critical problem. The keyword detection problem aims at searching
important information from massive text data to reflect the most important
events or topics. However, social media data usually has unique features: the
documents are usually short, the language is colloquial, and the data is likely
to have significant temporal patterns. Therefore, it could be challenging to
discover critical information from these text streams. In this paper, we
propose a novel method to address the keyword detection problem in social
media. Our model combines the Inverse Document Frequency (IDF) and Latent
Dirichlet Allocation (LDA) models to better cope with the distinct attributes
of social media data, such as the number of likes, comments, and retweets. By
weighting the importance of each document based on these attributes, our method
can effectively detect more representative keywords over time. Comprehensive
experiments conducted under various conditions on Weibo data illustrate that
our approach outperforms the baselines in various evaluation metrics, including
precision and recall for multiple problem settings.


------------------------------------------------------------------------------

Title:
MMD-FUSE: Learning and Combining Kernels for Two-Sample Testing Without  Data Splitting

Abstract: We propose novel statistics which maximise the power of a two-sample test
based on the Maximum Mean Discrepancy (MMD), by adapting over the set of
kernels used in defining it. For finite sets, this reduces to combining
(normalised) MMD values under each of these kernels via a weighted soft
maximum. Exponential concentration bounds are proved for our proposed
statistics under the null and alternative. We further show how these kernels
can be chosen in a data-dependent but permutation-independent way, in a
well-calibrated test, avoiding data splitting. This technique applies more
broadly to general permutation-based MMD testing, and includes the use of deep
kernels with features learnt using unsupervised models such as auto-encoders.
We highlight the applicability of our MMD-FUSE test on both synthetic
low-dimensional and real-world high-dimensional data, and compare its
performance in terms of power against current state-of-the-art kernel tests.


------------------------------------------------------------------------------

Title:
PrivaScissors: Enhance the Privacy of Collaborative Inference through  the Lens of Mutual Information

Abstract: Edge-cloud collaborative inference empowers resource-limited IoT devices to
support deep learning applications without disclosing their raw data to the
cloud server, thus preserving privacy. Nevertheless, prior research has shown
that collaborative inference still results in the exposure of data and
predictions from edge devices. To enhance the privacy of collaborative
inference, we introduce a defense strategy called PrivaScissors, which is
designed to reduce the mutual information between a model's intermediate
outcomes and the device's data and predictions. We evaluate PrivaScissors's
performance on several datasets in the context of diverse attacks and offer a
theoretical robustness guarantee.


------------------------------------------------------------------------------

Title:
Accurate Airway Tree Segmentation in CT Scans via Anatomy-aware  Multi-class Segmentation and Topology-guided Iterative Learning

Abstract: Intrathoracic airway segmentation in computed tomography (CT) is a
prerequisite for various respiratory disease analyses such as chronic
obstructive pulmonary disease (COPD), asthma and lung cancer. Unlike other
organs with simpler shapes or topology, the airway's complex tree structure
imposes an unbearable burden to generate the "ground truth" label (up to 7 or 3
hours of manual or semi-automatic annotation on each case). Most of the
existing airway datasets are incompletely labeled/annotated, thus limiting the
completeness of computer-segmented airway. In this paper, we propose a new
anatomy-aware multi-class airway segmentation method enhanced by
topology-guided iterative self-learning. Based on the natural airway anatomy,
we formulate a simple yet highly effective anatomy-aware multi-class
segmentation task to intuitively handle the severe intra-class imbalance of the
airway. To solve the incomplete labeling issue, we propose a tailored
self-iterative learning scheme to segment toward the complete airway tree. For
generating pseudo-labels to achieve higher sensitivity , we introduce a novel
breakage attention map and design a topology-guided pseudo-label refinement
method by iteratively connecting breaking branches commonly existed from
initial pseudo-labels. Extensive experiments have been conducted on four
datasets including two public challenges. The proposed method ranked 1st in
both EXACT'09 challenge using average score and ATM'22 challenge on weighted
average score. In a public BAS dataset and a private lung cancer dataset, our
method significantly improves previous leading approaches by extracting at
least (absolute) 7.5% more detected tree length and 4.0% more tree branches,
while maintaining similar precision.


------------------------------------------------------------------------------

Title:
A Recursive Newton Method for Smoothing in Nonlinear State Space Models

Abstract: In this paper, we use the optimization formulation of nonlinear Kalman
filtering and smoothing problems to develop second-order variants of iterated
Kalman smoother (IKS) methods. We show that Newton's method corresponds to a
recursion over affine smoothing problems on a modified state-space model
augmented by a pseudo measurement. The first and second derivatives required in
this approach can be efficiently computed with widely available automatic
differentiation tools. Furthermore, we show how to incorporate line-search and
trust-region strategies into the proposed second-order IKS algorithm in order
to regularize updates between iterations. Finally, we provide numerical
examples to demonstrate the method's efficiency in terms of runtime compared to
its batch counterpart.


------------------------------------------------------------------------------

Title:
Annotator Consensus Prediction for Medical Image Segmentation with  Diffusion Models

Abstract: A major challenge in the segmentation of medical images is the large inter-
and intra-observer variability in annotations provided by multiple experts. To
address this challenge, we propose a novel method for multi-expert prediction
using diffusion models. Our method leverages the diffusion-based approach to
incorporate information from multiple annotations and fuse it into a unified
segmentation map that reflects the consensus of multiple experts. We evaluate
the performance of our method on several datasets of medical segmentation
annotated by multiple experts and compare it with state-of-the-art methods. Our
results demonstrate the effectiveness and robustness of the proposed method.
Our code is publicly available at
this https URL


------------------------------------------------------------------------------

Title:
Wireless Point Cloud Transmission

Abstract: 3D point cloud is a three-dimensional data format generated by LiDARs and
depth sensors, and is being increasingly used in a large variety of
applications. This paper presents a novel solution called SEmantic Point cloud
Transmission (SEPT), for the transmission of point clouds over wireless
channels with limited bandwidth. At the transmitter, SEPT encodes the point
cloud via an iterative downsampling and feature extraction process. At the
receiver, SEPT reconstructs the point cloud with latent reconstruction and
offset-based upsampling. Extensive numerical experiments confirm that SEPT
significantly outperforms the standard approach with octree-based compression
followed by channel coding. Compared with a more advanced benchmark that
utilizes state-of-the-art deep learning-based compression techniques, SEPT
achieves comparable performance while eliminating the cliff and leveling
effects. Thanks to its improved performance and robustness against channel
variations, we believe that SEPT can be instrumental in collaborative sensing
and inference applications among robots and vehicles, particularly in the
low-latency and high-mobility scenarios.


------------------------------------------------------------------------------

Title:
A Comparison of Self-Supervised Pretraining Approaches for Predicting  Disease Risk from Chest Radiograph Images

Abstract: Deep learning is the state-of-the-art for medical imaging tasks, but requires
large, labeled datasets. For risk prediction, large datasets are rare since
they require both imaging and follow-up (e.g., diagnosis codes). However, the
release of publicly available imaging data with diagnostic labels presents an
opportunity for self and semi-supervised approaches to improve label efficiency
for risk prediction. Though several studies have compared self-supervised
approaches in natural image classification, object detection, and medical image
interpretation, there is limited data on which approaches learn robust
representations for risk prediction. We present a comparison of semi- and
self-supervised learning to predict mortality risk using chest x-ray images. We
find that a semi-supervised autoencoder outperforms contrastive and transfer
learning in internal and external validation.


------------------------------------------------------------------------------

Title:
Dynamic Interval Restrictions on Action Spaces in Deep Reinforcement  Learning for Obstacle Avoidance

Abstract: Deep reinforcement learning algorithms typically act on the same set of
actions. However, this is not sufficient for a wide range of real-world
applications where different subsets are available at each step. In this
thesis, we consider the problem of interval restrictions as they occur in
pathfinding with dynamic obstacles. When actions that lead to collisions are
avoided, the continuous action space is split into variable parts. Recent
research learns with strong assumptions on the number of intervals, is limited
to convex subsets, and the available actions are learned from the observations.
Therefore, we propose two approaches that are independent of the state of the
environment by extending parameterized reinforcement learning and ConstraintNet
to handle an arbitrary number of intervals. We demonstrate their performance in
an obstacle avoidance task and compare the methods to penalties, projection,
replacement, as well as discrete and continuous masking from the literature.
The results suggest that discrete masking of action-values is the only
effective method when constraints did not emerge during training. When
restrictions are learned, the decision between projection, masking, and our
ConstraintNet modification seems to depend on the task at hand. We compare the
results with varying complexity and give directions for future work.


------------------------------------------------------------------------------

Title:
PUGAN: Physical Model-Guided Underwater Image Enhancement Using GAN with  Dual-Discriminators

Abstract: Due to the light absorption and scattering induced by the water medium,
underwater images usually suffer from some degradation problems, such as low
contrast, color distortion, and blurring details, which aggravate the
difficulty of downstream underwater understanding tasks. Therefore, how to
obtain clear and visually pleasant images has become a common concern of
people, and the task of underwater image enhancement (UIE) has also emerged as
the times require. Among existing UIE methods, Generative Adversarial Networks
(GANs) based methods perform well in visual aesthetics, while the physical
model-based methods have better scene adaptability. Inheriting the advantages
of the above two types of models, we propose a physical model-guided GAN model
for UIE in this paper, referred to as PUGAN. The entire network is under the
GAN architecture. On the one hand, we design a Parameters Estimation subnetwork
(Par-subnet) to learn the parameters for physical model inversion, and use the
generated color enhancement image as auxiliary information for the Two-Stream
Interaction Enhancement sub-network (TSIE-subnet). Meanwhile, we design a
Degradation Quantization (DQ) module in TSIE-subnet to quantize scene
degradation, thereby achieving reinforcing enhancement of key regions. On the
other hand, we design the Dual-Discriminators for the style-content adversarial
constraint, promoting the authenticity and visual aesthetics of the results.
Extensive experiments on three benchmark datasets demonstrate that our PUGAN
outperforms state-of-the-art methods in both qualitative and quantitative
metrics.


------------------------------------------------------------------------------

Title:
Distributionally Robust Stratified Sampling for Stochastic Simulations  with Multiple Uncertain Input Models

Abstract: This paper presents a robust version of the stratified sampling method when
multiple uncertain input models are considered for stochastic simulation.
Various variance reduction techniques have demonstrated their superior
performance in accelerating simulation processes. Nevertheless, they often use
a single input model and further assume that the input model is exactly known
and fixed. We consider more general cases in which it is necessary to assess a
simulation's response to a variety of input models, such as when evaluating the
reliability of wind turbines under nonstationary wind conditions or the
operation of a service system when the distribution of customer inter-arrival
time is heterogeneous at different times. Moreover, the estimation variance may
be considerably impacted by uncertainty in input models. To address such
nonstationary and uncertain input models, we offer a distributionally robust
(DR) stratified sampling approach with the goal of minimizing the maximum of
worst-case estimator variances among plausible but uncertain input models.
Specifically, we devise a bi-level optimization framework for formulating DR
stochastic problems with different ambiguity set designs, based on the
$L_2$-norm, 1-Wasserstein distance, parametric family of distributions, and
distribution moments. In order to cope with the non-convexity of objective
function, we present a solution approach that uses Bayesian optimization.
Numerical experiments and the wind turbine case study demonstrate the
robustness of the proposed approach.


------------------------------------------------------------------------------

Title:
Optimal Hypothesis Testing Based on Information Theory

Abstract: There has a major problem in the current theory of hypothesis testing in
which no unified indicator to evaluate the goodness of various test methods
since the cost function or utility function usually relies on the specific
application scenario, resulting in no optimal hypothesis testing method. In
this paper, the problem of optimal hypothesis testing is investigated based on
information theory. We propose an information-theoretic framework of hypothesis
testing consisting of five parts: test information (TI) is proposed to evaluate
the hypothesis testing, which depends on the a posteriori probability
distribution function of hypotheses and independent of specific test methods;
accuracy with the unit of bit is proposed to evaluate the degree of validity of
specific test methods; the sampling a posteriori (SAP) probability test method
is presented, which makes stochastic selections on the hypotheses according to
the a posteriori probability distribution of the hypotheses; the probability of
test failure is defined to reflect the probability of the failed decision is
made; test theorem is proved that all accuracy lower than the TI is achievable.
Specifically, for every accuracy lower than TI, there exists a test method with
the probability of test failure tending to zero. Conversely, there is no test
method whose accuracy is more than TI. Numerical simulations are performed to
demonstrate that the SAP test is asymptotically optimal. In addition, the
results show that the accuracy of the SAP test and the existing test methods,
such as the maximum a posteriori probability, expected a posteriori
probability, and median a posteriori probability tests, are not more than TI.


------------------------------------------------------------------------------

Title:
Phase Transitions of Civil Unrest across Countries and Time

Abstract: Phase transitions, characterized by abrupt shifts between macroscopic
patterns of organization, are ubiquitous in complex systems. Despite
considerable research in the physical and natural sciences, the empirical study
of this phenomenon in societal systems is relatively underdeveloped. The goal
of this study is to explore whether the dynamics of collective civil unrest can
be plausibly characterized as a sequence of recurrent phase shifts, with each
phase having measurable and identifiable latent characteristics. We introduce a
macro-level statistical model of civil unrest and evaluate its plausibility
using a comprehensive dataset of civil unrest events in 170 countries from 1946
to 2017. Our findings demonstrate that the macro-level phase model effectively
captures the characteristics of civil unrest data from diverse countries
globally and that universal mechanisms may underlie certain aspects of the
dynamics of civil unrest. We also introduce a new scale to quantify a country's
long-term unrest per unit of time and show that civil unrest events tend to
cluster geographically, with the magnitude of civil unrest concentrated in
specific regions. Our approach has the potential to identify and measure phase
transitions in various collective human phenomena beyond civil unrest,
contributing to a better understanding of complex social systems.


------------------------------------------------------------------------------

Title:
A Hybrid Approach for Smart Alert Generation

Abstract: Anomaly detection is an important task in network management. However,
deploying intelligent alert systems in real-world large-scale networking
systems is challenging when we take into account (i) scalability, (ii) data
heterogeneity, and (iii) generalizability and maintainability. In this paper,
we propose a hybrid model for an alert system that combines statistical models
with a whitelist mechanism to tackle these challenges and reduce false positive
alerts. The statistical models take advantage of a large database to detect
anomalies in time-series data, while the whitelist filters out persistently
alerted nodes to further reduce false positives. Our model is validated using
qualitative data from customer support cases. Future work includes more feature
engineering and input data, as well as including human feedback in the model
development process.


------------------------------------------------------------------------------

Title:
DHBE: Data-free Holistic Backdoor Erasing in Deep Neural Networks via  Restricted Adversarial Distillation

Abstract: Backdoor attacks have emerged as an urgent threat to Deep Neural Networks
(DNNs), where victim DNNs are furtively implanted with malicious neurons that
could be triggered by the adversary. To defend against backdoor attacks, many
works establish a staged pipeline to remove backdoors from victim DNNs:
inspecting, locating, and erasing. However, in a scenario where a few clean
data can be accessible, such pipeline is fragile and cannot erase backdoors
completely without sacrificing model accuracy. To address this issue, in this
paper, we propose a novel data-free holistic backdoor erasing (DHBE) framework.
Instead of the staged pipeline, the DHBE treats the backdoor erasing task as a
unified adversarial procedure, which seeks equilibrium between two different
competing processes: distillation and backdoor regularization. In distillation,
the backdoored DNN is distilled into a proxy model, transferring its knowledge
about clean data, yet backdoors are simultaneously transferred. In backdoor
regularization, the proxy model is holistically regularized to prevent from
infecting any possible backdoor transferred from distillation. These two
processes jointly proceed with data-free adversarial optimization until a
clean, high-accuracy proxy model is obtained. With the novel adversarial
design, our framework demonstrates its superiority in three aspects: 1) minimal
detriment to model accuracy, 2) high tolerance for hyperparameters, and 3) no
demand for clean data. Extensive experiments on various backdoor attacks and
datasets are performed to verify the effectiveness of the proposed framework.
Code is available at \url{this https URL}


------------------------------------------------------------------------------

Title:
Dark web activity classification using deep learning

Abstract: The present article highlights the pressing need for identifying and
controlling illicit activities on the dark web. While only 4% of the
information available on the internet is accessible through regular search
engines, the deep web contains a plethora of information, including personal
data and online accounts, that is not indexed by search engines. The dark web,
which constitutes a subset of the deep web, is a notorious breeding ground for
various illegal activities, such as drug trafficking, weapon sales, and money
laundering. Against this backdrop, the authors propose a novel search engine
that leverages deep learning to identify and extract relevant images related to
illicit activities on the dark web. Specifically, the system can detect the
titles of illegal activities on the dark web and retrieve pertinent images from
websites with a .onion extension. The authors have collected a comprehensive
dataset named darkoob and the proposed method achieves an accuracy of 94% on
the test dataset. Overall, the proposed search engine represents a significant
step forward in identifying and controlling illicit activities on the dark web.
By contributing to internet and community security, this technology has the
potential to mitigate a wide range of social, economic, and political
challenges arising from illegal activities on the dark web.


------------------------------------------------------------------------------

Title:
Contrastive Attention Networks for Attribution of Early Modern Print

Abstract: In this paper, we develop machine learning techniques to identify unknown
printers in early modern (c.~1500--1800) English printed books. Specifically,
we focus on matching uniquely damaged character type-imprints in anonymously
printed books to works with known printers in order to provide evidence of
their origins. Until now, this work has been limited to manual investigations
by analytical bibliographers. We present a Contrastive Attention-based Metric
Learning approach to identify similar damage across character image pairs,
which is sensitive to very subtle differences in glyph shapes, yet robust to
various confounding sources of noise associated with digitized historical
books. To overcome the scarce amount of supervised data, we design a random
data synthesis procedure that aims to simulate bends, fractures, and inking
variations induced by the early printing process. Our method successfully
improves downstream damaged type-imprint matching among printed works from this
period, as validated by in-domain human experts. The results of our approach on
two important philosophical works from the Early Modern period demonstrate
potential to extend the extant historical research about the origins and
content of these books.


------------------------------------------------------------------------------

Title:
Tune As You Scale: Hyperparameter Optimization For Compute Efficient  Training

Abstract: Hyperparameter tuning of deep learning models can lead to order-of-magnitude
performance gains for the same amount of compute. Despite this, systematic
tuning is uncommon, particularly for large models, which are expensive to
evaluate and tend to have many hyperparameters, necessitating difficult
judgment calls about tradeoffs, budgets, and search bounds. To address these
issues and propose a practical method for robustly tuning large models, we
present Cost-Aware Pareto Region Bayesian Search (CARBS), a Bayesian
optimization algorithm that performs local search around the performance-cost
Pareto frontier. CARBS does well even in unbounded search spaces with many
hyperparameters, learns scaling relationships so that it can tune models even
as they are scaled up, and automates much of the "black magic" of tuning. Among
our results, we effectively solve the entire ProcGen benchmark just by tuning a
simple baseline (PPO, as provided in the original ProcGen paper). We also
reproduce the model size vs. training tokens scaling result from the Chinchilla
project (Hoffmann et al. 2022), while simultaneously discovering scaling laws
for every other hyperparameter, via an easy automated process that uses
significantly less compute and is applicable to any deep learning problem (not
just language models).


------------------------------------------------------------------------------

Title:
ExoMDN: Rapid characterization of exoplanet interior structures with  Mixture Density Networks

Abstract: Characterizing the interior structure of exoplanets is essential for
understanding their diversity, formation, and evolution. As the interior of
exoplanets is inaccessible to observations, an inverse problem must be solved,
where numerical structure models need to conform to observable parameters such
as mass and radius. This is a highly degenerate problem whose solution often
relies on computationally-expensive and time-consuming inference methods such
as Markov Chain Monte Carlo.
We present ExoMDN, a machine-learning model for the interior characterization
of exoplanets based on Mixture Density Networks (MDN). The model is trained on
a large dataset of more than 5.6 million synthetic planets below 25 Earth
masses consisting of an iron core, a silicate mantle, a water and high-pressure
ice layer, and a H/He atmosphere. We employ log-ratio transformations to
convert the interior structure data into a form that the MDN can easily handle.
Given mass, radius, and equilibrium temperature, we show that ExoMDN can
deliver a full posterior distribution of mass fractions and thicknesses of each
planetary layer in under a second on a standard Intel i5 CPU. Observational
uncertainties can be easily accounted for through repeated predictions from
within the uncertainties. We use ExoMDN to characterize the interior of 22
confirmed exoplanets with mass and radius uncertainties below 10% and 5%
respectively, including the well studied GJ 1214 b, GJ 486 b, and the
TRAPPIST-1 planets. We discuss the inclusion of the fluid Love number $k_2$ as
an additional (potential) observable, showing how it can significantly reduce
the degeneracy of interior structures. Utilizing the fast predictions of
ExoMDN, we show that measuring $k_2$ with an accuracy of 10% can constrain the
thickness of core and mantle of an Earth analog to $\approx13\%$ of the true
values.


------------------------------------------------------------------------------

Title:
Optimization on product manifolds under a preconditioned metric

Abstract: Since optimization on Riemannian manifolds relies on the chosen metric, it is
appealing to know that how the performance of a Riemannian optimization method
varies with different metrics and how to exquisitely construct a metric such
that a method can be accelerated. To this end, we propose a general framework
for optimization problems on product manifolds where the search space is
endowed with a preconditioned metric, and we develop the Riemannian gradient
descent and Riemannian conjugate gradient methods under this metric.
Specifically, the metric is constructed by an operator that aims to approximate
the diagonal blocks of the Riemannian Hessian of the cost function, which has a
preconditioning effect. We explain the relationship between the proposed
methods and the variable metric methods, and show that various existing
methods, e.g., the Riemannian Gauss--Newton method, can be interpreted by the
proposed framework with specific metrics. In addition, we tailor new
preconditioned metrics and adapt the proposed Riemannian methods to the
canonical correlation analysis and the truncated singular value decomposition
problems, and we propose the Gauss--Newton method to solve the tensor ring
completion problem. Numerical results among these applications verify that a
delicate metric does accelerate the Riemannian optimization methods.


------------------------------------------------------------------------------

Title:
MCPI: Integrating Multimodal Data for Enhanced Prediction of Compound  Protein Interactions

Abstract: The identification of compound-protein interactions (CPI) plays a critical
role in drug screening, drug repurposing, and combination therapy studies. The
effectiveness of CPI prediction relies heavily on the features extracted from
both compounds and target proteins. While various prediction methods employ
different feature combinations, both molecular-based and network-based models
encounter the common obstacle of incomplete feature representations. Thus, a
promising solution to this issue is to fully integrate all relevant CPI
features. This study proposed a novel model named MCPI, which is designed to
improve the prediction performance of CPI by integrating multiple sources of
information, including the PPI network, CCI network, and structural features of
CPI. The results of the study indicate that the MCPI model outperformed other
existing methods for predicting CPI on public datasets. Furthermore, the study
has practical implications for drug development, as the model was applied to
search for potential inhibitors among FDA-approved drugs in response to the
SARS-CoV-2 pandemic. The prediction results were then validated through the
literature, suggesting that the MCPI model could be a useful tool for
identifying potential drug candidates. Overall, this study has the potential to
advance our understanding of CPI and guide drug development efforts.


------------------------------------------------------------------------------

Title:
GHP-MOFassemble: Diffusion modeling, high throughput screening, and  molecular dynamics for rational discovery of novel metal-organic frameworks  for carbon capture at scale

Abstract: We introduce GHP-MOFassemble, a Generative artificial intelligence (AI), High
Performance framework to accelerate the rational design of metal-organic
frameworks (MOFs) with high CO2 capacity and synthesizable linkers. Our
framework combines a diffusion model, a class of generative AI, to generate
novel linkers that are assembled with one of three pre-selected nodes into MOFs
in a primitive cubic (pcu) topology. The CO2 capacities of these AI-generated
MOFs are predicted using a modified version of the crystal graph convolutional
neural network model. We then use the LAMMPS code to perform molecular dynamics
simulations to relax the AI-generated MOF structures, and identify those that
converge to stable structures, and maintain their porous properties throughout
the simulations. Among 120,000 pcu MOF candidates generated by the
GHP-MOFassemble framework, with three distinct metal nodes (Cu paddlewheel, Zn
paddlewheel, Zn tetramer), a total of 102 structures completed molecular
dynamics simulations at 1 bar with predicted CO2 capacity higher than 2 mmol/g
at 0.1 bar, which corresponds to the top 5% of hMOFs in the hypothetical MOF
(hMOF) dataset in the MOFX-DB database. Among these candidates, 18 have change
in density lower than 1% during molecular dynamics simulations, indicating
their stability. We also found that the top five GHP-MOFassemble's MOF
structures have CO2 capacities higher than 96.9% of hMOF structures. This new
approach combines generative AI, graph modeling, large-scale molecular dynamics
simulations, and extreme scale computing to open up new pathways for the
accelerated discovery of novel MOF structures at scale.


------------------------------------------------------------------------------

Title:
Adaptation of Student Behavioural Routines during COVID-19: A Multimodal  Approach

Abstract: One population group that had to significantly adapt and change their
behaviour during the COVID-19 pandemic is students. While previous studies have
extensively investigated the impact of the pandemic on their psychological
well-being and academic performance, limited attention has been given to their
activity routines. In this work, we analyze students' behavioural changes by
examining qualitative and quantitative differences in their daily routines
between two distinct periods (2018 and 2020). Using an Experience Sampling
Method (ESM) that captures multimodal self-reported data on students' activity,
locations and sociality, we apply Non-Negative Matrix Factorization (NMF) to
extract meaningful behavioural components, and quantified the variations in
behaviour between students in 2018 and 2020. Surprisingly, despite the presence
of COVID-19 restrictions, we find minimal changes in the activities performed
by students, and the diversity of activities also remains largely unaffected.
Leveraging the richness of the data at our disposal, we discovered that
activities adaptation to the pandemic primarily occurred in the location and
sociality dimensions.


------------------------------------------------------------------------------

Title:
Langevin Monte Carlo for strongly log-concave distributions: Randomized  midpoint revisited

Abstract: We revisit the problem of sampling from a target distribution that has a
smooth strongly log-concave density everywhere in $\mathbb R^p$. In this
context, if no additional density information is available, the randomized
midpoint discretization for the kinetic Langevin diffusion is known to be the
most scalable method in high dimensions with large condition numbers. Our main
result is a nonasymptotic and easy to compute upper bound on the Wasserstein-2
error of this method. To provide a more thorough explanation of our method for
establishing the computable upper bound, we conduct an analysis of the midpoint
discretization for the vanilla Langevin process. This analysis helps to clarify
the underlying principles and provides valuable insights that we use to
establish an improved upper bound for the kinetic Langevin process with the
midpoint discretization. Furthermore, by applying these techniques we establish
new guarantees for the kinetic Langevin process with Euler discretization,
which have a better dependence on the condition number than existing upper
bounds.


------------------------------------------------------------------------------

Title:
On Certified Generalization in Structured Prediction

Abstract: In structured prediction, target objects have rich internal structure which
does not factorize into independent components and violates common i.i.d.
assumptions. This challenge becomes apparent through the exponentially large
output space in applications such as image segmentation or scene graph
generation. We present a novel PAC-Bayesian risk bound for structured
prediction wherein the rate of generalization scales not only with the number
of structured examples but also with their size. The underlying assumption,
conforming to ongoing research on generative models, is that data are generated
by the Knothe-Rosenblatt rearrangement of a factorizing reference measure. This
allows to explicitly distill the structure between random output variables into
a Wasserstein dependency matrix. Our work makes a preliminary step towards
leveraging powerful generative models to establish generalization bounds for
discriminative downstream tasks in the challenging setting of structured
prediction.


------------------------------------------------------------------------------

Title:
The Universal Law of Generalization Holds for Naturalistic Stimuli

Abstract: Shepard's universal law of generalization is a remarkable hypothesis about
how intelligent organisms should perceive similarity. In its broadest form, the
universal law states that the level of perceived similarity between a pair of
stimuli should decay as a concave function of their distance when embedded in
an appropriate psychological space. While extensively studied, evidence in
support of the universal law has relied on low-dimensional stimuli and small
stimulus sets that are very different from their real-world counterparts. This
is largely because pairwise comparisons -- as required for similarity judgments
-- scale quadratically in the number of stimuli. We provide direct evidence for
the universal law in a naturalistic high-dimensional regime by analyzing an
existing dataset of 214,200 human similarity judgments and a newly collected
dataset of 390,819 human generalization judgments (N=2406 US participants)
across three sets of natural images.


------------------------------------------------------------------------------

Title:
Multichannel Active Noise Control with Exterior Radiation Suppression  Based on Riemannian Optimization

Abstract: A multichannel active noise control (ANC) method with exterior radiation
suppression is proposed. When applying ANC in a three-dimensional space by
using multiple microphones and loudspeakers, the loudspeaker output can amplify
noise outside a region of target positions because most of current ANC methods
do not take into consideration the exterior radiation of secondary
loudspeakers. We propose a normalized least mean square algorithm for
feedforward ANC in the frequency domain based on the Riemannian optimization to
update the control filter with the exterior radiation power constrained to a
target value. The advantages of the proposed method, compared with the
algorithm using a penalty term of exterior radiation, were validated by
numerical experiments: the exterior radiation power can be constrained during
the adaptation process and the parameter for the constraint can be determined
in advance.


------------------------------------------------------------------------------

Title:
A Survey on Blood Pressure Measurement Technologies: Addressing  Potential Sources of Bias

Abstract: Blood pressure is a vital sign that offers important insights into overall
health, particularly cardiovascular well-being. It plays a critical role in
medical settings and homes for disease prevention, diagnosis, treatment, and
management. Physicians heavily rely on blood pressure values for making crucial
decisions. Most commercial devices utilize cuffs for blood pressure
measurement, and automatic devices have gained popularity due to the high
prevalence of hypertension. Self-measurement and home monitoring of blood
pressure are also recommended. However, concerns arise regarding the accuracy
of blood pressure measurement technologies and the alignment of reported values
with actual values. People often adjust their medication based on these
reported values, making accuracy vital. This study focuses on the concept of
``bias'' to highlight potential discrepancies between reported and actual blood
pressure values. Previous research has identified biases originating from three
categories: (1) blood pressure measurement devices, (2) subject-specific
factors, and (3) measurement sessions. Specifically, this study examines biases
associated with cuff-based blood pressure technologies due to their widespread
use in medical applications and the growing trend of home monitoring.
Identifying and addressing the primary sources of biases is crucial to prevent
their propagation and mitigate potential consequences. Additionally, the study
explores the future prospects of blood pressure monitoring using machine
learning methods.


------------------------------------------------------------------------------

Title:
Exact Count of Boundary Pieces of ReLU Classifiers: Towards the Proper  Complexity Measure for Classification

Abstract: Classic learning theory suggests that proper regularization is the key to
good generalization and robustness. In classification, current training schemes
only target the complexity of the classifier itself, which can be misleading
and ineffective. Instead, we advocate directly measuring the complexity of the
decision boundary. Existing literature is limited in this area with few
well-established definitions of boundary complexity. As a proof of concept, we
start by analyzing ReLU neural networks, whose boundary complexity can be
conveniently characterized by the number of affine pieces. With the help of
tropical geometry, we develop a novel method that can explicitly count the
exact number of boundary pieces, and as a by-product, the exact number of total
affine pieces. Numerical experiments are conducted and distinctive properties
of our boundary complexity are uncovered. First, the boundary piece count
appears largely independent of other measures, e.g., total piece count, and
$l_2$ norm of weights, during the training process. Second, the boundary piece
count is negatively correlated with robustness, where popular robust training
techniques, e.g., adversarial training or random noise injection, are found to
reduce the number of boundary pieces.


------------------------------------------------------------------------------

Title:
Provably Efficient Offline Reinforcement Learning with Perturbed Data  Sources

Abstract: Existing theoretical studies on offline reinforcement learning (RL) mostly
consider a dataset sampled directly from the target task. In practice, however,
data often come from several heterogeneous but related sources. Motivated by
this gap, this work aims at rigorously understanding offline RL with multiple
datasets that are collected from randomly perturbed versions of the target task
instead of from itself. An information-theoretic lower bound is derived, which
reveals a necessary requirement on the number of involved sources in addition
to that on the number of data samples. Then, a novel HetPEVI algorithm is
proposed, which simultaneously considers the sample uncertainties from a finite
number of data samples per data source and the source uncertainties due to a
finite number of available data sources. Theoretical analyses demonstrate that
HetPEVI can solve the target task as long as the data sources collectively
provide a good data coverage. Moreover, HetPEVI is demonstrated to be optimal
up to a polynomial factor of the horizon length. Finally, the study is extended
to offline Markov games and offline robust RL, which demonstrates the
generality of the proposed designs and theoretical analyses.


------------------------------------------------------------------------------

Title:
Off-policy Evaluation in Doubly Inhomogeneous Environments

Abstract: This work aims to study off-policy evaluation (OPE) under scenarios where two
key reinforcement learning (RL) assumptions -- temporal stationarity and
individual homogeneity are both violated. To handle the ``double
inhomogeneities", we propose a class of latent factor models for the reward and
observation transition functions, under which we develop a general OPE
framework that consists of both model-based and model-free approaches. To our
knowledge, this is the first paper that develops statistically sound OPE
methods in offline RL with double inhomogeneities. It contributes to a deeper
understanding of OPE in environments, where standard RL assumptions are not
met, and provides several practical approaches in these settings. We establish
the theoretical properties of the proposed value estimators and empirically
show that our approach outperforms competing methods that ignore either
temporal nonstationarity or individual heterogeneity. Finally, we illustrate
our method on a data set from the Medical Information Mart for Intensive Care.


------------------------------------------------------------------------------

Title:
High-Dimensional MR Reconstruction Integrating Subspace and Adaptive  Generative Models

Abstract: We present a novel method that integrates subspace modeling with an adaptive
generative image prior for high-dimensional MR image reconstruction. The
subspace model imposes an explicit low-dimensional representation of the
high-dimensional images, while the generative image prior serves as a spatial
constraint on the "contrast-weighted" images or the spatial coefficients of the
subspace model. A formulation was introduced to synergize these two components
with complimentary regularization such as joint sparsity. A special pretraining
plus subject-specific network adaptation strategy was proposed to construct an
accurate generative-model-based representation for images with varying
contrasts, validated by experimental data. An iterative algorithm was
introduced to jointly update the subspace coefficients and the multiresolution
latent space of the generative image model that leveraged a recently developed
intermediate layer optimization technique for network inversion. We evaluated
the utility of the proposed method in two high-dimensional imaging
applications: accelerated MR parameter mapping and high-resolution MRSI.
Improved performance over state-of-the-art subspace-based methods was
demonstrated in both cases. Our work demonstrated the potential of integrating
data-driven and adaptive generative models with low-dimensional representation
for high-dimensional imaging problems.


------------------------------------------------------------------------------

Title:
BRUDEX Database: Binaural Room Impulse Responses with Uniformly  Distributed External Microphones

Abstract: There is an emerging need for comparable data for multi-microphone
processing, particularly in acoustic sensor networks. However, commonly
available databases are often limited in the spatial diversity of the
microphones or only allow for particular signal processing tasks. In this
paper, we present a database of acoustic impulse responses and recordings for a
binaural hearing aid setup, 36 spatially distributed microphones spanning a
uniform grid of (5x5) m^2 and 12 source positions. This database can be used
for a variety of signal processing tasks, such as (multi-microphone) noise
reduction, source localization, and dereverberation, as the measurements were
performed using the same setup for three different reverberation conditions
(T_60\approx{310, 510, 1300} ms). The usability of the database is demonstrated
for a noise reduction task using a minimum variance distortionless response
beamformer based on relative transfer functions, exploiting the availability of
spatially distributed microphones.


------------------------------------------------------------------------------

Title:
Qubit efficient quantum algorithms for the vehicle routing problem on  quantum computers of the NISQ era

Abstract: The vehicle routing problem with time windows (VRPTW) is a classic
optimization problem that arises in many different areas, such as logistics and
transportation. The goal of the VRPTW is to find the shortest possible route
for a fleet of vehicles to visit a set of destinations. In recent years, there
has been growing interest in using variational quantum algorithms (VQAs), to
find approximate solutions to problems that can be formulated as quadratic
unconstrained binary optimization (QUBO) problems. In this work, we formulate
the VRPTW as a QUBO and apply a quantum variational approach to the VRPTW using
our earlier suggested encoding scheme described in [1] to reduce drastically
the number of qubits required. We evaluate our approach on a set of VRPTW
instances ranging from 11 to 3964 routes constructed with data provided by
researchers from ExxonMobil. We compare the solutions obtained with standard
full encoding approaches for which the max problems size possible in NISQ era
are of the order of 20-30 routes. We run our algorithms in simulators as well
as cloud quantum hardware provided by IBMQ, AWS (Rigetti) and IonQ and
benchmark our results against each other as well as on the simulators. We show
that our approach can find approximate solutions to the VRPTW that are
comparable to the solutions found by quantum algorithms using the full
encoding. Our results suggest that our unique encoding approach, provides a
promising approach to drastically reducing the number of qubits required to
find decent approximate solutions for industry-based optimization problems.


------------------------------------------------------------------------------

Title:
TWIGMA: A dataset of AI-Generated Images with Metadata From Twitter

Abstract: Recent progress in generative artificial intelligence (gen-AI) has enabled
the generation of photo-realistic and artistically-inspiring photos at a single
click, catering to millions of users online. To explore how people use gen-AI
models such as DALLE and StableDiffusion, it is critical to understand the
themes, contents, and variations present in the AI-generated photos. In this
work, we introduce TWIGMA (TWItter Generative-ai images with MetadatA), a
comprehensive dataset encompassing over 800,000 gen-AI images collected from
Jan 2021 to March 2023 on Twitter, with associated metadata (e.g., tweet text,
creation date, number of likes). Through a comparative analysis of TWIGMA with
natural images and human artwork, we find that gen-AI images possess
distinctive characteristics and exhibit, on average, lower variability when
compared to their non-gen-AI counterparts. Additionally, we find that the
similarity between a gen-AI image and natural images (i) is inversely
correlated with the number of likes; and (ii) can be used to identify human
images that served as inspiration for the gen-AI creations. Finally, we observe
a longitudinal shift in the themes of AI-generated images on Twitter, with
users increasingly sharing artistically sophisticated content such as intricate
human portraits, whereas their interest in simple subjects such as natural
scenes and animals has decreased. Our analyses and findings underscore the
significance of TWIGMA as a unique data resource for studying AI-generated
images.


------------------------------------------------------------------------------

Title:
Permutation Invariant Recurrent Neural Networks for Sound Source  Tracking Applications

Abstract: Many multi-source localization and tracking models based on neural networks
use one or several recurrent layers at their final stages to track the movement
of the sources. Conventional recurrent neural networks (RNNs), such as the long
short-term memories (LSTMs) or the gated recurrent units (GRUs), take a vector
as their input and use another vector to store their state. However, this
approach results in the information from all the sources being contained in a
single ordered vector, which is not optimal for permutation-invariant problems
such as multi-source tracking. In this paper, we present a new recurrent
architecture that uses unordered sets to represent both its input and its state
and that is invariant to the permutations of the input set and equivariant to
the permutations of the state set. Hence, the information of every sound source
is represented in an individual embedding and the new estimates are assigned to
the tracked trajectories regardless of their order.


------------------------------------------------------------------------------

Title:
A high-order fully Lagrangian particle level-set method for dynamic  surfaces

Abstract: We present a fully Lagrangian particle level-set method based on high-order
polynomial regression. This enables closest-point redistancing without
requiring a regular Cartesian mesh, relaxing the need for particle-mesh
interpolation. Instead, we perform level-set redistancing directly on
irregularly distributed particles by polynomial regression in a Newton-Lagrange
basis on a set of unisolvent nodes. We demonstrate that the resulting particle
closest-point (PCP) redistancing achieves high-order accuracy for 2D and 3D
geometries discretized on highly irregular particle distributions and has
better robustness against particle distortion than regression in a monomial
basis. Further, we show convergence in a classic level-set benchmark case
involving ill-conditioned particle distributions, and we present an application
to an oscillating droplet simulation in multi-phase flow.


------------------------------------------------------------------------------

Title:
PathWise: a flexible, open-source library for the Resource Constrained  Shortest Path

Abstract: In this paper, we consider a fundamental and hard combinatorial problem: the
Resource Constrained Shortest Path Problem (RCSPP). We describe the
implementation of a flexible, open-source library for the solution of the
RCSPP, called PathWise, capable of tackling several variants of the problem. We
designed PathWise with the final user in mind, developing easy-to-use
interfaces without compromising performance. We provide computational
experiments on three classes of instances of the RCSPP, namely RCSPP on cyclic
networks, RCSPP on large acyclic networks, and RCSPP on ad-hoc cyclic networks.
We show that PathWise is packed off-the-shelf with algorithms capable of
tackling all classes. This paper represents the first step along the journey of
devising and implementing a comprehensive open-source library for a large
variety of RCSPPs. The current version of the library carries exact algorithms
for the RCSPP but new algorithms, both heuristic and exact, will be added
thanks to the flexible design. We also foresee PathWise becoming a platform
ready for data-driven and process-driven methodologies for these types of
problems.


------------------------------------------------------------------------------

Title:
Nonparametric regression using over-parameterized shallow ReLU neural  networks

Abstract: It is shown that over-parameterized neural networks can achieve minimax
optimal rates of convergence (up to logarithmic factors) for learning functions
from certain smooth function classes, if the weights are suitably constrained
or regularized. Specifically, we consider the nonparametric regression of
estimating an unknown $d$-variate function by using shallow ReLU neural
networks. It is assumed that the regression function is from the H\"older space
with smoothness $\alpha<(d+3)/2$ or a variation space corresponding to shallow
neural networks, which can be viewed as an infinitely wide neural network. In
this setting, we prove that least squares estimators based on shallow neural
networks with certain norm constraints on the weights are minimax optimal, if
the network width is sufficiently large. As a byproduct, we derive a new
size-independent bound for the local Rademacher complexity of shallow ReLU
neural networks, which may be of independent interest.


------------------------------------------------------------------------------

Title:
Exploiting the Brain's Network Structure for Automatic Identification of  ADHD Subjects

Abstract: Attention Deficit Hyperactive Disorder (ADHD) is a common behavioral problem
affecting children. In this work, we investigate the automatic classification
of ADHD subjects using the resting state Functional Magnetic Resonance Imaging
(fMRI) sequences of the brain. We show that the brain can be modeled as a
functional network, and certain properties of the networks differ in ADHD
subjects from control subjects. We compute the pairwise correlation of brain
voxels' activity over the time frame of the experimental protocol which helps
to model the function of a brain as a network. Different network features are
computed for each of the voxels constructing the network. The concatenation of
the network features of all the voxels in a brain serves as the feature vector.
Feature vectors from a set of subjects are then used to train a PCA-LDA
(principal component analysis-linear discriminant analysis) based classifier.
We hypothesized that ADHD-related differences lie in some specific regions of
the brain and using features only from those regions is sufficient to
discriminate ADHD and control subjects. We propose a method to create a brain
mask that includes the useful regions only and demonstrate that using the
feature from the masked regions improves classification accuracy on the test
data set. We train our classifier with 776 subjects and test on 171 subjects
provided by The Neuro Bureau for the ADHD-200 challenge. We demonstrate the
utility of graph-motif features, specifically the maps that represent the
frequency of participation of voxels in network cycles of length 3. The best
classification performance (69.59%) is achieved using 3-cycle map features with
masking. Our proposed approach holds promise in being able to diagnose and
understand the disorder.


------------------------------------------------------------------------------

Title:
Ramsey numbers of connected 4-clique matching

Abstract: We determine the exact value of the $2$-color Ramsey number of a connected
$4$-clique matching $\mathscr{C}(nK_4)$ which is a set of connected graphs
containing $n$ disjoint $K_4$. That is, we show that $R_2(\mathscr{C}(nK_4)) =
13n-3$ for any positive integer $n \geq 3$. The result is an extension of the
result by (Roberts, 2017) which gave that result when $n\geq 18$. We also show
that the result still holds when $n=2$ provided that $R_2(2K_4) \leq 23$.


------------------------------------------------------------------------------

Title:
Towards training Bilingual and Code-Switched Speech Recognition models  from Monolingual data sources

Abstract: Multilingual Automatic Speech Recognition (ASR) models are capable of
transcribing audios across multiple languages, eliminating the need for
separate models. In addition, they can perform Language Identification (LID)
and handle code-switched speech. However, training these models requires
special code-switch and multilingual speech corpora which are sparsely
available. In this paper, we evaluate different approaches towards training of
bilingual as well as code-switched ASR models using purely monolingual data
sources. We introduce the concept of aggregate tokenizers that differs from the
current prevalent technique of generating LIDs at the boundaries of monolingual
samples and produces LID for each emitted token instead. We compare bilingual
and monolingual model performance, showcase the efficacy of aggregate
tokenizers, present a synthetic code-switched ASR data generation technique and
demonstrate the effectiveness of the proposed code-switched ASR models for the
tasks of speech recognition and spoken language identification.


------------------------------------------------------------------------------

Title:
Taming Reversible Halftoning via Predictive Luminance

Abstract: Traditional halftoning usually drops colors when dithering images with binary
dots, which makes it difficult to recover the original color information. We
proposed a novel halftoning technique that converts a color image into a binary
halftone with full restorability to its original version. Our novel base
halftoning technique consists of two convolutional neural networks (CNNs) to
produce the reversible halftone patterns, and a noise incentive block (NIB) to
mitigate the flatness degradation issue of CNNs. Furthermore, to tackle the
conflicts between the blue-noise quality and restoration accuracy in our novel
base method, we proposed a predictor-embedded approach to offload predictable
information from the network, which in our case is the luminance information
resembling from the halftone pattern. Such an approach allows the network to
gain more flexibility to produce halftones with better blue-noise quality
without compromising the restoration quality. Detailed studies on the
multiple-stage training method and loss weightings have been conducted. We have
compared our predictor-embedded method and our novel method regarding spectrum
analysis on halftone, halftone accuracy, restoration accuracy, and the data
embedding studies. Our entropy evaluation evidences our halftone contains less
encoding information than our novel base method. The experiments show our
predictor-embedded method gains more flexibility to improve the blue-noise
quality of halftones and maintains a comparable restoration quality with a
higher tolerance for disturbances.


------------------------------------------------------------------------------

Title:
Explainable and Position-Aware Learning in Digital Pathology

Abstract: Encoding whole slide images (WSI) as graphs is well motivated since it makes
it possible for the gigapixel resolution WSI to be represented in its entirety
for the purpose of graph learning. To this end, WSIs can be broken into smaller
patches that represent the nodes of the graph. Then, graph-based learning
methods can be utilized for the grading and classification of cancer. Message
passing among neighboring nodes is the foundation of graph-based learning
methods. However, they do not take into consideration any positional
information for any of the patches, and if two patches are found in
topologically isomorphic neighborhoods, their embeddings are nearly similar to
one another. In this work, classification of cancer from WSIs is performed with
positional embedding and graph attention. In order to represent the positional
embedding of the nodes in graph classification, the proposed method makes use
of spline convolutional neural networks (CNN). The algorithm is then tested
with the WSI dataset for grading prostate cancer and kidney cancer. A
comparison of the proposed method with leading approaches in cancer diagnosis
and grading verify improved performance. The identification of cancerous
regions in WSIs is another critical task in cancer diagnosis. In this work, the
explainability of the proposed model is also addressed. A gradient-based
explainbility approach is used to generate the saliency mapping for the WSIs.
This can be used to look into regions of WSI that are responsible for cancer
diagnosis thus rendering the proposed model explainable.


------------------------------------------------------------------------------

Title:
Variance-Preserving-Based Interpolation Diffusion Models for Speech  Enhancement

Abstract: The goal of this study is to implement diffusion models for speech
enhancement (SE). The first step is to emphasize the theoretical foundation of
variance-preserving (VP)-based interpolation diffusion under continuous
conditions. Subsequently, we present a more concise framework that encapsulates
both the VP- and variance-exploding (VE)-based interpolation diffusion methods.
We demonstrate that these two methods are special cases of the proposed
framework. Additionally, we provide a practical example of VP-based
interpolation diffusion for the SE task. To improve performance and ease model
training, we analyze the common difficulties encountered in diffusion models
and suggest amenable hyper-parameters. Finally, we evaluate our model against
several methods using a public benchmark to showcase the effectiveness of our
approach


------------------------------------------------------------------------------

Title:
Quantum interactive proofs using quantum energy teleportation

Abstract: We present a simple quantum interactive proof (QIP) protocol using the
quantum state teleportation (QST) and quantum energy teleportation (QET)
protocols. QET is a technique that allows a receiver at a distance to extract
the local energy by local operations and classical communication (LOCC), using
the energy injected by the supplier as collateral. QET works for any local
Hamiltonian with entanglement and, for our study, it is important that getting
the ground state of a generic local Hamiltonian is quantum Merlin Arthur
(QMA)-hard. The key motivations behind employing QET for these purposes are
clarified. Firstly, in cases where a prover possesses the correct state and
executes the appropriate operations, the verifier can effectively validate the
presence of negative energy with a high probability (Completeness). Failure to
select the appropriate operators or an incorrect state renders the verifier
incapable of observing negative energy (Soundness). Importantly, the verifier
solely observes a single qubit from the prover's transmitted state, while
remaining oblivious to the prover's Hamiltonian and state (Zero-knowledge).
Furthermore, the analysis is extended to distributed quantum interactive
proofs, where we propose multiple solutions for the verification of each
player's measurement. The complexity class of our protocol in the most general
case belongs to QIP(3)=PSPACE, hence it provides a secure quantum
authentication scheme that can be implemented in small quantum communication
devices. It is straightforward to extend our protocol to Quantum Multi-Prover
Interactive Proof (QMIP) systems, where the complexity is expected to be more
powerful (PSPACE$\subset$QMIP=NEXPTIME). In our case, all provers share the
ground state entanglement, hence it should belong to a more powerful complexity
class QMIP$^*$.


------------------------------------------------------------------------------

Title:
Increasing subsequences, matrix loci, and Viennot shadows

Abstract: Let $\mathbf{x}_{n \times n}$ be an $n \times n$ matrix of variables and let
$\mathbb{F}[\mathbf{x}_{n \times n}]$ be the polynomial ring in these variables
over a field $\mathbb{F}$. We study the ideal $I_n \subseteq
\mathbb{F}[\mathbf{x}_{n \times n}]$ generated by all row and column variable
sums and all products of two variables drawn from the same row or column. We
show that the quotient $\mathbb{F}[\mathbf{x}_{n \times n}]/I_n$ admits a
standard monomial basis determined by Viennot's shadow line avatar of the
Schensted correspondence. As a corollary, the Hilbert series of
$\mathbb{F}[\mathbf{x}_{n \times n}]/I_n$ is the generating function of
permutations in $\mathfrak{S}_n$ by the length of their longest increasing
subsequence. Along the way, we describe a `shadow junta' basis of the vector
space of $k$-local permutation statistics. We also calculate the structure of
$\mathbb{F}[\mathbf{x}_{n \times n}]/I_n$ as a graded $\mathfrak{S}_n \times
\mathfrak{S}_n$-module.


------------------------------------------------------------------------------

Title:
Bayesian Non-linear Latent Variable Modeling via Random Fourier Features

Abstract: The Gaussian process latent variable model (GPLVM) is a popular probabilistic
method used for nonlinear dimension reduction, matrix factorization, and
state-space modeling. Inference for GPLVMs is computationally tractable only
when the data likelihood is Gaussian. Moreover, inference for GPLVMs has
typically been restricted to obtaining maximum a posteriori point estimates,
which can lead to overfitting, or variational approximations, which
mischaracterize the posterior uncertainty. Here, we present a method to perform
Markov chain Monte Carlo (MCMC) inference for generalized Bayesian nonlinear
latent variable modeling. The crucial insight necessary to generalize GPLVMs to
arbitrary observation models is that we approximate the kernel function in the
Gaussian process mappings with random Fourier features; this allows us to
compute the gradient of the posterior in closed form with respect to the latent
variables. We show that we can generalize GPLVMs to non-Gaussian observations,
such as Poisson, negative binomial, and multinomial distributions, using our
random feature latent variable model (RFLVM). Our generalized RFLVMs perform on
par with state-of-the-art latent variable models on a wide range of
applications, including motion capture, images, and text data for the purpose
of estimating the latent structure and imputing the missing data of these
complex data sets.


------------------------------------------------------------------------------

Title:
Imagery Tracking of Sun Activity Using 2D Circular Kernel Time Series  Transformation, Entropy Measures and Machine Learning Approaches

Abstract: The sun is highly complex in nature and its observatory imagery features is
one of the most important sources of information about the sun activity, space
and Earth's weather conditions. The NASA, solar Dynamics Observatory captures
approximately 70,000 images of the sun activity in a day and the continuous
visual inspection of this solar observatory images is challenging. In this
study, we developed a technique of tracking the sun's activity using 2D
circular kernel time series transformation, statistical and entropy measures,
with machine learning approaches. The technique involves transforming the solar
observatory image section into 1-Dimensional time series (1-DTS) while the
statistical and entropy measures (Approach 1) and direct classification
(Approach 2) is used to capture the extraction features from the 1-DTS for
machine learning classification into 'solar storm' and 'no storm'. We found
that the potential accuracy of the model in tracking the activity of the sun is
approximately 0.981 for Approach 1 and 0.999 for Approach 2. The stability of
the developed approach to rotational transformation of the solar observatory
image is evident. When training on the original dataset for Approach 1, the
match index (T90) of the distribution of solar storm areas reaches T90 ~ 0.993,
and T90 ~ 0.951 for Approach 2. In addition, when using the extended training
base, the match indices increased to T90 ~ 0.994 and T90 ~ 1, respectively.
This model consistently classifies areas with swirling magnetic lines
associated with solar storms and is robust to image rotation, glare, and
optical artifacts.


------------------------------------------------------------------------------

Title:
Analysis and Approximate Inference of Large and Dense Random Kronecker  Graphs

Abstract: Random graph models are playing an increasingly important role in science and
industry, and finds their applications in a variety of fields ranging from
social and traffic networks, to recommendation systems and molecular genetics.
In this paper, we perform an in-depth analysis of the random Kronecker graph
model proposed in \cite{leskovec2010kronecker}, when the number of graph
vertices $N$ is large. Built upon recent advances in random matrix theory, we
show, in the dense regime, that the random Kronecker graph adjacency matrix
follows approximately a signal-plus-noise model, with a small-rank (of order at
most $\log N$) signal matrix that is linear in the graph parameters and a
random noise matrix having a quarter-circle-form singular value distribution.
This observation allows us to propose a ``denoise-and-solve'' meta algorithm to
approximately infer the graph parameters, with reduced computational complexity
and (asymptotic) performance guarantee. Numerical experiments of graph
inference and graph classification on both synthetic and realistic graphs are
provided to support the advantageous performance of the proposed approach.


------------------------------------------------------------------------------

Title:
M^2UNet: MetaFormer Multi-scale Upsampling Network for Polyp  Segmentation

Abstract: Polyp segmentation has recently garnered significant attention, and multiple
methods have been formulated to achieve commendable outcomes. However, these
techniques often confront difficulty when working with the complex polyp
foreground and their surrounding regions because of the nature of convolution
operation. Besides, most existing methods forget to exploit the potential
information from multiple decoder stages. To address this challenge, we suggest
combining MetaFormer, introduced as a baseline for integrating CNN and
Transformer, with UNet framework and incorporating our Multi-scale Upsampling
block (MU). This simple module makes it possible to combine multi-level
information by exploring multiple receptive field paths of the shallow decoder
stage and then adding with the higher stage to aggregate better feature
representation, which is essential in medical image segmentation. Taken all
together, we propose MetaFormer Multi-scale Upsampling Network (M$^2$UNet) for
the polyp segmentation task. Extensive experiments on five benchmark datasets
demonstrate that our method achieved competitive performance compared with
several previous methods.


------------------------------------------------------------------------------

Title:
Pedestrian Recognition with Radar Data-Enhanced Deep Learning Approach  Based on Micro-Doppler Signatures

Abstract: As a hot topic in recent years, the ability of pedestrians identification
based on radar micro-Doppler signatures is limited by the lack of adequate
training data. In this paper, we propose a data-enhanced multi-characteristic
learning (DEMCL) model with data enhancement (DE) module and
multi-characteristic learning (MCL) module to learn more complementary
pedestrian micro-Doppler (m-D) signatures. In DE module, a range-Doppler
generative adversarial network (RDGAN) is proposed to enhance free walking
datasets, and MCL module with multi-scale convolution neural network (MCNN) and
radial basis function neural network (RBFNN) is trained to learn m-D signatures
extracted from enhanced datasets. Experimental results show that our model is
3.33% to 10.24% more accurate than other studies and has a short run time of
0.9324 seconds on a 25-minute walking dataset.


------------------------------------------------------------------------------

Title:
Efficient Adapters for Giant Speech Models

Abstract: Large pre-trained speech models are widely used as the de-facto paradigm,
especially in scenarios when there is a limited amount of labeled data
available. However, finetuning all parameters from the self-supervised learned
model can be computationally expensive, and becomes infeasiable as the size of
the model and the number of downstream tasks scales. In this paper, we propose
a novel approach called Two Parallel Adapter (TPA) that is inserted into the
conformer-based model pre-trained model instead. TPA is based on systematic
studies of the residual adapter, a popular approach for finetuning a subset of
parameters. We evaluate TPA on various public benchmarks and experiment results
demonstrates its superior performance, which is close to the full finetuning on
different datasets and speech tasks. These results show that TPA is an
effective and efficient approach for serving large pre-trained speech models.
Ablation studies show that TPA can also be pruned, especially for lower blocks.


------------------------------------------------------------------------------

Title:
Design and Formulation of a Hydromechanical Fin for AUV Operations and  Wave Parameter Estimation

Abstract: Ocean dynamics play a crucial role in global climate, ecosystems, and human
activities, necessitating accurate and efficient methods to characterize ocean
currents and waves. This paper presents the development of a novel bio-inspired
hydrofoil system for detecting and characterizing ocean currents and waves
based on a dolphin's flipper foil design. The prototype's performance was
assessed through controlled experiments, demonstrating the system's ability to
quickly and accurately orientate itself in the direction of flow. Wave
mechanics and a geometric proof were applied to estimate wave parameters such
as wave height and current from the hydrofoil's positioning. The proposed
hydrofoil system shows potential for use in various marine applications,
including oceanographic research, environmental monitoring, and navigation. The
bio-inspired hydrofoil system offers a promising approach to ocean current and
wave characterization, with the potential to significantly impact our
understanding and monitoring of ocean dynamics without greatly impacting
vehicle performance or increasing power utilization in operation.


------------------------------------------------------------------------------

Title:
Feature Normalization for Fine-tuning Self-Supervised Models in Speech  Enhancement

Abstract: Large, pre-trained representation models trained using self-supervised
learning have gained popularity in various fields of machine learning because
they are able to extract high-quality salient features from input data. As
such, they have been frequently used as base networks for various pattern
classification tasks such as speech recognition. However, not much research has
been conducted on applying these types of models to the field of speech signal
generation. In this paper, we investigate the feasibility of using pre-trained
speech representation models for a downstream speech enhancement task. To
alleviate mismatches between the input features of the pre-trained model and
the target enhancement model, we adopt a novel feature normalization technique
to smoothly link these modules together. Our proposed method enables
significant improvements in speech quality compared to baselines when combined
with various types of pre-trained speech models.


------------------------------------------------------------------------------

Title:
DCTX-Conformer: Dynamic context carry-over for low latency unified  streaming and non-streaming Conformer

Abstract: Conformer-based end-to-end models have become ubiquitous these days and are
commonly used in both streaming and non-streaming automatic speech recognition
(ASR). Techniques like dual-mode and dynamic chunk training helped unify
streaming and non-streaming systems. However, there remains a performance gap
between streaming with a full and limited past context. To address this issue,
we propose the integration of a novel dynamic contextual carry-over mechanism
in a state-of-the-art (SOTA) unified ASR system. Our proposed dynamic context
Conformer (DCTX-Conformer) utilizes a non-overlapping contextual carry-over
mechanism that takes into account both the left context of a chunk and one or
more preceding context embeddings. We outperform the SOTA by a relative 25.0%
word error rate, with a negligible latency impact due to the additional context
embeddings.


------------------------------------------------------------------------------

Title:
Self-supervised Deep Hyperspectral Inpainting with the Sparsity and  Low-Rank Considerations

Abstract: Hyperspectral images are typically composed of hundreds of narrow and
contiguous spectral bands, each containing information about the material
composition of the imaged scene. However, these images can be affected by
various sources of noise, distortions, or data losses, which can significantly
degrade their quality and usefulness. To address these problems, we introduce
two novel self-supervised Hyperspectral Images (HSI) inpainting algorithms: Low
Rank and Sparsity Constraint Plug-and-Play (LRS-PnP), and its extension
LRS-PnP-DIP, which features the strong learning capability, but is still free
of external training data. We conduct the stability analysis under some mild
assumptions which guarantees the algorithm to converge. It is specifically very
helpful for the practical applications. Extensive experiments demonstrate that
the proposed solution is able to produce visually and qualitatively superior
inpainting results, achieving state-of-the-art performance. The code for
reproducing the results is available at
\url{this https URL}.


------------------------------------------------------------------------------

Title:
Evaluation of Spatial Distortion in Multichannel Audio

Abstract: Despite the recent proliferation of spatial audio technologies, the
evaluation of spatial quality continues to rely on subjective listening tests,
often requiring expert listeners. Based on the duplex theory of spatial
hearing, it is possible to construct a signal model for frequency-independent
spatial distortion by accounting for inter-channel time and level differences
relative to a reference signal. By using a combination of least-square
optimization and heuristics, we propose a signal decomposition method to
isolate the spatial error from a processed signal. This allows the computation
of simple energy-ratio metrics, providing objective measures of spatial and
non-spatial signal qualities, with minimal assumption and no dataset
dependency. Experiments demonstrate robustness of the method against common
signal degradation as introduced by, e.g., audio compression and music source
separation. Implementation is available at
this https URL


------------------------------------------------------------------------------

Title:
A Quantum Fingerprinting Algorithm for Next Generation Cellular  Positioning

Abstract: The recent release of the third generation partnership project, Release 17,
calls for sub-meter cellular positioning accuracy with reduced latency in
calculation. To provide such high accuracy on a worldwide scale, leveraging the
received signal strength (RSS) for positioning promises ubiquitous availability
in the current and future equipment. RSS Fingerprint-based techniques have
shown a great potential for providing high accuracy in both indoor and outdoor
environments. However, fingerprint-based positioning faces the challenge of
providing a fast matching algorithm that can scale worldwide. In this paper, we
propose a cosine similarity-based quantum algorithm for enabling
fingerprint-based high accuracy and worldwide positioning that can be
integrated with the next generation of 5G and 6G networks and beyond. By
entangling the test RSS vector with the fingerprint RSS vectors, the proposed
quantum algorithm has a complexity that is exponentially better than its
classical version as well as the state-of-the-art quantum fingerprint
positioning systems, both in the storage space and the running time. We
implement the proposed quantum algorithm and evaluate it in a cellular testbed
on a real IBM quantum machine. Results show the exponential saving in both time
and space for the proposed quantum algorithm while keeping the same positioning
accuracy compared to the traditional classical fingerprinting techniques and
the state-of-the-art quantum algorithms.


------------------------------------------------------------------------------

Title:
Data Augmentation for Seizure Prediction with Generative Diffusion Model

Abstract: Objective: Seizure prediction is of great importance to improve the life of
patients. The focal point is to distinguish preictal states from interictal
ones. With the development of machine learning, seizure prediction methods have
achieved significant progress. However, the severe imbalance problem between
preictal and interictal data still poses a great challenge, restricting the
performance of classifiers. Data augmentation is an intuitive way to solve this
problem. Existing data augmentation methods generate samples by overlapping or
recombining data. The distribution of generated samples is limited by original
data, because such transformations cannot fully explore the feature space and
offer new information. As the epileptic EEG representation varies among
seizures, these generated samples cannot provide enough diversity to achieve
high performance on a new seizure. As a consequence, we propose a novel data
augmentation method with diffusion model called DiffEEG. Methods: Diffusion
models are a class of generative models that consist of two processes.
Specifically, in the diffusion process, the model adds noise to the input EEG
sample step by step and converts the noisy sample into output random noise,
exploring the distribution of data by minimizing the loss between the output
and the noise added. In the denoised process, the model samples the synthetic
data by removing the noise gradually, diffusing the data distribution to
outward areas and narrowing the distance between different clusters. Results:
We compared DiffEEG with existing methods, and integrated them into three
representative classifiers. The experiments indicate that DiffEEG could further
improve the performance and shows superiority to existing methods. Conclusion:
This paper proposes a novel and effective method to solve the imbalanced
problem and demonstrates the effectiveness and generality of our method.


------------------------------------------------------------------------------

Title:
Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for  Large Language Models

Abstract: Large Language Models (LLMs), with their remarkable task-handling
capabilities and innovative outputs, have catalyzed significant advancements
across a spectrum of fields. However, their proficiency within specialized
domains such as biomolecular studies remains limited. To address this
challenge, we introduce Mol-Instructions, a meticulously curated, comprehensive
instruction dataset expressly designed for the biomolecular realm.
Mol-Instructions is composed of three pivotal components: molecule-oriented
instructions, protein-oriented instructions, and biomolecular text
instructions, each curated to enhance the understanding and prediction
capabilities of LLMs concerning biomolecular features and behaviors. Through
extensive instruction tuning experiments on the representative LLM, we
underscore the potency of Mol-Instructions to enhance the adaptability and
cognitive acuity of large models within the complex sphere of biomolecular
studies, thereby promoting advancements in the biomolecular research community.
Mol-Instructions is made publicly accessible for future research endeavors and
will be subjected to continual updates for enhanced applicability.


------------------------------------------------------------------------------

Title:
Domain-Aware Few-Shot Learning for Optical Coherence Tomography Noise  Reduction

Abstract: Speckle noise has long been an extensively studied problem in medical
imaging. In recent years, there have been significant advances in leveraging
deep learning methods for noise reduction. Nevertheless, adaptation of
supervised learning models to unseen domains remains a challenging problem.
Specifically, deep neural networks (DNNs) trained for computational imaging
tasks are vulnerable to changes in the acquisition system's physical
parameters, such as: sampling space, resolution, and contrast. Even within the
same acquisition system, performance degrades across datasets of different
biological tissues. In this work, we propose a few-shot supervised learning
framework for optical coherence tomography (OCT) noise reduction, that offers a
dramatic increase in training speed and requires only a single image, or part
of an image, and a corresponding speckle suppressed ground truth, for training.
Furthermore, we formulate the domain shift problem for OCT diverse imaging
systems, and prove that the output resolution of a despeckling trained model is
determined by the source domain resolution. We also provide possible remedies.
We propose different practical implementations of our approach, verify and
compare their applicability, robustness, and computational efficiency. Our
results demonstrate significant potential for generally improving sample
complexity, generalization, and time efficiency, for coherent and non-coherent
noise reduction via supervised learning models, that can also be leveraged for
other real-time computer vision applications.


------------------------------------------------------------------------------

Title:
Large-scale Language Model Rescoring on Long-form Data

Abstract: In this work, we study the impact of Large-scale Language Models (LLM) on
Automated Speech Recognition (ASR) of YouTube videos, which we use as a source
for long-form ASR. We demonstrate up to 8\% relative reduction in Word Error
Eate (WER) on US English (en-us) and code-switched Indian English (en-in)
long-form ASR test sets and a reduction of up to 30\% relative on Salient Term
Error Rate (STER) over a strong first-pass baseline that uses a maximum-entropy
based language model. Improved lattice processing that results in a lattice
with a proper (non-tree) digraph topology and carrying context from the 1-best
hypothesis of the previous segment(s) results in significant wins in rescoring
with LLMs. We also find that the gains in performance from the combination of
LLMs trained on vast quantities of available data (such as C4) and conventional
neural LMs is additive and significantly outperforms a strong first-pass
baseline with a maximum entropy LM.


------------------------------------------------------------------------------

Title:
On relative clique number of triangle-free planar $(n,m)$-graphs

Abstract: An $(n,m)$-graph is a graph with $n$ types of arcs and $m$ types of edges. A
homomorphism of an $(n,m)$-graph $G$ to another $(n,m)$-graph $H$ is a vertex
mapping that preserves the adjacencies along with their types and directions.
The order of a smallest (with respect to the number of vertices) such $H$ is
the $(n,m)$-chromatic number of $G$.Moreover, an $(n,m)$-relative clique $R$ of
an $(n,m)$-graph $G$ is a vertex subset of $G$ for which no two distinct
vertices of $R$ get identified under any homomorphism of $G$. The
$(n,m)$-relative clique number of $G$, denoted by $\omega_{r(n,m)}(G)$, is the
maximum $|R|$ such that $R$ is an $(n,m)$-relative clique of $G$. In practice,
$(n,m)$-relative cliques are often used for establishing lower bounds of
$(n,m)$-chromatic number of graph families.
Generalizing an open problem posed by Sopena [Discrete Mathematics 2016] in
his latest survey on oriented coloring, Chakroborty, Das, Nandi, Roy and Sen
[Discrete Applied Mathematics 2022] conjectured that $\omega_{r(n,m)}(G) \leq 2
(2n+m)^2 + 2$ for any triangle-free planar $(n,m)$-graph $G$ and that this
bound is tight for all $(n,m) \neq (0,1)$.In this article, we positively settle
this conjecture by improving the previous upper bound of $\omega_{r(n,m)}(G)
\leq 14 (2n+m)^2 + 2$ to $\omega_{r(n,m)}(G) \leq 2 (2n+m)^2 + 2$, and by
finding examples of triangle-free planar graphs that achieve this bound. As a
consequence of the tightness proof, we also establish a new lower bound of $2
(2n+m)^2 + 2$ for the $(n,m)$-chromatic number for the family of triangle-free
planar graphs.


------------------------------------------------------------------------------

Title:
Model-Free Market Risk Hedging Using Crowding Networks

Abstract: Crowding is widely regarded as one of the most important risk factors in
designing portfolio strategies. In this paper, we analyze stock crowding using
network analysis of fund holdings, which is used to compute crowding scores for
stocks. These scores are used to construct costless long-short portfolios,
computed in a distribution-free (model-free) way and without using any
numerical optimization, with desirable properties of hedge portfolios. More
specifically, these long-short portfolios provide protection for both small and
large market price fluctuations, due to their negative correlation with the
market and positive convexity as a function of market returns. By adding our
long-short portfolio to a baseline portfolio such as a traditional 60/40
portfolio, our method provides an alternative way to hedge portfolio risk
including tail risk, which does not require costly option-based strategies or
complex numerical optimization. The total cost of such hedging amounts to the
total cost of rebalancing the hedge portfolio.


------------------------------------------------------------------------------

Title:
An update on $(n,m)$-chromatic numbers

Abstract: An $(n,m)$-graph is a graph with $n$ types of arcs and $m$ types of edges. A
homomorphism of an $(n,m)$-graph $G$ to another $(n,m)$-graph $H$ is a vertex
mapping that preserves adjacency, its direction, and its type. The minimum
value of $|V(H)|$ such that $G$ admits a homomorphism to $H$ is the
$(n,m)$-chromatic number of $G$, denoted by $\mychi_{n,m}(G)$. This parameter
was introduced by Ne\v{s}et\v{r}il and Raspaud (J. Comb. Theory. Ser. B 2000).
In this article, we show that the arboricity of $G$ is bounded by a function of
$\mychi_{n,m}(G)$, but not the other way round. We also show that acyclic
chromatic number of $G$ is bounded by a function of $\mychi_{n,m}(G)$, while
the other way round bound was known beforehand. Moreover, we show that
$(n,m)$-chromatic number for the family of graphs having maximum average degree
less than $2+ \frac{2}{4(2n+m)-1}$, which contains the family of planar graphs
having girth at least $8(2n+m)$ as a subfamily, is equal to $2(2n+m)+1$. This
improves the previously known result which proved that the $(n,m)$-chromatic
number for the family planar graphs having girth at least $10(2n+m)-4$ is equal
to $2(2n+m)+1$. It is known that the $(n,m)$-chromatic number for the family of
partial $2$-trees bounded below and above by quadratic functions of $(2n+m)$
and that the lower bound is tight when $(2n+m)=2$. We show that the lower bound
is not tight when $(2n+m)=3$ by improving the corresponding lower bounds by
one. We manage to improve some of the upper bounds in these cases as well.


------------------------------------------------------------------------------

Title:
Safe Use of Neural Networks

Abstract: Neural networks in modern communication systems can be susceptible to
internal numerical errors that can drastically effect decision results. Such
structures are composed of many sections each of which generally contain
weighting operations and activation function evaluations. The safe use comes
from methods employing number based codes that can detect arithmetic errors in
the network's processing steps. Each set of operations generates parity values
dictated by a code in two ways. One set of parities is obtained from a
section's outputs while a second comparable set is developed directly from the
original inputs. The parity values protecting the activation functions involve
a Taylor series approximation to the activation functions. We focus on using
long numerically based convolutional codes because of the large size of data
sets. The codes are based on Discrete Fourier Transform kernels and there are
many design options available. Mathematical program simulations show our
error-detecting techniques are effective and efficient.


------------------------------------------------------------------------------

Title:
Improving Zero-Shot Detection of Low Prevalence Chest Pathologies using  Domain Pre-trained Language Models

Abstract: Recent advances in zero-shot learning have enabled the use of paired
image-text data to replace structured labels, replacing the need for expert
annotated datasets. Models such as CLIP-based CheXzero utilize these
advancements in the domain of chest X-ray interpretation. We hypothesize that
domain pre-trained models such as CXR-BERT, BlueBERT, and ClinicalBERT offer
the potential to improve the performance of CLIP-like models with specific
domain knowledge by replacing BERT weights at the cost of breaking the original
model's alignment. We evaluate the performance of zero-shot classification
models with domain-specific pre-training for detecting low-prevalence
pathologies. Even though replacing the weights of the original CLIP-BERT
degrades model performance on commonly found pathologies, we show that
pre-trained text towers perform exceptionally better on low-prevalence
diseases. This motivates future ensemble models with a combination of
differently trained language models for maximal performance.


------------------------------------------------------------------------------

Title:
Artificial intelligence in digital pathology: a diagnostic test accuracy  systematic review and meta-analysis

Abstract: Ensuring diagnostic performance of AI models before clinical use is key to
the safe and successful adoption of these technologies. Studies reporting AI
applied to digital pathology images for diagnostic purposes have rapidly
increased in number in recent years. The aim of this work is to provide an
overview of the diagnostic accuracy of AI in digital pathology images from all
areas of pathology. This systematic review and meta-analysis included
diagnostic accuracy studies using any type of artificial intelligence applied
to whole slide images (WSIs) in any disease type. The reference standard was
diagnosis through histopathological assessment and / or immunohistochemistry.
Searches were conducted in PubMed, EMBASE and CENTRAL in June 2022. We
identified 2976 studies, of which 100 were included in the review and 48 in the
full meta-analysis. Risk of bias and concerns of applicability were assessed
using the QUADAS-2 tool. Data extraction was conducted by two investigators and
meta-analysis was performed using a bivariate random effects model. 100 studies
were identified for inclusion, equating to over 152,000 whole slide images
(WSIs) and representing many disease types. Of these, 48 studies were included
in the meta-analysis. These studies reported a mean sensitivity of 96.3% (CI
94.1-97.7) and mean specificity of 93.3% (CI 90.5-95.4) for AI. There was
substantial heterogeneity in study design and all 100 studies identified for
inclusion had at least one area at high or unclear risk of bias. This review
provides a broad overview of AI performance across applications in whole slide
imaging. However, there is huge variability in study design and available
performance data, with details around the conduct of the study and make up of
the datasets frequently missing. Overall, AI offers good accuracy when applied
to WSIs but requires more rigorous evaluation of its performance.


------------------------------------------------------------------------------

Title:
Investigation of Fractional Compartmental Models with Application to  Amiodarone Drug Diffusion in Pharmacokinetics

Abstract: This paper presents three fractional models formulated from a classical
Pharmacokinetics compartmental system: commensurable, non-commensurable, and
implicit non-commensurable models. Their distinguishing characteristics are
further examined comprehensively. Because analytic solutions for such models
are typically challenging to obtain, we study the application of the Fractional
Finite Difference Method (FFDM) to simulate approximate solutions. The
characteristic of the non-commensurable model is shown to be incompatible with
the concept of mass balance. However, it appeared to outlast fractional
calculus theory when simulating anomalous kinetics. We proved this by fitting
the proposed fractional and classical models to an experimental data set
(amiodarone) and estimated the parameters using the least-square approach. The
classical model diverged, but the non-commensurable model predicted a fit
comparable to the other two fractional models. The fractional models described
anomalous diffusion better than classical theories. The numerical results
showed that the proposed numerical method is equally efficient in solving any
complex compartmental models, as they performed well in simulations for the
classic example of the model.


------------------------------------------------------------------------------

Title:
Implicit Compressibility of Overparametrized Neural Networks Trained  with Heavy-Tailed SGD

Abstract: Neural network compression has been an increasingly important subject, due to
its practical implications in terms of reducing the computational requirements
and its theoretical implications, as there is an explicit connection between
compressibility and the generalization error. Recent studies have shown that
the choice of the hyperparameters of stochastic gradient descent (SGD) can have
an effect on the compressibility of the learned parameter vector. Even though
these results have shed some light on the role of the training dynamics over
compressibility, they relied on unverifiable assumptions and the resulting
theory does not provide a practical guideline due to its implicitness. In this
study, we propose a simple modification for SGD, such that the outputs of the
algorithm will be provably compressible without making any nontrivial
assumptions. We consider a one-hidden-layer neural network trained with SGD and
we inject additive heavy-tailed noise to the iterates at each iteration. We
then show that, for any compression rate, there exists a level of
overparametrization (i.e., the number of hidden units), such that the output of
the algorithm will be compressible with high probability. To achieve this
result, we make two main technical contributions: (i) we build on a recent
study on stochastic analysis and prove a 'propagation of chaos' result with
improved rates for a class of heavy-tailed stochastic differential equations,
and (ii) we derive strong-error estimates for their Euler discretization. We
finally illustrate our approach on experiments, where the results suggest that
the proposed approach achieves compressibility with a slight compromise from
the training and test error.


------------------------------------------------------------------------------

Title:
Leveraging Machine Learning for Multichain DeFi Fraud Detection

Abstract: Since the inception of permissionless blockchains with Bitcoin in 2008, it
became apparent that their most well-suited use case is related to making the
financial system and its advantages available to everyone seamlessly without
depending on any trusted intermediaries. Smart contracts across chains provide
an ecosystem of decentralized finance (DeFi), where users can interact with
lending pools, Automated Market Maker (AMM) exchanges, stablecoins,
derivatives, etc. with a cumulative locked value which had exceeded 160B USD.
While DeFi comes with high rewards, it also carries plenty of risks. Many
financial crimes have occurred over the years making the early detection of
malicious activity an issue of high priority. The proposed framework introduces
an effective method for extracting a set of features from different chains,
including the largest one, Ethereum and it is evaluated over an extensive
dataset we gathered with the transactions of the most widely used DeFi
protocols (23 in total, including Aave, Compound, Curve, Lido, and Yearn) based
on a novel dataset in collaboration with Covalent. Different Machine Learning
methods were employed, such as XGBoost and a Neural Network for identifying
fraud accounts detection interacting with DeFi and we demonstrate that the
introduction of novel DeFi-related features, significantly improves the
evaluation results, where Accuracy, Precision, Recall, F1-score and F2-score
where utilized.


------------------------------------------------------------------------------

Title:
Point spread function modelling for astronomical telescopes: a review  focused on weak gravitational lensing studies

Abstract: The accurate modelling of the Point Spread Function (PSF) is of paramount
importance in astronomical observations, as it allows for the correction of
distortions and blurring caused by the telescope and atmosphere. PSF modelling
is crucial for accurately measuring celestial objects' properties. The last
decades brought us a steady increase in the power and complexity of
astronomical telescopes and instruments. Upcoming galaxy surveys like Euclid
and LSST will observe an unprecedented amount and quality of data. Modelling
the PSF for these new facilities and surveys requires novel modelling
techniques that can cope with the ever-tightening error requirements. The
purpose of this review is three-fold. First, we introduce the optical
background required for a more physically-motivated PSF modelling and propose
an observational model that can be reused for future developments. Second, we
provide an overview of the different physical contributors of the PSF,
including the optic- and detector-level contributors and the atmosphere. We
expect that the overview will help better understand the modelled effects.
Third, we discuss the different methods for PSF modelling from the parametric
and non-parametric families for ground- and space-based telescopes, with their
advantages and limitations. Validation methods for PSF models are then
addressed, with several metrics related to weak lensing studies discussed in
detail. Finally, we explore current challenges and future directions in PSF
modelling for astronomical telescopes.


------------------------------------------------------------------------------

Title:
Epistemic and Aleatoric Uncertainty Quantification and Surrogate  Modelling in High-Performance Multiscale Plasma Physics Simulations

Abstract: This work suggests several methods of uncertainty treatment in multiscale
modelling and describes their application to a system of coupled turbulent
transport simulations of a tokamak plasma. We propose a method to quantify the
usually aleatoric uncertainty of a system in a quasi-stationary state,
estimating the mean values and their errors for quantities of interest, which
is average heat fluxes in the case of turbulence simulations. The method
defines the stationarity of the system and suggests a way to balance the
computational cost of simulation and the accuracy of estimation. This allows,
contrary to many approaches, to incorporate aleatoric uncertainties in the
analysis of the model and to have a quantifiable decision for simulation
runtime. Furthermore, the paper describes methods for quantifying the epistemic
uncertainty of a model and the results of such a procedure for turbulence
simulations, identifying the model's sensitivity to particular input parameters
and sensitivity to uncertainties in total. Finally, we introduce a surrogate
model approach based on Gaussian Process Regression and present a preliminary
result of training and analysing the performance of such a model based on
turbulence simulation data. Such an approach shows a potential to significantly
decrease the computational cost of the uncertainty propagation for the given
model, making it feasible on current HPC systems.


------------------------------------------------------------------------------

